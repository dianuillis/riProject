<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 00:15:06[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Spiking neural network</title>
<id>10159567</id>
<revision>
<id>227173171</id>
<timestamp>2008-07-22T09:30:15Z</timestamp>
<contributor>
<username>SoyYo</username>
<id>2116281</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Computational statistics</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<indent level="1">

<it>Main article: <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial neural network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</it>
</indent>
==Overview==
Spiking neural networks (SNNs) fall into the third generation of <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
neural network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 models, increasing the level of realism in a neural simulation. In addition to <link xlink:type="simple" xlink:href="../771/349771.xml">
neuronal</link> and <link xlink:type="simple" xlink:href="../228/441228.xml">
synaptic</link> state, SNNs also incorporate the concept of time into their <link xlink:type="simple" xlink:href="../282/11944282.xml">
operating model</link>. The idea is that <link xlink:type="simple" xlink:href="../771/349771.xml">
neurons</link> in the SNN do not fire at each propagation cycle (as it happens with typical multi-layer <link xlink:type="simple" xlink:href="../777/172777.xml">
perceptron networks</link>), but rather fire only when a <link xlink:type="simple" xlink:href="../161/563161.xml">
membrane potential</link> - an intrinsic quality of the neuron related to its membrane electrical charge - reaches a specific value. When a neuron fires, it generates a signal which travels to other neurons which, in turn, increase or decrease their potentials in accordance with this signal.<p>

In the context of spiking neural networks, the current activation level (modeled as some differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher, and then either firing or decaying over time. Various <it>coding methods</it> exist for interpreting the outgoing <it><link xlink:type="simple" xlink:href="../998/156998.xml">
spike train</link></it> as a real-value number, either relying on the frequency of  spikes, or the timing between spikes, to encode information.</p>

<sec>
<st>
Beginnings</st>

<p>

The first <link xlink:type="simple" xlink:href="../795/3224795.xml">
scientific model</link> of a spiking neuron was proposed by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../862/353862.xml">
Alan Lloyd Hodgkin</link></scientist>
</person>
 and <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../842/145842.xml">
Andrew Huxley</link></person>
 in 1952. This model describes how <link xlink:type="simple" xlink:href="../998/156998.xml">
action potential</link>s are initiated and propagated. Spikes, however are not generally transmitted directly between <link xlink:type="simple" xlink:href="../120/21120.xml">
neuron</link>s, communication requires the exchange of chemical substances in the <link xlink:type="simple" xlink:href="../809/27809.xml">
 synaptic</link> gap, called <link xlink:type="simple" xlink:href="../865/21865.xml">
neurotransmitter</link>s. The complexity and variability of biological models have resulted in various neuron models, such as the <link>
integrate-and-fire</link> (1907), <link xlink:type="simple" xlink:href="../867/9105867.xml">
FitzHugh-Nagumo</link> (1961-1962) and <link xlink:type="simple" xlink:href="../542/18004542.xml">
 Hindmarsh-Rose</link> model (1984). </p>
<p>

From the <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link> point of view, the problem is to propose a model that explains how information is encoded and decoded by a series of trains of pulses, i.e., action potentials. Thus, one of the early questions of neuroscience is to determined if neurons communicated by a rate code or by a pulse code.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>
<p>

Early results with spiking neural models suggested that by using temporal coding, networks of spiking neurons may gain more computational power than traditional neural networks. It was also suggested that, under certain conditions, any multilayered perceptron can be simulated closely by a network consisting of spiking neurons.</p>

</sec>
<sec>
<st>
References</st>


<p>

<reflist>
<entry id="1">
Maas, W. &amp; Bishop, M.B.(1999) "Pulsed Neural Networks" MIT Press</entry>
</reflist>
</p>

</sec>
<sec>
<st>
 External links </st>

<p>

<list>
<entry level="1" type="bullet">

 Full text of the book <weblink xlink:type="simple" xlink:href="http://icwww.epfl.ch/~gerstner/SPNM/SPNM.html">
Spiking Neuron Models. Single Neurons, Populations, Plasticity</weblink> by Wulfram Gerstner and Werner M. Kistler (ISBN 0521890799)</entry>
</list>
</p>




</sec>
</bdy>
</article>
