<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:01:17[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Computer experiment</title>
<id>1031713</id>
<revision>
<id>214672189</id>
<timestamp>2008-05-24T18:38:54Z</timestamp>
<contributor>
<username>Btyner</username>
<id>185327</id>
</contributor>
</revision>
<categories>
<category>Computational science</category>
</categories>
</header>
<bdy>

In the scientific context, a computer experiment typically implies two phases. <b>The modelization phase</b> and the <b>experimentation phase</b>. The modelization phase can be seen as a replacement of the traditional <link>
mathematical modelization</link>, and the experimentation phase as a replacement of the traditional <link xlink:type="simple" xlink:href="../187/15187.xml">
in vivo</link> and <link xlink:type="simple" xlink:href="../188/15188.xml">
in vitro</link> experiements. It has become common to call such experiements <link xlink:type="simple" xlink:href="../450/1035450.xml">
in silico</link>. 
<sec>
<st>
Computer simulation as a building block of a computer experiment</st>
<p>

In a <b>computer simulation</b>, a "computer" model typically replaces a traditional mathematical model. Whereas a mathematical model is traditionally solved analytically, a computer model can be solved numerically: this is what a <link xlink:type="simple" xlink:href="../416/375416.xml">
computer simulation</link> of a system (typically a physical systems) is about. (Sometimes, an analytical solution to a mathematical model is not known, however a computer simulation can find an approximate solution, typically this happens with <link xlink:type="simple" xlink:href="../309/1424309.xml">
differential equations</link>).</p>
<p>

In a <b>computer experiment</b> a computer model is used to make inferences about some underlying system.  The idea is that the computer model takes the place of an experiment we cannot do: the phrase <it><link xlink:type="simple" xlink:href="../450/1035450.xml">
in silico experiment</link></it> is also used.  At the moment, for example, the debate on climate change is being informed largely from evaluations of climate simulators running on some of the largest computers in the world, which are being used to investigate the impact of a substantial increase in the atmospheric concentration of greenhouse gases like carbon dioxide. In this case,
the accumulation of many simulations on different initial conditions form an experiment.</p>

</sec>
<sec>
<st>
Computer experiments and statistics</st>

<p>

Computer experiments can be seen a branch of <link xlink:type="simple" xlink:href="../059/1059.xml">
applied statistics</link>, because the user must account for three sources of uncertainty.  First, the models often contain parameters whose values are not certain; second, the models themselves are imperfect representations of the underlying system; and third, data collected from the system that might be used to calibrate the models are imperfectly measured.  However, it is fair to say that most practitioners of computer experiments do not see themselves as statisticians.</p>

</sec>
<sec>
<st>
History</st>

<p>

The first computer experiments were probably conducted at <site wordnetid="108651247" confidence="0.8">
<region wordnetid="108630985" confidence="0.8">
<position wordnetid="108621598" confidence="0.8">
<center wordnetid="108523483" confidence="0.8">
<point wordnetid="108620061" confidence="0.8">
<landmark wordnetid="108624891" confidence="0.8">
<area wordnetid="108497294" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<link xlink:type="simple" xlink:href="../145/38145.xml">
Los Alamos National Laboratory</link></geographical_area>
</tract>
</location>
</area>
</landmark>
</point>
</center>
</position>
</region>
</site>
 to study the behaviour of nuclear weapons. Since then, the use of computer models has branched out into large parts of the physical and environmental sciences (where they are sometimes referred to as <it>process models</it>), and in medicine.  Because computer experiments have developed in such a wide range of applications there is little standardisation of the terminology.  </p>

</sec>
<sec>
<st>
Preliminary remark</st>

<p>

As a general guide, in this article learning about the model parameters using data from the system is referred to as <it>(model) calibration</it>, while learning about the system behaviour itself as <it>(system) prediction</it>.  Combining both of these, e.g., using the model and system data to make predictions about the system, is referred to as <it>calibrated prediction</it>.  Other terminology is discussed below, in <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22The+traditional+approach%22])">
<list>
<entry level="1" type="number">

The "traditional" approach</entry>
</list>
</link>.</p>

<ss1>
<st>
Constructing a simulator</st>

<p>

The simulator is the computer code that we actually evaluate: the outputs of the simulator correspond, usually directly, to measurable aspects of the system.  It is important to understand the process of creating a simulator, because this allows us to make judgements about how similar two or more simulators of the same system are.  Without this information it is difficult to combine information from different simulators, because we do not know to what extent we can treat them as independent sources of information.  See also <link xlink:type="simple" xlink:href="../416/375416.xml">
Computer simulation</link>.</p>
<p>

In most applications there are typically three parts to a simulator: the model, the treatment and the solver.  Thus we might write</p>
<p>

Simulator = Model + Treatment + Solver</p>
<p>

Differences in each of these three components give rise to different simulators.</p>

</ss1>
<ss1>
<st>
The model</st>

<p>

The subject of models is a large one: see <link xlink:type="simple" xlink:href="../795/3224795.xml">
Model (abstract)</link> for an introduction and further links.  Our starting point is a <link xlink:type="simple" xlink:href="../590/20590.xml">
mathematical model</link> for the system of interest.  In the physical sciences a model typically describes the state variables, plus fundamental laws and equations of state that variables exist and evolve in space and time.</p>
<p>

For example, suppose you were interested in building an ocean model. Then you might proceed as follows:</p>
<p>

<list>
<entry level="1" type="bullet">

<b>State variables</b>: Velocity (in each of three directions), pressure, temperature, salinity, density </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

<b>Fundamental laws</b>: <link xlink:type="simple" xlink:href="../395/48395.xml">
Navier-Stokes equations</link> for conservation of momentum, <link xlink:type="simple" xlink:href="../793/410793.xml">
continuity equation</link> (conservation of mass), conservation of temperature and salinity</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

<b>Equations of state</b>: Relationship of density to temperature, salinity and pressure, and perhaps also a model for the formation of sea-ice</entry>
</list>
</p>
<p>

The state variables for the ocean model are expressed as a continuum in space and time, and the fundamental laws as <link xlink:type="simple" xlink:href="../564/52564.xml">
partial differential equation</link>s.  Even at this stage, though, simplifications may be made. For example, it is common to treat seawater as incompressible.  Furthermore, equations of state are often specified by empirical
relationships based on laboratory experiments.</p>
<p>

In the "less-physical" sciences the models tend to be constructed with respect to the key processes, using what are sometimes referred to as <it>compartmental models</it>.  (Example to be supplied here.)  This also happens with some physical models.  For example, a simple model of an ocean such as the Atlantic might divide the ocean into four compartments: 'south', 'tropical', 'north' and 'deep'.</p>

</ss1>
<ss1>
<st>
The treatment</st>

<p>

The treatment is what makes the model applicable to a particular instance.  For example, it is what makes our model of the ocean applicable to the earth during the period 1750 - 2100.  The treatment in this case comprises <it>boundary conditions</it> that describe the ocean margins and topography, <it>initial conditions</it> that quantify the state vector (velocity, pressure, <it>etc</it>) at every location at the start of 1750, and <it>forcing functions</it> that describe external influences on the oceans over the period 1750 - 2100.  These forcings mainly describe events at the surface of the ocean, such as temperature, winds, and exchanges of freshwater through evaporation and precipitation.  `Historic' values of forcing can be inferred from data, while `future' values, ie those from today to 2100, will be specified according to a particular scenario.</p>
<p>

Note that in large-scale climate modelling we <it>couple</it> an ocean model with an atmosphere model, so that the forcing at the margin between the ocean and the atmosphere does not have to be prescribed, but can be inferred.  At the moment this type of coupling is a bit of a black art.  The forcing in these coupled models tends to be on the atmosphere: things like orbital effects, and atmospheric concentrations of greenhouse gases like CO2.</p>

</ss1>
<ss1>
<st>
The solver</st>

<p>

Finally, the solver turns the model and the treatment into a calculation that approximates the evolution of the state vector.  At this point it is usually necessary to <it>discretise</it> the problem, which involves replacing the continuum with a lattice of discrete points.  For an ocean simulator, the earth's surface might be divided into rectangles, and the ocean itself into a number of layers.  This division is typically fixed for a given simulator, and the number of cells is referred to as the simulator's <it>resolution</it>.  Time is also discretised, although it is often possible to treat the step-size between adjacent time-points in quite a sophisticated manner, so that it adapts to the needs of the calculation.</p>
<p>

Discretisation allows us to approximate the solution of the model for our given treatment, but it introduces problems that can necessitate further adjustment.  There may be processes with characteristic scales that are smaller than a grid cell, or a time-step.  These don't get picked up by the simulator, which behaves as though the state vector is constant over each cell and time-step.  These so-called <it>sub-grid-scale</it> processes need to be put back in if they are thought to be a large component of the model.  So the solver also includes an approximation for these processes: the impact of this approximation should go to zero as the simulator resolution becomes large.</p>

</ss1>
<ss1>
<st>
The simulator as a function</st>

<p>

The simulator is implemented as a piece of computer code that can be evaluated to produce a collection of outputs, normally written to file.  In the code itself, or in files that are read by the code, we find all of the numbers that are required before the code will run.  Typically these comprise (a) coefficients in the underlying model; and possibly also (b) initial conditions and (c) forcing functions.  It is natural to see the simulator as a deterministic function that maps these <it>inputs</it> into a collection of <it>outputs</it>.  As an aside, this notion can be extended to <it>stochastic</it> simulators, if we think of one of the inputs as being the seed to a random number generator.</p>
<p>

On the basis of seeing our simulator this way, it is common to refer to the collection of inputs as <it>x</it>, the simulator itself as <it>f</it>, and the resulting output as <it>f(x)</it>.  Both <it>x</it> and <it>f(x)</it> are vector quantities, and they can be very large collections of values, often indexed by space, or by time, or by both space and time.</p>
<p>

It is worth noting here that the simulator itself is a worthy object of inference.  Many simulators embody, both formally and informally, the expertise of large sections of the relevant scientific community and as such are interesting objects in their own right. </p>
<p>

Although <math>f(\cdot)</math> is known in principle, in practice this is not the case.  Many simulators comprise tens of thousands of lines of high-level computer code, which is not accessible to intuition.  As discussed in the next section, without actually running the code it is impossible to predict exactly what the outputs will be.</p>
<p>

Such a view lends itself to a <link xlink:type="simple" xlink:href="../526/38526.xml">
Bayesian</link> analysis, in which <math>f(\cdot)</math> is treated as a random function, and the set of simulator runs as observations.  The next section implicitly treats <math>f(\cdot)</math> as a random function  about which inferences may be made using the Bayesian paradigm.</p>

</ss1>
</sec>
<sec>
<st>
Sources of uncertainty</st>

<p>

There are four different sources of uncertainty in a computer experiment, which are discussed in turn.</p>

<ss1>
<st>
Uncertainty about the simulator behaviour</st>

<p>

We are uncertain about what would happen if we evaluated the simulator at a particular input value <it>x</it>; that is, until we actually make this evaluation, the outcome <it>f(x)</it> is uncertain.  This is because the simulator is usually sufficiently complicated that we cannot know <it>f(x)</it> in the same way that we know <it>x1</it> + <it>x2</it> for given values of <it>x1</it> and <it>x2</it>.  Where the simulator is cheap to evaluate and the number of components in <it>x</it> is quite small this is not usually a problem, as we can either evaluate the simulator at little cost, or find an input close by at which we already have an evaluation.  In this case our uncertainty about <it>f(x)</it> for any given <it>x</it> is not really an issue.  But where the simulator is expensive to evaluate or there are lots of components in <it>x</it>, our typical state is to be uncertain about <it>f(x)</it>, and this uncertainty can be quite large.</p>

</ss1>
<ss1>
<st>
Uncertainty about the 'correct' simulator input</st>

<p>

There are two reasons why we might be uncertain about the 'correct' simulator input.  First, we may not have a precise measurement: this applies to components of <it>x</it> which have an analogue in the system.  These <it>measurable</it> inputs would typically include initial conditions, since these are starting values of the state vector and the state vector is typically a quantity with an analogue in the system, like water temperature.  Many forcing functions are also measurable.  But just being measurable doesn't mean that we actually have the measurements, made without error.</p>
<p>

Second, some components of <it>x</it> may not correspond to any measurable quantity in the system.  These <it>tuning</it> inputs are typically found in the model parameters, in places where the model has been simplified, or where processes that are not completely understood have been represented by flexible but non-physical sub-models.  These tuning inputs tend to represent quite general concepts, often highly aggregated.</p>
<p>

For example, in a hydrocarbon reservoir it is common to parameterise each fault with a "transmissibility".  We know that the nature of a fault varies at the microscale level, so obviously a single number is a gross simplification, but a reasonable one if the treatment has 100 faults.  But what is the right value for "transmissibility" in this case?  Exactly the same problems apply to rock permeability when averaged over regions, to give an aggregated "permeability".  These are two examples where simplifying the model can lead to problems with the definitions of some of the input components, resulting in uncertainty about the 'correct' value.  We cannot even be sure that there is a correct value, hence the use of 'correct' instead.</p>
<p>

Add something on flexible non-physical sub-models, example coupling</p>

</ss1>
<ss1>
<st>
Uncertainty about the simulator output and the system</st>
<p>

ทดสอบ</p>

</ss1>
<ss1>
<st>
Uncertainty about measurements on the system</st>


</ss1>
</sec>
<sec>
<st>
The "traditional" approach</st>


</sec>
<sec>
<st>
The probabilistic or Bayesian approach</st>


</sec>
<sec>
<st>
Challenges in large computer experiments</st>


</sec>
<sec>
<st>
Further reading</st>
<p>

<cite id="Reference-Santner-2003" style="font-style:normal" class="book">Santner, Thomas&#32;(2003). The Design and Analysis of Computer Experiments.&#32;Berlin:&#32;Springer. ISBN 0387954201.</cite>&nbsp;</p>

</sec>
</bdy>
</article>
