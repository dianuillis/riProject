<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:04:28[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Granular computing</title>
<id>1041204</id>
<revision>
<id>202286181</id>
<timestamp>2008-03-31T13:13:24Z</timestamp>
<contributor>
<username>Vigilius</username>
<id>6052252</id>
</contributor>
</revision>
<categories>
<category>Theoretical computer science</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Granular computing</b> is an emerging computing paradigm of <link xlink:type="simple" xlink:href="../578/315578.xml">
information processing</link>.  It concerns the processing of complex information entities called information granules, which arise in the process of data abstraction and derivation of knowledge from information.  Generally speaking, information granules are collections of entities that usually originate at the numeric level and  are arranged together due to their similarity, functional adjacency, indistinguishability, coherency, or the like.  <p>

At present, granular computing is more a <it>theoretical perspective</it> than a coherent set of methods or principles.  As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales.  In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented. </p>

<sec>
<st>
 Types of granulation </st>
<p>

<image width="200 px" src="Catarina_26_mar_2004_1310Z.jpg" type="thumb">
<caption>

Satellite view of cyclone.
</caption>
</image>
 
<image width="200 px" src="NASA_Manhattan.jpg" type="thumb">
<caption>

Satellite view of Manhattan.
</caption>
</image>
 
As mentioned above, <it>granular computing</it> is not an algorithm or process; there is not a particular method that is called "granular computing".  It is rather an approach  to looking at data that recognizes that different and interesting regularities in the data can appear at different levels of granularity, much as different features become salient in <link xlink:type="simple" xlink:href="../557/1184557.xml">
satellite images</link> of  greater or lesser resolution.  On a low resolution satellite image, for example, one might notice interesting cloud patterns representing <link xlink:type="simple" xlink:href="../970/910970.xml">
cyclones</link> or other large-scale weather phenomena, while in a higher resolution image one misses these large-scale atmospheric phenomena but instead notices smaller-scale phenomena, such as the interesting pattern that is the streets of <village wordnetid="108672738" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../470/45470.xml">
Manhattan</link></village>
.  The same is generally true of all data: At different resolutions or granularities, different features and relationships emerge.  The aim of granular computing is ultimately simply to try to take advantage of this fact in designing more effective machine learning and reasoning systems. </p>
<p>

There are several types of granularity that are often encountered in <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link> and <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, and we review them below:</p>

<ss1>
<st>
 Value granulation (discretization/quantization) </st>
<p>

One type of granulation is the <link xlink:type="simple" xlink:href="../321/25321.xml">
quantization</link> of variables.  It is very common that in data mining or machine learning applications that the resolution of variables needs to be <it>decreased</it> in order to extract meaningful regularities.  An example of this would be a variable such as "outside temperature", (<math>temp</math>), which in a given application might be recorded to several decimal places of accuracy (depending on the sensing apparatus).  However, for purposes of extracting relationships between "outside temperature" and, say, "number of health club applications", (<math>club </math>), it will generally be advantageous to quantize "outside temperature" into a smaller number of intervals.  </p>

<ss2>
<st>
 Motivations </st>
<p>

There are several interrelated reasons for granulating variables in this fashion:
<list>
<entry level="1" type="bullet">

 Based on prior domain knowledge, we do not expect that minute variations in temperature (e.g., the difference between 80°F and 80.7°F) could have an influence on behaviors driving the number of health club applications.  For this reason, any "regularity" which our learning algorithms might detect at this level of resolution would have to be <it>spurious</it>, an artifact of overfitting.  By coarsening the temperature variable into intervals the difference between which we <it>do</it> anticipate (based on prior domain knowledge) might influence  number of health club applications, we eliminate the possibility of detecting these spurious  patterns.  Thus, in this case, reducing resolution is a method of controlling <link xlink:type="simple" xlink:href="../332/173332.xml">
overfitting</link>.</entry>
<entry level="1" type="bullet">

 By reducing the number of intervals in the temperature variable (i.e., increasing its <it>grain size</it>), we increase the amount of sample data indexed by each interval designation.  Thus, by coarsening the variable, we increase sample sizes and achieve better statistical estimation.  In this sense, increasing granularity provides an antidote to the so-called <it><link xlink:type="simple" xlink:href="../776/787776.xml">
curse of dimensionality</link></it>, which relates to the exponentially decrease in statistical power with increase in number of dimensions or variable cardinality.</entry>
<entry level="1" type="bullet">

Independent of prior domain knowledge, it is often the case that meaningful regularities (i.e., which can be detected by a given learning methodology, representational language, etc.) may exist at one level of resolution and not at another.  </entry>
</list>
</p>
<p>

<image width="200 px" src="Value_granulation.png" type="thumb">
<caption>

Benefits of value granulation: Implications here exist at the resolution of <math>\{X_i,Y_j\}</math>  that do not exist at the higher resolution of <math>\{x_i,y_j\}</math>; in particular,  <math>\forall x_i,y_j: x_i \not\to y_j</math>, while at the same time, <math>\forall X_i \exists Y_j: X_i \leftrightarrow Y_j</math>.
</caption>
</image>

For example, a simple learner or pattern recognition system may seek to extract regularities satisfying a <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probability</link> threshold such as <math>p(Y=y_j|X=x_i) \ge \alpha </math>.  In the special case where <math>\alpha = 1 </math>, this recognition system is essentially detecting <it><link xlink:type="simple" xlink:href="../333/169333.xml">
logical implication</link></it> of the form <math>X=x_i \rightarrow Y=y_j </math> or, in words, "if <math>X=x_i</math>, then <math>Y=y_j </math>".  The systems ability to recognize such implications (or, in general, conditional probabilities exceeding threshold)  is partially contingent on the resolution with which the system analyzes the variables.</p>
<p>

As an example of this last point, consider the feature space shown to the right.  The variables may each be regarded at two different resolutions.  Variable <math>X</math> may be regarded at a high (quaternary) resolution wherein it takes on the four values <math>\{x_1, x_2, x_3, x_4\}</math> or at a lower (binary) resolution wherein it takes on the two values <math>\{X_1, X_2\}</math>.  Similarly, variable <math>Y</math> may be regarded at a high (quaternary) resolution or at a lower (binary) resolution, where it takes on the values <math>\{y_1, y_2, y_3, y_4\}</math> or <math>\{Y_1, Y_2\}</math>, respectively.  It will be noted that at the high resolution, there are <b>no</b> detectable implications of the form <math>X=x_i \rightarrow Y=y_j </math>, since every <math>x_i</math> is associated with more than one <math>y_j</math>, and thus, for all <math>x_i</math>, <math>p(Y=y_j|X=x_i) &amp;lt; 1 </math>.  However, at the low (binary) variable resolution, two bilateral implications become detectable:    <math>X=X_1 \leftrightarrow Y=Y_1 </math> and <math>X=X_2 \leftrightarrow Y=Y_2 </math>, since every <math>X_1</math> occurs <it>iff</it> <math>Y_1</math> and <math>X_2</math> occurs <it>iff</it> <math>Y_2</math>.  Thus, a pattern recognition system scanning for implications of this kind would find them at the binary variable resolution, but would fail to find them at the   higher quaternary variable resolution.</p>

</ss2>
<ss2>
<st>
Issues and methods</st>
<p>

It is not feasible to exhaustively test all possible discretization resolutions on all variables in order to see which combination of resolutions yields interesting or significant results.  Instead, the feature space must be preprocessed (often by an <link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link> analysis of some kind) so that some guidance can be given as to how the discretization process should proceed.  Moreover, one cannot generally achieve good results by naively analyzing and discretizing each variable independently, since this may obliterate the very interactions that we had hoped to discover. </p>
<p>

A sample of papers that address the problem of variable discretization in general, and multiple-variable discretization in particular, are as follows: <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFChiuWongCheung1991%22])">
Chiu, Wong &amp; Cheung (1991)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFBay2001%22])">
Bay (2001)</link>,  <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFLiuHussainTanDasii2002%22])">
Liu et al. (2002)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFWangLiu1998%22])">
Wang &amp; Liu (1998)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFZighedRabas=C3=A9daRakotomalala1998%22])">
Zighed, Rabaséda &amp; Rakotomalala (1998)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCatlett1991%22])">
Catlett (1991)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDoughertyKohaviSahami1995%22])">
Dougherty, Kohavi &amp; Sahami (1995)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFMontiCooper1999%22])">
Monti &amp; Cooper (1999)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFFayyadIrani1993%22])">
Fayyad &amp; Irani (1993)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFChiuCheungWong1990%22])">
Chiu, Cheung &amp; Wong (1990)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFNguyenNguyen1998%22])">
Nguyen &amp; Nguyen (1998)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFGrzymala-BusseStefanowski2001%22])">
Grzymala-Busse &amp; Stefanowski (2001)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTing1994%22])">
Ting (1994)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFLudlWidmer2000%22])">
Ludl &amp; Widmer (2000)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFPfahringer1995%22])">
Pfahringer (1995)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFAnCercone1999%22])">
An &amp; Cercone (1999)</link>, 
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFChiuCheung1989%22])">
Chiu &amp; Cheung (1989)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFChmielewskiGrzymala-Busse1996%22])">
Chmielewski &amp; Grzymala-Busse (1996)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFLeeShin1994%22])">
Lee &amp; Shin (1994)</link>.</p>

</ss2>
</ss1>
<ss1>
<st>
 Variable granulation (clustering/aggregation/transformation) </st>
<p>

Variable granulation is a term that could describe a variety of techniques, most of which are aimed at reducing dimensionality, redundancy, and storage requirements.  We briefly describe some of the ideas here, and present pointers to the literature.
</p>
<ss2>
<st>
Variable transformation</st>
<p>

A number of classical methods, such as <link xlink:type="simple" xlink:href="../340/76340.xml">
principal component analysis</link>, <link xlink:type="simple" xlink:href="../786/398786.xml">
multidimensional scaling</link>, <link xlink:type="simple" xlink:href="../492/253492.xml">
factor analysis</link>, and <link xlink:type="simple" xlink:href="../748/2007748.xml">
structural equation modeling</link>, and their relatives, fall under the genus of "variable transformation."  Also in this category are more modern areas of study such as <link xlink:type="simple" xlink:href="../867/579867.xml">
dimensionality reduction</link>, <link xlink:type="simple" xlink:href="../368/2830368.xml">
projection pursuit</link>, and <link xlink:type="simple" xlink:href="../031/598031.xml">
independent component analysis</link>.  The common goal of these methods in general is to find a representation of the data in terms of new variables, which are a linear or nonlinear transformation of the original variables, and in which important statistical relationships emerge. The resulting variable sets are almost always smaller than the original variable set, and hence these methods can be loosely said to impose a granulation on the feature space.  These dimensionality reduction methods are all reviewed in the standard texts, such as <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDudaHartStork2001%22])">
Duda, Hart &amp; Stork (2001)</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFWittenFrank2005%22])">
Witten &amp; Frank (2005)</link>, and <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHastieTibshiraniFriedman2001%22])">
Hastie, Tibshirani &amp; Friedman (2001)</link>.</p>

</ss2>
<ss2>
<st>
Variable aggregation</st>
<p>

A different class of variable granulation methods derive more from <link xlink:type="simple" xlink:href="../675/669675.xml">
data clustering</link> methodologies than from the linear systems theory informing the above methods.  It was noted fairly early that one may consider "clustering" related variables in just  the same way that one considers clustering related data.  In data clustering, one identifies a group of similar entities (using a measure of "similarity" suitable to the domain), and then in some sense <it>replaces</it> those entities with a prototype of some kind.  The prototype may be the simple average of the data in the identified cluster, or some other representative measure.  But the key idea is that in subsequent operations, we may be able to use the single prototype for the data cluster (along with perhaps a statistical model describing how exemplars are derived from the prototype) to <it>stand in</it> for the much larger set of exemplars.   These prototypes are generally such as to capture most of the information of interest concerning the entities.</p>
<p>

<image width="400 px" src="Kraskov_tree.png" type="thumb">
<caption>

A Watanabe-Kraskov variable agglomeration tree. Variables are agglomerated (or "unitized") from the bottom-up, with each merge-node representing a (constructed) variable having entropy equal to the joint entropy of the agglomerating variables. Thus, the agglomeration of two m-ary variables <math>X_1</math> and <math>X_2</math> having individual entropies <math>H(X_1)</math> and <math>H(X_2)</math> yields a single <math>m^2</math>-ary variable <math>X_{1,2}</math> with entropy <math>H(X_{1,2})=H(X_1,X_2)</math>. When <math>X_1</math> and <math>X_2</math> are highly dependent (i.e., redundant) and have large mutual information <math>I(X_1;X_2)</math>, then <math>H(X_{1,2})</math> &amp;#x226A; <math>H(X_1)+H(X_2)</math> because <math>H(X_1,X_2)=H(X_1)+H(X_2)-I(X_1;X_2)</math>, and this would be considered a parsimonious unitization or aggregation.
</caption>
</image>

Similarly, it is reasonable to ask whether a large set of variables might be aggregated in to a smaller set of <it>prototype</it> variables that capture the most salient relationships between the variables.    Although variable clustering methods based on <link xlink:type="simple" xlink:href="../057/157057.xml">
linear correlation</link> have been proposed (<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDudaHartStork2001%22])">
Duda, Hart &amp; Stork 2001</link>;<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRencher2002%22])">
Rencher 2002</link>), more powerful methods of variable clustering are based on the <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link> between variables. Watanabe has shown (<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFWatanabe1960%22])">
Watanabe 1960</link>;<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFWatanabe1969%22])">
Watanabe 1969</link>) that for any set of variables one can construct a <it>polytomic</it> (i.e., n-ary) tree representing a series of variable agglomerations in which the ultimate "total" correlation  among the complete variable set is the sum of the "partial" correlations exhibited by each agglomerating subset (see figure). Watanabe suggests that an observer might seek to thus partition a system in such a way as to minimize the interdependence between the parts "... as if they were looking for a natural division or a hidden crack." </p>
<p>

One practical approach to building such a tree is to successively choose for agglomeration the two variables (either atomic variables or previously agglomerated variables) which have the highest pairwise mutual information  <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFKraskovSt=C3=B6gbauerAndrzejakGrassberger2003%22])">
Kraskov et al. 2003</link>)</cite>. The product of each agglomeration is a new (constructed) variable that reflects the local <link xlink:type="simple" xlink:href="../637/879637.xml">
joint distribution</link> of the two agglomerating variables, and thus possesses an entropy equal to their <link xlink:type="simple" xlink:href="../967/910967.xml">
joint entropy</link>.
(From a procedural standpoint, this agglomeration step involves replacing two columns in the attribute-value table—representing the two agglomerating variables—with a single column that has a unique value for every unique combination of values in the replaced columns  <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFKraskovSt=C3=B6gbauerAndrzejakGrassberger2003%22])">
Kraskov et al. 2003</link>)</cite>.  No information is lost by such an operation; however, it should be noted that if one is exploring the data for inter-variable relationships, it would generally <it>not</it> be desirable to merge redundant variables in this way, since in such a context it is liklely to be precisely the redundancy or <it>dependency</it> between variables that is of interest;  and once redundant variables are merged, their relationship to one another can no longer be studied.</p>
<p>

See also <link xlink:type="simple" xlink:href="../239/189239.xml">
OLAP aggregation</link> for an application of aggregation in <link xlink:type="simple" xlink:href="../513/8513.xml">
database systems</link>.</p>

</ss2>
</ss1>
<ss1>
<st>
 Concept granulation (component analysis) </st>
<p>

The origins of the <it>granular computing</it> ideology are to be found in the <link xlink:type="simple" xlink:href="../778/1634778.xml">
rough sets</link> and <link xlink:type="simple" xlink:href="../601/56601.xml">
fuzzy sets</link> literatures.  One of the key insights of rough set research—although by no means unique to it—is that, in general, the selection of different sets of features or variables will yield different <it>concept</it> granulations.  Here, as in elementary rough set theory, by "concept" we mean a set of entities that are <it>indistinguishable</it> or <it>indiscernible</it> to the observer (i.e., a simple concept), or a set of entities that is composed from such simple concepts (i.e., a complex concept).  To put it in other words, by projecting a data set (<link xlink:type="simple" xlink:href="../482/7512482.xml">
value-attribute system</link>) onto different sets of variables, we recognize alternative sets of equivalence-class "concepts" in the data, and these different sets of concepts will in general be conducive to the extraction of different relationships and regularities.
</p>
<ss2>
<st>
Equivalence class granulation</st>
<p>

We illustrate with an example.  Consider the attribute-value system below:</p>
<p>

<indent level="1">

| class="wikitable" style="text-align:center; width:30%" border="1"
</indent>
|+ Sample Information System
! Object !! <math>P_{1}</math> !! <math>P_{2}</math> !! <math>P_{3}</math> !! <math>P_{4}</math> !! <math>P_{5}</math>
|-
! <math>O_{1}</math>
| 1 || 2 || 0 || 1 || 1
|-
! <math>O_{2}</math>
| 1 || 2 || 0 || 1 || 1
|-
! <math>O_{3}</math>
| 2 || 0 || 0 || 1 || 0
|-
! <math>O_{4}</math>
| 0 || 0 || 1 || 2 || 1
|-
! <math>O_{5}</math>
| 2 || 1 || 0 || 2 || 1
|-
! <math>O_{6}</math>
| 0 || 0 || 1 || 2 || 2
|-
! <math>O_{7}</math>
| 2 || 0 || 0 || 1 || 0
|-
! <math>O_{8}</math>
| 0 || 1 || 2 || 2 || 1
|-
! <math>O_{9}</math>
| 2 || 1 || 0 || 2 || 2
|-
! <math>O_{10}</math>
| 2 || 0 || 0 || 1 || 0
|}</p>
<p>

When the full set of attributes <math>P = \{P_{1},P_{2},P_{3},P_{4},P_{5}\}</math> is considered, we see that we have the following seven equivalence classes or primitive (simple) concepts:</p>
<p>

<indent level="1">

<math>
\begin{cases} 
\{O_{1},O_{2}\} \\ 
\{O_{3},O_{7},O_{10}\} \\ 
\{O_{4}\} \\ 
\{O_{5}\} \\
\{O_{6}\} \\
\{O_{8}\} \\
\{O_{9}\} \end{cases}
</math>
</indent>

Thus, the two objects within the first equivalence class, <math>\{O_{1},O_{2}\}</math>,  cannot be distinguished from one another based on the available attributes, and the three objects within the second equivalence class, <math>\{O_{3},O_{7},O_{10}\}</math>, cannot be distinguished from one another based on the available attributes.  The remaining five objects are each discernible from all other objects.  Now, let us imagine a projection of the attribute value system onto attribute <math>P_{1}</math> alone, which would represent, for example, the  view from an observer which is only capable of detecting this single attribute. Then we obtain the following much coarser equivalence class structure.  </p>
<p>

<indent level="1">

<math>
\begin{cases} 
\{O_{1},O_{2}\} \\ 
\{O_{3},O_{5},O_{7},O_{9},O_{10}\} \\ 
\{O_{4},O_{6},O_{8}\} \end{cases}
</math>
</indent>

This is in a certain regard the same structure as before, but at a lower degree of resolution (larger grain size).  Just as in the case of <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Value+granulation+(discretization/quantization)%22])">
value granulation (discretization/quantization)</link>, it is possible that relationships (dependencies) may emerge at one level of granularity that are not present at another.  As an example of this, we can consider the effect of concept granulation on the measure known as <it>attribute dependency</it> (a simpler relative of the <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>).</p>
<p>

To establish this notion of dependency (see also <link xlink:type="simple" xlink:href="../778/1634778.xml">
rough sets</link>), let <math>[x]_Q = \{Q_1, Q_2, Q_3, \dots, Q_N \}</math> represent a particular concept granulation, where each <math>Q_i</math> is an equivalence class from the concept structure induced by attribute set <math>Q</math>.  For example, if the  attribute set <math>Q</math> consists of attribute <math>P_{1}</math> alone, as above,  then the concept structure <math>[x]_Q</math> will be composed of    <math>Q_1 = \{O_{1},O_{2}\}</math>, <math>Q_2 = \{O_{3},O_{5},O_{7},O_{9},O_{10}\}</math>, and <math>Q_3 = \{O_{4},O_{6},O_{8}\}</math>.  The <b>dependency</b> of attribute set <math>Q</math> on another attribute set <math>P</math>, <math>\gamma_{P}(Q)</math>, is given by</p>
<p>

<indent level="1">

<math>
\gamma_{P}(Q) =  \frac{\left | \sum_{i=1}^N {\underline P}Q_i \right |} {\left | \mathbb{U} \right |} \leq 1
</math>
</indent>

That is, for each equivalence class <math>Q_i</math> in <math>[x]_Q</math>, we add up the size of its "lower approximation" (see <link xlink:type="simple" xlink:href="../778/1634778.xml">
rough sets</link>) by the attributes in <math>P</math>, i.e., <math>{\underline P}Q_i</math>.  More simply, this approximation  is the number of objects which on attribute set <math>P</math> can be positively identified as belonging to target set <math>Q_i</math>.  Added across all equivalence classes in <math>[x]_Q</math>, the numerator above represents the total number of objects which—based on attribute set <math>P</math>—can be positively categorized according to the classification induced by  attributes <math>Q</math>.  The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects, in a sense capturing the "synchronization" of the two concept structures <math>[x]_Q</math> and <math>[x]_P</math>.  The dependency <math>\gamma_{P}(Q)</math> "can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in <math>P</math> to determine the values of attributes in <math>Q</math>" (Ziarko &amp; Shan 1995).</p>
<p>

Having gotten definitions now out of the way, we can make the simple observation that the choice of concept granularity (i.e., choice of attributes) will influence the detected dependencies among attributes.  Consider again the attribute value table from above:</p>
<p>

<indent level="1">

| class="wikitable" style="text-align:center; width:30%" border="1"
</indent>
|+ Sample Information System
! Object !! <math>P_{1}</math> !! <math>P_{2}</math> !! <math>P_{3}</math> !! <math>P_{4}</math> !! <math>P_{5}</math>
|-
! <math>O_{1}</math>
| 1 || 2 || 0 || 1 || 1
|-
! <math>O_{2}</math>
| 1 || 2 || 0 || 1 || 1
|-
! <math>O_{3}</math>
| 2 || 0 || 0 || 1 || 0
|-
! <math>O_{4}</math>
| 0 || 0 || 1 || 2 || 1
|-
! <math>O_{5}</math>
| 2 || 1 || 0 || 2 || 1
|-
! <math>O_{6}</math>
| 0 || 0 || 1 || 2 || 2
|-
! <math>O_{7}</math>
| 2 || 0 || 0 || 1 || 0
|-
! <math>O_{8}</math>
| 0 || 1 || 2 || 2 || 1
|-
! <math>O_{9}</math>
| 2 || 1 || 0 || 2 || 2
|-
! <math>O_{10}</math>
| 2 || 0 || 0 || 1 || 0
|}</p>
<p>

Let us consider the dependency of attribute set  <math>Q = \{P_4, P_5\}</math>
on attribute set <math>P = \{P_2, P_3\}</math>.  That is, we wish to know what proportion of objects can be correctly classified into classes of <math>[x]_Q</math> based on knowledge of <math>[x]_P</math>.  The equivalence classes of <math>[x]_Q</math> and of <math>[x]_P</math> are shown below.</p>
<p>

<indent level="1">

| class="wikitable"
</indent>
|-
! <math>[x]_Q</math>
! <math>[x]_P</math>
|-
| <math>
\begin{cases} 
\{O_{1},O_{2}\} \\ 
\{O_{3},O_{7},O_{10}\} \\ 
\{O_{4},O_{5},O_{8}\} \\
\{O_{6},O_{9}\}\end{cases}
</math>
| <math>
\begin{cases} 
\{O_{1},O_{2}\} \\ 
\{O_{3},O_{7},O_{10}\} \\ 
\{O_{4},O_{6}\} \\
\{O_{5},O_{9}\} \\
\{O_{8}\}\end{cases}
</math>
|}</p>
<p>

The objects that can be <it>definitively</it> categorized according to concept structure <math>[x]_Q</math> based on <math>[x]_P</math> are those in the set <math>\{O_{1},O_{2},O_{3},O_{7},O_{8},O_{10}\}</math>, and since there are six of these, the dependency of <math>Q</math> on <math>P</math>,  <math>\gamma_{P}(Q) = 6/10</math>.  This might be considered an interesting dependency in its own right, but perhaps in a particular data mining application only stronger dependencies are desired.  </p>
<p>

We might then consider the dependency of the smaller attribute set  <math>Q = \{P_4\}</math>
on the attribute set <math>P = \{P_2, P_3\}</math>.  The move from <math>Q = \{P_4, P_5\}</math> to <math>Q = \{P_4\}</math> induces a coarsening of the class structure <math>[x]_Q</math>, as will be seen shortly.  We wish again to know what proportion of objects can be correctly classified into the (now larger) classes of <math>[x]_Q</math> based on knowledge of <math>[x]_P</math>.  The equivalence classes of the new <math>[x]_Q</math> and of <math>[x]_P</math> are shown below.</p>
<p>

<indent level="1">

| class="wikitable"
</indent>
|-
! <math>[x]_Q</math>
! <math>[x]_P</math>
|-
| <math>
\begin{cases} 
\{O_{1},O_{2},O_{3},O_{7},O_{10}\} \\ 
\{O_{4},O_{5},O_{6},O_{8},O_{9}\} \end{cases}
</math>
| <math>
\begin{cases} 
\{O_{1},O_{2}\} \\ 
\{O_{3},O_{7},O_{10}\} \\ 
\{O_{4},O_{6}\} \\
\{O_{5},O_{9}\} \\
\{O_{8}\}\end{cases}
</math>
|}</p>
<p>

Clearly, <math>[x]_Q</math> has a coarser granularity than it did earlier.  The objects that can now be <it>definitively</it> categorized according to the concept structure <math>[x]_Q</math> based on <math>[x]_P</math> constitute the complete universe <math>\{O_{1},O_{2},\ldots,O_{10}\}</math>, and thus  the dependency of <math>Q</math> on <math>P</math>,  <math>\gamma_{P}(Q) = 1</math>.  That is, knowledge of membership according to category set  <math>[x]_P</math> is adequate to determine category membership in <math>[x]_Q</math> with complete certainty; In this case we might say that <math>P \rightarrow Q</math>.  Thus, by coarsening the concept structure, we were able to find a stronger (deterministic) dependency.  However, we also note that the classes induced in <math>[x]_Q</math> from the reduction in resolution necessary to obtain this deterministic dependency are now themselves large and few in number; as a result, the dependency we found, while strong, may be less valuable to us than the weaker dependency found earlier under the higher resolution view of <math>[x]_Q</math>. </p>
<p>

In general it is not possible to test all sets of attributes to see which induced concept structures yield the strongest dependencies, and this search must be therefore be guided with some intelligence.  Papers which discuss this issue, and others relating to intelligent use of granulation, are those by Y.Y. Yao and <link xlink:type="simple" xlink:href="../155/201155.xml">
Lofti Zadeh</link> listed in the <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22References%22])">
<list>
<entry level="1" type="number">

References</entry>
</list>
</link> below.</p>

</ss2>
<ss2>
<st>
Component granulation</st>
<p>

Another perspective on concept granulation may be obtained from work on parametric models of categories.  In <link xlink:type="simple" xlink:href="../681/871681.xml">
mixture model</link> learning, for example, a set of data is explained as a mixture of distinct <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian</link> (or other) distributions.  Thus, a large amount of data is "replaced" by a small number of distributions.  The choice of the number of these distributions, and their size, can again be viewed as a problem of <it>concept granulation</it>.  In general, a better fit to the data is obtained by a larger number of distributions or parameters, but in order to extract meaningful patterns, it is necessary to constrain the number of distributions, thus deliberately  <it>coarsening</it> the concept resolution.  Finding the "right" concept resolution is a tricky problem for which many methods have been proposed (e.g., <link xlink:type="simple" xlink:href="../512/690512.xml">
AIC</link>, <link xlink:type="simple" xlink:href="../272/2473272.xml">
BIC</link>, <link xlink:type="simple" xlink:href="../325/331325.xml">
MDL</link>, etc.), and these are frequently  considered under the rubric of "<link xlink:type="simple" xlink:href="../061/2009061.xml">
model regularization</link>".</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
Different Interpretations of Granular Computing</st>
<p>

Granular computing can be conceived as a framework of theories, methodologies, techniques, and tools that make use of information granules in the process of problem solving.  In this sense, granular computing is used as an umbrella term to cover topics that have been studied in various fields in isolation.  By examining all of these existing studies in light of the unified framework of granular computing and extracting their commonalities, it may be possible to develop a general theory for problem solving. </p>
<p>

In a more philosophical sense, granular computing can describe a way of thinking that relies on the human ability to perceive the real world under various levels of granularity (i.e., abstraction) in order to abstract and consider only those things that serve a specific interest and to switch among different granularities. By focusing on different levels of granularity, one can obtain different levels of knowledge, as well as a greater understanding of the inherent knowledge structure.  Granular computing is thus essential in human problem solving and hence has a very significant impact on the design and implementation of intelligent systems.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../778/1634778.xml">
Rough set</link></instrumentality>
</artifact>
</system>
, <link xlink:type="simple" xlink:href="../017/330017.xml">
Discretization</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFAnCercone1999">An, Aijun&#32;&amp;&#32;Nick Cercone&#32;(1999),&#32;<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/l56xxg751cjx2hu7/">
"Discretization of continuous attributes for learning classification rules"</weblink>, written at <link xlink:type="simple" xlink:href="../746/18603746.xml">
Beijing, China</link>, in&#32;Ning Zhong &amp; Lizhu Zhou,&#32;<it>Methodologies for Knowledge Discovery and Data Mining: Proceedings of the Third Pacific-Asia Conference, PAKDD-99</it>,  509–514, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

Bargiela, A. and Pedrycz, W. (2003) <it>Granular Computing. An introduction</it>, Kluwer Academic Publishers</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFBay2001">Bay, Stephen D.&#32;(2001),&#32;"<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/x2ceg05lgaecqfcg/">
Multivariate discretization for set mining</weblink>",&#32;<it>Knowledge and Information Systems</it>&#32;<b>3</b>&amp;#x00a0;(4):  491–512, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFCatlett1991">Catlett, J.&#32;(1991),&#32;<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?coll=GUIDE&amp;dl=GUIDE&amp;id=112164">
"On changing continuous attributes into ordered discrete attributes"</weblink>, written at <link xlink:type="simple" xlink:href="../715/100715.xml">
Porto, Portugal</link>, in&#32;Y. Kodratoff,&#32;<it>Machine Learning—EWSL-91: European Working Session on Learning</it>,  164–178, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFChiuCheung1989">Chiu, David K. Y.&#32;&amp;&#32;Benny Cheung&#32;(1989),&#32;"Hierarchical maximum entropy discretization", written at <link xlink:type="simple" xlink:href="../646/64646.xml">
Toronto, Canada</link>, in&#32;Ryszard Janicki &amp; Waldemar W. Koczkodaj,&#32;<it>Computing and Information: Proceedings of the International Conference on Computing and Information (ICCI '89)</it>, North-Holland,  237–242</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFChiuCheungWong1990">Chiu, David K. Y.; Benny Cheung&#32;&amp; Andrew K. C. Wong&#32;(1990),&#32;"Information synthesis based on hierarchical maximum entropy discretization",&#32;<it>Journal of Experimental and Theoretical Artificial Intelligence</it>&#32;<b>2</b>:  117–129</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFChiuWongCheung1991">Chiu, David K. Y.; Andrew K. C. Wong&#32;&amp; Benny Cheung&#32;(1991),&#32;"Information discovery through hierarchical maximum entropy discretization and synthesis", written at <link xlink:type="simple" xlink:href="../685/5685.xml">
Cambridge, MA</link>, in&#32;Gregory Piatetsky-Shapiro &amp; William J. Frawley,&#32;<it>Knowledge Discovery in Databases</it>, MIT Press,  126–140</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFChmielewskiGrzymala-Busse1996">Chmielewski, Michal R.&#32;&amp;&#32;Jerzy W. Grzymala-Busse&#32;(1996),&#32;"<weblink xlink:type="simple" xlink:href="http://kuscholarworks.ku.edu/dspace/bitstream/1808/412/1/j36-draft.pdf">
Global discretization of continuous attributes as preprocessing for machine learning</weblink>",&#32;<it>International Journal of Approximate Reasoning</it>&#32;<b>15</b>:  319–331, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFDoughertyKohaviSahami1995">Dougherty, James; Ron Kohavi&#32;&amp; Mehran Sahami&#32;(1995),&#32;<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/dougherty95supervised.html">
"Supervised and unsupervised discretization of continuous features"</weblink>, written at <link xlink:type="simple" xlink:href="../855/107855.xml">
Tahoe City, CA</link>, in&#32;Armand Prieditis &amp; Stuart Russell,&#32;<it>Machine Learning: Proceedings of the Twelfth International Conference (ICML 1995)</it>, Morgan Kaufmann,  194–202, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFDudaHartStork2001">Duda, Richard O.; Peter E. Hart&#32;&amp; David G. Stork&#32;(2001), written at <region wordnetid="108630985" confidence="0.8">
<administrative_district wordnetid="108491826" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<district wordnetid="108552138" confidence="0.8">
<country wordnetid="108544813" confidence="0.8">
<link xlink:type="simple" xlink:href="../131/8210131.xml">
New York</link></country>
</district>
</location>
</administrative_district>
</region>
,&#32;<it><weblink xlink:type="simple" xlink:href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html">
Pattern Classification</weblink></it>&#32;(2 ed.), John Wiley &amp; Sons, </cite></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFFayyadIrani1993">Fayyad, Usama M.&#32;&amp;&#32;Keki B. Irani&#32;(1993),&#32;"Multi-interval discretization of continuous-valued attributes for classification learning", written at <link>
Chambéry, France</link>, in&#32;edited volume,&#32;<it>Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93)</it>,  1022–1027</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFGrzymala-BusseStefanowski2001">Grzymala-Busse, Jerzy W.&#32;&amp;&#32;Jerzy Stefanowski&#32;(2001),&#32;"<weblink xlink:type="simple" xlink:href="http://www3.interscience.wiley.com/cgi-bin/abstract/76501018/ABSTRACT?CRETRY=1&amp;SRETRY=0">
Three discretization methods for rule induction</weblink>",&#32;<it>International Journal of Intelligent Systems</it>&#32;<b>16</b>&amp;#x00a0;(1):  29–38, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFHastieTibshiraniFriedman2001">Hastie, Trevor; Robert Tibshirani&#32;&amp; Jerome Friedman&#32;(2001), written at <region wordnetid="108630985" confidence="0.8">
<administrative_district wordnetid="108491826" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<district wordnetid="108552138" confidence="0.8">
<country wordnetid="108544813" confidence="0.8">
<link xlink:type="simple" xlink:href="../131/8210131.xml">
New York</link></country>
</district>
</location>
</administrative_district>
</region>
,&#32;<it><weblink xlink:type="simple" xlink:href="http://www.springer.com/west/home/generic/order?SGWID=4-40110-22-2190214-0">
The Elements of Statistical Learning: Data Mining, Inference, and Prediction</weblink></it>, Springer, </cite></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFKraskovStögbauerAndrzejakGrassberger2003">Kraskov, Alexander; Harald Stögbauer&#32;&amp; Ralph G. Andrzejak&#32;et al.&#32;(2003),&#32;"<weblink xlink:type="simple" xlink:href="http://arxiv.org/abs/q-bio/0311039">
Hierarchical clustering based on mutual information</weblink>",&#32;<it>q-bio.QM/0311039 manuscript</it>, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFLeeShin1994">Lee, Changhwan&#32;&amp;&#32;Dong-Guk Shin&#32;(1994),&#32;"A context-sensitive discretization of numeric attributes for classification learning", written at <link xlink:type="simple" xlink:href="../844/844.xml">
Amsterdam, The Netherlands</link>, in&#32;A. G. Cohn,&#32;<it>Proceedings of the 11th European Conference on Artificial Intelligence (ECAI 94)</it>,  428–432</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFLiuHussainTanDasii2002">Liu, Huan; Farhad Hussain&#32;&amp; Chew Lim Tan&#32;et al.&#32;(2002),&#32;"<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/tuxy32pw4lg6832m/">
Discretization: An enabling technique</weblink>",&#32;<it>Data Mining and Knowledge Discovery</it>&#32;<b>6</b>&amp;#x00a0;(4):  393–423, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFLudlWidmer2000">Ludl, Marcus-Christopher&#32;&amp;&#32;Gerhard Widmer&#32;(2000),&#32;<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/37yrbu7fyg7484lt/">
"Relative unsupervised discretization for association rule mining"</weblink>, written at <link xlink:type="simple" xlink:href="../634/8638634.xml">
Lyon, France</link>, in&#32;Djamel A. Zighed, Jan Komorowski &amp; Jan Zytkow,&#32;<it>Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery (PKDD 2000)</it>,  148–158, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFMontiCooper1999">Monti, Stefano&#32;&amp;&#32;Gregory F. Cooper&#32;(1999),&#32;<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/monti99latent.html">
"A latent variable model for multivariate discretization"</weblink>, written at <link xlink:type="simple" xlink:href="../028/109028.xml">
Fort Lauderdale, FL</link>, in&#32;edited volume,&#32;<it>Uncertainty 99: The 7th International Workshop on Artificial Intelligence and Statistics</it>, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFNguyenNguyen1998">Nguyen, Hung Son&#32;&amp;&#32;Sinh Hoa Nguyen&#32;(1998),&#32;"Discretization methods in data mining", written at <location wordnetid="100027167" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../743/46743.xml">
Heidelberg</link></location>
, in&#32;Lech Polkowski &amp; Andrzej Skowron,&#32;<it>Rough Sets in Knowledge Discovery 1: Methodology and Applications</it>, Physica-Verlag,  451–482</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFPfahringer1995">Pfahringer, Bernhard&#32;(1995),&#32;<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/pfahringer95compressionbased.html">
"Compression-based discretization of continuous attributes"</weblink>, written at <link xlink:type="simple" xlink:href="../855/107855.xml">
Tahoe City, CA</link>, in&#32;Armand Prieditis &amp; Stuart Russell,&#32;<it>Machine Learning: Proceedings of the Twelfth International Conference (ICML 1995)</it>, Morgan Kaufmann,  456–463, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFRencher2002">Rencher, Alvin C.&#32;(2002), written at <region wordnetid="108630985" confidence="0.8">
<administrative_district wordnetid="108491826" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<district wordnetid="108552138" confidence="0.8">
<country wordnetid="108544813" confidence="0.8">
<link xlink:type="simple" xlink:href="../131/8210131.xml">
New York</link></country>
</district>
</location>
</administrative_district>
</region>
,&#32;<it>Methods of Multivariate Analysis</it>, Wiley</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFSimonAndo1963">Simon, Herbert A.&#32;&amp;&#32;Albert Ando&#32;(1963),&#32;"Aggregation of variables in dynamic systems", written at Cambridge, MA, in&#32;Albert Ando, Franklin M. Fisher, &amp; Herbert A. Simon,&#32;<it>Essays on the Structure of Social Science Models</it>, MIT Press,  64-91</cite></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFSimon1996">Simon, Herbert A.&#32;(1996),&#32;"The architecture of complexity: Hierarchic systems", written at Cambridge, MA, in&#32;Herbert A. Simon,&#32;<it>The Sciences of the Artificial</it>&#32;(2 ed.), MIT Press,  183-216</cite></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFTing1994">Ting, Kai Ming&#32;(1994), written at <site wordnetid="108651247" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../862/27862.xml">
Sydney</link></site>
,&#32;<it><weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/145651.html">
Discretization of continuous-valued attributes and instance-based learning (Technical Report No.491)</weblink></it>, Basser Department of Computer Science, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFWangLiu1998">Wang, Ke&#32;&amp;&#32;Bing Liu&#32;(1998),&#32;<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/wang98concurrent.html">
"Concurrent discretization of multiple attributes"</weblink>, written at <village wordnetid="108672738" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../867/17867.xml">
London</link></village>
, in&#32;Springer,&#32;<it>Proceedings of the 5th Pacific Rim International Conference on Artificial Intelligence</it>, Springer-Verlag,  250–259, </cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFWatanabe1960">Watanabe, Satosi&#32;(1960),&#32;"Information theoretical analysis of multivariate correlation",&#32;<it>IBM Journal of Research and Development</it>&#32;<b>4</b>&amp;#x00a0;(1):  66–82</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFWatanabe1969">Watanabe, Satosi&#32;(1969), written at <region wordnetid="108630985" confidence="0.8">
<administrative_district wordnetid="108491826" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<district wordnetid="108552138" confidence="0.8">
<country wordnetid="108544813" confidence="0.8">
<link xlink:type="simple" xlink:href="../131/8210131.xml">
New York</link></country>
</district>
</location>
</administrative_district>
</region>
,&#32;<it>Knowing and Guessing: A Quantitative Study of Inference and Information</it>, Wiley</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFWittenFrank2005">Witten, Ian H.&#32;&amp;&#32;Eibe Frank&#32;(2005), written at <village wordnetid="108672738" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../844/844.xml">
Amsterdam</link></village>
,&#32;<it><weblink xlink:type="simple" xlink:href="http://www.cs.waikato.ac.nz/~ml/weka/book.html">
Data Mining: Practical Machine Learning Tools and Techniques</weblink></it>&#32;(2 ed.), Morgan Kaufmann, </cite></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

Yao, Y.Y. (2004) "A Partition Model of Granular Computing", Lecture Notes in Computer Science (to appear)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Yao, Y. Y.&#32;(2001). "<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=675398">
On modeling data mining with granular computing</weblink>".&#32;<it>Proceedings of the 25th Annual International Computer Software and Applications Conference (COMPSAC 2001)</it>: 638–643.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Yao, Yiyu&#32;(2006). "<weblink xlink:type="simple" xlink:href="http://www2.cs.uregina.ca/~yyao/PAPER_PDF/grcfordm06.pdf">
Granular computing for data mining</weblink>".&#32;<physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../791/12259791.xml">
Dasarathy, Belur V.</link></associate>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
&#32;<it>Proceedings of the SPIE Conference on Data Mining, Intrusion Detection, Information Assurance, and Data Networks Security</it>.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Yao, J. T.;&#32;Yao, Y. Y.&#32;(2002). "<weblink xlink:type="simple" xlink:href="http://www2.cs.uregina.ca/~jtyao/Papers/53_RSCTC02.pdf">
Induction of classification rules by granular computing</weblink>".&#32;<it>Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing (TSCTC'02)</it>: 331–338, London, UK:&#32;Springer-Verlag.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

Zadeh, L.A. (1997) "Toward a Theory of Fuzzy Information Granulation and its Centrality in Human Reasoning and Fuzzy Logic"<it>, Fuzzy Sets and Systems</it>, 90:111-127</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFZighedRabasédaRakotomalala1998">Zighed, D. A.; S. Rabaséda&#32;&amp; R. Rakotomalala&#32;(1998),&#32;"<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=353472">
FUSINTER: A method for discretization of continuous attributes</weblink>",&#32;<it>International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</it>&#32;<b>6</b>&amp;#x00a0;(3):  307–326, </cite>.</entry>
</list>
</p>


</sec>
</bdy>
</article>
