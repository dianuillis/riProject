<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 00:18:28[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Variable-order Markov model</title>
<id>10770999</id>
<revision>
<id>217911052</id>
<timestamp>2008-06-08T07:45:44Z</timestamp>
<contributor>
<username>DOI bot</username>
<id>6652755</id>
</contributor>
</revision>
<categories>
<category>Stochastic processes</category>
<category>Probability theory</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Variable-order Markov (VOM) models</b> are an important class of models that extend the well known <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 models. In contrast to the Markov chain models, where each random variable  in a sequence with a <link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link> depends on a fixed number of <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>s, in VOM models this number of conditioning random variables may vary based on the specific observed realization. <p>

This realization sequence is often called the <it>context</it>; therefore the VOM models are also called <it>context trees</it> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. The flexibility in the number of conditioning <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>s turns out to be of real advantage for many applications, such as statistical analysis, classification and prediction.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></p>

<sec>
<st>
Example</st>

<p>

Consider for example a sequence of <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>s, each of which takes a value from the ternary <link xlink:type="simple" xlink:href="../670/670.xml">
alphabet</link> {<it>a</it>,&nbsp;<it>b</it>,&nbsp;<it>c</it>}. Specifically, consider the string <it>aaabcaaabcaaabcaaabc...aaabc</it> constructed from infinite concatenations of the sub-string <it>aaabc</it>.</p>
<p>

The VOM model of maximal order 2 can approximate the above string using <it>only</it> the following five <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probability</link> components: {Pr(<it>a</it>|<it>aa</it>) = 0.5, Pr(<it>b</it>|<it>aa</it>) = 0.5, Pr(<it>c</it>|<it>b</it>) = 1.0, Pr(<it>a</it>|<it>c</it>)= 1.0, Pr(<it>a</it>|<it>ca</it>)= 1.0}. </p>
<p>

In this example, Pr(<it>c</it>|<it>ab</it>) = Pr(<it>c</it>|<it>b</it>) = 1.0; therefore, the shorter context <it>b</it> is sufficient to determine the next character. Similarly, the VOM model of maximal order 3 can generate the string exactly using only five conditional probability components, which are all equal to 1.0.</p>
<p>

To construct the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 of order 1 for the next character in that string, one must estimate the following 9 conditional probability components: {Pr(<it>a</it>|<it>a</it>), Pr(<it>a</it>|<it>b</it>), Pr(<it>a</it>|<it>c</it>), Pr(<it>b</it>|<it>a</it>), Pr(<it>b</it>|<it>b</it>), Pr(<it>b</it>|<it>c</it>), Pr(<it>c</it>|<it>a</it>), Pr(<it>c</it>|<it>b</it>), Pr(<it>c</it>|<it>c</it>)}. To construct the Markov chain of order 2 for the next character, one must estimate 27 conditional probability components: {Pr(<it>a</it>|<it>aa</it>), Pr(<it>a</it>|<it>ab</it>), ..., Pr(<it>c</it>|<it>cc</it>)}. And to construct the Markov chain of order three for the next character one must estimate the following 81 conditional probability components: {Pr(<it>a</it>|<it>aaa</it>), Pr(<it>a</it>|<it>aab</it>), ..., Pr(<it>c</it>|<it>ccc</it>)}.</p>
<p>

In practical settings there is seldom sufficient data to accurately estimate the <link xlink:type="simple" xlink:href="../933/191933.xml">
exponentially increasing</link> number of conditional probability components as the order of the Markov chain increases.</p>
<p>

The variable-order Markov model assumes that in realistic settings, there are certain realizations of states (represented by contexts) in which some past states are independent from the future states; accordingly, "a great reduction in the number of model parameters can be achieved."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

</sec>
<sec>
<st>
Definition</st>
<p>

Let <math>A</math> be a state space (finite <link xlink:type="simple" xlink:href="../670/670.xml">
alphabet</link>) of size |A|. </p>
<p>

Consider a sequence with the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov property</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 <math>x_1^{n}=x_1x_2\dots x_n</math> of <math>n</math> realizations of <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>s, where <math> x_i\in  A</math> is the state (symbol) at position <math>i</math> 1≤<math>i</math>≤<math>n</math>, and the concatenation of states <math>x_i</math> and <math>x_{i+1}</math> is denoted by <math>x_ix_{i+1}</math>.</p>
<p>

Given a training set of observed states, <math>x_1^{n}</math>, the construction algorithm of the VOM models<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> learns a model <math>P</math> that provides a <link xlink:type="simple" xlink:href="../934/22934.xml">
probability</link> assignment for each state in the sequence given its past (previously observed symbols) or future states.</p>
<p>

Specifically, the learner generates a <link xlink:type="simple" xlink:href="../458/504458.xml">
conditional probability distribution</link> <math>P(x_i|s)</math> for a symbol <math>x_i \in A</math>  given a context <math>s\in A^*</math>, where the * sign represents a sequence of states of any length, including the empty context. </p>
<p>

VOM models attempt to estimate <link xlink:type="simple" xlink:href="../458/504458.xml">
conditional distribution</link>s of the form <math>P(x_i|s)</math> where the context length |<math>s</math>|≤<math>D</math> varies depending on the available statistics. 
In contrast, conventional <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov models</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 attempt to estimate these <link xlink:type="simple" xlink:href="../458/504458.xml">
conditional distribution</link>s by assuming a fixed contexts' length |<math>s</math>|=<math>D</math> and, hence, can be considered as special cases of the VOM models. </p>
<p>

Effectively, for a given training sequence, the VOM models are found to obtain better model parameterization than the fixed-order <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov models</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 that leads to a better <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link>-bias tradeoff of the learned models.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></p>

</sec>
<sec>
<st>
Application areas</st>
<p>

Various efficient algorithms have been devised for estimating the parameters of the VOM model.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

VOM models have been successfully applied to areas such as <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link> and <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>, including specific applications such as <link xlink:type="simple" xlink:href="../225/5225.xml">
coding</link> and <link xlink:type="simple" xlink:href="../013/8013.xml">
data compression</link>,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> document compression,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> classification and identification of <link xlink:type="simple" xlink:href="../955/7955.xml">
DNA</link> and <link xlink:type="simple" xlink:href="../634/23634.xml">
protein sequences</link>,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> <link xlink:type="simple" xlink:href="../680/593680.xml">
statistical process control</link>,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> <link xlink:type="simple" xlink:href="../001/1336001.xml">
spam filtering</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> and others.</p>

</sec>
<sec>
<st>
See also</st>

<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../196/195196.xml">
Examples of Markov chains</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../012/11228012.xml">
Variable order Bayesian network</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../772/98772.xml">
Markov process</link></entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../801/236801.xml">
Markov chain Monte Carlo</link></method>
</know-how>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../736/1598736.xml">
Semi-Markov process</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../214/4214.xml">
Bioinformatics</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../164/1164.xml">
Artificial intelligence</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">Rissanen, J.&#32;(Sep 1983).&#32;"<weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?isnumber=22734&amp;arnumber=1056741">
A Universal Data Compression System</weblink>". <it>IEEE Transactions on Information Theory</it>&#32;<b>29</b>&#32;(5): 656–664. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1109%2FTIT.1983.1056741">
10.1109/TIT.1983.1056741</weblink>.</cite>&nbsp;</entry>
<entry id="2">
 <cite style="font-style:normal">Shmilovici, A.; Ben-Gal, I.&#32;(2007).&#32;"<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/a447865604519210/">
Using a VOM Model for Reconstructing Potential Coding Regions in EST Sequences</weblink>". <it>Computational Statistics</it>&#32;<b>22</b>&#32;(1): 49–69. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2Fs00180-007-0021-8">
10.1007/s00180-007-0021-8</weblink>.</cite>&nbsp;</entry>
<entry id="3">
 <cite style="font-style:normal">Begleiter, R.; El-Yaniv, R. and Yona, G.&#32;(2004).&#32;"<weblink xlink:type="simple" xlink:href="http://www.jair.org/media/1491/live-1491-2335-jair.pdf">
On Prediction Using Variable Order Markov Models</weblink>". <it>Journal of Artificial Intelligence Research</it>&#32;<b>22</b>: 385–421.</cite>&nbsp;</entry>
<entry id="4">
 <cite style="font-style:normal">Ben-Gal, I.; Morag, G. and Shmilovici, A.&#32;(2003).&#32;"<weblink xlink:type="simple" xlink:href="http://www.eng.tau.ac.il/~bengal/Technometrics_final.pdf">
CSPC: A Monitoring Procedure for State Dependent Processes</weblink>". <it>Technometrics</it>&#32;<b>45</b>&#32;(4): 293–311. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1198%2F004017003000000122">
10.1198/004017003000000122</weblink>.</cite>&nbsp;</entry>
<entry id="5">
 <cite style="font-style:normal">Bratko, A.; Cormack, G. V., Filipic, B., Lynam, T. and Zupan, B.&#32;(2006).&#32;"<weblink xlink:type="simple" xlink:href="http://www.jmlr.org/papers/volume7/bratko06a/bratko06a.pdf">
Spam Filtering Using Statistical Data Compression Models</weblink>". <it>Journal of Machine Learning Research</it>&#32;<b>7</b>: 2673–2698.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
</bdy>
</article>
