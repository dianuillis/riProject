<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:07:07[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Generative topographic map</title>
<id>1091054</id>
<revision>
<id>230450806</id>
<timestamp>2008-08-07T18:14:50Z</timestamp>
<contributor>
<username>Bookgrrl</username>
<id>1322058</id>
</contributor>
</revision>
<categories>
<category>Articles contradicting other articles</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-content" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="39px" src="Ambox_contradict.svg">
<caption>

Accuracy dispute
</caption>
</image>
</p>
</col>
<col style="" class="mbox-text">
 This article appears to <b>contradict</b> another article.
Please see discussion on the linked . Please do not remove this message until the contradictions are resolved.</col>
</row>
</table>


<b>Generative topographic map (GTM)</b> is a <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> method that is a probabilistic counterpart of the <link xlink:type="simple" xlink:href="../996/76996.xml">
self-organizing map</link> (SOM), is provably convergent and does not require a shrinking <link xlink:type="simple" xlink:href="../485/1529485.xml">
 neighborhood</link> or a decreasing step size. It is a <link xlink:type="simple" xlink:href="../578/1222578.xml">
generative model</link>: the data is assumed to arise by first probabilistically picking a point in a low-dimensional space, mapping the point to the observed high-dimensional input space (via a smooth function), then adding noise in that space. The parameters of the low-dimensional probability distribution, the smooth map and the noise are all learned from the training data using the <link xlink:type="simple" xlink:href="../752/470752.xml">
expectation-maximization</link> (EM) algorithm.  GTM was introduced in <link xlink:type="simple" xlink:href="../636/34636.xml">
1996</link> in a paper by Bishop, Svensen, and Williams.
<sec>
<st>
 Details of the algorithm </st>

<p>

The approach is strongly related to <link>
density networks</link> which use <link xlink:type="simple" xlink:href="../671/867671.xml">
importance sampling</link> and a <link>
multi-layer perceptron</link> to form a non-linear <link xlink:type="simple" xlink:href="../572/4237572.xml">
latent variable model</link>. In the GTM the latent space is a discrete grid of points which is assumed to be non-linearly projected into data space. A <link xlink:type="simple" xlink:href="../827/1387827.xml">
Gaussian noise</link> assumption is then made in data space so that the model becomes a constrained <link>
mixture of Gaussians</link>. Then the model's likelihood can be maximized by EM. </p>
<p>

In theory, an arbitrary nonlinear parametric deformation could be used.  The optimal parameters could be found by gradient descent etc.</p>
<p>

The suggested approach to the nonlinear mapping is to use a <link xlink:type="simple" xlink:href="../443/9651443.xml">
radial basis function network</link> (RBF) to create a nonlinear mapping between the latent space and the data space. The nodes of the  
RBF network then form a feature space and the nonlinear mapping can then be taken as a <link xlink:type="simple" xlink:href="../102/18102.xml">
linear transform</link> of this feature space.  This approach has the advantage over the suggested density network approach that it can be optimised analytically.</p>

</sec>
<sec>
<st>
 Uses </st>

<p>

In data analysis, GTMs are like a nonlinear version of <link xlink:type="simple" xlink:href="../340/76340.xml">
principal components analysis</link>, which allows high dimensional data to be modelled as resulting from Gaussian noise added to sources in lower-dimensional latent space.  For example, to locate stocks in plottable 2D space based on their hi-D time-series shapes.  Other applications may want to have fewer sources than data points, for example mixture models.</p>
<p>

In generative <link>
deformational modelling</link>, the latent and data spaces have the same dimensions, for example, 2D images or 1 audio sound waves.  Extra 'empty' dimensions are added to the source (known as the 'template' in this form of modelling), for example locating the 1D sound wave in 2D space.  Further nonlinear dimensions are then added, produced by combining the original dimensions.  The enlarged latent space is then projected back into the 1D data space.  The probability of a given projection is, as before, given by the product of the likelihood of the data under the Gaussian noise model with the prior on the deformation parameter.  Unlike conventional spring-based deformation modelling, this has the advantage of being analytically optimizable.  The disadvantage is that it is a 'data-mining' approach, ie. the shape of the deformation prior is unlikely to be meaningful as an explanation of the possible deformations, as it is based on a very high, artificial- and arbitrarily constructed nonlinear latent space.  For this reason the prior is learned from data rather than created by a human expert, as is possible for spring-based models.</p>

</sec>
<sec>
<st>
 Comparison with Kohonen's SOM </st>

<p>

While nodes in the SOM can wander around at will, GTM nodes are constrained by the allowable transformations and their probabilities.  If the deformations are well-behaved the topology of the latent space is preserved.</p>
<p>

SOM was created as a biological model of neurons and is a heuristic algorithm. By contrast, GTM has nothing to do with neuroscience or cognition and is a probabilistically principled model. Thus, it has a number of advantages over SOM, namely:
<list>
<entry level="1" type="bullet">

 it explicitly formulates a density model over the data.</entry>
<entry level="1" type="bullet">

 it uses a cost function that quantifies how well the map is trained.</entry>
<entry level="1" type="bullet">

 it uses a sound optimization procedure (<link>
EM</link> algorithm).</entry>
</list>
</p>
<p>

GTM was introduced by Bishop, Svensen and Williams in their Technical Report in 1997 (Technical Report NCRG/96/015, Aston University, UK) published later in Neural Computation. It was also described in <link xlink:type="simple" xlink:href="../775/8775.xml">
PhD</link> thesis of Markus Svensen (Aston, 1998).</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial Neural Network</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../636/263636.xml">
Connectionism</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../253/42253.xml">
Data mining</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../261/309261.xml">
Nonlinear dimensionality reduction</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../924/3712924.xml">
Neural network software</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../706/126706.xml">
Pattern recognition</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://research.microsoft.com/~cmbishop/downloads/Bishop-GTM-Ncomp-98.pdf">
Bishop, Svensen and Williams Generative Topographic Mapping paper</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
