<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:08:01[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Divide-and-conquer eigenvalue algorithm</title>
<id>1103352</id>
<revision>
<id>242510634</id>
<timestamp>2008-10-02T15:16:09Z</timestamp>
<contributor>
<username>Giftlite</username>
<id>37986</id>
</contributor>
</revision>
<categories>
<category>Numerical linear algebra</category>
</categories>
</header>
<bdy>

<b>Divide-and-conquer eigenvalue algorithms</b> are a class of <link xlink:type="simple" xlink:href="../560/516560.xml">
eigenvalue algorithm</link>s for <link xlink:type="simple" xlink:href="../682/189682.xml">
Hermitian</link> or <link xlink:type="simple" xlink:href="../491/19725491.xml">
real</link> <link xlink:type="simple" xlink:href="../474/126474.xml">
symmetric matrices</link> that have recently (circa 1990s) become competitive in terms of <link xlink:type="simple" xlink:href="../807/233807.xml">
stability</link> and <link xlink:type="simple" xlink:href="../543/7543.xml">
efficiency</link> with more traditional algorithms such as the <link xlink:type="simple" xlink:href="../072/594072.xml">
QR algorithm</link>.  The basic concept behind these algorithms is of course the famous <link xlink:type="simple" xlink:href="../154/201154.xml">
divide-and-conquer</link> approach from <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>.  An <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalue</link> problem is divided into two problems of roughly half the size, each of these are solved <link xlink:type="simple" xlink:href="../407/25407.xml">
recursively</link>, and the eigenvalues of the original problem are computed from the results of these smaller problems.<p>

Here we present the simplest version of a divide-and-conquer algorithm, similar to the one originally proposed by Cuppen in 1981.  Many details that lie outside the scope of this article will be omitted; it should be noted, however, that without considering these details, the algorithm is not fully stable.</p>

<sec>
<st>
Background</st>
<p>

As with most eigenvalue algorithms for Hermitian matrices, divide-and-conquer begins with a reduction to <link xlink:type="simple" xlink:href="../218/519218.xml">
tridiagonal</link> form.  For an <math>m \times m</math> matrix, the standard method for this, via <link xlink:type="simple" xlink:href="../424/485424.xml">
Householder reflection</link>s, takes <math>\frac{4}{3}m^{3}</math> <link xlink:type="simple" xlink:href="../930/82930.xml">
flops</link>, or <math>\frac{8}{3}m^{3}</math> if <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link>s are needed as well.  There are other algorithms, such as the <link xlink:type="simple" xlink:href="../614/1134614.xml">
Arnoldi iteration</link>, which may do better for certain classes of matrices; we will not consider this further here.</p>
<p>

In certain cases, it is possible to <it>deflate</it> an eigenvalue problem into smaller problems.  Consider a <link xlink:type="simple" xlink:href="../464/457464.xml">
block diagonal matrix</link>
<indent level="1">

<math>T = \begin{bmatrix} T_{1} &amp; 0 \\ 0 &amp; T_{2}\end{bmatrix}.</math>
</indent>
The eigenvalues and eigenvectors of <math>T</math> are simply those of <math>T_{1}</math> and <math>T_{2}</math>, and it will almost always be faster to solve these two smaller problems than to solve the original problem all at once.  This technique can used to improve the efficiency of many eigenvalue algorithms, but it has special significance to divide-and-conquer.</p>
<p>

For the rest of this article, we will assume the input to the divide-and-conquer algorithm is an <math>m \times m</math> real symmetric tridiagonal matrix <math>T</math>.  Although the algorithm can be modified for Hermitian matrices, we do not give the details here.</p>

</sec>
<sec>
<st>
Divide</st>

<p>

The <it>divide</it> part of the divide-and-conquer algorithm comes from the realization that a tridiagonal matrix is "almost" block diagonal.</p>
<p>

<indent level="1">

<image width="150px" src="Almost_block_diagonal.png">
<caption>

Almost block diagonal.png
</caption>
</image>

</indent>

The size of submatrix <math>T_{1}</math> we will call <math>n \times n</math>, and then <math>T_{2}</math> is <math>(m - n) \times (m - n)</math>.  Note that the remark about <math>T</math> being almost block diagonal is true regardless of how <math>n</math> is chosen (i.e., there are many ways to so decompose the matrix).  However, it makes sense, from an efficiency standpoint, to choose <math>n \approx m/2</math>. </p>
<p>

We write <math>T</math> as a block diagonal matrix, plus a <link xlink:type="simple" xlink:href="../561/26561.xml">
rank-1</link> correction:</p>
<p>

<indent level="1">

<image width="150px" src="Block_diagonal_plus_correction.png">
<caption>

Block diagonal plus correction.png
</caption>
</image>

</indent>

The only difference between <math>T_{1}</math> and <math>\hat{T}_{1}</math> is that the lower right entry <math>t_{nn}</math> in <math>\hat{T}_{1}</math> has been replaced with <math>t_{nn} - \beta</math> and similarly, in <math>\hat{T}_{2}</math> the top left entry <math>t_{n+1,n+1}</math> has been replaced with <math>t_{n+1,n+1} - \beta</math>.</p>
<p>

The remainder of the divide step is to solve for the eigenvalues (and if desired the eigenvectors) of <math>\hat{T}_{1}</math> and <math>\hat{T}_{2}</math>, that is to find the <link xlink:type="simple" xlink:href="../672/59672.xml">
diagonalization</link>s <math>\hat{T}_{1} = Q_{1} D_{1} Q_{1}^{T}</math> and <math>\hat{T}_{2} = Q_{2} D_{2} Q_{2}^{T}</math>.  This can be accomplished with recursive calls to the divide-and-conquer algorithm, although practical implementations often switch to the QR algorithm for small enough submatrices.</p>

</sec>
<sec>
<st>
Conquer</st>

<p>

The <it>conquer</it> part of the algorithm is the unintuitive part.  Given the diagonalizations of the submatrices, calculated above, how do we find the diagonalization of the original matrix?</p>
<p>

First, define <math>z^{T} = (q_{1}^{T},q_{2}^{T})</math>, where <math>q_{1}^{T}</math> is the last row of <math>Q_{1}</math> and <math>q_{2}^{T}</math> is the first row of <math>Q_{2}</math>.  It is now elementary to show that
<indent level="1">

<math>T = \begin{bmatrix} Q_{1} &amp; \\ &amp; Q_{2} \end{bmatrix} \left( \begin{bmatrix} D_{1} &amp; \\ &amp; D_{2} \end{bmatrix} + \beta z z^{T} \right) \begin{bmatrix} Q_{1}^{T} &amp; \\ &amp; Q_{2}^{T} \end{bmatrix}</math>
</indent>

The remaining task has been reduced to finding the eigenvalues of a diagonal matrix plus a rank-one correction.  Before showing how to do this, let us simplify the notation.  We are looking for the eigenvalues of the matrix <math>D + w w^{T}</math>, where <math>D</math> is diagonal and <math>w</math> is any vector.</p>
<p>

If <math>\lambda</math> is an eigenvalue, we have:
<indent level="1">

<math>(D + w w^{T})q = \lambda q</math>
</indent>
where <math>q</math> is the corresponding eigenvector.  Now
<indent level="1">

<math>(D - \lambda I)q + w(w^{T}q) = 0</math>
</indent>
:<math>q + (D - \lambda I)^{-1} w(w^{T}q) = 0</math>
<indent level="1">

<math>w^{T}q + w^{T}(D - \lambda I)^{-1} w(w^{T}q) = 0</math>
</indent>
Keep in mind that <math>w^{T}q</math> is a nonzero scalar. Neither <math>w</math> nor <math>q</math> are zero. If <math>w^{T}q</math> were to be zero, <math>q</math> would be an eigenvector of <math>D</math> by <math>(D + w w^{T})q = \lambda q</math>. If that were the case, <math>q</math> would contain only one nonzero position since <math>D</math> is diagonal and thus the inner product <math>w^{T}q</math> can not be zero after all. Therefore, we have:
<indent level="1">

<math>1 + w^{T}(D - \lambda I)^{-1} w = 0</math>
</indent>
or written as a scalar equation,
<indent level="1">

<math>1 + \sum_{j=1}^{m} \frac{w_{j}^{2}}{d_{j} - \lambda} = 0.</math>
</indent>
This equation is known as the <it>secular equation</it>. The problem has therefore been reduced to finding the roots of the <link xlink:type="simple" xlink:href="../210/361210.xml">
rational function</link> defined by the left-hand side of this equation.</p>
<p>

All general eigenvalue algorithms must be iterative, and the divide-and-conquer algorithm is no different.  Solving the <link xlink:type="simple" xlink:href="../103/146103.xml">
nonlinear</link> secular equation requires an iterative technique, such as the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../145/22145.xml">
Newtonâ€“Raphson method</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
.  However, each root can be found in <link xlink:type="simple" xlink:href="../578/44578.xml">
O</link>(1) iterations, each of which requires <math>\Theta(m)</math> flops (for an <math>m</math>-degree rational function), making the cost of the iterative part of this algorithm <math>\Theta(m^{2})</math>.</p>

</sec>
<sec>
<st>
Analysis</st>

<p>

As is common for divide and conquer algorithms, we will use the <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../585/561585.xml">
Master theorem</link></proposition>
</theorem>
</message>
</statement>
 to analyze the running time.  Remember that above we stated we choose <math>n \approx m/2</math>.  We can write the <link xlink:type="simple" xlink:href="../806/146806.xml">
recurrence relation</link>:
<indent level="1">

<math>T(m) = 2 \times T\left(\frac{m}{2}\right) + \Theta(m^{2})</math>
</indent>
In the notation of the Master theorem, <math>a = b = 2</math> and thus <math>\log_{b} a = 1</math>.  Clearly, <math>\Theta(m^{2}) = \Omega(m^{1})</math>, so we have
<indent level="1">

<math>T(m) = \Theta(m^{2})</math>
</indent>

Remember that above we pointed out that reducing a Hermitian matrix to tridiagonal form takes <math>\frac{4}{3}m^{3}</math> flops.  This dwarfs the running time of the divide-and-conquer part, and at this point it is not clear what advantage the divide-and-conquer algorithm offers over the QR algorithm (which also takes <math>\Theta(m^{2})</math> flops for tridiagonal matrices).</p>
<p>

The advantage of divide-and-conquer comes when eigenvectors are needed as well.  If this is the case, reduction to tridiagonal form takes <math>\frac{8}{3}m^{3}</math>, but the second part of the algorithm takes <math>\Theta(m^{3})</math> as well.  For the QR algorithm with a reasonable target precision, this is <math>\approx 6 m^{3}</math>, whereas for divide-and-conquer it is <math>\approx \frac{4}{3}m^{3}</math>.  The reason for this improvement is that in divide-and-conquer, the <math>\Theta(m^{3})</math> part of the algorithm (multiplying <math>Q</math> matrices) is separate from the iteration, whereas in QR, this must occur in every iterative step.  Adding the <math>\frac{8}{3}m^{3}</math> flops for the reduction, the total improvement is from <math>\approx 9 m^{3}</math> to <math>\approx 4 m^{3}</math> flops.</p>
<p>

Practical use of the divide-and-conquer algorithm has shown that in most realistic eigenvalue problems, the algorithm actually does better than this.  The reason is that very often the matrices <math>Q</math> and the vectors <math>z</math> tend to be <it>numerically sparse</it>, meaning that they have many entries with values smaller than the <link xlink:type="simple" xlink:href="../376/11376.xml">
floating point</link> precision, allowing for <it>numerical deflation</it>, i.e. breaking the problem into uncoupled subproblems.</p>

</sec>
<sec>
<st>
Variants and implementation</st>

<p>

The algorithm presented here is the simplest version.  In many practical implementations, more complicated rank-1 corrections are used to guarantee stability; some variants even use rank-2 corrections.</p>
<p>

There exist specialized root-finding techniques for rational functions that may do better than the Newton-Raphson method in terms of both performance and stability.  These can be used to improve the iterative part of the divide-and-conquer algorithm.</p>
<p>

The divide-and-conquer algorithm is readily <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../840/148840.xml">
parallelized</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, and <link xlink:type="simple" xlink:href="../422/18422.xml">
linear algebra</link> computing packages such as <link xlink:type="simple" xlink:href="../768/1013768.xml">
LAPACK</link> contain high-quality parallel implementations.</p>

</sec>
</bdy>
</article>
