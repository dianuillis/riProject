<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:09:02[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<know-how  confidence="0.8" wordnetid="105616786">
<method  confidence="0.8" wordnetid="105660268">
<header>
<title>Monte Carlo integration</title>
<id>1112960</id>
<revision>
<id>242502103</id>
<timestamp>2008-10-02T14:32:13Z</timestamp>
<contributor>
<username>Lightbot</username>
<id>7178666</id>
</contributor>
</revision>
<categories>
<category>Monte Carlo methods</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link>, <b>Monte Carlo integration</b> is <link xlink:type="simple" xlink:href="../089/170089.xml">
numerical quadrature</link> using <link xlink:type="simple" xlink:href="../210/23210.xml">
pseudorandom number</link>s. That is, Monte Carlo integration methods are <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>s for the approximate evaluation of <link xlink:type="simple" xlink:href="../532/15532.xml">
definite integral</link>s, usually multidimensional ones. The usual algorithms evaluate the integrand at a regular grid. <link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo methods</link>, however, randomly choose the points at which the integrand is evaluated.<p>

Informally, to estimate the area of a domain D, first pick a simple domain d whose area is easily calculated and which contains D.  Now pick a sequence of random points that fall within d.  Some fraction of these points will also fall within D.  The area of D is then estimated as this fraction multiplied by the area of d.</p>
<p>

The traditional Monte Carlo algorithm distributes the evaluation points <link xlink:type="simple" xlink:href="../835/5509835.xml">
uniformly</link> over the integration region. Adaptive algorithms such as VEGAS and MISER use <link xlink:type="simple" xlink:href="../671/867671.xml">
importance sampling</link> and <link xlink:type="simple" xlink:href="../596/27596.xml">
stratified sampling</link> techniques to get a better result.</p>

<sec>
<st>
 The traditional algorithm </st>

<p>

The algorithm computes an estimate of a multidimensional definite integral of the form,</p>
<p>

<indent level="1">

<math>I = \int_{x_l}^{x_u} \int_{y_l}^{y_u} f(x, y, \ldots) \, dx  \, dy \ldots =\int_{V}f(x, y, \ldots) \, dx  \, dy \ldots</math>
</indent>

over the <link xlink:type="simple" xlink:href="../783/39783.xml">
hypercube</link> with volume <math>V</math> defined by { (<it>x</it>, <it>y</it>, &amp;hellip;) | <it>xl</it> &amp;le; <it>x</it> &amp;le; <it>xu</it>, <it>yl</it> &amp;le; <it>y</it> &amp;le; <it>yu</it>, &amp;hellip; }.</p>
<p>

The plain Monte Carlo algorithm samples points uniformly from the integration region to estimate the integral and its error. Suppose that the sample has size <it>N</it> and denote the points in the sample by <it>x</it>1, &amp;hellip;, <it>xN</it>. Then the estimate for the integral is given by
<indent level="1">

<math> E(f;N) = V \cdot \langle f \rangle = V\frac{1}{N} \sum_{i=1}^N f(x_i), </math>
</indent>
where <math>\langle f \rangle</math> denotes the <link xlink:type="simple" xlink:href="../612/612.xml">
sample mean</link> of the integrand.</p>
<p>

The <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link> of the function can be estimated using
<indent level="1">

<math> \sigma^2(E;N) = \frac{1}{N} \sum_{i=1}^N (f(x_i) - \langle f \rangle)^2. </math>
</indent>

The variance of the estimate of the integral can be estimated using
<indent level="1">

<math> \sigma^2(E;N) = \frac{V^2}{N^2} \sum_{i=1}^N (f(x_i) - \langle f \rangle)^2. </math>
</indent>

For large <it>N</it> this variance decreases asymptotically as <it>V2</it> var(<it>f</it>) / <it>N</it>, where var(<it>f</it>) is the true variance of the function over the integration region. The error estimate itself decreases as <it>V</it> &amp;sigma;(<it>f</it>) / &amp;radic;<it>N</it>. The familiar law of errors decreasing as 1 / &amp;radic;<it>N</it> applies: to reduce the error by a factor of 10 requires a 100-fold increase in the number of sample points. </p>
<p>

The above expression provides a statistical estimate of the error on the result. This error estimate is not a strict error bound &mdash; random sampling of the region may not uncover all the important features of the function, resulting in an underestimate of the error.</p>




<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-content" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_content.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 The following sections should be <b>rewritten</b>, because their explanations of the algorithms are too dependent on the specific implementations in the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
. See the discussion on the .</col>
</row>
</table>
</p>


</sec>
<sec>
<st>
 MISER Monte Carlo </st>

<p>

The MISER algorithm of Press and Farrar is based on recursive stratified sampling. This technique aims to reduce the overall integration error by concentrating integration points in the regions of highest variance. </p>
<p>

The idea of stratified sampling begins with the observation that for two disjoint regions a and b with Monte Carlo estimates of the integral <math>E_a(f)</math> and <math>E_b(f)</math> and variances <math>\sigma_a^2(f)</math> and <math>\sigma_b^2(f)</math>, the variance <math>Var(f)</math> of the combined estimate <math>E(f) = (1/2) (E_a(f) + E_b(f))</math> is given by, </p>
<p>

<indent level="1">

<math>\mathrm{Var}(f) = (\sigma_a^2(f) / 4 N_a) + (\sigma_b^2(f) / 4 N_b)</math>
</indent>

It can be shown that this variance is minimized by distributing the points such that, </p>
<p>

<indent level="1">

<math>N_a / (N_a + N_b) = \sigma_a / (\sigma_a + \sigma_b)</math>
</indent>

Hence the smallest error estimate is obtained by allocating sample points in proportion to the standard deviation of the function in each sub-region. </p>
<p>

The MISER algorithm proceeds by bisecting the integration region along one coordinate axis to give two sub-regions at each step. The direction is chosen by examining all d possible bisections and selecting the one which will minimize the combined variance of the two sub-regions. The variance in the sub-regions is estimated by sampling with a fraction of the total number of points available to the current step. The same procedure is then repeated recursively for each of the two half-spaces from the best bisection. The remaining sample points are allocated to the sub-regions using the formula for N_a and N_b. This recursive allocation of integration points continues down to a user-specified depth where each sub-region is integrated using a plain Monte Carlo estimate. These individual values and their error estimates are then combined upwards to give an overall result and an estimate of its error. </p>
<p>

This routines uses the MISER Monte Carlo algorithm to integrate the function f over the dim-dimensional hypercubic region defined by the lower and upper limits in the arrays xl and xu, each of size dim. The integration uses a fixed number of function calls, and obtains random sampling points using the random number generator r. A previously allocated workspace s must be supplied. The result of the integration is returned in result, with an estimated absolute error abserr. </p>

<ss1>
<st>
 Configurable Parameters </st>
<p>

The MISER algorithm has several configurable parameters. </p>

<ss2>
<st>
 estimate_frac </st>
<p>
 
This parameter specifies the fraction of the currently available number of function calls which are allocated to estimating the variance at each recursive step. In the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
's (GSL) implementation, the default value is 0.1.</p>

</ss2>
<ss2>
<st>
 min_calls </st>
<p>
 
This parameter specifies the minimum number of function calls required for each estimate of the variance. If the number of function calls allocated to the estimate using estimate_frac falls below min_calls then min_calls are used instead. This ensures that each estimate maintains a reasonable level of accuracy. In the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
's implementation, the default value of min_calls is 16 * dim. </p>

</ss2>
<ss2>
<st>
 min_calls_per_bisection </st>
<p>
 
This parameter specifies the minimum number of function calls required to proceed with a bisection step. When a recursive step has fewer calls available than min_calls_per_bisection it performs a plain Monte Carlo estimate of the current sub-region and terminates its branch of the recursion. In the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
's implementation, the default value of this parameter is 32 * min_calls. </p>

</ss2>
<ss2>
<st>
 alpha </st>
<p>
 
This parameter controls how the estimated variances for the two sub-regions of a bisection are combined when allocating points. With recursive sampling the overall variance should scale better than 1/N, since the values from the sub-regions will be obtained using a procedure which explicitly minimizes their variance. To accommodate this behavior the MISER algorithm allows the total variance to depend on a scaling parameter \alpha, </p>
<p>

<indent level="1">

<math>\mathrm{Var}(f) = {\sigma_a \over N_a^\alpha} + {\sigma_b \over N_b^\alpha}</math>
</indent>

The authors of the original paper describing MISER recommend the value <math>\alpha = 2</math> as a good choice, obtained from numerical experiments, and this is used as the default value in the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
's implementation.</p>

</ss2>
<ss2>
<st>
 dither </st>
<p>
 
This parameter introduces a random fractional variation of size dither into each bisection, which can be used to break the symmetry of integrands which are concentrated near the exact center of the hypercubic integration region. In the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
's implementation, the default value of dither is zero, so no variation is introduced. If needed, a typical value of dither is around 0.1.</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
 VEGAS Monte Carlo </st>

<p>

The <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../188/345188.xml">
VEGAS algorithm</link></method>
</know-how>
 of G.P.Lepage is based on importance sampling. It samples points from the probability distribution described by the function <math>|f|</math>, so that the points are concentrated in the regions that make the largest contribution to the integral. </p>
<p>

In general, if the Monte Carlo integral of <math>f</math> is sampled with points distributed according to a probability distribution described by the function <math>g</math>, we obtain an estimate <math>E_g(f; N)</math>, </p>
<p>

<math>E_g(f; N) = E(f/g; N)</math></p>
<p>

with a corresponding variance, </p>
<p>

<math>Var_g(f; N) = Var(f/g; N)</math></p>
<p>

If the probability distribution is chosen as <math>g = |f|/I(|f|)</math> then it can be shown that the variance <math>V_g(f; N)</math> vanishes, and the error in the estimate will be zero. In practice it is not possible to sample from the exact distribution <math>g</math> for an arbitrary function, so importance sampling algorithms aim to produce efficient approximations to the desired distribution. </p>
<p>

The VEGAS algorithm approximates the exact distribution by making a number of passes over the integration region while histogramming the function <math>f</math>. Each histogram is used to define a sampling distribution for the next pass. Asymptotically this procedure converges to the desired distribution. In order to avoid the number of histogram bins growing like <math>K^d</math> the probability distribution is approximated by a separable function: <math>g(x_1, x_2, \ldots) = g_1(x_1) g_2(x_2) \ldots</math> so that the number of bins required is only <math>Kd</math>. This is equivalent to locating the peaks of the function from the projections of the integrand onto the coordinate axes. The efficiency of VEGAS depends on the validity of this assumption. It is most efficient when the peaks of the integrand are well-localized. If an integrand can be rewritten in a form which is approximately separable this will increase the efficiency of integration with VEGAS. </p>
<p>

VEGAS incorporates a number of additional features, and combines both stratified sampling and importance sampling. The integration region is divided into a number of "boxes", with each box getting a fixed number of points (the goal is 2). Each box can then have a fractional number of bins, but if bins/box is less than two, Vegas switches to a kind variance reduction (rather than importance sampling). </p>
<p>

This routines uses the VEGAS Monte Carlo algorithm to integrate the function <math>f</math> over the dim-dimensional hypercubic region defined by the lower and upper limits in the arrays <math>xl</math> and <math>xu</math>, each of size <math>dim</math>. The integration uses a fixed number of function calls calls, and obtains random sampling points using the random number generator <math>r</math>. A previously allocated workspace <math>s</math> must be supplied. The result of the integration is returned in <math>result</math>, with an estimated absolute error <math>abserr</math>. The result and its error estimate are based on a weighted average of independent samples. The chi-squared per degree of freedom for the weighted average is returned via the state struct component, <math>s\to chisq</math>, and must be consistent with 1 for the weighted average to be reliable. </p>
<p>

The VEGAS algorithm computes a number of independent estimates of the integral internally, according to the iterations parameter described below, and returns their weighted average. Random sampling of the integrand can occasionally produce an estimate where the error is zero, particularly if the function is constant in some regions. An estimate with zero error causes the weighted average to break down and must be handled separately. In the original Fortran implementations of VEGAS the error estimate is made non-zero by substituting a small value (typically 1e-30). The implementation in GSL differs from this and avoids the use of an arbitrary constant -- it either assigns the value a weight which is the average weight of the preceding estimates, or discards it according to the following procedure:</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Current estimate has zero error, weighted average has finite error</b>The current estimate is assigned a weight which is the average weight of the preceding estimates.</entry>
<entry level="1" type="bullet">

 <b>Current estimate has finite error, previous estimates had zero error</b>The previous estimates are discarded and the weighted averaging procedure begins with the current estimate.</entry>
<entry level="1" type="bullet">

 <b>Current estimate has zero error, previous estimates had zero error</b>The estimates are averaged using the arithmetic mean, but no error is computed. </entry>
</list>
</p>

<ss1>
<st>
 Configurable Parameters </st>

<p>

The VEGAS algorithm is configurable. </p>

<ss2>
<st>
 chisq </st>
<p>
 
This parameter gives the chi-squared per degree of freedom for the weighted estimate of the integral. The value of chisq should be close to 1. A value of chisq which differs significantly from 1 indicates that the values from different iterations are inconsistent. In this case the weighted error will be under-estimated, and further iterations of the algorithm are needed to obtain reliable results. </p>

</ss2>
<ss2>
<st>
 alpha </st>
<p>
 
The parameter alpha controls the stiffness of the rebinning algorithm. It is typically set between one and two. A value of zero prevents rebinning of the grid. In the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
's implementation, the default value is 1.5. </p>

</ss2>
<ss2>
<st>
 iterations </st>
<p>
 
The number of iterations to perform for each call to the routine. In the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
's implementation, the default value is 5 iterations. </p>

</ss2>
<ss2>
<st>
 stage </st>
<p>
 
Setting this determines the stage of the calculation. Normally, stage = 0 which begins with a new uniform grid and empty weighted average. Calling vegas with stage = 1 retains the grid from the previous run but discards the weighted average, so that one can "tune" the grid using a relatively small number of points and then do a large run with stage = 1 on the optimized grid. Setting stage = 2 keeps the grid and the weighted average from the previous run, but may increase (or decrease) the number of histogram bins in the grid depending on the number of calls available. Choosing stage = 3 enters at the main loop, so that nothing is changed, and is equivalent to performing additional iterations in a previous call. </p>

</ss2>
<ss2>
<st>
 mode </st>
<p>
 
The possible choices are GSL_VEGAS_MODE_IMPORTANCE, GSL_VEGAS_MODE_STRATIFIED, GSL_VEGAS_MODE_IMPORTANCE_ONLY. This determines whether VEGAS will use importance sampling or stratified sampling, or whether it can pick on its own. In low dimensions VEGAS uses strict stratified sampling (more precisely, stratified sampling is chosen if there are fewer than 2 bins per box).</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
 References and further reading </st>

<p>

The following reference about Monte Carlo and quasi-Monte Carlo methods in general (with a description of the variance reduction techniques) is excellent to start with:
<list>
<entry level="1" type="bullet">

 R. E. Caflisch, <it>Monte Carlo and quasi-Monte Carlo methods</it>, Acta Numerica vol. 7, Cambridge University Press, 1998, pp. 1-49.</entry>
</list>
</p>
<p>

Nice survey on arXiv, based on lecture for graduate students in high energy physics:
<list>
<entry level="1" type="bullet">

 S. Weinzierl, <it><weblink xlink:type="simple" xlink:href="http://arxiv.org/abs/hep-ph/0006269/">
Introduction to Monte Carlo methods</weblink></it>, </entry>
</list>
</p>
<p>

The MISER algorithm is described in the following article, </p>
<p>

<list>
<entry level="1" type="bullet">

 W.H. Press, G.R. Farrar, Recursive Stratified Sampling for Multidimensional Monte Carlo Integration, Computers in Physics, v4 (1990), pp190-195. </entry>
</list>
</p>
<p>

The VEGAS algorithm is described in the following papers, </p>
<p>

<list>
<entry level="1" type="bullet">

 G.P. Lepage, A New Algorithm for Adaptive Multidimensional Integration, Journal of Computational Physics 27, 192-203, (1978) </entry>
<entry level="1" type="bullet">

 G.P. Lepage, VEGAS: An Adaptive Multi-dimensional Integration Program, Cornell preprint CLNS 80-447, March 1980 </entry>
</list>

----
<it>Based on the <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
's manual, which is published under the GFDL (and hence free to use for Wikipedia). Original available <weblink xlink:type="simple" xlink:href="http://www.gnu.org/software/gsl/manual/gsl-ref_23.html">
here</weblink>.</it></p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../018/5474018.xml">
Auxiliary field Monte Carlo</link></method>
</know-how>
</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://math.fullerton.edu/mathews/n2003/MonteCarloMod.html">
Module for Monte Carlo Integration</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://math.fullerton.edu/mathews/n2003/montecarlo/MonteCarloBib/Links/MonteCarloBib_lnk_1.html">
Internet Resources for Monte Carlo Integration</weblink></entry>
</list>
</p>


</sec>
</bdy>
</method>
</know-how>
</article>
