<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 00:44:08[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>In-place matrix transposition</title>
<id>11174336</id>
<revision>
<id>235888506</id>
<timestamp>2008-09-02T20:42:27Z</timestamp>
<contributor>
<username>Thenickdude</username>
<id>486141</id>
</contributor>
</revision>
<categories>
<category>Numerical linear algebra</category>
<category>Permutations</category>
</categories>
</header>
<bdy>

<b>In-place matrix transposition</b>, also called <b>in-situ matrix transposition</b>, is the problem of <link xlink:type="simple" xlink:href="../844/173844.xml">
transposing</link> an <math>N \times M</math> <link xlink:type="simple" xlink:href="../728/19008728.xml">
matrix</link> <link xlink:type="simple" xlink:href="../861/219861.xml">
in-place</link> in <link xlink:type="simple" xlink:href="../847/25847.xml">
computer memory</link>: ideally with <math>O(1)</math> (bounded) additional storage, or at most with additional storage much less than <math>NM</math>.  Typically, the matrix is assumed to be stored in <link xlink:type="simple" xlink:href="../786/1620786.xml">
row-major order</link> or <link xlink:type="simple" xlink:href="../786/1620786.xml">
column-major order</link> (i.e., contiguous rows or columns, respectively, arranged consecutively).<p>

Performing an in-place transpose (in-situ transpose) is most difficult when <math>N \neq M</math>, i.e. for a non-square (rectangular) matrix, where it involves a complicated <link xlink:type="simple" xlink:href="../027/44027.xml">
permutation</link> of the data elements, with many <link xlink:type="simple" xlink:href="../394/2539394.xml">
cycle</link>s of length greater than 2.  In contrast, for a square matrix (<math>N = M</math>), all of the cycles of are length 1 or 2, and the transpose can achieved by a simple loop to swap the upper triangle of the matrix with the lower triangle.  Further complications arise if one wishes to maximize <link>
memory locality</link>, however, to improve <link xlink:type="simple" xlink:href="../181/849181.xml">
cache line</link> utilization or to operate <link xlink:type="simple" xlink:href="../722/1881722.xml">
out-of-core</link> (where the matrix does not fit into main memory), since transposes inherently involve non-consecutive memory accesses.</p>
<p>

The problem of non-square in-place transposition has been studied since at least the late <link xlink:type="simple" xlink:href="../573/34573.xml">
1950s</link>, and several algorithms are known, including several which attempt to optimize locality for cache, out-of-core, or similar memory-related contexts.</p>

<sec>
<st>
Background</st>

<p>

On a <link xlink:type="simple" xlink:href="../457/7878457.xml">
computer</link>, one can often avoid explicitly transposing a matrix in <link xlink:type="simple" xlink:href="../847/25847.xml">
memory</link> by simply accessing the same data in a different order.  For example, <link xlink:type="simple" xlink:href="../421/106421.xml">
software libraries</link> for <link xlink:type="simple" xlink:href="../422/18422.xml">
linear algebra</link>, such the <link xlink:type="simple" xlink:href="../721/1060721.xml">
BLAS</link>, typically provide options to specify that certain matrices are to be interpreted in transposed order to avoid the necessity of data movement.</p>
<p>

However, there remain a number of circumstances in which it is necessary or desirable to physically reorder a matrix in memory to its transposed ordering.  For example, with a matrix stored in <link xlink:type="simple" xlink:href="../786/1620786.xml">
row-major order</link>, the rows of the matrix are contiguous in memory and the columns are discontiguous.  If repeated operations need to be performed on the columns, for example in a <link xlink:type="simple" xlink:href="../512/11512.xml">
fast Fourier transform</link> algorithm (e.g. Frigo &amp; Johnson, 2005), transposing the matrix in memory (to make the columns contiguous) may improve performance by increasing <link>
memory locality</link>.  Since these situations normally coincide with the case of very large matrices (which exceed the cache size), performing the transposition in-place with minimal additional storage becomes desirable.</p>
<p>

Also, as a purely mathematical problem, in-place transposition involves a number of interesting <link xlink:type="simple" xlink:href="../527/21527.xml">
number theory</link> puzzles that have been worked out over the course of several decades.</p>

</sec>
<sec>
<st>
Example</st>

<p>

For example, consider the <math>2\times4</math> matrix:</p>
<p>

<indent level="1">

<math>\begin{bmatrix} 0 &amp; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 &amp; 7\end{bmatrix}.</math>
</indent>

In row-major format, this would be stored in computer memory as the sequence (0,1,2,3,4,5,6,7), i.e. the two rows stored consecutively.  If we transpose this, we obtain the <math>4\times2</math> matrix:</p>
<p>

<indent level="1">

<math>\begin{bmatrix} 0 &amp; 4 \\ 1 &amp; 5 \\ 2 &amp; 6 \\ 3 &amp; 7\end{bmatrix}</math>
</indent>

which is stored in computer memory as the sequence (0,4,1,5,2,6,3,7).</p>
<p>

If we number the storage locations 0 to 7, from left to right, then this permutation consists of four cycles:</p>
<p>

<indent level="1">

(0), (1 2 4), (3 6 5), (7)
</indent>

That is, position 0 goes to position 0 (a cycle of length 1, no data motion).  And position 1 (in 0,1,2,...) goes to position 2 (in 0,4,1,...), while 2 goes to position 4 (in 0,4,1,5,2,...), while position 4 goes back to position 1 (in 0,4,1,...).  Similarly for position 7 and positions (3 6 5).</p>

</sec>
<sec>
<st>
Properties of the permutation</st>

<p>

In the following, we assume that the <math>N \times M</math> matrix is stored in row-major order with zero-based indices.  This means that the <math>(n,m)</math> element, for <math>n = 0,\ldots,N-1</math> and <math>m = 0,\ldots,M-1</math>, is stored at an address <math>a = Mn + m</math> (plus some offset in memory, which we ignore).  In the transposed <math>M \times N</math> matrix, the corresponding <math>(m,n)</math> element is stored at the address <math>a' = Nm + n</math>, again in row-major order.  We define the <it>transposition permutation</it> to be the function <math>a' = P(a)</math> such that:
<indent level="1">

<math>Nm + n = P(Mn + m) \,</math> for all <math>(n,m) \in [0,N-1]\times[0,M-1] \,.</math>
</indent>
This defines a permutation on the numbers <math>n = 0,\ldots,MN-1</math>.</p>
<p>

It turns out that one can define simple formulas for <it>P</it> and its inverse (Cate &amp; Twigg, 1977).  First:</p>
<p>

<indent level="1">

<math>P(a) = \left\{ \begin{matrix}
MN - 1 &amp; \mbox{if } a = MN - 1, \\
Na \mod MN - 1 &amp; \mbox{otherwise},
\end{matrix} \right.
</math>
</indent>

where "mod" is the <link xlink:type="simple" xlink:href="../428/1352428.xml">
modulo operation</link>.  Proof: if <math>0 \leq a = Mn + m &amp;lt; MN - 1</math>, then <math>Na \mod (MN-1) = MN n + Nm \mod (MN - 1) = n + Nm</math>.  [Note that <math>MN x \mod (MN-1) = (MN - 1) x  + x \mod (MN-1) = x</math> for <math>0 \leq x &amp;lt; MN - 1</math>.]  Note that the first (<math>a = 0</math>) and last (<math>a = MN-1</math>) elements are always left invariant under transposition.  Second, the inverse permutation is given by:</p>
<p>

<indent level="1">

<math>P^{-1}(a') = \left\{ \begin{matrix}
MN - 1 &amp; \mbox{if } a' = MN - 1, \\
Ma' \mod MN - 1 &amp; \mbox{otherwise}.
\end{matrix} \right.
</math>
</indent>

(This is just a consequence of the fact that the inverse of an <math>N \times M</math> transpose is an <math>M \times N</math> transpose, although it is also easy to show explicitly that <math>P^{-1}</math> composed with <math>P</math> gives the identity.)</p>
<p>

As proved by Cate &amp; Twigg (1977), the number of <link xlink:type="simple" xlink:href="../738/449738.xml">
fixed points</link> (cycles of length 1) of the permutation is precisely 1&nbsp;+&nbsp;gcd(<it>N</it>&amp;minus;1,<it>M</it>&amp;minus;1), where gcd is the <link xlink:type="simple" xlink:href="../354/12354.xml">
greatest common divisor</link>.  For example, with <it>N</it> = <it>M</it> the number of fixed points is simply <it>N</it> (the diagonal of the matrix).  If <it>N</it>&nbsp;&amp;minus;&nbsp;1 and <it>M</it>&nbsp;&amp;minus;&nbsp;1 are <link xlink:type="simple" xlink:href="../556/6556.xml">
coprime</link>, on the other hand, the only two fixed points are the upper-left and lower-right corners of the matrix.</p>
<p>

The number of cycles of any length <it>k</it>&amp;gt;1 is given by (Cate &amp; Twigg, 1977):</p>
<p>

<indent level="1">

<math>\frac{1}{k} \sum_{d | k} \mu(k/d) \gcd(N^d - 1, MN - 1) ,</math>
</indent>

where μ is the <link>
Möbius function</link> and the sum is over the <link xlink:type="simple" xlink:href="../492/49492.xml">
divisor</link>s <it>d</it> of <it>k</it>.</p>
<p>

Furthermore, the cycle containing <it>a</it>=1 (i.e. the second element of the first row of the matrix) is always a cycle of maximum length <it>L</it>, and the lengths <it>k</it> of all other cycles must be divisors of <it>L</it>  (Cate &amp; Twigg, 1977).</p>
<p>

For a given cycle <it>C</it>, every element <math>x \in C</math> has the same greatest common divisor <math>d = \gcd(x, MN - 1)</math>.  Proof (Brenner, 1973):  Let <it>s</it> be the smallest element of the cycle, and <math>d = \gcd(s, MN - 1)</math>.  From the definition of the permutation <it>P</it> above, every other element <it>x</it> of the cycle is obtained by repeatedly multiplying <it>s</it> by <it>N</it> modulo <it>MN</it>&amp;minus;1, and therefore every other element is divisible by <it>d</it>.  But, since <it>N</it> and <it>MN</it>&nbsp;&amp;minus;&nbsp;1 are coprime, <it>x</it> cannot be divisible by any factor of <it>MN</it>&nbsp;&amp;minus;&nbsp;1 larger than <it>d</it>, and hence <math>d = \gcd(x, MN - 1)</math>.  This theorem is useful in searching for cycles of the permutation, since an efficient search can look only at multiples of divisors of <it>MN</it>&amp;minus;1 (Brenner, 1973).</p>
<p>

Laflin &amp; Brebner (1970) pointed out that the cycles often come in pairs, which is exploited by several algorithms that permute pairs of cycles at a time.  In particular, let <it>s</it> be the smallest element of some cycle <it>C</it> of length <it>k</it>.  It follows that <it>MN</it>&amp;minus;1&amp;minus;<it>s</it> is also an element of a cycle of length <it>k</it> (possibly the same cycle).  Proof: by the definition of <it>P</it> above, the length <it>k</it> of the cycle containing <it>s</it> is the smallest <it>k</it> &amp;gt; 0 such that <math>s N^k = s \mod (MN - 1)</math>.  Clearly, this is the same as the smallest <it>k</it>&amp;gt;0 such that <math>(-s) N^k = -s \mod (MN - 1)</math>, since we are just multiplying both sides by &amp;minus;1, and <math>MN-1-s = -s \mod (MN - 1)</math>.</p>

</sec>
<sec>
<st>
Algorithms</st>

<p>

The following briefly summarizes the published algorithms to perform in-place matrix transposition.  <link xlink:type="simple" xlink:href="../661/27661.xml">
Source code</link> implementing some of these algorithms can be found in the references, below.</p>

<ss1>
<st>
Square matrices</st>

<p>

For a square <math>N \times N</math> matrix <math>A_{n,m} = A(n,m)</math>, in-place transposition is easy because all of the cycles have length 1 (the diagonals <math>A_{n,n}</math>) or length 2 (the upper triangle is swapped with the lower triangle.  <message wordnetid="106598915" confidence="0.8">
<subject wordnetid="106599788" confidence="0.8">
<link xlink:type="simple" xlink:href="../185/24185.xml">
Pseudocode</link></subject>
</message>
 to accomplish this (assuming zero-based <link xlink:type="simple" xlink:href="../052/2052.xml">
array</link> indices) is:</p>
<p>

<b>for</b> n = 0 to N - 2
<b>for</b> m = n + 1 to N - 1
swap A(n,m) with A(m,n)</p>
<p>

This type of implementation, while simple, can exhibit poor performance due to poor cache-line utilization, especially when <it>N</it> is a <link xlink:type="simple" xlink:href="../948/376948.xml">
power of two</link> (due to cache-line conflicts in a <link xlink:type="simple" xlink:href="../181/849181.xml">
CPU cache</link> with limited associativity).  The reason for this is that, as <it>m</it> is incremented in the inner loop, the memory address corresponding to <it>A</it>(<it>n</it>,<it>m</it>) or <it>A</it>(<it>m</it>,<it>n</it>) jumps discontiguously by <it>N</it> in memory (depending on whether the array is in column-major or row-major format, respectively).  That is, the algorithm does not exploit  the possibility of <link>
spatial locality</link>.</p>
<p>

One solution to improve the cache utilization is to "block" the algorithm to operate on several numbers at once, in blocks given by the cache-line size; unfortunately, this means that the algorithm depends on the size of the cache line (it is "cache-aware"), and on a modern computer with multiple levels of cache it requires multiple levels of machine-dependent blocking. Instead, it has been suggested (Frigo <it>et al.</it>, 1999) that better performance can be obtained by a <link xlink:type="simple" xlink:href="../407/25407.xml">
recursive</link> algorithm: divide the matrix into four submatrices of roughly equal size, transposing the two submatrices along the diagonal recursively and transposing and swapping the two submatrices above and below the diagonal.  (When <it>N</it> is sufficiently small, the simple algorithm above is used as a base case, as naively recursing all the way down to <it>N</it>=1 would have excessive function-call overhead.)  This is a <link xlink:type="simple" xlink:href="../377/1773377.xml">
cache-oblivious</link> algorithm, in the sense that it can exploit the cache line without the cache-line size being an explicit parameter.</p>

</ss1>
<ss1>
<st>
Following the cycles</st>

<p>

For non-square matrices, the algorithms are more complicated.  Many of the algorithms prior to 1980 could be described as "follow-the-cycles" algorithms.  That is, they loop over the cycles, moving the data from one location to the next in the cycle.  In pseudocode form:</p>
<p>

<b>for each</b> length&amp;gt;1 cycle <it>C</it> of the permutation
pick a starting address <it>s</it> in <it>C</it>
let <it>D</it> = data at <it>s</it>
let <it>x</it> = predecessor of <it>s</it> in the cycle
<b>while</b> <it>x</it> ≠ <it>s</it>
move data from <it>x</it> to successor of <it>x</it>
let <it>x</it> = predecessor of <it>x</it>
move data from <it>D</it> to successor of <it>s</it></p>
<p>

The differences between the algorithms lie mainly in how they locate the cycles, how they find the starting addresses in each cycle, and how they ensure that each cycle is moved exactly once.  Typically, as discussed above, the cycles are moved in pairs, since <it>s</it> and <it>MN</it>&amp;minus;1&amp;minus;<it>s</it> are in cycles of the same length (possibly the same cycle).  Sometimes, a small scratch array, typically of length <it>M</it>+<it>N</it> (e.g. Brenner, 1973; Cate &amp; Twigg, 1977) is used to keep track of a subset of locations in the array that have been visited, to accelerate the algorithm.</p>
<p>

In order to determine whether a given cycle has been moved already, the simplest scheme would be to use <it>O</it>(<it>MN</it>) auxiliary storage, one <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link> per element, to indicate whether a given element has been moved.  To use only <it>O</it>(<it>M</it>+<it>N</it>) or even <it>O</it>(log&nbsp;<it>MN</it>) auxiliary storage, more complicated algorithms are required, and the known algorithms have a worst-case <link xlink:type="simple" xlink:href="../768/239768.xml">
linearithmic</link> computational cost of <it>O</it>(<it>MN</it>&nbsp;log&nbsp;<it>MN</it>) at best, as first proved by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Knuth</link></scientist>
</person>
 (Fich <it>et al.</it>, 1995; Gustavson &amp; Swirszcz, 2007).</p>
<p>

Such algorithms are designed to move each data element exactly once.  However, they also involve a considerable amount of arithmetic to compute the cycles, and require heavily non-consecutive memory accesses since the adjacent elements of the cycles differ by multiplicative factors of <it>N</it>, as discussed above.</p>

</ss1>
<ss1>
<st>
Improving memory locality at the cost of greater total data movement</st>

<p>

Several algorithms have been designed to achieve greater memory locality at the cost of greater data movement, as well as slightly greater storage requirements.  That is, they may move each data element more than once, but they involve more consecutive memory access (greater spatial locality), which can improve performance on modern CPUs that rely on caches, as well as on <link xlink:type="simple" xlink:href="../359/55359.xml">
SIMD</link> architectures optimized for processing consecutive data blocks.  The oldest context in which the spatial locality of transposition seems to have been studied is for out-of-core operation (by Alltop, 1975), where the matrix is too large to fit into main memory ("core").</p>
<p>

For example, if <it>d</it> = gcd(<it>N</it>,<it>M</it>) is not small, one can perform the transposition using a small amount (<it>NM</it>/<it>d</it>) of additional storage, with at most three passes over the array (Alltop, 1975; Dow, 1995).  Two of the passes involve a sequence of separate, small transpositions (which can be performed efficiently out of place using a small buffer) and one involves an in-place <it>d</it>&amp;times;<it>d</it> square transposition of <math>NM/d^2</math> blocks (which is efficient since the blocks being moved are large and consecutive, and the cycles are of length at most 2).   For the case where |<it>N</it>&nbsp;&amp;minus;&nbsp;<it>M</it>| is small, Dow (1995) describes another algorithm requiring |<it>N</it>&nbsp;&amp;minus;&nbsp;<it>M</it>|⋅min(<it>N</it>,<it>M</it>) additional storage, involving a min(<it>N</it>,&nbsp;<it>M</it>) &amp;times; min(<it>N</it>,&nbsp;<it>M</it>) square transpose preceded or followed by a small out-of-place transpose.  Frigo &amp; Johnson (2005) describe the adaptation of these algorithms to use cache-oblivious techniques for general-purpose CPUs relying on cache lines to exploit spatial locality.</p>
<p>

Work on out-of-core matrix transposition, where the matrix does not fit in main memory and must be stored largely on a <link xlink:type="simple" xlink:href="../777/13777.xml">
hard disk</link>, has focused largely on the <it>N</it> = <it>M</it> square-matrix case, with some exceptions (e.g. Alltop, 1975).  Recent reviews of out-of-core algorithms, especially as applied to <link xlink:type="simple" xlink:href="../162/145162.xml">
parallel computing</link>, can be found in e.g. Suh &amp; Prasanna (2002) and Krishnamoorth et al. (2004).</p>

</ss1>
</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 P. F. Windley, "Transposing matrices in a digital computer," <it>Computer Journal</it> <b>2</b>, p. 47-48 (1959).</entry>
<entry level="1" type="bullet">

 G. Pall, and E. Seiden, "A problem in Abelian Groups, with application to the transposition of a matrix on an electronic computer," <it>Math. Comp.</it> <b>14</b>, p. 189-192 (1960).</entry>
<entry level="1" type="bullet">

 J. Boothroyd, "<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=363304&amp;dl=GUIDE&amp;coll=GUIDE&amp;CFID=436989&amp;CFTOKEN=18491885">
Algorithm 302: Transpose vector stored array</weblink>," <it>ACM Transactions on Mathematical Software</it> <b>10</b> (5), p. 292-293 (1967). </entry>
<entry level="1" type="bullet">

 Susan Laflin and M. A. Brebner, "<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=362368&amp;dl=GUIDE&amp;coll=GUIDE&amp;CFID=436989&amp;CFTOKEN=18491885">
Algorithm 380: in-situ transposition of a rectangular matrix</weblink>," <it>ACM Transactions on Mathematical Software</it> <b>13</b> (5), p. 324-326 (1970).  <weblink xlink:type="simple" xlink:href="http://www.netlib.org/toms/380">
Source code</weblink>.</entry>
<entry level="1" type="bullet">

 Norman Brenner, "<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=362542&amp;dl=GUIDE&amp;coll=GUIDE&amp;CFID=436989&amp;CFTOKEN=18491885">
Algorithm 467: matrix transposition in place</weblink>," <it>ACM Transactions on Mathematical Software</it> <b>16</b> (11), p. 692-694 (1973). <weblink xlink:type="simple" xlink:href="http://www.netlib.org/toms/467">
Source code</weblink>.</entry>
<entry level="1" type="bullet">

 W. O. Alltop, "A computer algorithm for transposing nonsquare matrices," <it>IEEE Trans. Comput.</it> <b>24</b> (10), p. 1038-1040 (1975).</entry>
<entry level="1" type="bullet">

 Esko G. Cate and David W. Twigg, "<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=355719.355729&amp;coll=GUIDE&amp;dl=GUIDE&amp;CFID=436989&amp;CFTOKEN=18491885">
Algorithm 513: Analysis of In-Situ Transposition</weblink>," <it>ACM Transactions on Mathematical Software</it> <b>3</b> (1), p. 104-110 (1977). <weblink xlink:type="simple" xlink:href="http://www.netlib.org/toms/513">
Source code</weblink>.</entry>
<entry level="1" type="bullet">

 Murray Dow, "Transposing a matrix on a vector computer," <it>Parallel Computing</it> <b>21</b> (12), p. 1997-2005 (1995).</entry>
<entry level="1" type="bullet">

 Donald E. Knuth, <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../358/31358.xml">
The Art of Computer Programming</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
 Volume 1: Fundamental Algorithms</it>, third edition, section 1.3.3 exercise 12 (Addison-Wesley: New York, 1997).</entry>
<entry level="1" type="bullet">

 M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran, "<weblink xlink:type="simple" xlink:href="http://supertech.lcs.mit.edu/cilk/papers/abstracts/abstract4.html">
Cache-oblivious algorithms</weblink>," in <it>Proceedings of the 40th IEEE Symposium on Foundations of Computer Science</it> (FOCS 99), p. 285-297 (1999). <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/iel5/6604/17631/00814600.pdf?arnumber=814600">
Extended abstract at IEEE</weblink>, <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/307799.html">
at Citeseer</weblink>.</entry>
<entry level="1" type="bullet">

 J. Suh and V. K. Prasanna, "<weblink xlink:type="simple" xlink:href="http://www.east.isi.edu/~jsuh/publ/tc_113280.pdf">
An efficient algorithm for out-of-core matrix transposition</weblink>," <it>IEEE Trans. Computers</it> <b>51</b> (4), p. 420-438 (2002).</entry>
<entry level="1" type="bullet">

 S. Krishnamoorthy, G. Baumgartner, D. Cociorva, C.-C. Lam, and P. Sadayappan, "<weblink xlink:type="simple" xlink:href="http://csc.lsu.edu/~gb/TCE//Publications/ParTranspose2.pdf">
Efficient parallel out-of-core matrix transposition</weblink>," <it>International Journal of High Performance Computing and Networking</it> <b>2</b> (2-4), p. 110-119 (2004).</entry>
<entry level="1" type="bullet">

 M. Frigo and S. G. Johnson, "<weblink xlink:type="simple" xlink:href="http://fftw.org/fftw-paper-ieee.pdf">
The Design and Implementation of FFTW3</weblink>," <it>Proceedings of the IEEE</it> <b>93</b> (2), 216–231 (2005). <weblink xlink:type="simple" xlink:href="http://www.fftw.org">
Source code</weblink> of the <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../334/2969334.xml">
FFTW</link></software>
 library, which includes optimized serial and <link xlink:type="simple" xlink:href="../162/145162.xml">
parallel</link> square and non-square transposes, in addition to <link xlink:type="simple" xlink:href="../512/11512.xml">
FFT</link>s.</entry>
<entry level="1" type="bullet">

 Faith E. Fich, J. Ian Munro, and Patricio V. Poblete, "Permuting in place," <it>SIAM Journal on Computing</it> <b>24</b> (2), p. 266-278 (1995).</entry>
<entry level="1" type="bullet">

 Fred G. Gustavson and Tadeusz Swirszcz, "In-place transposition of rectangular matrices," <it>Lecture Notes in Computer Science</it> <b>4699</b>, p. 560-569 (2007), from the Proceedings of the 2006 Workshop on State-of-the-Art [<it>sic</it>] in Scientific and Parallel Computing (PARA 2006) (Umeå, Sweden, June 2006).</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.research.att.com/~njas/sequences/A093055">
Sequence A093055</weblink>, number of non-singleton cycles for in-situ transpositions, <it><link xlink:type="simple" xlink:href="../004/500004.xml">
The On-Line Encyclopedia of Integer Sequences</link></it>.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.research.att.com/~njas/sequences/A093056">
Sequence A093056</weblink>, length of the longest cycle for in-situ transpositions, <it><link xlink:type="simple" xlink:href="../004/500004.xml">
The On-Line Encyclopedia of Integer Sequences</link></it>.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.research.att.com/~njas/sequences/A093057">
Sequence A093057</weblink>, number of fixed points &amp;minus; 2 for in-situ transpositions, <it><link xlink:type="simple" xlink:href="../004/500004.xml">
The On-Line Encyclopedia of Integer Sequences</link></it>.</entry>
</list>
</p>

</sec>
<sec>
<st>
External Links</st>

<ss1>
<st>
Source code</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://romo661.free.fr/offt.html">
OFFT</weblink> - recursive block in-place transpose of square matrices, in Fortran</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://groups.google.com/group/sci.math.num-analysis/msg/680211b3fbac30c4?hl=en">
Jason Stratos Papadopoulos</weblink>, blocked in-place transpose of square matrices, in <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
, <it>sci.math.num-analysis</it> newsgroup (April 7, 1998).</entry>
<entry level="1" type="bullet">

 See "Source code" links in the references section above, for additional code to perform in-place transposes of both square and non-square matrices.</entry>
</list>
</p>

</ss1>
</sec>
</bdy>
</article>
