<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:10:38[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Markov decision process</title>
<id>1125883</id>
<revision>
<id>243493283</id>
<timestamp>2008-10-06T19:25:46Z</timestamp>
<contributor>
<username>Entangle</username>
<id>7559964</id>
</contributor>
</revision>
<categories>
<category>Stochastic processes</category>
<category>Machine learning</category>
<category>Dynamic programming</category>
<category>Mathematical optimization</category>
</categories>
</header>
<bdy>

<b>Markov decision processes (MDPs)</b>, named after <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../915/915.xml">
Andrey Markov</link></scientist>
</person>
, provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of the decision maker.  MDPs are useful for studying a wide range of <link xlink:type="simple" xlink:href="../536/1126536.xml">
optimization problem</link>s solved via <link xlink:type="simple" xlink:href="../297/125297.xml">
dynamic programming</link> and <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link>. MDPs were known at least as early as the 1950s (cf. Bellman 1957). Much research in the area was spawned due to <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../101/1219101.xml">
Ronald A. Howard</link></research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
's book, <it>Dynamic Programming and Markov Processes</it>, in 1960.  Today they are used in a variety of areas, including robotics, automated control, economics and in manufacturing.<p>

More precisely a Markov Decision Process is a <link xlink:type="simple" xlink:href="../137/305137.xml">
discrete time</link> <link xlink:type="simple" xlink:href="../222/292222.xml">
stochastic</link> <link>
control</link> process characterized by a set of states; in each state there are several actions from which the decision maker must choose.  For a state <math>s</math> and an action <math>a</math>, a state transition function <math>P_a(s)</math> determines the transition probabilities to the next state.  The decision maker earns a reward for each state transition. The state transitions of an MDP possess the <it><link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link></it>: given the state of the MDP at time <math>t</math> is known, transition probabilities to the state at time <math>t+1</math> are independent of all previous states or actions.</p>
<p>

Markov decision processes are an extension of <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s; the difference is the addition of actions (allowing choice) and rewards (giving motivation). If there were only one action, or if the action to take were fixed for each state, a Markov decision process would reduce to a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
.</p>

<sec>
<st>
Definition</st>
<p>

A Markov Decision Process is a list of four objects <math>(S,A,P_a(\cdot,\cdot),R_a(\cdot,\cdot))</math>, where</p>
<p>

<list>
<entry level="1" type="bullet">

 <math>S</math> is the state space,</entry>
<entry level="1" type="bullet">

 <math>A</math> is the action space,</entry>
<entry level="1" type="bullet">

 <math>P_a(s,s') = \Pr(s_{t+1}=s' \mid s_t = s,\, a_t=a)</math> is the probability that action <math>a</math> in state <math>s</math> at time <math>t</math> will lead to state <math>s'</math> at time <math>t+1</math>,</entry>
<entry level="1" type="bullet">

<math>R_a(s,s')</math> is the immediate reward (or <link xlink:type="simple" xlink:href="../494/1567494.xml">
expected</link> immediate reward) received after transition to state <math>s'</math> from state <math>s</math> with transition probability <math>P_a(s,s')</math>.</entry>
</list>
</p>
<p>

The goal is to maximize some cumulative function of the rewards, typically the discounted sum over a potentially infinite horizon:</p>
<p>

<list>
<entry level="1" type="bullet">

<math>\sum^{\infty}_{t=0}\gamma^t R_{a_t}(s_t, s_{t+1})</math></entry>
</list>
</p>
<p>

where <math>\ \gamma \ </math> is the <link xlink:type="simple" xlink:href="../297/1047297.xml">
discount rate</link> and satisfies <math>0 &amp;lt; \gamma \le 1</math>. It is typically close to 1.</p>

</sec>
<sec>
<st>
Solution</st>

<p>

The solution to a Markov Decision Process can be expressed as a <it>policy</it> <math>\pi</math>, a function from states to actions. Note that once a Markov decision process is combined with a policy in this way, this fixes the action for each state and the resulting combination behaves like a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
.</p>
<p>

The standard family of algorithms to calculate the policy requires storage for two arrays indexed by state: <it>value</it> <math>V</math>, which contains real values, and <it>policy</it> <math>\pi</math> which contains actions. At the end of the algorithm, <math>\pi</math> will contain the solution and <math>V(s_0)</math> will contain the discounted sum of the rewards to be earned (on average) by following that solution.</p>
<p>

The algorithm then has the following two kinds of steps, which are repeated in some order for all the states until no further changes take place.</p>
<p>

<indent level="1">

<math>\ \pi(s) := \arg \max_a \sum_{s'} P_a(s,s') V(s')\ </math>
</indent>

<indent level="1">

<math>\ V(s) := R(s) + \gamma \sum_{s'} P_{\pi(s)}(s,s') V(s')\ </math>
</indent>

Their order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution.</p>

<ss1>
<st>
Notable variants</st>

<ss2>
<st>
Value iteration</st>
<p>

In value iteration (Bellman 1957), which is also called <link xlink:type="simple" xlink:href="../912/2060912.xml">
backward induction</link>,
the <math>\pi</math> array is not used; instead, the value of <math>\pi(s)</math> is calculated whenever it is needed.</p>
<p>

Substituting the calculation of <math>\pi(s)</math> into the calculation of <math>V(s)</math> gives the combined step:
<indent level="1">

<math>\ V(s) := R(s) + \gamma \max_a \sum_{s'} P_a(s,s') V(s').\ </math>
</indent>

</p>
</ss2>
<ss2>
<st>
Policy iteration</st>
<p>

In policy iteration (Howard 1960), step one is performed once, and then step two is repeated until it converges. Then step one is again performed once and so on.</p>
<p>

Instead of repeating step two to convergence, it may be formulated and solved as a set of linear equations.</p>
<p>

This variant has the advantage that there is a definite stopping condition: when the array <math>\pi</math> does not change in the course of applying step 1 to all states, the algorithm is completed.</p>

</ss2>
<ss2>
<st>
Modified policy iteration</st>
<p>

In modified policy iteration (Puterman and Shin 1978), step one is performed once, and then step two is repeated several times. Then step one is again performed once and so on.</p>

</ss2>
<ss2>
<st>
Prioritized sweeping</st>
<p>

In this variant, the steps are preferentially applied to states which are in some way important - whether based on the algorithm (there were large changes in <math>V</math> or <math>\pi</math> around those states recently) or based on use (those states are near the starting state, or otherwise of interest to the person or program using the algorithm).</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
Extensions</st>

<ss1>
<st>
Partial observability</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../552/3063552.xml">
partially observable Markov decision process</link></it>
</indent>
The solution above assumes that the state <math>s</math> is known when action is to be taken; otherwise <math>\pi(s)</math> cannot be calculated. When this assumption is not true, the problem is called a partially observable Markov decision process or POMDP.</p>

</ss1>
<ss1>
<st>
Learning</st>
<p>

If the probabilities are unknown, the problem is one of <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link>.</p>
<p>

For this purpose it is useful to define a further function, which corresponds to taking the action <math>a</math> and then continuing optimally (or according to whatever policy one currently has):
<indent level="1">

<math>\ Q(s,a) = R(s) + \gamma \sum_{s'} P_a(s,s') V(s').\ </math>
</indent>

While this function is also unknown, experience during learning is based on <math>(s, a)</math> pairs (together with the outcome <math>s'</math>); that is, "I was in state <math>s</math> and I tried doing <math>a</math> and <math>s'</math> happened)". Thus, one has an array <math>Q</math> and uses experience to update it directly. This is known as <link xlink:type="simple" xlink:href="../850/1281850.xml">
Q-learning</link>.</p>

</ss1>
<ss1>
<st>
Minor extensions</st>

<p>

These extensions are minor in that they complicate the notation, but make no real difference to the problem or its solution.</p>
<p>

<list>
<entry level="1" type="bullet">

 The reward may be a function of the action as well as the state, <math>R(s,a)</math>.</entry>
<entry level="1" type="bullet">

 The reward may be a function of the resulting state as well as the action and state, <math>R(s,a,s')</math>.</entry>
<entry level="1" type="bullet">

 The action space may be different at each state, so that it is <math>A_s</math> rather than <math>A</math>.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
Alternative notations</st>
<p>

The terminology and notation for MDPs are not entirely settled; there are two main streams — one using action, reward, value and <math>\gamma</math>, while the other uses control, cost, cost-to-go and <math>\alpha</math>. In addition, the notation for the transition probability varies.</p>
<p>

<table >
<header>
in this article</header>
<header>
alternative</header>
<header>
comment</header>
<row>
<col>
action <math>a</math></col>
<col>
control <math>u</math></col>

</row>
<row>
<col>
reward <math>R</math></col>
<col>
cost <math>g</math></col>
<col>
<math>g</math> is the negative of <math>R</math></col>
</row>
<row>
<col>
value <math>V</math></col>
<col>
cost-to-go <math>J</math></col>
<col>
<math>J</math> is the negative of <math>V</math></col>
</row>
<row>
<col>
policy <math>\pi</math></col>
<col>
policy <math>\mu</math></col>

</row>
<row>
<col>
discounting factor <math>\ \gamma \ </math></col>
<col>
discounting factor <math>\alpha</math></col>

</row>
<row>
<col>
transition probability <math>P_a(s,s')</math></col>
<col>
transition probability <math>p_{ss'}(a)</math></col>

</row>
</table>
</p>
<p>

In addition, transition probability is sometimes written <math>Pr(s,a,s')</math>, <math>Pr(s'|s,a)</math> or, rarely, <math>p_{s's}(a).</math></p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../552/3063552.xml">
Partially observable Markov decision process</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../297/125297.xml">
Dynamic programming</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../458/1236458.xml">
Bellman equation</link> for applications to economics.</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 R. Bellman. <it>A Markovian Decision Process</it>. Journal of Mathematics and Mechanics 6, 1957.</entry>
<entry level="1" type="bullet">

 R. E. Bellman. <it>Dynamic Programming</it>. Princeton University Press, Princeton, NJ, 1957. Dover paperback edition (2003), ISBN 0486428095.</entry>
<entry level="1" type="bullet">

 Ronald A. Howard <it>Dynamic Programming and Markov Processes</it>, The M.I.T. Press, 1960.</entry>
<entry level="1" type="bullet">

 M. L. Puterman. <it>Markov Decision Processes</it>. Wiley, 1994.</entry>
<entry level="1" type="bullet">

 H.C. Tijms. <it>A First Course in Stochastic Models</it>. Wiley, 2003.</entry>
<entry level="1" type="bullet">

 Sutton, R.S. <it>On the significance of Markov decision processes</it> . In W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud (Eds.) Artificial Neural Networks -- ICANN'97, pp. 273-282. Springer.</entry>
<entry level="1" type="bullet">

 Sutton, R. S. and Barto A. G. <it>Reinforcement Learning: An Introduction</it>. The MIT Press, Cambridge, MA, 1998.</entry>
<entry level="1" type="bullet">

 S. P. Meyn, 2007.  <weblink xlink:type="simple" xlink:href="http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html">
Control Techniques for Complex Networks</weblink>, Cambridge University Press, 2007. ISBN-13: 9780521884419. Appendix contains abridged <weblink xlink:type="simple" xlink:href="http://decision.csl.uiuc.edu/~meyn/pages/book.html">
Meyn &amp; Tweedie</weblink>.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ai.mit.edu/~murphyk/Software/MDP/mdp.html">
MDP Toolbox for Matlab</weblink> - An excellent tutorial and Matlab toolbox for working with MDPs.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~sutton/book/ebook">
Reinforcement Learning</weblink> An Introduction by Richard S. Sutton and Andrew G. Barto</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.computing.dundee.ac.uk/staff/jessehoey/spudd/index.html">
SPUDD</weblink> A structured MDP solver for download by Jesse Hoey</entry>
</list>




</p>

</sec>
</bdy>
</article>
