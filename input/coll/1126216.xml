<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:10:39[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Anticipation (artificial intelligence)</title>
<id>1126216</id>
<revision>
<id>233264943</id>
<timestamp>2008-08-21T05:42:58Z</timestamp>
<contributor>
<username>StigBot</username>
<id>5316354</id>
</contributor>
</revision>
<categories>
<category>Artificial intelligence</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link>, <b>anticipation</b> is the concept of an <link xlink:type="simple" xlink:href="../317/2711317.xml">
agent</link> making decisions based on predictions, expectations, or beliefs about the future. It is widely considered that anticipation is a vital component of complex natural <link xlink:type="simple" xlink:href="../238/106238.xml">
cognitive</link> systems.  As a branch of AI, anticipatory systems is a specialization still echoing the debates from the 1980s about the necessity for AI for an <link xlink:type="simple" xlink:href="../795/3224795.xml">
internal model</link>.
<sec>
<st>
Reaction, proaction and anticipation</st>

<p>

Elementary forms of artificial intelligence can be constructed using a policy based on simple if-then rules. An example of such a system would be an agent following the rules</p>
<p>

If it rains outside, 
take the umbrella. 
Otherwise 
leave the umbrella home</p>
<p>

A system such as the one defined above might be viewed as inherently <link xlink:type="simple" xlink:href="../376/5034376.xml">
reactive</link> because the decision making is based on the current state of the environment with no explicit regard to the future. An agent employing anticipation would try to predict the future state of the environment (weather in this case) and make use of the  predictions in the decision making. For example</p>
<p>

If the sky is cloudy and the air pressure is low, 
it will probably rain soon 
so take the umbrella with you. 
Otherwise 
leave the umbrella home.</p>
<p>

These rules appear more <link xlink:type="simple" xlink:href="../145/3038145.xml">
proactive</link>, because they explicitly take into account possible future events.  Notice though that in terms of <link xlink:type="simple" xlink:href="../920/16920.xml">
representation</link> and <link xlink:type="simple" xlink:href="../755/89755.xml">
reasoning</link>, these two rule sets are identical, both behave in response to existing conditions.  Note too that both systems assume the agent is proactively
<list>
<entry level="1" type="bullet">

 leaving the house, and</entry>
<entry level="1" type="bullet">

 trying to stay dry.</entry>
</list>
</p>
<p>

In practice, systems incorporating <link xlink:type="simple" xlink:href="../376/5034376.xml">
reactive planning</link> tend to be <link xlink:type="simple" xlink:href="../145/191145.xml">
autonomous</link> systems proactively pursuing at least one, and often many, goals.  What define anticipation in an AI model is the explicit existence of an inner model of the environment for the anticipatory system (sometimes including the system itself). For example, if the phrase <it>it will probably rain</it> were computed on line in real time, the system would be seen as anticipatory.  </p>
<p>

In the 1985, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../026/275026.xml">
Robert Rosen</link></scientist>
</causal_agent>
</biologist>
</person>
</physical_entity>
 defined an anticipatory system as follows <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>:</p>
<p>

A system containing a predictive model of itself and/or its
environment, which allows it to change state at an instant in accord
with the model's predictions pertaining to a latter instant.</p>
<p>

To some extent, this applies to any system incorporating <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>.  At issue is how much of a system's behaviour should or indeed can be determined by reasoning over dedicated representations, how much by on-line <link xlink:type="simple" xlink:href="../641/1505641.xml">
planning</link>, and how much must be provided by the system's designers.</p>

</sec>
<sec>
<st>
Anticipation in evolution and cognition</st>

<p>

The anticipation of future states is also a major evolutionary and cognitive advance (Sjolander 1995). 
Anticipatory agents belonging to Rosen's definition are closer to humans capabilities of taking decisions at a certain time T taking into account the effects of their own actions at different future timescales T+k. Machine learning methods started to integrate these capabilities in an implicit form as in <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link> systems (Sutton &amp; Barto, 1998; Balkenius, 1995<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>) where they learn to anticipate future rewards and punishments caused by current actions (Sutton &amp; Barto, 1998). Moreover
anticipation enhanced performance of machine learning techniques to face with complex environments where agents have to guide their attention to collect important information to act (Balkenius &amp; Hulth, 1999).</p>

</sec>
<sec>
<st>
From Anticipation to Curiosity</st>

<p>

Juergen Schmidhuber modifies error <link xlink:type="simple" xlink:href="../091/1360091.xml">
back propagation</link> algorithm to change neural network weights in order to decrease the mismatch between anticipated states and states actually experienced in the future (Schmidhuber - Adaptive curiosity and adaptive confidence, 1991). He introduces the concept of <it>curiosity</it> for agents as a measure of the mismatch between expectations and future experienced reality. Agents able to monitor and control their own curiosity explore situations where they expect to engage with novel experiences and are generally able to deal with complex environments more than the others.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../373/5033373.xml">
Action selection</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../238/106238.xml">
Cognition</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../376/5034376.xml">
Dynamic planning</link></entry>
<entry level="1" type="bullet">

 The <link xlink:type="simple" xlink:href="../560/2894560.xml">
History of artificial intelligence</link></entry>
<entry level="1" type="bullet">

 <work wordnetid="100575741" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../217/10477217.xml">
MindRACES</link></activity>
</psychological_feature>
</act>
</undertaking>
</event>
</work>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../807/39807.xml">
Nature-nurture</link></entry>
<entry level="1" type="bullet">

 The <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../999/2685999.xml">
Physical symbol system</link></instrumentality>
</artifact>
</system>
 hypothesis</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../357/586357.xml">
Strong AI</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
Anticipatory Systems, Robert Rosen, 1985, Pergamon Press</entry>
<entry id="2">
 <cite id="Reference-Balkenius-1995" style="font-style:normal" class="book">Balkenius, C.&#32;(1995). <weblink xlink:type="simple" xlink:href="http://www.lucs.lu.se/People/Christian.Balkenius/Thesis/">
Natural Intelligence in Artificial Creatures</weblink>.&#32;Lund University Cognitive Studies, 37. ISBN 91-628-1599-7.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
<sec>
<st>
Links</st>

<p>

<list>
<entry level="1" type="bullet">

 <it>MindRACES: From Reactive to Anticipatory Cognitive Embodied Systems</it>, http://www.mindraces.org, 2004</entry>
</list>

</p>
</sec>
</bdy>
</article>
