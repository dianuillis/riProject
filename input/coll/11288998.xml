<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 00:37:08[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<plant  confidence="0.8" wordnetid="100017222">
<tree  confidence="0.8" wordnetid="113104059">
<vascular_plant  confidence="0.8" wordnetid="113083586">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<woody_plant  confidence="0.8" wordnetid="113103136">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Alternating decision tree</title>
<id>11288998</id>
<revision>
<id>242536266</id>
<timestamp>2008-10-02T17:21:05Z</timestamp>
<contributor>
<username>SmackBot</username>
<id>433328</id>
</contributor>
</revision>
<categories>
<category>Knowledge discovery in databases</category>
<category>Decision trees</category>
<category>Machine learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

An Alternating Decision Tree (ADTree) is a <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> method
for classification.  The ADTree <link xlink:type="simple" xlink:href="../519/8519.xml">
data structure</link> and <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>
are a generalization of <link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link>s and have connections to
<link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link>.  ADTrees were introduced by <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../826/8000826.xml">
Yoav Freund</link></scientist>
 and <link>
Llew Mason</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.
<sec>
<st>
Motivation</st>

<p>

Original boosting algorithms typically combined either <link xlink:type="simple" xlink:href="../179/9903179.xml">
decision stump</link>s 
or <link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link>s.  Boosting <link xlink:type="simple" xlink:href="../179/9903179.xml">
decision stump</link>s creates
a set of <math>T</math> weighted weak hypotheses (where <math>T</math>
is the number of boosting iterations), which can be visualized as a
set for reasonable values of <math>T</math>.  Boosting decision trees
could result in a final combined classifier with thousands (or
millions) of nodes for modest values of <math>T</math>.  Both of these
scenarios produce final classifiers in which it is either difficult to
visualize correlations or difficult to visualize at all.
Alternating decision trees provide a method for visualizing decision
stumps in an ordered and logical way to demonstrate correlations.  In
doing so, they simultaneously generalize decision trees and can be
used to essentially grow boosted decision trees in parallel.</p>

</sec>
<sec>
<st>
Description of the structure</st>

<p>

The alternating decision tree structure consists of two components:
decision nodes and prediction nodes.  <b>Decision nodes</b> specify a
predicate condition.  <b>Prediction nodes</b> specify a value to add to
the <b>score</b> based on the result of the decision node.  Each
decision node can be seen as a conjunction between a
<b>precondition</b> (the decision node was reached) and the condition
specified in the decision node.</p>
<p>

Perhaps the easiest way to understand the interaction of decision and
prediction nodes is through an example.The following example is taken 
from <link>
JBoost</link> performing boosting for 6 iterations on the
<weblink xlink:type="simple" xlink:href="http://www.ics.uci.edu/~mlearn/databases/spambase/">
spambase dataset</weblink>
(available from the <weblink xlink:type="simple" xlink:href="http://www.ics.uci.edu/~mlearn/MLRepository.html">
UCI Machine Learning Repository</weblink>).
Positive examples indicate that the message is spam and negative
examples are not spam.  During each iteration, a single node is added to the ADTree.  
The ADTree determined by the learning algorithm implemented in <link>
JBoost</link> is:</p>
<p>

<image location="center" width="800px" src="spambase_adtree.png">
<caption>

An ADTree for 6 iterations on the
Spambase dataset.
</caption>
</image>
</p>
<p>

The tree construction algorithm is described below in the
<b>Description of the algorithm</b> section.  We now show how to
interpret the tree once it has been constructed.  We focus
on one specific instance:</p>
<p>

<table class="wikitable">
<caption>
An instance to be classified</caption>
<row>
<header>
Feature</header>
<header>
Value</header>
</row>
<row>
<col>
char_freq_bang</col>
<col>
0.08</col>
</row>
<row>
<col>
word_freq_hp</col>
<col>
0.4</col>
</row>
<row>
<col>
capital_run_length_longest</col>
<col>
4</col>
</row>
<row>
<col>
char_freq_dollar</col>
<col>
0</col>
</row>
<row>
<col>
word_freq_remove</col>
<col>
0.9</col>
</row>
<row>
<col>
word_freq_george</col>
<col>
0</col>
</row>
<row>
<col>
Other features</col>
<col>
...</col>
</row>
</table>
</p>
<p>

For this instance, we obtain a score that determines the
classification of the instance.  This score not only acts as a
classification, but also as a measure of confidence.  The actual order
that the ADTree nodes are evaluated will likely be different then the
order in which they were created.  That is, the node from iteration 4
can be evaluated before the node from iteration 1.  There are
constraints to this (e.g. node from iteration 2 must be evaluated
before the node from iteration 5).  In general, either breadth-first
or depth-first evaluation will yield the correct interpretation.</p>
<p>

The following table shows how the score is created (progressive
score) for our above example instance:</p>
<p>

<table class="wikitable">
<caption>
Score for the above instance</caption>
<row>
<header>
Iteration</header>
<col>
0</col>
<col>
1</col>
<col>
2</col>
<col>
3</col>
<col>
4</col>
<col>
5</col>
<col>
6</col>
</row>
<row>
<header>
Instance values</header>
<col>
N/A</col>
<col>
.08  .052 = n</col>
<col>
.4  .195 = n</col>
<col>
0  .01 = y</col>
<col>
0  0.005 = y</col>
<col>
N/A</col>
<col>
.9  .225 = n</col>
</row>
<row>
<header>
Prediction</header>
<col>
-0.093</col>
<col>
0.74</col>
<col>
-1.446</col>
<col>
-0.38</col>
<col>
0.176</col>
<col>
0</col>
<col>
1.66</col>
</row>
<row>
<header>
Progressive Score</header>
<col>
-0.093</col>
<col>
0.647</col>
<col>
-0.799</col>
<col>
-1.179</col>
<col>
-1.003</col>
<col>
-1.003</col>
<col>
0.657</col>
</row>
</table>
</p>
<p>

There are a few observations that we should make
<list>
<entry level="1" type="bullet">

 The final classification of the example is positive (0.657), meaning that the example is considered to be spam.</entry>
<entry level="1" type="bullet">

 All nodes at depth 1 have their predicate evaluated and one of their prediction nodes contributes to the score.  Thus a tree with depth 1 is the equivalent of boosted decision stumps.</entry>
<entry level="1" type="bullet">

 If a decision node is not reached (the node from iteration 5 in the above example) then the node's predicate and subsequent prediction nodes will not be evaluated.</entry>
</list>
</p>

</sec>
<sec>
<st>
Description of the algorithm</st>

<p>

The alternating decision tree learning algorithm is described in the
original paper<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.  The general idea involves a
few main concepts:
<list>
<entry level="1" type="bullet">

 The root decision node is always TRUE or FALSE</entry>
<entry level="1" type="bullet">

 The tree is grown iteratively.  The total number of iterations is generally decided prior to starting the algorithm.</entry>
<entry level="1" type="bullet">

 Each decision node (<math>c_2</math>) is selected by the algorithm based on how well it discriminates between positive and negative examples.  </entry>
<entry level="1" type="bullet">

 Once a decision node is created, the prediction node is determined by how well the decision node discriminates.</entry>
</list>
</p>
<p>

Before the algorithm, we first define some notation. Let <math>c</math> be a predicate, then 
<list>
<entry level="1" type="bullet">

 <math>W_+(c)</math> is the weight of all positively labeled examples that satisfy <math>c</math></entry>
<entry level="1" type="bullet">

 <math>W_-(c)</math> is the weight of all negatively labeled examples that satisfy <math>c</math></entry>
<entry level="1" type="bullet">

 <math>W(c)</math> is the weight of all  examples that satisfy <math>c</math></entry>
<entry level="1" type="bullet">

 We call <math>c</math> a precondition when it is a conjunction of previous base conditions and negations of previous base conditions</entry>
</list>
</p>
<p>

The exact algorithm is:</p>
<p>

INPUT: <math>m</math> examples and labels
<math>(x_1,y_1),\ldots,(x_m,y_m)</math></p>
<p>

Set the weight of all examples to <math>W_1(x_j) = 1/m</math></p>
<p>

Set the margin of all examples to <math>r_1(x_j) = 0</math></p>
<p>

The root decision node is always <math>c=</math>TRUE, with a single prediction node</p>
<p>

<math>a=\frac{1}{2}\textrm{ln}\frac{W_+(T)}{W_-(T)}</math></p>
<p>

For <math>t = 1, \ldots, T</math> do:
<list>
<entry level="1" type="bullet">

 Let <math>c_1 \in P_t</math> be a precondition (that is, the node being created can be reached via <math>c_1</math>) and <math>c_2</math> be a condition (the new node). Then each decision node (<math>c_2</math>) is selected by the algorithm based on how well it discriminates between positive and negative examples.  The original ADTree algorithm minimizes the criterion <math>2 \left(  \sqrt{W_+(c_1\wedge c_2) W_-(c_1 \wedge c_2)} + \sqrt{W_+(c_1\wedge \neg c_2) W_-(c_1 \wedge \neg c_2)} \right) +W(\neg c_2) </math>.</entry>
<entry level="1" type="bullet">

 Once a decision node is created, the prediction nodes are determined by <math>a=\frac{1}{2}\textrm{ln}\frac{W_+(c_1\wedge c_2)}{W_-(c_1 \wedge c_2)}</math>  and <math>b=\frac{1}{2}\textrm{ln}\frac{W_+(c_1\wedge \neg c_2)}{W_-(c_1 \wedge \neg c_2)}</math></entry>
<entry level="1" type="bullet">

 Add the conditions <math>c_1 \wedge c_2</math>  and  <math>c_1 \wedge \neg c_2</math> to the set of possible preconditions  <math>P_{t+1}</math> </entry>
<entry level="1" type="bullet">

 Update the weights: <math>W_{t+1}(x_j) = W_{t}(x_j)e^{r_t(x_j)y_j}</math></entry>
</list>
</p>

</sec>
<sec>
<st>
Empirical Results</st>

<p>

Figure 6 in the original paper<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> demonstrates that
ADTrees are typically as robust as boosted <link xlink:type="simple" xlink:href="../602/232602.xml">
decision trees</link> and boosted <link xlink:type="simple" xlink:href="../179/9903179.xml">
decision stump</link>s.</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
Yoav Freund and Llew Mason.  The
Alternating Decision Tree Algorithm.  Proceedings of the 16th
International Conference on Machine Learning, pages 124-133 (1999)
</entry>
</reflist>
</p>


</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.ucsd.edu/~aarvey/jboost/presentations/BoostingLightIntro.pdf">
An introduction to Boosting and ADTrees</weblink>  (Has many graphical examples of alternating decision trees in practice)</entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</article>
