<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:11:36[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Arnoldi iteration</title>
<id>1134614</id>
<revision>
<id>233506295</id>
<timestamp>2008-08-22T09:51:37Z</timestamp>
<contributor>
<username>Maias</username>
<id>1827467</id>
</contributor>
</revision>
<categories>
<category>Numerical linear algebra</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../506/21506.xml">
numerical</link> <link xlink:type="simple" xlink:href="../422/18422.xml">
linear algebra</link>, the <b>Arnoldi iteration</b> is an <link xlink:type="simple" xlink:href="../560/516560.xml">
eigenvalue algorithm</link> and an important example of <link xlink:type="simple" xlink:href="../237/15237.xml">
iterative method</link>s.  Arnoldi finds the <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalue</link>s of general (possibly non-<link xlink:type="simple" xlink:href="../682/189682.xml">
Hermitian</link>) <link xlink:type="simple" xlink:href="../728/19008728.xml">
matrices</link>; an analogous method for Hermitian matrices is the <link xlink:type="simple" xlink:href="../852/2593852.xml">
Lanczos iteration</link>.  The Arnoldi iteration was invented by <link>
W. E. Arnoldi</link> in 1951.<p>

The term <it>iterative method</it>, used to describe Arnoldi, can perhaps be somewhat confusing.  Note that all general eigenvalue algorithms must be iterative.  This is not what is referred to when we say Arnoldi is an iterative method.  Rather, Arnoldi belongs to a class of linear algebra <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>s (based on the idea of <link xlink:type="simple" xlink:href="../615/2542615.xml">
Krylov subspace</link>s) that give a partial result after a relatively small number of iterations.  This is in contrast to so-called <it>direct methods</it>, which must complete to give any useful results.</p>
<p>

Arnoldi iteration is a typical large sparse matrix algorithm: It does not access the elements of the matrix directly, but rather makes the matrix map vectors and makes its conclusions from their images. This is the motivation for building the <link xlink:type="simple" xlink:href="../615/2542615.xml">
Krylov subspace</link>.</p>

<sec>
<st>
Krylov subspaces and the power iteration</st>

<p>

An intuitive method for finding an eigenvalue (specifically the largest eigenvalue) of a given <it>m</it> &amp;times; <it>m</it> matrix <math>A</math> is the <link xlink:type="simple" xlink:href="../550/5975550.xml">
power iteration</link>.  Starting with an initial <link xlink:type="simple" xlink:href="../523/19196523.xml">
random</link> <link xlink:type="simple" xlink:href="../370/32370.xml">
vector</link> b, this method calculates <it>Ab</it>, <it>A</it>2<it>b</it>, <it>A</it>3<it>b</it>,&amp;hellip; iteratively storing and normalizing the result into b on every turn. This sequence converges to the <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link> corresponding to the largest eigenvalue, <math>\lambda_{1}</math>. However, much potentially useful computation is wasted by using only the final result, <math>A^{n-1}b</math>. This suggests that instead, we form the so-called <it>Krylov matrix</it>:
<indent level="1">

<math>K_{n} = \begin{bmatrix}b &amp; Ab &amp; A^{2}b &amp; \cdots &amp; A^{n-1}b \end{bmatrix}.</math>
</indent>

The columns of this matrix are not <link xlink:type="simple" xlink:href="../221/102221.xml">
orthogonal</link>, but in principle, we can extract an orthogonal <link xlink:type="simple" xlink:href="../420/18420.xml">
basis</link>, via a method such as <link>
Gram–Schmidt orthogonalization</link>.  The resulting vectors are a basis of the <it><link xlink:type="simple" xlink:href="../615/2542615.xml">
Krylov subspace</link></it>, <math>\mathcal{K}_{n}</math>.  We may expect the vectors of this basis to give good approximations of the eigenvectors corresponding to the <math>n</math> largest eigenvalues, for the same reason that <math>A^{n-1}b</math> approximates the dominant eigenvector.</p>

</sec>
<sec>
<st>
The Arnoldi iteration</st>

<p>

The process described above is intuitive.  Unfortunately, it is also <link xlink:type="simple" xlink:href="../807/233807.xml">
unstable</link>.  This is where the Arnoldi iteration enters.</p>
<p>

The Arnoldi iteration uses the stabilized <link>
Gram–Schmidt process</link> to produce a sequence of orthonormal vectors, <it>q</it>1, <it>q</it>2, <it>q</it>3, &amp;hellip;, called the <it>Arnoldi vectors</it>, such that for every <it>n</it>, the vectors <it>q</it>1, &amp;hellip;, <it>qn</it> span the Krylov subspace <math>\mathcal{K}_n</math>. Explicitly, the algorithm is as follows:</p>
<p>

<list>
<entry level="1" type="bullet">

 Start with an arbitrary vector <it>q</it>1 with norm 1.</entry>
<entry level="1" type="bullet">

 Repeat for <it>k</it> = 2, 3, &amp;hellip;</entry>
<entry level="2" type="bullet">

 <math> q_k \leftarrow Aq_{k-1} \,</math></entry>
<entry level="2" type="bullet">

 <b>for</b> <it>j</it> from 1 to <it>k</it> &amp;minus; 1</entry>
<entry level="3" type="bullet">

 <math> h_{j,k-1} \leftarrow q_j^* q_k \, </math></entry>
<entry level="3" type="bullet">

 <math> q_k \leftarrow q_k - h_{j,k-1} q_j \, </math></entry>
<entry level="2" type="bullet">

 <math> h_{k,k-1} \leftarrow \|q_k\| \, </math></entry>
<entry level="2" type="bullet">

 <math> q_k \leftarrow \frac{q_k}{h_{k,k-1}} \, </math></entry>
</list>
</p>
<p>

The <it>j</it>-loop projects out the component of <math>q_k</math> in the directions of <math>q_1,\dots,q_{k-1}</math>.  This ensures the orthogonality of all the generated vectors.</p>
<p>

The algorithm breaks down when <it>qk</it> is the zero vector. This happens when the <link xlink:type="simple" xlink:href="../678/254678.xml">
minimal polynomial</link> of <it>A</it> is of degree <it>k</it>. In most applications of the Arnoldi iteration, including the eigenvalue algorithm below and <link xlink:type="simple" xlink:href="../521/4329521.xml">
GMRES</link>, the algorithm has converged at this point.</p>
<p>

Every step of the <it>k</it>-loop takes one matrix-vector product and approximately 4<it>km</it> floating point operations.</p>

</sec>
<sec>
<st>
Properties of the Arnoldi iteration</st>

<p>

Let <it>Qn</it> denote the <it>m</it>-by-<it>n</it> matrix formed by the first <it>n</it> Arnoldi vectors <it>q</it>1, <it>q</it>2, &amp;hellip;, <it>qn</it>, and let <it>Hn</it> be the (upper <link xlink:type="simple" xlink:href="../204/519204.xml">
Hessenberg</link>) matrix formed by the numbers <it>hj</it>,<it>k</it> computed by the algorithm:
<indent level="1">

<math> H_n = \begin{bmatrix}
   h_{1,1} &amp; h_{1,2} &amp; h_{1,3} &amp; \cdots  &amp; h_{1,n} \\
   h_{2,1} &amp; h_{2,2} &amp; h_{2,3} &amp; \cdots  &amp; h_{2,n} \\
   0       &amp; h_{3,2} &amp; h_{3,3} &amp; \cdots  &amp; h_{3,n} \\
   \vdots  &amp; \ddots  &amp; \ddots  &amp; \ddots  &amp; \vdots  \\
   0       &amp; \cdots  &amp; 0     &amp; h_{n,n-1} &amp; h_{n,n} 
\end{bmatrix}. </math>
</indent>
We then have
<indent level="1">

<math> H_n = Q_n^* A Q_n. \, </math>
</indent>
This yields an alternative interpretation of the Arnoldi iteration as a (partial) orthogonal reduction of <it>A</it> to Hessenberg form. The matrix <it>Hn</it> can be viewed as the representation in the basis formed by the Arnoldi vectors of the orthogonal projection of <it>A</it> onto the Krylov subspace <math>\mathcal{K}_n</math>. </p>
<p>

The matrix <it>Hn</it> can be characterized by the following optimality condition. The <link xlink:type="simple" xlink:href="../268/218268.xml">
characteristic polynomial</link> of <it>Hn</it> minimizes ||<it>p</it>(<it>A</it>)<it>q</it>1||2 among all monic polynomials of degree <it>n</it> (the word <it>monic</it> means that the leading coefficient is 1). This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down.</p>
<p>

The relation between the <it>Q</it> matrices in subsequent iterations is given by
<indent level="1">

<math> A Q_n = Q_{n+1} \tilde{H}_n </math>
</indent>
where
<indent level="1">

<math> \tilde{H}_n = \begin{bmatrix}
   h_{1,1} &amp; h_{1,2} &amp; h_{1,3} &amp; \cdots  &amp; h_{1,n} \\
   h_{2,1} &amp; h_{2,2} &amp; h_{2,3} &amp; \cdots  &amp; h_{2,n} \\
   0       &amp; h_{3,2} &amp; h_{3,3} &amp; \cdots  &amp; h_{3,n} \\
   \vdots  &amp; \ddots  &amp; \ddots  &amp; \ddots  &amp; \vdots  \\
   \vdots  &amp;         &amp; 0       &amp; h_{n,n-1} &amp; h_{n,n} \\
   0       &amp; \cdots  &amp; \cdots  &amp; 0       &amp; h_{n+1,n} 
\end{bmatrix} </math>
</indent>
is an (<it>n</it>+1)-by-<it>n</it> matrix formed by adding an extra row to <it>Hn</it>.</p>

</sec>
<sec>
<st>
Finding eigenvalues with the Arnoldi iteration</st>

<p>

The idea of the Arnoldi iteration as an <link xlink:type="simple" xlink:href="../560/516560.xml">
eigenvalue algorithm</link> is to compute the eigenvalues of the orthogonal projection of <it>A</it> onto the Krylov subspace. This projection is represented by <it>Hn</it>. The eigenvalues of <it>Hn</it> are called the <it>Ritz eigenvalues</it>. Since <it>Hn</it> is a Hessenberg matrix of modest size, its eigenvalues can be computed efficiently, for instance with the <link xlink:type="simple" xlink:href="../072/594072.xml">
QR algorithm</link>.</p>
<p>

It is often observed in practice that some of the Ritz eigenvalues converge to eigenvalues of <it>A</it>. Since <it>Hn</it> is <it>n</it>-by-<it>n</it>, it has at most <it>n</it> eigenvalues, and not all eigenvalues of <it>A</it> can be approximated. Typically, the Ritz eigenvalues converge to the extreme eigenvalues of <it>A</it>. This can be related to the characterization of <it>Hn</it> as the matrix whose characteristic polynomial minimizes ||<it>p</it>(<it>A</it>)<it>q</it>1|| in the following way. A good way to get <it>p</it>(<it>A</it>) small is to choose the polynomial <it>p</it> such that <it>p</it>(<it>x</it>) is small whenever <it>x</it> is an eigenvalue of <it>A</it>. Hence, the zeros of <it>p</it> (and thus the Ritz eigenvalues) will be close to the eigenvalues of <it>A</it>.</p>
<p>

However, the details are not fully understood yet. This is in contrast to the case where <it>A</it> is <link xlink:type="simple" xlink:href="../474/126474.xml">
symmetric</link>. In that situation, the Arnoldi iteration becomes the <link xlink:type="simple" xlink:href="../852/2593852.xml">
Lanczos iteration</link>, for which the theory is more complete.</p>

</sec>
<sec>
<st>
Common variations</st>
<p>

Due to practical storage consideration, common implementations of Arnoldi methods typically restarts after some number of iterations. One major innovation in restarting was due to Lehoucq and Sorensen who proposed the Implicitly Restarted Arnoldi Method <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.  They also implemented the algorithm in a freely available software package called ARPACK<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.  This has spurred a number of other variations including Implicitly Restarted Lanczos method<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.  It also influenced how other restarted methods are analyzed <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>.</p>

</sec>
<sec>
<st>
See also</st>

<p>

The <link xlink:type="simple" xlink:href="../521/4329521.xml">
generalized minimal residual method</link> (GMRES) is a method for solving <it>Ax</it> = <it>b</it> based on Arnoldi iteration.</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 W. E. Arnoldi, "The principle of minimized iterations in the solution of the matrix eigenvalue problem," <it>Quarterly of  Applied Mathematics</it>, volume 9, pages 17–29, 1951.</entry>
<entry level="1" type="bullet">

 Yousef Saad, <it>Numerical Methods for Large Eigenvalue Problems</it>, Manchester University Press, 1992. ISBN 0-7190-3386-1.</entry>
<entry level="1" type="bullet">

 Lloyd N. Trefethen and David Bau, III, <it>Numerical Linear Algebra</it>, Society for Industrial and Applied Mathematics, 1997. ISBN 0-89871-361-7.</entry>
<entry level="1" type="bullet">

 Jaschke, Leonhard: <it>Preconditioned Arnoldi Methods for Systems of Nonlinear Equations</it>. <weblink xlink:type="simple" xlink:href="http://www.wiku-editions-paris.com">
| WiKu Editions Paris E.U.R.L.</weblink> (2004). ISBN 2-84976-001-3</entry>
</list>
</p>
<p>

<reflist>
<entry id="1">
R. B. Lehoucq and D. C. Sorensen&#32;(1996).&#32;"<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1137/S0895479895281484">
Deflation Techniques for an Implicitly Restarted Arnoldi  Iteration</weblink>".&#32;  SIAM.</entry>
<entry id="2">
R. B. Lehoucq, D. C. Sorensen, and C. Yang&#32;(1998).&#32;"<weblink xlink:type="simple" xlink:href="http://www.ec-securehost.com/SIAM/SE06.html">
ARPACK Users Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods</weblink>".&#32;  SIAM.</entry>
<entry id="3">
D. CALVETTI, L. REICHEL, AND D.C. SORENSEN&#32;(1994).&#32;"<weblink xlink:type="simple" xlink:href="http://etna.mcs.kent.edu/vol.2.1994/pp1-21.dir/pp1-21.ps">
An Implicitly Restarted Lanczos Method for Large Symmetric Eigenvalue Problems</weblink>".&#32;  ETNA.</entry>
<entry id="4">
E. Kokiopoulou, C. Bekas, and E. Gallopoulos&#32;(2003).&#32;"<weblink xlink:type="simple" xlink:href="http://www.siam.org/meetings/la03/proceedings/LA03proc.pdf">
An Implicitly Restarted Lanczos Bidiagonalization Method for Computing Smallest Singular Triplets</weblink>".&#32;  SIAM.</entry>
<entry id="5">
Zhongxiao Jia&#32;(2002).&#32;"<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/S0168-9274(01)00132-5">
The refined harmonic Arnoldi method and an implicitly restarted refined algorithm for computing interior eigenpairs of large matrices</weblink>".&#32;  Appl. Numer. Math..</entry>
<entry id="6">
Andreas Stathopoulos and Yousef Saad and Kesheng Wu&#32;(1998).&#32;"<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1137/S1064827596304162">
Dynamic Thick Restarting of the Davidson,  and the Implicitly Restarted Arnoldi Methods</weblink>".&#32;  SIAM.</entry>
</reflist>
</p>



</sec>
</bdy>
</article>
