<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 00:51:48[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>BrownBoost</title>
<id>11448016</id>
<revision>
<id>214234410</id>
<timestamp>2008-05-22T17:49:07Z</timestamp>
<contributor>
<username>AaronArvey</username>
<id>4110958</id>
</contributor>
</revision>
<categories>
<category>Ensemble learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

<b>BrownBoost</b> is a <link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link> algorithm that may be robust to noisy datasets.  BrownBoost is an adaptive version of the <link>
boost by majority</link> algorithm.  As is true for all <link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link> algorithms, BrownBoost is used in conjunction with other <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> methods.  BrownBoost was introduced by <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../826/8000826.xml">
Yoav Freund</link></scientist>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>
<sec>
<st>
Motivation</st>

<p>

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 performs  well on a variety of datasets; however, it can be shown that AdaBoost does not perform well on noisy data sets.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>  This is a result of AdaBoost's focus on examples that are repeatedly misclassified.  In contrast, BrownBoost effectively "gives up" on examples that are repeatedly misclassified.  The core assumption of BrownBoost is that noisy examples will be repeatedly mislabeled by the weak hypotheses and correctly non-noisy examples will be correctly labeled frequently enough to not be "given up on."      Thus only noisy examples will be "given up on," whereas non-noisy examples will form contribute to the final classifier.  In turn, if the final classifier is learned from the non-noisy examples, the <link xlink:type="simple" xlink:href="../249/2456249.xml">
generalization error</link> of the final classifier may be much better than if learned from noisy and non-noisy examples.</p>
<p>

The user of the algorithm can set the amount of error to be tolerated in the training set.  Thus, if the training set is noisy (say 10% of all examples are assumed to be mislabeled), the booster can be told to accept a 10% error rate.  Since the noisy examples may be ignored, only the true examples will contribute to the learning process.</p>

</sec>
<sec>
<st>
Algorithm Description</st>

<p>

BrownBoost uses a non-convex potential loss function, thus it does not fit into the <link>
AnyBoost</link> framework.  The non-convex optimization provides a method to avoid overfitting noisy data sets.  However, in contrast to boosting algorithms that analytically minimize a convex loss function (e.g. <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 and <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../465/17627465.xml">
LogitBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
), BrownBoost solves a system of two equations and two unknowns using standard numerical methods.</p>
<p>

The only parameter of BrownBoost (<math>c</math> in the algorithm) is the "time" the algorithm runs.  The theory of BrownBoost states that each hypothesis takes a variable amount of time (<math>t</math> in the algorithm) which is directly related to the weight given to the hypothesis <math>\alpha</math>.  The time parameter in BrownBoost is analogous to the number of iterations <math>T</math> in AdaBoost.  </p>
<p>

A larger value of <math>c</math> means that BrownBoost will treat the data as if it were less noisy and therefore will give up on fewer examples.  Conversely, a smaller value of <math>c</math> means that <math>c</math> BrownBoost will treat the data as more noisy and give up on more examples.</p>
<p>

During each iteration of the algorithm, a hypothesis is selected with some advantage over random guessing.  The weight of this hypothesis <math>\alpha</math> and the "amount of time passed" <math>t</math> during the iteration are simultaneously solved in a system of two non-linear equations ( 1. uncorrelate hypothesis w.r.t example weights and 2. hold the potential constant) with two unknowns (weight of hypothesis <math>\alpha</math> and time passed <math>t</math>).  This can be solved by bisection (as implemented in the <link>
JBoost</link> software package) or <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../145/22145.xml">
Newton's method</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (as described in the original paper by Freund).  Once these equations are solved, the margins of each example (<math>r_i(x_j)</math> in the algorithm) and the amount of time remaining <math>s</math> are updated appropriately.  This process is repeated until there is no time remaining.</p>
<p>

The initial potential is defined to be <math>\frac{1}{m}\sum_{j=1}^m 1-erf(\sqrt{c}) = 1-erf(\sqrt{c})</math>.  Since a constraint of each iteration is that the potential be held constant, the final potential is <math>\frac{1}{m}\sum_{j=1}^m 1-erf(r_i(x_j)/\sqrt{c}) = 1-erf(\sqrt{c})</math>.  Thus the final error is <it>likely</it> to be near <math>1-erf(\sqrt(c))</math>.  However, the final potential function is not the 0-1 loss error function.  For the final error to be exactly  <math>1-erf(\sqrt(c))</math>, the variance of the loss function must decrease linearly w.r.t. time to form the 0-1 loss function at the end of boosting iterations.  This is not yet discussed in the literature and is not in the definition of the algorithm below.</p>
<p>

The final classifier is a linear combination of weak hypotheses and is evaluated in the same manner as most other boosting algorithms.</p>

</sec>
<sec>
<st>
BrownBoost Learning Algorithm Definition</st>

<p>

Input: 
<list>
<entry level="1" type="bullet">

 <math>m</math> training examples <math>(x_{1},y_{1}),\ldots,(x_{m},y_{m})</math> where <math>x_{j} \in X,\, y_{j} \in Y = \{-1, +1\}</math></entry>
<entry level="1" type="bullet">

 The parameter <math>c</math></entry>
</list>
</p>
<p>

Initialise: 
<list>
<entry level="1" type="bullet">

 <math>s=c</math>.  The value of <math>s</math> is the amount of time remaining in the game)</entry>
<entry level="1" type="bullet">

 <math>r_1(x_j) = 0</math> &nbsp; <math>\forall j</math>.  The value of <math>r_1(x_j)</math> is the margin at iteration <math>i</math> for example <math>x_j</math>.</entry>
</list>
</p>
<p>

While <math>s &amp;gt; 0</math>:
<list>
<entry level="1" type="bullet">

 Set the weights of each example: <math>W_{i}(x_j) = e^{- \frac{(r_i(x_j)+s)^2}{c}}</math>, where <math>r_i(x_j)</math> is the margin of example <math>x_j</math></entry>
<entry level="1" type="bullet">

 Find a classifier <math>h_i : X \to \{-1,+1\}</math> such that <math>\sum_j W_i(x_j) h_i(x_j) y_j &amp;gt; 0</math></entry>
<entry level="1" type="bullet">

 Find values <math>\alpha, t</math> that satisfy the equation:  <math>\sum_j h_i(x_j) y_j e^{-\frac{(r_i(x_j)+\alpha h_i(x_j) y_j + s - t)^2}{c}} = 0</math>.  (Note this is similar to the condition <math>E_{W_{i+1}}[h_i(x_j) y_j]=0</math> set forth by Schapire and Singer<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.  In this setting, we are numerically finding the <math>W_{i+1} = exp(\frac{\ldots}{\ldots})</math> such that <math>E_{W_{i+1}}[h_i(x_j) y_j]=0</math>.)  This update is subject to the constraint          <math> \sum \left(\Phi\left(r_i(x_j) + \alpha h(x_j) y_j + s - t\right)   -    \Phi\left( r_i(x_j) + s  \right)  \right) = 0 </math>,         where        <math>     \Phi(z) = 1-erf(z/\sqrt{c}) </math> is the potential loss for a point with margin <math>r_i(x_j)</math></entry>
<entry level="1" type="bullet">

 Update the margins for each example: <math>r_{i+1}(x_j) = r_i(x_j) + \alpha h(x_j) y_j</math></entry>
<entry level="1" type="bullet">

 Update the time remaining: <math>s = s - t</math></entry>
</list>
</p>
<p>

Output: <math>H(x) = \textrm{sign}\left( \sum_i \alpha_{i} h_{i}(x) \right)</math></p>

</sec>
<sec>
<st>
Empirical Results</st>

<p>

In preliminary experimental results with noisy datasets, BrownBoost outperformed AdaBoost's generalization error; however, LogitBoost performed as well as BrownBoost.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>  An implementation of BrownBoost can be found in the open source software <link>
JBoost</link>.</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
Yoav Freund. An adaptive version of the boost by majority algorithm. Machine Learning, 43(3):293--318, June 2001. </entry>
<entry id="2">
Dietterich, T. G., (2000). An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization.  Machine Learning, 40 (2) 139-158.</entry>
<entry id="3">
Robert Schapire and Yoram Singer.  Improved Boosting Using Confidence-rated Predictions. Journal of Machine Learning, Vol 37(3), pages 297-336. 1999</entry>
<entry id="4">
Ross A. McDonald, David J. Hand, Idris A. Eckley.  An Empirical Comparison of Three Boosting Algorithms on Real Data Sets with Artificial Class Noise.   Multiple Classifier Systems, In Series Lecture Notes in Computer Science, pages 35-44, 2003.</entry>
</reflist>
</p>


</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../500/90500.xml">
Boosting</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../998/11288998.xml">
Alternating decision tree</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
s</entry>
<entry level="1" type="bullet">

 <link>
JBoost</link></entry>
</list>
</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
