<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 00:46:29[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Factorial code</title>
<id>11612350</id>
<revision>
<id>220128343</id>
<timestamp>2008-06-18T12:15:33Z</timestamp>
<contributor>
<username>Melcombe</username>
<id>4682566</id>
</contributor>
</revision>
<categories>
<category>Signal processing</category>
<category>Multivariate statistics</category>
</categories>
</header>
<bdy>

Most real world data sets consist of data vectors whose individual components are not <link xlink:type="simple" xlink:href="../593/27593.xml">
statistically independent</link>, that is, they are <link xlink:type="simple" xlink:href="../633/40633.xml">
redundant</link> in the <link xlink:type="simple" xlink:href="../685/26685.xml">
statistical</link> sense. Then it is desirable to create a <b><link xlink:type="simple" xlink:href="../606/10606.xml">
factorial</link> code</b> of the data, i. e., a new vector-valued <link xlink:type="simple" xlink:href="../211/348211.xml">
representation</link> of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent.<p>

Later <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link> usually works much better when the raw input data is first translated into such a factorial code. For example, suppose the final goal is to <link xlink:type="simple" xlink:href="../426/232426.xml">
classify</link> images with highly redundant pixels. A <link xlink:type="simple" xlink:href="../339/87339.xml">
naive Bayes classifier</link> will assume the pixels are <link xlink:type="simple" xlink:href="../593/27593.xml">
statistically independent</link> <link xlink:type="simple" xlink:href="../685/25685.xml">
random variables</link> and therefore fail to produce good results. If the data are first encoded in a factorial way, however, then the naive Bayes classifier will achieve its <link xlink:type="simple" xlink:href="../285/914285.xml">
optimal</link> performance (compare Schmidhuber et al. 1996).</p>
<p>

To create factorial codes, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<neurobiologist wordnetid="110353928" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<neuroscientist wordnetid="110354580" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../440/842440.xml">
Horace Barlow</link></associate>
</neuroscientist>
</scientist>
</causal_agent>
</colleague>
</neurobiologist>
</biologist>
</person>
</peer>
</physical_entity>
 and co-workers suggested to minimize the sum of the <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link> entropies of the code components of <link xlink:type="simple" xlink:href="../686/238686.xml">
binary</link> codes (1989). <link>
Jürgen Schmidhuber</link> (1992) re-formulated the problem in terms of predictors and  binary  <link xlink:type="simple" xlink:href="../752/187752.xml">
feature</link> <link xlink:type="simple" xlink:href="../757/235757.xml">
detectors</link>, each receiving the raw data as an input.  For each detector there is a predictor that sees the other detectors and learns to predict the output of its own detector in response to the various input vectors or images. But each detector uses a <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> algorithm to become as unpredictable as possible. The <link xlink:type="simple" xlink:href="../285/914285.xml">
global optimum</link> of this <link xlink:type="simple" xlink:href="../033/52033.xml">
objective function</link> corresponds to a factorial code represented in a <link xlink:type="simple" xlink:href="../333/1967333.xml">
distributed</link> fashion across the outputs of the feature detectors.</p>

<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../893/431893.xml">
Blind signal separation (BSS)</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../340/76340.xml">
Principal component analysis (PCA)</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../492/253492.xml">
Factor analysis</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../497/233497.xml">
Unsupervised learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../332/76332.xml">
Image processing</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../324/29324.xml">
Signal processing</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>


<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<neurobiologist wordnetid="110353928" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<neuroscientist wordnetid="110354580" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../440/842440.xml">
Horace Barlow</link></associate>
</neuroscientist>
</scientist>
</causal_agent>
</colleague>
</neurobiologist>
</biologist>
</person>
</peer>
</physical_entity>
, T. P. Kaushal, and G. J. Mitchison. Finding minimum entropy codes.  Neural Computation, 1:412-423, 1989.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <link>
Jürgen Schmidhuber</link>. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863-879, 1992</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 J. Schmidhuber and M. Eldracher and B. Foltin. Semilinear predictability minimzation produces well-known feature detectors. Neural Computation, 8(4):773-786, 1996</entry>
</list>
</p>



</sec>
</bdy>
</article>
