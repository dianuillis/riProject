<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:14:39[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<algorithm  confidence="0.9511911446218017" wordnetid="105847438">
<header>
<title>Gauss–Newton algorithm</title>
<id>1164753</id>
<revision>
<id>241852745</id>
<timestamp>2008-09-29T20:34:08Z</timestamp>
<contributor>
<username>Rjwilmsi</username>
<id>203434</id>
</contributor>
</revision>
<categories>
<category>Optimization algorithms</category>
</categories>
</header>
<bdy>

The <b>Gauss–Newton algorithm</b> is a method used to solve <link xlink:type="simple" xlink:href="../764/15652764.xml">
non-linear least squares</link> problems. It can be seen as a modification of <link xlink:type="simple" xlink:href="../523/1244523.xml">
Newton's method</link> for finding a <link xlink:type="simple" xlink:href="../420/298420.xml">
minimum</link> of a <link xlink:type="simple" xlink:href="../427/185427.xml">
function</link>. Unlike Newton's method, the Gauss–Newton algorithm can <it>only</it> be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required. <p>

Non-linear least squares problems arise for instance in <link xlink:type="simple" xlink:href="../012/1045012.xml">
non-linear regression</link>, where parameters in a model are sought such that the model is in good agreement with available observations.</p>
<p>

The method is due to the renowned mathematician <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../125/6125.xml">
Carl Friedrich Gauss</link></scientist>
</person>
.</p>

<sec>
<st>
 The algorithm </st>
<p>

Given <it>m</it> functions <math>r_i</math> (<math>i=1,\ldots,m</math>) of <it>n</it> variables <math>{\boldsymbol \beta}=(\beta_1, \beta_2, \dots, \beta_n),</math> with <it>m</it>≥<it>n</it>, the Gauss–Newton algorithm finds the minimum of the sum of squares</p>
<p>

<indent level="1">

<math> S(\boldsymbol \beta)= \sum_{i=1}^m r_i^2(\boldsymbol \beta).</math> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> 
</indent>

Starting with an initial guess <math>\boldsymbol \beta^0</math> for the minimum, the method proceeds by the <link xlink:type="simple" xlink:href="../237/15237.xml">
iterations</link></p>
<p>

<indent level="1">

<math> \boldsymbol \beta^{s+1} = \boldsymbol
\beta^s+\delta\boldsymbol\beta,</math> 
</indent>

with the increment <math>\delta\boldsymbol\beta</math> satisfying the <b>normal equations</b></p>
<p>

<indent level="1">

<math>\left(\mathbf{J_r^T J_r} \right)\delta\boldsymbol\beta= -
\mathbf{ J_r^T r}. </math>
</indent>

Here, <b>r</b> is the vector of functions <it>ri</it>, and <b>J</b>r<b> is the <it>m</it></b><b>&amp;times;<it>n</it></b><b> <link xlink:type="simple" xlink:href="../351/195351.xml">
Jacobian matrix</link> of </b>r<b> with respect to </b>β<b>, both evaluated at </b>β<b><it>s</it></b><b>. The superscript </b>T<b> denotes the matrix <link xlink:type="simple" xlink:href="../844/173844.xml">
transpose</link>.</b></p>
<p>

In data fitting, where the goal is to find the parameters
<math>\boldsymbol \beta</math> such that  a given model function
<math>y=f(x, \boldsymbol \beta)</math> fits best some data points <math>(x_i, y_i),</math> the functions <it>ri</it> are the <link xlink:type="simple" xlink:href="../509/461509.xml">
residuals</link></p>
<p>

<indent level="1">

 <math>r_i(\boldsymbol \beta)= y_i - f(x_i, \boldsymbol \beta).</math>
</indent>

Then, the increment <math>\delta\boldsymbol\beta</math> can be expressed in terms of the Jacobian of the
function <it>f</it>, as</p>
<p>

<indent level="1">

 <math>\left( \mathbf{ J_f^T  J_f} \right)\delta\boldsymbol\beta= {\mathbf{J_f^T r}}. </math>
</indent>

</p>
</sec>
<sec>
<st>
Notes</st>

<p>

The assumption <it>m</it>≥<it>n</it> in the algorithm statement is necessary, as otherwise the matrix <math>\mathbf{J_r^T  J_r}</math> is not invertible and the normal equations cannot be solved.</p>
<p>

The Gauss–Newton algorithm can be derived by <link xlink:type="simple" xlink:href="../596/1107596.xml">
linearly approximating</link> the vector of functions <math>r_i.</math> Using <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../714/51714.xml">
Taylor's theorem</link></proposition>
</theorem>
</message>
</statement>
, we can write at every iteration</p>
<p>

<indent level="1">

 <math>\mathbf{r}(\boldsymbol \beta)\approx \mathbf{r}(\boldsymbol \beta^s)+\mathbf{J_r}(\boldsymbol \beta^s)\delta\boldsymbol\beta </math>
</indent>

with <math>\delta\boldsymbol \beta=\boldsymbol \beta-\boldsymbol \beta^s.</math> The task of finding <math>\delta\boldsymbol \beta</math> minimizing the sum of squares of the right-hand side is a <link xlink:type="simple" xlink:href="../872/484872.xml">
linear least squares</link> problem, which can be solved explicitly, yielding the normal equations in the algorithm.</p>
<p>

The normal equations are <it>m</it> linear simultaneous equations in the unknown increments, <math> \delta \boldsymbol\beta</math>. They may be solved in one step, using <link xlink:type="simple" xlink:href="../872/484872.xml#xpointer(//*[./st=%22computation%22])">
Choleski factorization</link>, or, better, the <link xlink:type="simple" xlink:href="../223/305223.xml">
QR factorization</link> of <b>J</b>r<b>. For large systems, an <link xlink:type="simple" xlink:href="../237/15237.xml">
iterative method</link>, such as the <link xlink:type="simple" xlink:href="../821/1448821.xml">
conjugate gradient</link> method, may be more efficient. If there is a linear dependence between columns of </b>J<b>r</b>, the iterations will fail as <math>\mathbf{J_r^T  J_r}</math> becomes singular.</p>

</sec>
<sec>
<st>
Example</st>
<p>

<image location="right" width="280px" src="Gauss_Newton_illustration.png" type="thumb">
<caption>

Calculated curve obtained with <math>\hat\beta_1=0.362</math> and <math>\hat\beta_2=0.556</math> (in blue) versus the observed data (in red).
</caption>
</image>

In this example, the Gauss–Newton algorithm will be used to fit a model to some data by minimizing the sum of squares of errors between the data and model's predictions. </p>
<p>

In a biology experiment studying the relation between substrate concentration [''S''] and reaction rate in an enzyme-mediated reaction, the data in the following table were obtained. 
<indent level="1">

|class="wikitable" style="text-align: center;" 
</indent>
|<it>i</it> || 1 || 2 || 3 || 4 || 5 || 6 || 7 
|-
| [S] || 0.038 || 0.194 || 0.425 || 0.626 || 1.253 ||  2.500 || 3.740
|-
| rate || 0.050 || 0.127 || 0.094  ||  0.2122 ||   0.2729 ||   0.2665 || 0.3317
|}</p>
<p>

It is desired to find a curve (model function) of the form </p>
<p>

<indent level="1">

<math>\text{rate}=\frac{V_{max}[S]}{K_M+[S]}</math>
</indent>

that fits best the data in the least squares sense, with the parameters <math>V_{max}</math> and <math>K_M</math> to be determined. </p>
<p>

Denote by <math>x_i</math> and <math>y_i</math> the value of <math>[S]</math> and the rate from the table, <math>i=1, \dots, 7.</math> Let <math>\beta_1=V_{max}</math> and <math>\beta_2=K_M.</math> We will find <math>\beta_1</math> and <math>\beta_2</math> such that the sum of squares of the residuals
<indent level="1">

 <math>r_i = y_i - \frac{\beta_1x_i}{\beta_2+x_i}</math>  &nbsp; (<math>i=1,\dots, 7</math>)
</indent>
is minimized.</p>
<p>

The Jacobian <math>\mathbf{J_r}</math> of the vector of residuals <math>r_i</math> in respect to the unknowns <math>\beta_j</math> is an <math>7\times 2</math> matrix with the <math>i</math>-th row having the entries</p>
<p>

<indent level="1">

<math>\frac{\partial r_i}{\partial \beta_1}= -\frac{x_i}{\beta_2+x_i},\  \frac{\partial r_i}{\partial \beta_2}= \frac{\beta_1x_i}{\left(\beta_2+x_i\right)^2}.</math>
</indent>

Starting with the initial estimates of <math>\beta_1</math>=0.9 and <math>\beta_2</math>=0.2, after five iterations of the Gauss–Newton algorithm the optimal values <math>\hat\beta_1=0.362</math> and <math>\hat\beta_2=0.556</math> are obtained. The sum of squares of residuals decreased from the initial value of 1.445 to 0.00784 after the fifth iteration. The plot in the figure on the right shows the curve determined by the model for the optimal parameters versus the observed data.</p>

</sec>
<sec>
<st>
Convergence properties</st>

<p>

It can be shown that the increment <math>\delta\beta</math> is a <link xlink:type="simple" xlink:href="../992/1537992.xml">
descent direction</link> for <it>S</it> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>, and, if the algorithm converges, then the limit is a <link xlink:type="simple" xlink:href="../651/885651.xml">
stationary point</link> of <it>S</it>. However, convergence is not guaranteed, not even <link xlink:type="simple" xlink:href="../093/16344093.xml">
local convergence</link> as in <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/1244523.xml">
Newton's method</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
. </p>
<p>

The rate of convergence of the Gauss–Newton algorithm can approach <link xlink:type="simple" xlink:href="../701/999701.xml">
quadratic</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> The algorithm may converge slowly or not at all  if the initial guess is far from the minimum or  the matrix <math>\mathbf{J_r^T  J_r}</math> is <link xlink:type="simple" xlink:href="../934/6934.xml">
ill-conditioned</link>.</p>
<p>

The Gauss–Newton algorithm may fail to converge. For example, consider the problem with <math>m=2</math> equations and <math>n=1</math> variable, given by
<indent level="1">

<math> \begin{align}
r_1(\beta) &amp;= \beta + 1 \\
r_2(\beta) &amp;= \lambda \beta^2 + \beta - 1.
\end{align} </math>
</indent>
The optimum is at <math>\beta = 0</math>. If <math>\lambda = 0</math> then the problem is in fact linear and the method finds the optimum in one iteration. If |λ|  1 then the method converges linearly and the error decreases asymptotically with a factor |λ| at every iteration. However, if |λ| &amp;gt; 1, then the method does not even converge locally.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></p>

</sec>
<sec>
<st>
 Derivation from Newton's method </st>

<p>

In what follows, the Gauss–Newton algorithm will be derived from <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/1244523.xml">
Newton's method</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 for function optimization via an approximation. As a consequence, the rate of convergence of the Gauss–Newton algorithm is at most quadratic.  </p>
<p>

The recurrence relation for Newton's method for minimizing a function <it>S</it> of parameters, <math>\boldsymbol\beta</math>, is
<indent level="1">

<math> \boldsymbol\beta^{s+1} = \boldsymbol\beta^s - \mathbf H^{-1} \mathbf g \, </math>
</indent>
where <b>g</b> denotes the <link xlink:type="simple" xlink:href="../461/12461.xml">
gradient vector</link> of <it>S</it> and <b>H</b> denotes the <link xlink:type="simple" xlink:href="../108/412108.xml">
Hessian matrix</link> of <it>S</it>.
Since <math> S = \sum_{i=1}^m r_i^2</math>, the gradient is given by
<indent level="1">

<math>g_j=2\sum_{i=1}^m r_i\frac{\partial r_i}{\partial \beta_j}.</math>
</indent>
Elements of the Hessian are calculated by differentiating the gradient elements, <math>g_j</math>, with respect to <math>\beta_k</math>
<indent level="1">

<math>H_{jk}=2\sum_{i=1}^m \left(\frac{\partial r_i}{\partial \beta_j}\frac{\partial r_i}{\partial \beta_k}+r_i\frac{\partial^2 r_i}{\partial \beta_j \partial \beta_k} \right).</math>
</indent>

The Gauss–Newton method is obtained by ignoring the second-order derivative terms (the second term in this expression). That is, the Hessian is approximated by
<indent level="1">

<math>H_{jk}\approx 2\sum_{i=1}^m J_{ij}J_{ik}</math>
</indent>
where <math>J_{ij}=\frac{\partial r_i}{\partial \beta_j}</math> are entries of the Jacobian <math>\mathbf{J_r}</math>. The gradient and the approximate Hessian can be written in matrix notation as
<indent level="1">

<math>\mathbf g=2\mathbf{J_r^Tr, \quad H \approx 2J_r^TJ_r}.\,</math>
</indent>
These expressions are substituted into the recurrence relation above to obtain the operational equations
<indent level="1">

<math> \boldsymbol{\beta}^{s+1} = \boldsymbol\beta^s+\delta\boldsymbol\beta;\ \delta\boldsymbol\beta= - \mathbf{\left( J_r^T J_r \right)^{-1} J_r^T r}. </math>
</indent>

Convergence of the Gauss–Newton method is not guaranteed in all instances. The approximation
<indent level="1">

<math>\left|r_i\frac{\partial^2 r_i}{\partial \beta_j \partial \beta_k}\right| \ll \left|\frac{\partial r_i}{\partial \beta_j}\frac{\partial r_i}{\partial \beta_k}\right|</math>
</indent>
that needs to hold to be able to ignore the second-order derivative terms may be valid in two cases, for which convergence is to be expected.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>
<list>
<entry level="1" type="number">

The function values <math>r_i</math> are small in magnitude, at least around the minimum. </entry>
<entry level="1" type="number">

The functions are only "mildly" non linear, so that <math>\frac{\partial^2 r_i}{\partial \beta_j \partial \beta_k}</math> is relatively small in magnitude.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Improved versions </st>

<p>

With the Gauss–Newton method the sum of squares <it>S</it> may not decrease at every iteration. However, since <math>\delta\boldsymbol\beta</math> is a descent direction, unless <math>S(\boldsymbol \beta^s)</math> is a stationary point, it holds that <math>S(\boldsymbol \beta^s+\alpha\  \delta\boldsymbol\beta) &amp;lt; S(\boldsymbol \beta^s)</math> for all sufficiently small <math>\alpha&amp;gt;0</math>. Thus, if divergence occurs, one solution is to employ a fraction, <math>\alpha</math>, of the increment vector, <math>\delta\boldsymbol\beta</math> in the updating formula   
<indent level="1">

<math> \boldsymbol \beta^{s+1} = \boldsymbol \beta^s+\alpha\  \delta\boldsymbol\beta</math>.
</indent>
In other words, the increment vector is too long, but it points in "downhill", so going just a part of the way will decrease the objective function <it>S</it>. An optimal value for <math>\alpha</math> can be found by using a <link xlink:type="simple" xlink:href="../058/1537058.xml">
line search</link> algorithm, that is, the magnitude of <math>\alpha</math> is determined by finding the value that minimizes <it>S</it>, usually using a <link xlink:type="simple" xlink:href="../058/1537058.xml">
direct search method</link> in the interval <math>0&amp;lt;\alpha&amp;lt;1</math>.</p>
<p>

In cases where the direction of the shift vector is such that the optimal fraction, <math> \alpha </math>, is close to zero, an alternative method for handling divergence is the use of the <link>
Levenberg–Marquardt algorithm</link>, also known as the "trust region method".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> The normal equations are modified in such a way that the increment vector is rotated towards the direction of <link xlink:type="simple" xlink:href="../489/201489.xml">
steepest descent</link>,    
<indent level="1">

<math>\left(\mathbf{J^TJ+\lambda D}\right)\delta\boldsymbol\beta=\mathbf{J}^T \mathbf{r}</math>,
</indent>
where <b>D</b> is a positive diagonal matrix. Note that when <it>D</it> is the identity matrix and <math>\lambda\to+\infty</math>, then  <math> \delta\boldsymbol\beta/\lambda\to \mathbf{J}^T \mathbf{r}</math>, therefore the <link xlink:type="simple" xlink:href="../947/4767947.xml">
direction</link> of <math> \delta\boldsymbol\beta</math> approaches the direction of the gradient <math> \mathbf{J}^T \mathbf{r}</math>.</p>
<p>

The so-called Marquardt parameter, <math>\lambda</math>, may also be optimized by a line search, but this is inefficient as the shift vector must be re-calculated every time <math>\lambda</math> is changed. A more efficient strategy is this. When divergence occurs increase the Marquardt parameter until there is a decrease in S. Then, retain the value from one iteration to the next, but decrease it if possible until a cut-off value is reached when the Marquardt parameter can be set to zero; the minimization of <it>S</it> then becomes a standard Gauss–Newton minimization.</p>

</sec>
<sec>
<st>
 Related algorithms </st>

<p>

In a <link xlink:type="simple" xlink:href="../127/7883127.xml">
quasi-Newton method</link>, such as that due to <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../265/11231265.xml">
Davidon, Fletcher and Powell</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 an estimate of the full Hessian, <math>\frac{\partial^2 S}{\partial \beta_j \partial\beta_k}</math>, is built up numerically using first derivatives <math>\frac{\partial r_i}{\partial\beta_j}</math> only so that after <it>n</it> refinement cycles the method closely approximates to Newton's method in performance.  </p>
<p>

Another method for solving least squares problems using only first derivatives is <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link>. However, this method does not take into account the second derivatives even approximately. Consequently, it is highly inefficient for many functions.</p>

</sec>
<sec>
<st>
References and notes</st>
<p>

<reflist>
<entry id="1">
 content</entry>
<entry id="2">
Björck, p260</entry>
<entry id="3">
Björck, p341, 342</entry>
<entry id="4">
 <cite id="CITEREFFletcher1987" style="font-style:normal">Fletcher, Roger&#32;(1987),&#32;<it>Practical methods of optimization</it>&#32;(2nd ed.), New York: <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../930/537930.xml">
John Wiley &amp; Sons</link></company>
, p. 113, ISBN 978-0-471-91547-8</cite>&nbsp;.</entry>
<entry id="5">
 <cite id="Reference-Nocedal-1999" style="font-style:normal" class="book">Nocedal, Jorge;&#32;Wright, Stephen&#32;(1999). Numerical optimization.&#32;New York: Springer. ISBN 0387987932.</cite>&nbsp;
</entry>
</reflist>
</p>
<p>

<table style=";" class="navbox" cellspacing="0">
<row>
<col style="padding:2px;">
<table style="width:100%;background:transparent;color:inherit;;" class="nowraplinks collapsible autocollapse " cellspacing="0">
<row>
<header colspan="2" style=";" class="navbox-title">
Least squares and regression analysis</header>
</row>
<row style="height:2px;">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../359/82359.xml">
Least squares</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../872/484872.xml">
Linear least squares</link> - <link xlink:type="simple" xlink:href="../764/15652764.xml">
Non-linear least squares</link> - <link xlink:type="simple" xlink:href="../736/1046736.xml">
Partial least squares</link> -<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../437/971437.xml">
Total least squares</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 - <link>
Gauss–Newton algorithm</link> - <link>
Levenberg–Marquardt algorithm</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../997/826997.xml">
Regression analysis</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../903/17903.xml">
Linear regression</link> - <link xlink:type="simple" xlink:href="../012/1045012.xml">
Nonlinear regression</link> - <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../904/17904.xml">
Linear model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 - <link xlink:type="simple" xlink:href="../122/747122.xml">
Generalized linear model</link> - <link xlink:type="simple" xlink:href="../327/2713327.xml">
Robust regression</link> - <link xlink:type="simple" xlink:href="../324/3675324.xml">
Least-squares estimation of linear regression coefficients</link>- <link xlink:type="simple" xlink:href="../740/16234740.xml">
Mean and predicted response</link> - <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../102/2708102.xml">
Poisson regression</link></datum>
</information>
 - <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../631/226631.xml">
Logistic regression</link></datum>
</information>
 - <link xlink:type="simple" xlink:href="../143/2836143.xml">
Isotonic regression</link> - <link xlink:type="simple" xlink:href="../328/954328.xml">
Ridge regression</link> - <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../021/11683021.xml">
Segmented regression</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 - <link xlink:type="simple" xlink:href="../068/4536068.xml">
Nonparametric regression</link> - <link xlink:type="simple" xlink:href="../187/16722187.xml">
Regression discontinuity</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Statistics</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link>
Gauss–Markov theorem</link> - <link xlink:type="simple" xlink:href="../509/461509.xml">
Errors and residuals in statistics</link> - <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../821/2474821.xml">
Goodness of fit</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
 - <link xlink:type="simple" xlink:href="../197/609197.xml">
Studentized residual</link> - <link xlink:type="simple" xlink:href="../816/201816.xml">
Mean squared error</link> - <link xlink:type="simple" xlink:href="../559/6004559.xml">
R-factor (crystallography)</link> - <link xlink:type="simple" xlink:href="../288/3244288.xml">
Mean squared prediction error</link> - <link xlink:type="simple" xlink:href="../545/1761545.xml">
Minimum mean-square error</link> - <link xlink:type="simple" xlink:href="../608/8648608.xml">
Root mean square deviation</link> - <link xlink:type="simple" xlink:href="../367/9830367.xml">
Squared deviations</link> - <link xlink:type="simple" xlink:href="../388/4225388.xml">
M-estimator</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Applications</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../425/555425.xml">
Curve fitting</link> - <link xlink:type="simple" xlink:href="../787/532787.xml">
Calibration curve</link> - <link xlink:type="simple" xlink:href="../617/10684617.xml">
Numerical smoothing and differentiation</link> - <link xlink:type="simple" xlink:href="../311/2017311.xml">
Least mean squares filter</link> - <link xlink:type="simple" xlink:href="../338/2017338.xml">
Recursive least squares filter</link> - <link xlink:type="simple" xlink:href="../659/6582659.xml">
Moving least squares</link> - <link xlink:type="simple" xlink:href="../002/4229002.xml">
BHHH algorithm</link></col>
</row>
</table>
</col>
</row>
</table>
</p>



</sec>
</bdy>
</algorithm>
</article>
