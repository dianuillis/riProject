<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:15:28[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Feature selection</title>
<id>1179950</id>
<revision>
<id>237122142</id>
<timestamp>2008-09-08T19:29:39Z</timestamp>
<contributor>
<username>SmackBot</username>
<id>433328</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Feature selection</b>, also known as <b>variable selection</b>, <b>feature reduction</b>, <b>attribute selection</b> or <b>variable subset selection</b>, is the technique, commonly used in machine learning, of selecting a subset of relevant <link xlink:type="simple" xlink:href="../752/187752.xml">
features</link> for building robust learning models. When applied in <link xlink:type="simple" xlink:href="../632/9127632.xml">
biology</link> domain, the technique is also called <link>
discriminative gene selection</link>, which detects influential <link xlink:type="simple" xlink:href="../553/4250553.xml">
genes</link> based on <link xlink:type="simple" xlink:href="../954/255954.xml">
DNA microarray</link> experiments. By removing most irrelevant and redundant features from the data, feature selection helps improve the performance of learning models by:
<list>
<entry level="2" type="bullet">

 Alleviating the effect of the <link xlink:type="simple" xlink:href="../776/787776.xml">
curse of dimensionality</link>.</entry>
<entry level="2" type="bullet">

 Enhancing generalization capability.</entry>
<entry level="2" type="bullet">

 Speeding up learning process.</entry>
<entry level="2" type="bullet">

 Improving model interpretability.</entry>
</list>

Feature selection also helps people to acquire better understanding about their data by telling them that which are the important features and how they are related with each other. 
<sec>
<st>
Introduction</st>
<p>

Simple feature selection algorithms are ad hoc, but there are also more methodical approaches. From a theoretical perspective, it can be shown that optimal feature selection for <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link> problems requires an exhaustive search of all possible subsets of features of the chosen cardinality. If large numbers of features are available, this is impractical. For practical supervised learning algorithms, the search is for a satisfactory set of features instead of an optimal set. </p>
<p>

Feature selection algorithms typically fall into two categories; Feature Ranking and Subset Selection. Feature Ranking ranks the features by a metric and eliminates all features that do not achieve an adequate score. Subset Selection searches the set of possible features for the optimal subset. </p>
<p>

In statistics, the most popular form of feature selection is <link xlink:type="simple" xlink:href="../759/4877759.xml">
stepwise regression</link>.  It is a greedy algorithm that adds the best feature (or deletes the worst feature) at each round.  The main control issue is deciding when to stop the algorithm.  In machine learning, this is typically done by <link xlink:type="simple" xlink:href="../612/416612.xml">
cross validation</link>.  In statistics, some criteria are optimized.  This leads to the inherent problem of nesting. More robust methods have been explored, such as <link xlink:type="simple" xlink:href="../580/456580.xml">
Branch and Bound</link> and Piecewise Linear Networks.</p>

</sec>
<sec>
<st>
Subset Selection</st>
<p>

Subset selection evaluates a subset of features as a group for suitability. Subset selection algorithms can be broken into Wrappers, Filters and Embedded. Wrappers use a search algorithm to search through the space of possible features and evaluate each subset by running a model on the subset. Wrappers can be computationally expensive and have a risk of over fitting to the model. Filters are similar to Wrappers in the search approach, but instead of evaluating against a model, a simpler filter is evaluated. Embedded techniques are embedded in and specific to a model.</p>
<p>

Many popular search approaches use <link xlink:type="simple" xlink:href="../247/89247.xml">
greedy</link> <link xlink:type="simple" xlink:href="../002/364002.xml">
hill climbing</link>, which iteratively evaluates a candidate subset of features, then modifies the subset and evaluates if the new subset is an improvement over the old. Evaluation of the subsets requires a scoring <link xlink:type="simple" xlink:href="../658/731658.xml">
metric</link> that grades a subset of features.  Exhaustive search is generally impractical, so at some implementor (or operator) defined stopping point, the subset of features with the highest score discovered up to that point is selected as the satisfactory feature subset.  The stopping criterion varies by algorithm; possible criteria include: a subset score exceeds a threshold, a program's maximum allowed run time has been surpassed, etc.</p>
<p>

Search approaches include:</p>
<p>

<list>
<entry level="1" type="bullet">

 Exhaustive</entry>
<entry level="1" type="bullet">

 Best first</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../244/172244.xml">
Simulated annealing</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../254/40254.xml">
Genetic algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../247/89247.xml">
Greedy</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 forward selection</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../247/89247.xml">
Greedy</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 backward elimination</entry>
</list>
</p>
<p>

Two popular filter metrics for classification problems are <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation</link> and <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>, although neither are true <link xlink:type="simple" xlink:href="../467/1561467.xml">
metrics</link> or 'distance measures' in the mathematical sense, since they fail to obey the <link xlink:type="simple" xlink:href="../941/53941.xml">
triangle inequality</link> and thus do not compute any actual 'distance' -- they should rather be regarded as 'scores'.  These scores are computed between a candidate feature (or set of features) and the desired output category.</p>
<p>

Other available filter metrics include:</p>
<p>

<list>
<entry level="1" type="bullet">

 Class separability</entry>
<entry level="2" type="bullet">

 Error probability</entry>
<entry level="2" type="bullet">

 Inter-class distance</entry>
<entry level="2" type="bullet">

 Probabilistic distance</entry>
<entry level="2" type="bullet">

 <link xlink:type="simple" xlink:href="../891/9891.xml">
Entropy</link></entry>
<entry level="1" type="bullet">

 Consistency-based feature selection</entry>
<entry level="1" type="bullet">

 Correlation-based feature selection</entry>
</list>
</p>

</sec>
<sec>
<st>
Optimality criteria</st>

<p>

There are a variety of optimality criteria that can be used for controlling feature selection.  The oldest are <link xlink:type="simple" xlink:href="../591/8284591.xml">
Mallows' Cp</link> statistic and <link xlink:type="simple" xlink:href="../512/690512.xml">
Akaike information criterion</link> (AIC).  These add variables if the t statistic is bigger than <math>\sqrt{2}</math>.  </p>
<p>

Other criteria are <link xlink:type="simple" xlink:href="../272/2473272.xml">
Bayesian information criterion</link> (BIC) which uses <math>\sqrt{\log{n}}</math>, <link xlink:type="simple" xlink:href="../325/331325.xml">
minimum description length</link> (MDL) which asymptotically uses <math>\sqrt{\log{n}}</math> but some argue this asymptote is not computed correctly, Bonnferroni / RIC which use <math>\sqrt{2\log{p}}</math>, and a variety of new criteria that are motivated by <link xlink:type="simple" xlink:href="../271/4574271.xml">
false discovery rate</link> (FDR) which use something close to <math>\sqrt{2\log{\frac{p}{q}}}</math>.</p>

</sec>
<sec>
<st>
minimum-Redundancy-Maximum-Relevance </st>

<p>

Selecting features that correlate strongest to the classification variable has been called the "maximum-relevance selection". Many heuristic algorithms can be used, such as the sequential forward, backward, or floating selections.</p>
<p>

On the other hand, features can be selected to be different from each other, while they still have high correlation to the classification variable. This scheme, called "minimum-Redundancy-Maximum-Relevance" selection (<weblink xlink:type="simple" xlink:href="http://research.janelia.org/peng/proj/mRMR/index.htm">
mRMR</weblink>), has been found to be more powerful than the maximum relevance selection.</p>
<p>

As a special case, statistical dependence between variables can be used instead of correlation. Mutual information can be used to quantify the dependency. In this case, it is shown that mRMR is an approximation to maximizing the dependency between the joint distribution of the selected features and the classification variable.</p>

</sec>
<sec>
<st>
Embedded methods incorporating Feature Selection</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../880/1363880.xml">
Random forests</link> (RF)</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../079/9448079.xml">
Random multinomial logit</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (RMNL)</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../328/954328.xml">
Ridge regression</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../602/232602.xml">
Decision tree</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 Auto-encoding networks with a bottleneck-layer</entry>
<entry level="1" type="bullet">

 Many other <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> methods applying a <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../075/5462075.xml">
pruning</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 step.</entry>
</list>
</p>

</sec>
<sec>
<st>
Software for Feature Selection</st>

<p>

<list>
<entry level="1" type="bullet">

 <work wordnetid="100575741" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../865/5100865.xml">
RapidMiner</link></activity>
</psychological_feature>
</act>
</undertaking>
</event>
</work>
, a freely available <link xlink:type="simple" xlink:href="../758/18938758.xml">
open-source</link> software for intelligent data analysis, <link xlink:type="simple" xlink:href="../947/2044947.xml">
knowledge discovery</link>, <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link>, <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, visualization, etc. featuring numerous feature generation and feature selection operators.</entry>
<entry level="1" type="bullet">

 <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../034/3829034.xml">
Weka</link></software>
, a Java software package including a collection of machine learning algorithms for data mining tasks.</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../675/669675.xml">
Cluster analysis</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../867/579867.xml">
Dimensionality reduction</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../190/242190.xml">
Feature extraction</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../253/42253.xml">
Data mining</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://jmlr.csail.mit.edu/papers/special/feature03.html">
JMLR Special Issue on Variable and Feature Selection</weblink></entry>
<entry level="1" type="bullet">

 Peng, H.C., Long, F., and Ding, C., "Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy," IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 27, No. 8, pp.1226-1238, 2005. <weblink xlink:type="simple" xlink:href="http://research.janelia.org/peng/proj/mRMR/index.htm">
Program</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.springer.com/west/home?SGWID=4-102-22-33327495-0&amp;changeHeader=true&amp;referer=www.wkap.nl&amp;SHORTCUT=www.springer.com/prod/b/0-7923-8198-X">
Feature Selection for Knowledge Discovery and Data Mining</weblink> (Book)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.kernel-machines.org/jmlr/volume3/guyon03a/guyon03a.pdf">
An Introduction to Variable and Feature Selection</weblink> (Survey)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/iel5/69/30435/01401889.pdf">
Toward integrating feature selection algorithms for classification and clustering</weblink> (Survey)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ijcai.org/papers07/Papers/IJCAI07-187.pdf">
Searching for Interacting Features</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.icml2006.org/icml_documents/camera-ready/107_Feature_Subset_Selec.pdf">
Feature Subset Selection Bias for Classification Learning</weblink></entry>
<entry level="1" type="bullet">

 M. Hall 1999, <weblink xlink:type="simple" xlink:href="http://www.cs.waikato.ac.nz/~mhall/thesis.pdf">
Correlation-based Feature Selection for Machine Learning</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.clopinet.com/isabelle/Projects/NIPS2003/">
NIPS challenge 2003</weblink> (see also <link xlink:type="simple" xlink:href="../156/1175156.xml">
NIPS</link>)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://paul.luminos.nl/documents/show_document.php?d=198">
Naive Bayes implementation with feature selection in Visual Basic</weblink> (includes executable and source code)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://research.janelia.org/peng/proj/mRMR/index.htm">
minimum-Redundancy-Maximum-Relevance (mRMR) feature selection program</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
