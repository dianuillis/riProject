<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:15:30[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Stochastic gradient descent</title>
<id>1180641</id>
<revision>
<id>230899901</id>
<timestamp>2008-08-09T22:56:38Z</timestamp>
<contributor>
<username>Ogrisel</username>
<id>227630</id>
</contributor>
</revision>
<categories>
<category>Optimization algorithms</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<image location="right" width="150px" src="stogra.png" type="thumb">
<caption>

Fluctuations in the total objective function as gradient steps with respect to mini-batches are taken
</caption>
</image>
<p>

<b>Stochastic gradient descent</b> is a general <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>, but is typically used to fit the <link xlink:type="simple" xlink:href="../065/25065.xml">
parameter</link>s of a <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> <link xlink:type="simple" xlink:href="../795/3224795.xml">
model</link>.</p>
<p>

In standard (or "batch") <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link>, the true <link xlink:type="simple" xlink:href="../461/12461.xml">
gradient</link> is used to update the parameters of the model. The true gradient is usually the sum of the gradients caused by each individual training example. The parameter <link xlink:type="simple" xlink:href="../370/32370.xml">
vector</link>s are adjusted by the negative of the true gradient multiplied by a step size. Therefore, batch gradient descent requires one sweep through the training set before any parameters can be changed.</p>
<p>

In stochastic (or "on-line") gradient descent, the true gradient is approximated by the gradient of the cost function only evaluated on a single training example. The parameters are then adjusted by an amount proportional to this approximate gradient. Therefore, the parameters of the model are updated after each training example. For large data sets, on-line gradient descent can be much faster than batch gradient descent.</p>
<p>

There is a compromise between the two forms, which is often called "mini-batches", where the true gradient is approximated by a sum over a small number of training examples.</p>
<p>

Stochastic gradient descent is a form of <link xlink:type="simple" xlink:href="../437/8979437.xml">
stochastic approximation</link>. The theory of stochastic approximations gives conditions on when stochastic gradient descent converges.</p>
<p>

Some of the most popular stochastic gradient descent algorithms are the <link>
least mean squares (LMS)</link> adaptive filter and the <link xlink:type="simple" xlink:href="../091/1360091.xml">
backpropagation</link> algorithm.</p>

<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://leon.bottou.org/papers/bottou-mlss-2004">
<it>Stochastic Learning''</it></weblink>. Lecture by <link>
Léon Bottou</link> for the Machine Learning Summer School 2003 in <link>
Tübingen</link>. Also in <it>Advanced Lectures on Machine Learning</it> edited by <link>
Olivier Bousquet</link> and <link>
Ulrike von Luxburg</link>, ISBN 3-540-23122-6, <link xlink:type="simple" xlink:href="../524/35524.xml">
2004</link></entry>
<entry level="1" type="bullet">

<it>Introduction to Stochastic Search and Optimization</it> by James C. Spall, ISBN 0-471-33052-3, <link xlink:type="simple" xlink:href="../163/36163.xml">
2003</link></entry>
<entry level="1" type="bullet">

<it>Pattern Classification</it> by Richard O. Duda, Peter E. Hart, David G. Stork, ISBN 0-471-05669-3, <link xlink:type="simple" xlink:href="../548/34548.xml">
2000</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Implementation </st>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://leon.bottou.org/projects/sgd">
sgd</weblink>: An LGPL C++ library implementing Stochastic Gradient Descent with application to learning <link xlink:type="simple" xlink:href="../309/65309.xml">
Support Vector Machine</link> and <link xlink:type="simple" xlink:href="../276/4118276.xml">
Conditional random field</link></entry>
</list>
</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
