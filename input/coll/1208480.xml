<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:18:12[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Variational Bayesian methods</title>
<id>1208480</id>
<revision>
<id>222923389</id>
<timestamp>2008-07-01T19:30:12Z</timestamp>
<contributor>
<username>Patrick</username>
<id>4388</id>
</contributor>
</revision>
<categories>
<category>Wikipedia articles needing context</category>
<category>Bayesian statistics</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_style.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>This article only describes one highly specialized aspect of its associated subject.</b>
Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Variational_Bayesian_methods&amp;action=edit">
improve this article</weblink>by adding more general information.</col>
</row>
</table>


<b>Variational Bayesian</b> methods, also called <b>ensemble learning</b>, are a family of techniques for approximating intractable integrals arising in Bayesian statistics and machine learning. They can be used to lower bound the <link xlink:type="simple" xlink:href="../771/979771.xml">
marginal likelihood</link> (i.e. "evidence") of several models with a view to performing <link xlink:type="simple" xlink:href="../073/3664073.xml">
model selection</link>, and often provide an analytical approximation to the parameter posterior which is useful for prediction. It is an alternative to Monte Carlo sampling methods for making use of a posterior distribution that is difficult to sample from directly.
<sec>
<st>
Mathematical derivation</st>
<p>

In variational inference, the posterior distribution over a set of <link xlink:type="simple" xlink:href="../330/2649330.xml">
latent variable</link>s <math>X = \{X_1 \dots X_n\}</math> given some data <math>D</math> is approximated
by a variational distribution</p>
<p>

<indent level="1">

<math>P(X|D) \approx Q(X).</math>
</indent>

The variational distribution <math>Q(X)</math> is restricted to belong to a family of distributions of simpler
form than <math>P(X|D)</math>. This family is selected with the intention that <math>Q</math> can be made very similar
to the true posterior. The difference between <math>Q</math> and this true posterior is measured in terms of
a dissimilarity function <math>d(Q; P)</math> and hence inference is performed by selecting the distribution
<math>Q</math> that minimises <math>d</math>. One choice of dissimilarity function where this minimisation is tractable
is the <link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler divergence</link> (KL divergence), defined as</p>
<p>

<indent level="1">

<math>\text{KL}(Q || P) = \sum_X  Q(X) \log \frac{Q(X)}{P(X|D)}.</math>
</indent>

We can write the log <link xlink:type="simple" xlink:href="../854/5236854.xml">
evidence</link> as</p>
<p>

<indent level="1">

|
</indent>
|-
|<math>\log P(D)\!</math>
|<math>= \text{KL}(Q||P) - \sum_X Q(X) \log \frac{Q(X)}{P(X,D)}</math>
|-
|
|<math>= \text{KL}(Q||P) + \mathcal{L}(Q)</math>.
|}</p>
<p>

As the log evidence is fixed with respect to <math>Q</math>, maximising the final term <math>\mathcal{L}(Q)</math> will minimise the KL divergence between <math>Q</math> and <math>P</math>.  By appropriate choice of <math>Q</math>, we can make <math>\mathcal{L}(Q)</math> tractable to compute and to maximise. Hence we have both a lower bound on the evidence <math>\mathcal{L}(Q)</math> and an analytical approximation to the posterior <math>Q</math>.</p>

<ss1>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <system wordnetid="108435388" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<network wordnetid="108434259" confidence="0.8">
<link xlink:type="simple" xlink:href="../612/2461612.xml">
Variational message passing</link></network>
</group>
</system>
: a modular algorithm for variational Bayesian inference.</entry>
<entry level="1" type="bullet">

 <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<numerical_quantity wordnetid="105856066" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<value wordnetid="105856388" confidence="0.8">
<quantity wordnetid="105855125" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../752/470752.xml">
Expectation-maximization algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</quantity>
</value>
</rule>
</event>
</numerical_quantity>
</concept>
</idea>
: a related approach which corresponds to a special case of variational Bayesian inference.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.variational-bayes.org">
Variational-Bayes.org</weblink> - a repository of papers, software, and links related to the use of variational Bayesian methods.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/">
The on-line textbook: Information Theory, Inference, and Learning Algorithms</weblink>, by <link xlink:type="simple" xlink:href="../315/2679315.xml">
David J.C. MacKay</link> provides an introduction to variational methods (p. 422).</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cse.buffalo.edu/faculty/mbeal/thesis/index.html">
Variational Algorithms for Approximate Bayesian Inference</weblink>, by M. J. Beal includes comparisons of EM to Variational Bayesian EM and derivations of several models including Variational Bayesian HMMs. </entry>
</list>
</p>

</sec>
</bdy>
</article>
