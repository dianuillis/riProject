<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:18:23[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Temporal difference learning</title>
<id>1209759</id>
<revision>
<id>221902225</id>
<timestamp>2008-06-26T16:34:55Z</timestamp>
<contributor>
<username>GregorB</username>
<id>179697</id>
</contributor>
</revision>
<categories>
<category>Wikify from June 2008</category>
<category>Articles with invalid date parameter in template</category>
<category>All pages needing to be wikified</category>
<category>Machine learning</category>
<category>Computational neuroscience</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Wikitext.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>Please  this article or section.</b>
Help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Temporal_difference_learning&amp;action=edit">
improve this article</weblink> by adding  . <it>(June 2008)''</it></col>
</row>
</table>


<b>Temporal difference learning</b> is a prediction method. It has been mostly used for solving the <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link> problem. "TD learning is a combination of <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo</link></method>
</know-how>
</technique>
 ideas and <link xlink:type="simple" xlink:href="../297/125297.xml">
dynamic programming</link> (DP) ideas." [2] TD resembles a <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo method</link></method>
</know-how>
</technique>
 because it learns by <link xlink:type="simple" xlink:href="../361/160361.xml">
sampling</link> the environment according to some <b>policy</b>. TD is related to <link xlink:type="simple" xlink:href="../297/125297.xml">
dynamic programming</link> techniques because it approximates its current estimate based on previously learned estimates (a process known as <link xlink:type="simple" xlink:href="../211/4211.xml">
bootstrapping</link>). The TD learning algorithm is related to the Temporal difference model of animal learning.<p>

As a prediction method, TD learning takes into account the fact that subsequent predictions are often correlated in some sense. In standard supervised predictive learning, one only learns from actually observed values: A prediction is made, and when the observation is available, the prediction is adjusted to better match the observation. The core idea, as elucidated in [1], of TD learning is that we adjust predictions to match other, more accurate predictions, about the feature. This procedure is a form of bootstrapping as illustrated with the following example (taken from [1]):</p>
<p>

<indent level="1">

 Suppose you wish to predict the weather for Saturday and that you have some model that predicts Saturday's weather given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example Friday, you should have a pretty good idea of what the weather would be on Saturday - and thus be able to change, say, Monday's model before Saturday arrives.
</indent>

Mathematically speaking, both in a standard and a TD approach, we would try to optimise some cost function, related to the error in our predictions of the expectation of some random variable, E[z]. However, while in the standard approach we in some sense assume E[z]=z (the actual observed value), in the TD approach we use a model. For the particular case of reinforcement learning, which is the major application of TD methods, z is the total return and E[z] is given by the <link xlink:type="simple" xlink:href="../458/1236458.xml">
Bellman equation</link> of the return.</p>

<sec>
<st>
 TD algorithm in neuroscience </st>
<p>

The TD algorithm has also received attention in the field of <link xlink:type="simple" xlink:href="../245/21245.xml">
Neuroscience</link>. Researchers discovered that the firing rate of <link xlink:type="simple" xlink:href="../548/48548.xml">
dopamine</link> neurons in the <link xlink:type="simple" xlink:href="../908/716908.xml">
ventral tegmental area</link> (VTA) and <link xlink:type="simple" xlink:href="../425/46425.xml">
substantia nigra</link> (SNc) appear to mimic the error function in the algorithm [3]. The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward the error can be used to associate the stimulus with the future <link xlink:type="simple" xlink:href="../684/8582684.xml">
reward</link>. </p>
<p>

<link xlink:type="simple" xlink:href="../548/48548.xml">
Dopamine</link> cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice [4]. Initially the dopamine cells increased firing rates when exposed to the juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained the dopamine cells stopped firing. This mimics closely how the error function in TD is used for <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link>.</p>
<p>

The relationship between the model and potential neurological function has produced research attempting to use TD to explain many aspects of behavioral research [5]. It has also been used to study conditions such as schizophrenia or the consequences of pharmacological manipulations of dopamine on learning [6].</p>

</sec>
<sec>
<st>
 Mathematical Background </st>
<p>

Let <math> \lambda_t </math> be the reinforcement on time step <it>t</it>. Let <math> \bar V_t </math> be the correct prediction that is equal to discounted sum of all future reinforcement. The discounting is done by powers of factor of <math> \gamma </math> such that reinforcement at distant time step is less important.  
<indent level="1">

<math> \bar V_t = \sum_{i=0}^{\infty} \gamma^i \lambda_{t+i} </math>
</indent>
::<math> 0 \le \gamma &amp;lt; 1 </math>
This formula can be expanded 
<indent level="1">

<math> \bar V_t = \lambda_{t} + \sum_{i=1}^{\infty} \gamma^i \lambda_{t+i} </math>
</indent>
by changing the index of i to start from 0.
<indent level="1">

<math> \bar V_t = \lambda_{t} + \sum_{i=0}^{\infty} \gamma^{i+1} \lambda_{t+i+1} </math>
</indent>
:<math> \bar V_t = \lambda_{t} + \gamma \sum_{i=0}^{\infty} \gamma^i \lambda_{t+1+i} </math>
<indent level="1">

<math> \bar V_t = \lambda_{t} + \gamma \bar V_{t+1} </math>
</indent>

Thus, the reinforcement is the difference between the ideal prediction and the current prediction.
<indent level="1">

<math> \lambda_{t} = \bar V_{t} - \gamma \bar V_{t+1} </math>
</indent>
</p>
<p>

<b>TD-Lambda</b> is a learning algorithm invented by Richard Sutton based on earlier work on <link xlink:type="simple" xlink:href="../759/1209759.xml">
temporal difference learning</link> by Arthur Samuel [2]. This algorithm was famously applied by Gerald Tesauro to create <link>
TD-Gammon</link>, a program that can learn to play the game of <link xlink:type="simple" xlink:href="../329/4329.xml">
backgammon</link> nearly as well as expert human players. The lambda (<math>\lambda</math>) parameter here refers to the trace decay parameter, with <math>0 \le \lambda \le 1</math>. Higher settings lead to longer lasting traces; that is, a larger proportion of credit from a reward can be given to more distal states and actions when <math>\lambda</math> is higher, with <math>\lambda = 1</math> producing parallel learning to Monte Carlo RL algorithms.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../294/66294.xml">
Reinforcement learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../850/1281850.xml">
Q-learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../297/10584297.xml">
SARSA</link></entry>
<entry level="1" type="bullet">

 <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../571/1221571.xml">
Rescorla-Wagner model</link></concept>
</idea>
</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://scholarpedia.org/article/Temporal_Difference_Learning">
Scholarpedia Temporal difference Learning</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.research.ibm.com/massive/tdl.html#h3:stochastic_environment">
TD-Gammon</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://rlai.cs.ualberta.ca/TDNets/index.html">
TD-Networks Research Group</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

[0] Sutton, R.S., Barto A.G. (1990) <it>Time Derivative Models of Pavlovian Reinforcement, Learning and Computational Neuroscience</it> (available <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~sutton/papers/sutton-barto-90.pdf">
here</weblink>).</p>
<p>

[1] Richard Sutton.  Learning to predict by the methods of temporal differences.  <it>Machine Learning</it> 3:9-44. 1988.  (A revised version is available on <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~sutton/publications.html">
Richard Sutton's publication page</weblink>)</p>
<p>

[2] Richard Sutton and Andrew Barto. <it>Reinforcement Learning</it>. MIT Press, 1998. (available <weblink xlink:type="simple" xlink:href="http://www-anw.cs.umass.edu/~rich/book/the-book.html">
online</weblink>)</p>
<p>

[3] Schultz, W, Dayan, P &amp; Montague, PR. 1997. A neural substrate of prediction and reward. Science 275:1593-1599.</p>
<p>

[4] Schultz W. 1998. Predictive reward signal of dopamine neurons. J Neurophysiology 80:1-27.</p>
<p>

[5] Dayan P. 2002. Motivated reinforcement learning. In: Ghahramani T, editor. Advances in neural information processing system, Cambridge, MA: MIT Press.</p>
<p>

[6] Smith, A., Li, M., Becker, S. and Kapur, S. (2006), Dopamine, prediction error, and associative learning: a model-based account. Network: Computation in Neural Systems 17(1):61-84.</p>
<p>

[7] Gerald Tesauro.  Temporal Difference Learning and TD-Gammon.  <it>Communications of the ACM</it>, March 1995 / Vol. 38, No. 3. (available at <weblink xlink:type="simple" xlink:href="http://www.research.ibm.com/massive/tdl.html">
Temporal Difference Learning and TD-Gammon</weblink>)</p>
<p>

[8] Imran Ghory. Reinforcement Learning in Board Games. http://www.cs.bris.ac.uk/Publications/Papers/2000100.pdf</p>
<p>

[9] S. P. Meyn, 2007.  <weblink xlink:type="simple" xlink:href="http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html">
Control Techniques for Complex Networks</weblink>, Cambridge University Press, 2007. See final chapter, and appendix with abridged <weblink xlink:type="simple" xlink:href="http://decision.csl.uiuc.edu/~meyn/pages/book.html">
Meyn &amp; Tweedie</weblink>.</p>

</sec>
</bdy>
</article>
