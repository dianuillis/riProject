<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:19:17[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<physical_entity  confidence="0.8" wordnetid="100001930">
<person  confidence="0.8" wordnetid="100007846">
<model  confidence="0.8" wordnetid="110324560">
<assistant  confidence="0.8" wordnetid="109815790">
<worker  confidence="0.8" wordnetid="109632518">
<causal_agent  confidence="0.8" wordnetid="100007347">
<header>
<title>Generative model</title>
<id>1222578</id>
<revision>
<id>242916552</id>
<timestamp>2008-10-04T06:57:09Z</timestamp>
<contributor>
<username>Awaterl</username>
<id>246</id>
</contributor>
</revision>
<categories>
<category>Statistical models</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, a <b>generative model</b> is a model for randomly generating observed data, typically given some hidden parameters. It specifies a <link xlink:type="simple" xlink:href="../637/879637.xml">
joint probability distribution</link> over observation and label sequences. Generative models are used in <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> for either modeling data directly (i.e., modeling observed draws from a <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link>), or as an intermediate step to forming a <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probability density function</link>. A conditional distribution can be formed from a generative model through the use of <link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' rule</link>.<p>

Stated another way: if one has a generative model and a dictionary for a language one can <it>generate</it> well-formed sentences of that language.  This is done by selecting a word from the dictionary and then applying a rule from the model, which results in a two-word phrase.  Applying the rules from the model again produce a three-word phrase, etc.  The resultant sentence will be a valid construction of the language, even though no human may have ever spoken it.  In <work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../061/828061.xml">
A Mathematical Theory of Communication</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
 Shannon gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with <it>representing and speedily is an good</it>; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.</p>
<p>

Generative models contrast with <link xlink:type="simple" xlink:href="../912/12155912.xml">
discriminative model</link>s, in that a generative model is a full probability model of all variables, whereas a discriminative model provides a model only of the target variable(s) conditional on the observed variables. Thus a generative model can be used, for example, to simulate (i.e. <it>generate</it>) values of any variable in the model, whereas a discriminative model allows only sampling of the target variables conditional on the observed quantities.</p>
<p>

Examples of generative models include:
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian distribution</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../681/871681.xml">
Gaussian mixture model</link></entry>
<entry level="1" type="bullet">

 <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../553/1045553.xml">
Multinomial distribution</link></distribution>
</arrangement>
</structure>
</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../770/98770.xml">
Hidden Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../339/87339.xml">
Naive Bayes</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../351/4605351.xml">
Latent Dirichlet allocation</link></entry>
</list>
</p>
<p>

If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to <link xlink:type="simple" xlink:href="../806/140806.xml">
maximize the data likelihood</link> is a common method. However, since most statistical models are only approximates to the <it>true</it> distribution, if the model's application is to inference about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it is often more accurate to model the conditional density functions directly: i.e., performing <link xlink:type="simple" xlink:href="../244/1579244.xml">
classification</link> or <link xlink:type="simple" xlink:href="../997/826997.xml">
regression analysis</link>.</p>

<sec>
<st>
 External links </st>



</sec>
</bdy>
</causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</article>
