<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 01:21:15[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Ensembles of classifiers</title>
<id>12304987</id>
<revision>
<id>234549102</id>
<timestamp>2008-08-27T11:58:27Z</timestamp>
<contributor>
<username>EverGreg</username>
<id>3874455</id>
</contributor>
</revision>
<categories>
<category>All pages needing cleanup</category>
<category>Wikipedia articles needing style editing from December 2007</category>
<category>Articles lacking in-text citations</category>
<category>All articles needing style editing</category>
<category>Ensemble learning</category>
<category>Articles with disputed statements </category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_style.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>The  or style of this article or section may not be appropriate for Wikipedia.</b>
Specific concerns may be found on the . See Wikipedia's for suggestions. <it>(December 2007)''</it></col>
</row>
</table>


<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Text_document_with_red_question_mark.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 This article includes a  or , but its sources remain unclear because it lacks <b>.</b>
You can  this article by introducing more precise citations . <it>(February 2008)''</it></col>
</row>
</table>


Recently in the area of Machine Learning the concept of combining classifiers is proposed as a new direction for the improvement of the performance of individual classifiers. These classifiers could be based on a variety of classification methodologies, and could achieve different rate of correctly classified individuals. The goal of classification result integration algorithms is to generate more certain, precise and accurate system results. Dietterich (2001) provides an accessible and informal reasoning, from statistical, computational and representational viewpoints, of why ensembles can improve results.
<ss1>
<st>
Methods</st>
<p>

Numerous methods have been suggested for the creation of ensemble of classifiers.
<list>
<entry level="1" type="bullet">

 Using different subset of training data with a single learning method</entry>
<entry level="1" type="bullet">

 Using different training parameters with a single training method (e.g. using different initial weights for each neural network in an ensemble)</entry>
<entry level="1" type="bullet">

 Using different learning methods.</entry>
</list>
</p>

</ss1>
<ss1>
<st>
Weaknesses</st>
<p>

<list>
<entry level="1" type="bullet">

 Increased storage</entry>
<entry level="1" type="bullet">

 Increased computation</entry>
<entry level="1" type="bullet">

 Decreased comprehensibility</entry>
</list>
</p>
<p>

The first weakness, increased storage, is a direct consequence of the requirement that all component classifiers, instead of a single classifier, need to be stored after training. The total storage depends on the size of each component classifier itself and the size of the ensemble (number of classifiers in the ensemble). The second weakness is increased computation: to classify an input query, all component classifiers (instead of a single classifier) must be processed, and thus it requires more execution time. The last weakness is decreased comprehensibility. With involvement of multiple classifiers in decision-making, it is more difficult for users to perceive the underlying reasoning process leading to a decision.&#91;&#32; &ndash; &#93;</p>

</ss1>
<ss1>
<st>
Bagging</st>
<p>

<link xlink:type="simple" xlink:href="../862/811862.xml">
Bagging</link> is a method of the first category (Breiman, 1996). If there is a training set of size t, then it is possible to draw t random instances from it with replacement (i.e. using a uniform distribution), these t instances can be learned, and this process can be repeated several times. Since the draw is with replacement, usually the instances drawn will contain some duplicates and some omissions as compared to the original training set. Each cycle through the process results in one classifier. After the construction of several classifiers, taking a vote of the predictions of each classifier performs the final prediction.</p>

</ss1>
<ss1>
<st>
Boosting</st>
<p>

Another method of the first category is called <link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link>. AdaBoost is a practical version of the boosting approach (Freund and Schapire, 1996). Boosting is similar in overall structure to bagging, except that one keeps track of the performance of the learning algorithm and forces it to concentrate its efforts on instances that have not been correctly learned. Instead of choosing the t training instances randomly using a uniform distribution, one chooses the training instances in such a manner as to favour the instances that have not been accurately learned. After several cycles, the prediction is performed by taking a weighted vote of the predictions of each classifier, with the weights being proportional to each classifier’s accuracy on its training set.</p>
<p>

Boosting algorithms are considered stronger than bagging on noise free data. However, there are strong empirical indications that bagging is much more robust than boosting in noisy settings. For this reason, Kotsiantis and Pintelas (2004) built an ensemble using a voting methodology of bagging and boosting ensembles that give better classification accuracy.</p>

</ss1>
<sec>
<st>
References</st>

<p>

Breiman L. (1996): Bagging Predictors. Machine Learning, 24(3), 123--140. Kluwer Academic Publishers.
Dietterich, T.G. (2001): Ensemble methods in machine learning. In Kittler, J., Roli, F., eds.: Multiple Classifier Systems. LNCS Vol. 1857, Springer (2001) 1–15
Yoav Freund and Robert E. Schapire, Experiments with a New Boosting Algorithm, Proceedings: ICML’96, p. 148-156, 1996
S. Kotsiantis, P. Pintelas, Combining Bagging and Boosting, International Journal of Computational Intelligence, Vol. 1, No. 4 (324-333), 2004.

</p>
</sec>
</bdy>
</article>
