<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 01:34:23[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Chow-Liu tree</title>
<id>12680566</id>
<revision>
<id>219716511</id>
<timestamp>2008-06-16T15:36:23Z</timestamp>
<contributor>
<username>Mark cummins</username>
<id>2989703</id>
</contributor>
</revision>
<categories>
<category>Knowledge representation</category>
</categories>
</header>
<bdy>

<image width="400 px" src="Chow-liu.png" type="thumb">
<caption>

A second-order dependency tree representing the product below.
</caption>
</image>
A <b>Chow-Liu tree</b> is an efficient method for constructing a second-order product approximation of a <link xlink:type="simple" xlink:href="../637/879637.xml">
joint distribution</link>, first described in a paper by <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFChowLiu1968%22])">
Chow &amp; Liu (1968)</link>.  The goals of such a decomposition, as with such <link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian networks</link> in general, may be either <link xlink:type="simple" xlink:href="../013/8013.xml">
data compression</link> or <link xlink:type="simple" xlink:href="../465/317465.xml">
inference</link>.
<sec>
<st>
The Chow-Liu representation</st>
<p>

The Chow-Liu method describes a <link xlink:type="simple" xlink:href="../637/879637.xml">
joint probability distribution</link> <math>P(X_{1},X_{2},\ldots,X_{n})</math> as a product of second-order conditional and marginal distributions.  For example, the six-dimensional distribution <math>P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})</math> might be approximated as </p>
<p>

<indent level="1">

<math>
P^{\prime
}(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})=P(X_{6}|X_{5})P(X_{5}|X_{2})P(X_{4}|X_{2})P(X_{3}|X_{2})P(X_{2}|X_{1})P(X_{1})
</math>
</indent>

where each new term in the product introduces just one new variable, and the product can be represented as a first-order dependency tree, as shown in the figure.  The Chow-Liu algorithm (below) determines which conditional probabilities are to be used in the product approximation.   In general, unless there are no third or higher-order interactions, the Chow-Liu approximation is indeed an <it>approximation</it>, and cannot capture the complete structure of the original distribution.  <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFPearl1988%22])">
Pearl (1988)</link> provides a modern analysis of the Chow-Liu tree as a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian network</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
.</p>

</sec>
<sec>
<st>
The Chow-Liu algorithm</st>
<p>

Chow and Liu show how to select second-order terms for the product approximation so that among all such second-order approximations (first-order dependency trees), the constructed approximation <math>P^{\prime}</math> has the minimum <link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler distance</link> to the actual distribution <math>P</math>, and is thus the <it>closest</it> approximation in the classical <link xlink:type="simple" xlink:href="../773/14773.xml">
information-theoretic</link> sense. The Kullback-Leibler distance between a second-order product approximation and the actual distribution is shown to be</p>
<p>

<indent level="1">

<math>
D(P\parallel P^{\prime })=-\sum I(X_{i};X_{i-1})+\sum
H(X_{i})-H(X_{1},X_{2},\ldots ,X_{n})
</math>
</indent>

where <math>I(X_{i};X_{i-1})</math> is the <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link> between variable <math>X_{i}</math> and <math>X_{i-1}</math> and <math>H(X_{1},X_{2},\ldots ,X_{n})</math> is the <link xlink:type="simple" xlink:href="../967/910967.xml">
joint entropy</link> of variable set <math>\{X_{1},X_{2},\ldots ,X_{n}\}</math>.   Since the terms <math>\sum H(X_{i})</math> and  <math>H(X_{1},X_{2},\ldots ,X_{n})</math> are independent of the dependency ordering in the tree, only the sum of the pairwise <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>s, <math>\sum I(X_{i};X_{i-1})</math>, determines the quality of the approximation. Thus, if every branch (edge) on the tree is given a weight corresponding to the mutual information between the variables at its vertices, then the tree which provides the optimal second-order approximation to the target distribution is just the <it>maximum-weight tree</it>. The equation above also highlights the role of the dependencies in the approximation: When no dependencies exist, and the first term in the equation is absent, we have only an approximation based on first-order marginals, and the distance between the approximation and the true distribution is due to the redundancies that are not accounted for when the variables are treated as independent. As we specify second-order dependencies, we begin to capture some of that structure and reduce the distance between the two distributions.</p>
<p>

Chow and Liu provide a simple algorithm for constructing the optimal tree; at each stage of the procedure the algorithm simply adds the maximum <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link> pair to the tree.  See the original paper, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFChowLiu1968%22])">
Chow &amp; Liu (1968)</link>, for full details. A more efficient tree construction algorithm for the common case of sparse data was outlined in <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFMeil=C4=831999%22])">
Meilă (1999)</link>.</p>

</sec>
<sec>
<st>
Variations on Chow-Liu trees</st>
<p>

The obvious problem which occurs when the actual distribution is not in fact a second-order dependency tree can still in some cases be addressed by fusing or aggregating together densely connected subsets of variables to obtain a "large-node" Chow-Liu tree  <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHuangKing2002%22])">
Huang &amp; King 2002</link>)</cite>, or by extending the idea of greedy maximum branch weight selection to non-tree (multiple parent) structures  <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFWilliamson2000%22])">
Williamson 2000</link>)</cite>. (Similar techniques of variable substitution and construction are common in the <link xlink:type="simple" xlink:href="../996/203996.xml">
Bayes network</link> literature, e.g., for dealing with loops.  See <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFPearl1988%22])">
Pearl (1988)</link>.)</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFChowLiu1968">Chow, C. K.&#32;&amp;&#32;C. N. Liu&#32;(1968),&#32;"Approximating discrete probability distributions with dependence trees",&#32;<it>IEEE Transactions on Information Theory</it>&#32;<b>IT-14</b>&amp;#x00a0;(3):  462-467</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFHuangKingLyu2002">Huang, Kaizhu; Irwin King&#32;&amp; Michael R. Lyu&#32;(2002),&#32;"Constructing a large node Chow-Liu tree based on frequent itemsets", written at <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../318/27318.xml">
Singapore</link></country>
, in&#32;Wang, Lipo &amp; Rajapakse, Jagath C. &amp; Fukushima, Kunihiko &amp; Lee, Soo-Young &amp; Yao, Xin,&#32;<it>Proceedings of the 9th International Conference on Neural Information Processing ({ICONIP}'02)</it>,  498-502</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFPearl1988">Pearl, Judea&#32;(1988), written at <link xlink:type="simple" xlink:href="../124/108124.xml">
San Mateo, CA</link>,&#32;<it>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</it>, Morgan Kaufmann</cite></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFWilliamson2000">Williamson, Jon&#32;(2000),&#32;"Approximating discrete probability distributions with Bayesian networks", written at <body wordnetid="107965085" confidence="0.8">
<social_group wordnetid="107950920" confidence="0.8">
<colony wordnetid="108374049" confidence="0.8">
<land wordnetid="109334396" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<island wordnetid="109316454" confidence="0.8">
<link xlink:type="simple" xlink:href="../944/29944.xml">
Tasmania</link></island>
</group>
</land>
</colony>
</social_group>
</body>
,&#32;<it>Proceedings of the International Conference on Artificial Intelligence in Science and Technology</it>,  16-20</cite>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFMeilă1999">Meilă, Marina&#32;(1999),&#32;"An Accelerated Chow and Liu Algorithm: Fitting Tree Distributions to High-Dimensional Sparse Data",&#32;<it>Proceedings of the Sixteenth International Conference on Machine Learning</it>, Morgan Kaufman,  249-257</cite>.</entry>
</list>
</p>


</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian network</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../920/16920.xml">
Knowledge representation</link></entry>
</list>
</p>


</sec>
</bdy>
</article>
