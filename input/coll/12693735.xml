<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 01:26:15[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Winnow (algorithm)</title>
<id>12693735</id>
<revision>
<id>238442261</id>
<timestamp>2008-09-14T21:29:31Z</timestamp>
<contributor>
<username>JonathanWilliford</username>
<id>7799841</id>
</contributor>
</revision>
<categories>
<category>Wikipedia articles needing context</category>
<category>Wikipedia introduction cleanup</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_style.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 The introduction to this article provides <b>insufficient context</b> for those unfamiliar with the subject.
Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Winnow_%28algorithm%29&amp;action=edit">
improve the article</weblink> with a .</col>
</row>
</table>


The <b>winnow algorithm</b> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> is a technique from machine learning. It is closely related to the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../777/172777.xml">
Perceptron</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, but it uses a multiplicative weight-update scheme that allows it perform much better than the perceptron when many dimensions are irrelevant (hence its name).  It is not a sophisticated algorithm but it scales well to high-dimensional spaces. During training, winnow is shown a sequence of positive and negative examples. From these it learns a decision hyperplane. It can also be used in the online learning setting, where the learning phase is not separated from the training phase.
<sec>
<st>
The algorithm</st>

<p>

The basic algorithm, Winnow1, is given as follows.
The instance space is <math>X=\{0,1\}^n</math>.  The algorithm maintains non-negative weights <math>w_i</math> for <math>i\in \{1...n\}</math> which are initially set to 1.  When the learner is given an example <math>(x_1,...x_n)</math>, the learner follows the following prediction rule:</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>If</b> <math>\sum_{i=1}^n w_i x_i &amp;gt; \Theta </math>, <b>then</b> it predicts 1</entry>
<entry level="1" type="bullet">

 <b>Otherwise</b> it predicts 0</entry>
</list>
</p>
<p>

Where <math>\Theta</math> is a real number that is called the 'threshold'.  Good bounds are obtained if <math>\Theta=n/2</math>.</p>
<p>

The update rule is (loosely):</p>
<p>

<list>
<entry level="1" type="bullet">

 If an example is correctly classified, do nothing.</entry>
<entry level="1" type="bullet">

 If an example is predicted to be 1 but the correct result was 0, all of the weights involved in the mistake are set to zero (demotion step).</entry>
<entry level="1" type="bullet">

 If an example is predicted to be 0 but the correct result was 1, all of the weights involved in the mistake are multiplied by <math>\alpha</math> (promotion step).</entry>
</list>
</p>
<p>

A good value for <math>\alpha</math> is 2.</p>
<p>

Variations are also used.  For example, Winnow2 is the same as Winnow1 except that in the demotion step the weights are multiplied by <math>\frac{1}{\alpha}</math> instead of being set to 0.</p>

</sec>
<sec>
<st>
Mistake bounds</st>
<p>

If Winnow1 is run with <math>\alpha &amp;gt; 1</math> and <math>\Theta\geq 1/\alpha</math> on a target function that is a <math>k</math>-literal monotone disjunction given by <math>f(x_1,...x_n)=x_{i_1}\cup ... \cup x_{i_k}</math>, then for any sequence of instances the total number of mistakes is bounded by <math>\alpha k ( \log_\alpha \Theta+1)+\frac{n}{\Theta}</math>.</p>

</sec>
<sec>
<st>
References</st>

<ss1>
<st>
Citations and notes</st>
<p>

<reflist>
<entry id="1">
Littlestone, N. (1988) "Learning Quickly When Irrelevant Attributes About: A New Linear-threshold Algorithm" Machine Learning 285-318(2)</entry>
</reflist>
</p>

</ss1>
</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
