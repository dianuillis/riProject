<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:23:28[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Q-learning</title>
<id>1281850</id>
<revision>
<id>244441150</id>
<timestamp>2008-10-10T20:35:18Z</timestamp>
<contributor>
<username>Maelgwnbot</username>
<id>4594058</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Q-learning</b> is a <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link> technique that works by learning an action-value function that gives the expected utility of taking a given action in a given state and following a fixed policy thereafter. A strength with Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment.  A recent variation called delayed-Q learning has shown substantial improvements, bringing <link xlink:type="simple" xlink:href="../008/380008.xml">
PAC</link> bounds to <link xlink:type="simple" xlink:href="../883/1125883.xml">
Markov Decision Processes</link>.
<sec>
<st>
 Algorithm </st>
<p>

The core of the algorithm is a simple <link>
value iteration update</link>.
For each state, <it>s</it>, from the state set <it>S</it>, and for each action, <it>a</it>, from the action set <it>A</it>, we can calculate an update to its expected discounted reward with the following expression: </p>
<p>

<indent level="1">

<math>Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha_t(s_t,a_t) [r_{t+1} + \gamma \max_{a}Q(s_{t+1}, a) - Q(s_t,a_t)]</math>
</indent>

where <it><math>r_t</math></it> is an observed real reward at time <math>t</math>, <math>\alpha_t(s, a)</math> are the learning rates such that 0 ≤<math>\alpha_t(s, a)</math>≤ 1, and <math>\gamma</math> is the discount factor such that 0 ≤<math> \gamma</math>  1.</p>

</sec>
<sec>
<st>
 Implementation</st>
<p>

Q-Learning at its simplest uses tables to store data. This very quickly loses viability with increasing levels of complexity of the system it is monitoring/controlling. One answer to this problem is to use an (adapted) <link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial Neural Network</link> as a function approximator, as demonstrated by Tesauro in his <link xlink:type="simple" xlink:href="../329/4329.xml">
Backgammon</link> playing <link xlink:type="simple" xlink:href="../759/1209759.xml">
Temporal Difference Learning</link> research. An adaptation of the standard neural network is required because the required result (from which the error signal is generated) is itself generated at run-time.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../294/66294.xml">
Reinforcement learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../759/1209759.xml">
Temporal difference learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../297/10584297.xml">
SARSA</link></entry>
<entry level="1" type="bullet">

 <work wordnetid="100575741" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../717/43717.xml#xpointer(//*[./st=%22The+iterated+prisoner.27s+dilemma%22])">
Iterated prisoner's dilemma</link></activity>
</psychological_feature>
</research>
</act>
</investigation>
</experiment>
</event>
</scientific_research>
</work>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../924/11924.xml">
Game theory</link></entry>
<entry level="1" type="bullet">

 <link>
Fitted Q iteration algorithm</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://knol.google.com/k/christian-eder/q-learning/xfqw1gyel5ga/3#">
Q-Learning on Google Knols</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.rhul.ac.uk/~chrisw/thesis.html">
Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge University, Cambridge, England.</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=1143955">
Strehl, Li, Wiewiora, Langford, Littman (2006). PAC model-free reinforcement learning</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/index.html">
Q-Learning by examples</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/%7Esutton/book/the-book.html">
Reinforcement Learning online book</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://elsy.gdan.pl/index.php">
Connectionist Q-learning Java Framework</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://sourceforge.net/projects/piqle/">
Piqle : a Generic Java Platform for Reinforcement Learning</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://ccl.northwestern.edu/netlogo/models/community/Reinforcement%20Learning%20Maze">
Online demonstration of Q-learning (bug in a maze)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.research.ibm.com/infoecon/paps/html/ijcai99_qnn/node4.html">
Q-learning work by Tesauro</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://citeseer.comp.nus.edu.sg/352693.html">
Q-learning work by Tesauro Citeseer Link</weblink></entry>
</list>
</p>





</sec>
</bdy>
</article>
