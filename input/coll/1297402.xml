<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:24:18[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<theorem  confidence="0.9511911446218017" wordnetid="106752293">
<header>
<title>No free lunch in search and optimization</title>
<id>1297402</id>
<revision>
<id>241319070</id>
<timestamp>2008-09-27T13:21:16Z</timestamp>
<contributor>
<username>GregorB</username>
<id>179697</id>
</contributor>
</revision>
<categories>
<category>Mathematical theorems</category>
<category>Mathematical optimization</category>
</categories>
</header>
<bdy>

This article is about mathematical analysis of computing.&#32;&#32;For associated folklore, see <link xlink:type="simple" xlink:href="../317/1297317.xml">
No free lunch theorem</link>.&#32;&#32;<p>

<image location="right" width="305px" src="No_free_lunch_theorems_figure.png" type="thumb">
<caption>

The problem is to rapidly find a solution among candidates a, b, and c that is as good as any other, where goodness is either 0 or 1. There are eight instances ("lunch plates") f<it>xyz</it> of the problem, where <it>x,</it> <it>y,</it> and <it>z</it> indicate the goodness of a, b, and c, respectively. Procedure ("restaurant") A evaluates candidates in the order a, b, c, and B evaluates candidates in reverse that order, but each "charges" 1 evaluation in 5 cases, 2 evaluations in 2 cases, and 3 evaluations in 1 case.
</caption>
</image>
</p>

<p>

In computing, there are circumstances in which the outputs of all procedures solving a particular type of problem are statistically identical. A colorful way of describing such a circumstance, introduced by David H. Wolpert and William G. Macready in connection with the problems of search<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>
and optimization,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>
is to say that  <b><link xlink:type="simple" xlink:href="../704/144704.xml">
there is no free lunch</link></b>. Cullen Schaffer had previously established that there is no free lunch in machine learning problems of a particular sort, but used different terminology.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

In the "no free lunch" <link xlink:type="simple" xlink:href="../518/20518.xml">
metaphor</link>, each "restaurant" (problem-solving procedure) has a "menu" associating each "lunch plate" (problem) with a "price" (the performance of the procedure in solving the problem). The menus of restaurants are identical except in one regard â€” the prices are shuffled from one restaurant to the next. For an <link xlink:type="simple" xlink:href="../403/159403.xml">
omnivore</link> who is as likely to order each plate as any other, the average cost of lunch does not depend on the choice of restaurant. But a <link xlink:type="simple" xlink:href="../587/32587.xml">
vegan</link> who goes to lunch regularly with a <link xlink:type="simple" xlink:href="../543/6543.xml">
carnivore</link> who seeks economy pays a high average cost for lunch. To methodically reduce the average cost, one must use advance knowledge of a) what one will order and b) what the order will cost at various restaurants. That is, improvement of performance in problem-solving hinges on using prior information to match procedures to problems.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

In formal terms, there is no free lunch when the <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> on problem instances is such that all problem solvers have identically distributed results. In the case of <link xlink:type="simple" xlink:href="../149/19386149.xml">
search</link>, a problem instance is an <link xlink:type="simple" xlink:href="../033/52033.xml">
objective function</link>, and a result is a <link xlink:type="simple" xlink:href="../838/27838.xml">
sequence</link> of values obtained in evaluation of <link xlink:type="simple" xlink:href="../808/1556808.xml">
candidate solutions</link> in the <link xlink:type="simple" xlink:href="../263/50263.xml">
domain</link> of the function. For typical interpretations of results, search is an <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> process. There is no free lunch in search if and only if the distribution on objective functions is <link>
invariant</link> under <link xlink:type="simple" xlink:href="../027/44027.xml">
permutation</link> of the space of candidate solutions.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> This condition does not hold precisely in practice,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> but an "(almost) no free lunch" theorem suggests that it holds approximately.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></p>

<sec>
<st>
Overview</st>

<p>

Some computational problems are solved by searching for good solutions in a space of <link xlink:type="simple" xlink:href="../808/1556808.xml">
candidate solution</link>s. A description of how to repeatedly select candidate solutions for evaluation is called a <link xlink:type="simple" xlink:href="../249/28249.xml">
search algorithm</link>. On a particular problem, different search algorithms may obtain different results, but over all problems, they are indistinguishable. It follows that if an algorithm achieves superior results on some problems, it must pay with inferiority on other problems. In this sense there is <link xlink:type="simple" xlink:href="../704/144704.xml">
no free lunch</link> in search.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> Alternatively, following Schaffer,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> search performance is <system wordnetid="104377057" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../956/6956.xml">
conserved</link></instrumentality>
</artifact>
</concept>
</idea>
</system>
. Usually search is interpreted as <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link>, and this leads to the observation that there is no free lunch in optimization.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

"The 'no free lunch' theorem of Wolpert and Macready," as stated in plain language by Wolpert and Macready themselves, is that "any two algorithms are equivalent when their performance is averaged across all possible problems."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> The "no free lunch" results indicate that matching algorithms to problems gives higher average performance than does applying a fixed algorithm to all.  Igel and Toussaint<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> and English<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> have established a general condition under which there is no free lunch. While it is physically possible, it does not hold precisely.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> Droste, Jansen, and Wegener have proved a theorem they interpret as indicating that there is "(almost) no free lunch" in practice.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></p>
<p>

To make matters more concrete, consider an optimization practitioner confronted with a problem. Given some knowledge of how the problem arose, the practitioner may be able to exploit the knowledge in selection of an algorithm that will perform well in solving the problem. If the practitioner does not understand how to exploit the knowledge, or simply has no knowledge, then he or she faces the question of whether some algorithm generally outperforms others on real-world problems. The authors of the "(almost) no free lunch" theorem say that the answer is essentially no, but admit some reservations as to whether the theorem addresses practice.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></p>

</sec>
<sec>
<st>
No free lunch (NFL)</st>

<p>

A "problem" is, more formally, an <link xlink:type="simple" xlink:href="../033/52033.xml">
objective function</link> that associates <link xlink:type="simple" xlink:href="../808/1556808.xml">
candidate solution</link>s with goodness values. A <link xlink:type="simple" xlink:href="../249/28249.xml">
search algorithm</link> takes an objective function as input and evaluates candidate solutions one-by-one. The output of the algorithm is the <link xlink:type="simple" xlink:href="../838/27838.xml">
sequence</link> of observed goodness values.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></p>
<p>

Wolpert and Macready stipulate that an algorithm never reevaluates a candidate solution, and that algorithm performance is measured on outputs.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>  For simplicity, we disallow randomness in algorithms. Under these conditions, when a search algorithm is run on every possible input, it generates each possible output exactly once.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> Because performance is measured on the outputs, the algorithms are indistinguishable in how often they achieve particular levels of performance.</p>
<p>

Some measures of performance indicate how well search algorithms do at <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> of the objective function. Indeed, there seems to be no interesting application of search algorithms in the class under consideration but to optimization problems. A common performance measure is the least index of the least value in the output sequence. This is the number of evaluations required to minimize the objective function. For some algorithms, the time required to find the minimum is proportional to the number of evaluations.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></p>
<p>

The original no free lunch (NFL) theorems assume that all objective functions are equally likely to be input to search algorithms.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> It has since been established that there is NFL if and only if, loosely speaking, "shuffling" objective functions has no impact on their probabilities.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> Although this condition for NFL is physically possible, it has been argued that it certainly does not hold precisely.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>
<p>

The obvious interpretation of "not NFL" is "free lunch," but this is misleading. NFL is a matter of degree, not an all-or-nothing proposition. If the condition for NFL holds approximately, then all algorithms yield approximately the same results over all objective functions.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> Note also that "not NFL" implies only that algorithms are inequivalent overall by <it>some</it> measure of performance. For a performance measure of interest, algorithms may remain equivalent, or nearly so.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></p>
<p>

In theory, all algorithms perform well in optimization almost always. That is, an algorithm obtains good solutions with relatively few evaluations for almost all objective functions.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref> The reason is that almost all objective functions exhibit a high degree of <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov randomness</link>. This equates to extreme irregularity and unpredictability. All levels of goodness are equally represented among candidate solutions, and good solutions are scattered all about the space of candidates. A search algorithm will rarely evaluate more than a small fraction of the candidates before locating a very good solution.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></p>
<p>

In practice, almost all objective functions and algorithms are of such high <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov complexity</link> that they cannot arise.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> There is more information in the typical objective function or algorithm than <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../702/1451702.xml">
Seth Lloyd</link></scientist>
 estimates the observable universe is capable of registering.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref> For instance, if each candidate solution is encoded as a sequence of 300 0's and 1's, and the goodness values are 0 and 1, then most objective functions have Kolmogorov complexity of at least 2300 bits,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref> and this is greater than Lloyd's bound of 1090 â‰ˆ 2299 bits. It follows that not all of "no free lunch" theory applies to physical reality. In a practical sense, algorithms "small enough" for application in physical reality are superior in performance to those that are not.</p>

</sec>
<sec>
<st>
Formal synopsis of NFL</st>

<p>

The set of all <link xlink:type="simple" xlink:href="../033/52033.xml">
objective function</link>s is <math>Y^X</math>, where <math>X</math> is a finite <link xlink:type="simple" xlink:href="../808/1556808.xml">
solution space</link> and <math>Y</math> is a finite <link xlink:type="simple" xlink:href="../572/23572.xml">
poset</link>. The set of all <link xlink:type="simple" xlink:href="../027/44027.xml">
permutation</link>s of <it>X</it> is <it>J</it>. Random variable <it>F</it> is distributed on <math>Y^X</math>. For all <it>j</it> in <it>J</it>, <it>F</it> o <it>j</it> is a random variable distributed on <math>Y^X</math>, with Pr{<it>F</it> o <it>j</it> = <it>f</it>} = Pr{<it>F</it> = <it>f</it> o <it>j</it>â€“1} for all <it>f</it> in <math>Y^X</math>.</p>
<p>

Let <it>a</it>(<it>f</it>) denote the output of search algorithm <it>a</it> on input <it>f</it>. If <it>a</it>(<it>F</it>) and <it>b</it>(<it>F</it>) are identically distributed for all search algorithms <it>a</it> and <it>b</it>, then <it>F</it> has an <it>NFL distribution</it>. This condition holds if and only if <it>F</it> and <it>F</it> o <it>j</it> are identically distributed for all <it>j</it> in <it>J</it>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> In other words, there is no free lunch for search algorithms if and only if the distribution of objective functions is invariant under permutation of the solution space.  </p>
<p>

The "only if" part was first published by C. Schumacher in his PhD dissertation "Black Box Search - Framework and Methods" (The University of Tennessee, Knoxville (2000)).
Set-theoretic NFL theorems have recently been generalized to arbitrary cardinality <math>X</math> and <math>Y</math> ("Reinterpreting No Free Lunch", accepted, Evolutionary Computation Journal).</p>

</sec>
<sec>
<st>
Original NFL theorems</st>

<p>

Wolpert and Macready give two principal NFL theorems, the first regarding objective functions that do not change while search is in progress, and the second regarding objective functions that may change.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> </p>

<p>

<indent level="1">

<it>Theorem 1</it>: For any pair of algorithms <it>a</it>1 and <it>a</it>2
</indent>

<indent level="2">

<math>\sum_f P(h_m^y | f, m, a_1) = \sum_f P(h_m^y | f, m, a_2).</math>
</indent>
</p>
<p>

In essence, this says that when all functions <it>f</it> are equally likely, the probability of observing an arbitrary sequence of <it>m</it> values in the course of search does not depend upon the search algorithm. Theorem 2 establishes a "more subtle" NFL result for time-varying objective functions.</p>

</sec>
<sec>
<st>
Interpretations of NFL results</st>

<p>

A conventional, but not entirely accurate, interpretation of the NFL results is that "a general-purpose universal optimization strategy is theoretically impossible, and the only way one strategy can outperform another is if it is specialized to the specific problem under consideration"<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref>. Several comments are in order:</p>
<p>

<indent level="1">

<it>A general-purpose almost-universal optimizer exists theoretically.</it> Each search algorithm performs well on almost all objective functions.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref>
</indent>

<indent level="1">

<it>An algorithm may outperform another on a problem when neither is specialized to the problem.</it> It may be that both algorithms are among the worst for the problem. Wolpert and Macready have developed a measure of the degree of "match" between an algorithm and a problem.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> To say that one algorithm matches a problem better than another is not to say that either is specialized to the problem. 
</indent>

<indent level="1">

<it>In practice, some algorithms reevaluate candidate solutions.</it> The superiority of an algorithm that never reevaluates candidates over another that does on a particular problem may have nothing to do with specialization to the problem.
</indent>

<indent level="1">

<it>For almost all objective functions, specialization is essentially accidental.</it> Incompressible, or <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov random</link>, objective functions have no regularity for an algorithm to exploit. Given an incompressible objective function, there is no basis for choosing one algorithm over another. If a chosen algorithm performs better than most, the result is happenstance.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref>
</indent>

In practice, only highly compressible (far from random) objective functions fit in the storage of computers, and it is not the case that each algorithm performs well on almost all compressible functions. There is generally a performance advantage in incorporating prior knowledge of the problem into the algorithm. While the NFL results constitute, in a strict sense, <link xlink:type="simple" xlink:href="../289/5723289.xml">
full employment theorem</link>s for optimization professionals, it is important not to take the term literally. For one thing, humans often have little prior knowledge to work with. For another, incorporating prior knowledge does not give much of a performance gain on some problems. Finally, human time is very expensive relative to computer time. There are many cases in which a company would choose to optimize a function slowly with an unmodified computer program rather than rapidly with a human-modified program.</p>
<p>

The NFL results do not indicate that it is futile to take "pot shots" at problems with unspecialized algorithms. No one has determined the fraction of practical problems for which an algorithm yields good results rapidly. And there is a practical free lunch, not at all in conflict with theory. Running an implementation of an algorithm on a computer costs very little relative to the cost of human time and the benefit of a good solution. If an algorithm succeeds in finding a satisfactory solution in an acceptable amount of time, a small investment has yielded a big payoff. If the algorithm fails, then little is lost.</p>

</sec>
<sec>
<st>
Coevolutionary free lunches</st>

<p>

Wolpert and Macready have proved that there are free lunches in <link xlink:type="simple" xlink:href="../835/190835.xml">
coevolution</link>ary optimization.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> Their analysis "covers 'self-play' problems. In these problems, the set of players work together to produce a champion, who then engages one or more antagonists in a subsequent multiplayer game."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> That is, the objective is to obtain a good player, but without an objective function. The goodness of each player (candidate solution) is assessed by observing how well it plays against others. An algorithm attempts to use players and their quality of play to obtain better players. The player deemed best of all by the algorithm is the champion. Wolpert and Macready have demonstrated that some coevolutionary algorithms are generally superior to other algorithms in quality of champions obtained. Generating a champion through self-play is of interest in <link xlink:type="simple" xlink:href="../020/268020.xml">
evolutionary computation</link> and <link xlink:type="simple" xlink:href="../924/11924.xml">
game theory</link>. The results are inapplicable to coevolution of biological species, which does not yield champions.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref></p>

</sec>
<sec>
<st>
 References and notes </st>

<p>

<reflist>
<entry id="1">
Wolpert, D.H., Macready, W.G. (1995), No Free Lunch Theorems for Search, Technical Report SFI-TR-95-02-010 (Santa Fe Institute).</entry>
<entry id="2">
Wolpert, D.H., Macready, W.G. (1997), "No Free Lunch Theorems for Optimization," <it>IEEE Transactions on Evolutionary Computation</it> <b>1</b>, 67. http://ic.arc.nasa.gov/people/dhw/papers/78.pdf</entry>
<entry id="3">
Schaffer, Cullen (1994), "A conservation law for
generalization performance," <it>International Conference on Machine Learning,</it> H. Willian and W. Cohen, Editors. San Francisco: Morgan Kaufmann, pp.295-265.
</entry>
<entry id="4">
Streeter, M. (2003) "Two Broad Classes of Functions for Which a No Free Lunch Result Does Not Hold," <it>Genetic and Evolutionary Computationâ€”GECCO 2003</it>, pp. 1418â€“1430.</entry>
<entry id="5">
Igel, C., and Toussaint, M. (2004) "A No-Free-Lunch Theorem for Non-Uniform Distributions of Target Functions," <it>Journal of Mathematical Modelling and Algorithms</it> <b>3</b>, pp. 313â€“322.</entry>
<entry id="6">
English, T. (2004) No More Lunch: Analysis of Sequential Search, <it>Proceedings of the 2004 IEEE Congress on Evolutionary Computation</it>, pp. 227-234. http://BoundedTheoretics.com/CEC04.pdf</entry>
<entry id="7">
S. Droste, T. Jansen, and I. Wegener. 2002. "Optimization with randomized search heuristics: the (A)NFL theorem, realistic scenarios, and difficult functions," <it>Theoretical Computer Science,</it> vol. 287, no. 1, pp. 131-144.</entry>
<entry id="8">
Wolpert, D.H., and Macready, W.G. (2005) "Coevolutionary free lunches," <it>IEEE Transactions on Evolutionary Computation</it>, 9(6): 721-735</entry>
<entry id="9">
A search algorithm also outputs the sequence of candidate solutions evaluated, but that output is unused in this article.</entry>
<entry id="10">
English, T. M. 2000. "Optimization Is Easy and Learning Is Hard in the Typical Function," <it>Proceedings of the 2000 Congress on Evolutionary Computation: CEC00</it>, pp. 924-931. http://www.BoundedTheoretics.com/CEC2000.pdf</entry>
<entry id="11">
Lloyd, S. (2002) "Computational capacity of the universe," <it>Physics Review Letters</it> <b>88</b>, pp. 237901-237904. http://arxiv.org/abs/quant-ph/0110141</entry>
<entry id="12">
Li, M., and VitÃ¡nyi, P. (1997) <it>An Introduction to Kolmogorov Complexity and Its Applications</it> (2nd ed.), New York: Springer.</entry>
<entry id="13">
Ho, Y.C., Pepyne, D.L. (2002), "Simple Explanation of the No-Free-Lunch Theorem and Its Implications," <it>Journal of Optimization Theory and Applications</it> <b>115</b>, 549.</entry>
</reflist>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../804/15158804.xml">
Evolutionary informatics</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../926/173926.xml">
Inductive bias</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../797/36797.xml">
Occam's razor</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../682/238682.xml">
Simplicity</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../439/5077439.xml">
Ugly duckling theorem</link></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 http://www.no-free-lunch.org</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.uwyo.edu/~wspears/yin-yang.html">
Yin-Yang: No-Free-Lunch Theorems for Search</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/radcliffe95fundamental.html">
Radcliffe and Surry, 1995, "Fundamental Limitations on Search Algorithms: Evolutionary Computing in Perspective" (the first published paper on NFL, available in various formats)</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://EvoInfo.org">
Evolutionary Informatics Lab</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://BoundedTheoretics.com/Papers.html">
NFL publications by Thomas English</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.neuroinformatik.ruhr-uni-bochum.de/PEOPLE/igel/publications.html">
NFL publications by Christian Igel and Marc Toussaint</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.colostate.edu/~genitor/Pubs.html">
NFL and "free lunch" publications by Darrell Whitley</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://ic.arc.nasa.gov/people/dhw/optimization.php">
Publications by David Wolpert, William Macready, and Mario Koeppen on optimization and search</weblink></entry>
</list>
</p>


</sec>
</bdy>
</theorem>
</article>
