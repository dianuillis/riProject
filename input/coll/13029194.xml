<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 01:37:42[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<datum  confidence="0.9511911446218017" wordnetid="105816622">
<header>
<title>Single-linkage clustering</title>
<id>13029194</id>
<revision>
<id>229225316</id>
<timestamp>2008-08-01T13:52:15Z</timestamp>
<contributor>
<username>Michael Hardy</username>
<id>4626</id>
</contributor>
</revision>
<categories>
<category>Data clustering algorithms</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, <b>single linkage</b> (or "nearest neighbor") is a method of calculating distances between clusters in <link xlink:type="simple" xlink:href="../675/669675.xml#xpointer(//*[./st=%22Agglomerative+hierarchical+clustering%22])">
 hierarchical cluster analysis</link>. In single linkage, the distance between two clusters is computed as the distance between the two closest elements in the two clusters.<p>

Mathematically, the linkage function &mdash; the distance <math>D(X,Y)</math> between clusters <math>X</math> and <math>Y</math> &mdash; is described by the following expression :
<math>D(X,Y)= \min(d(x,y))</math></p>
<p>

where
<list>
<entry level="1" type="bullet">

 <math>d(x,y)</math> is the distance between elements <math>x \in X</math> and <math>y \in Y</math> ;</entry>
<entry level="1" type="bullet">

 <math>X</math> and <math>Y</math> are two sets of elements (clusters)</entry>
</list>
</p>
<p>

A drawback of this method is the so-called <it>chaining phenomenon</it>: clusters may be forced together due to single elements being close to each other, even though many of the elements in each cluster may be very distant to each other.</p>

<sec>
<st>
 Algorithm </st>

<p>

The following algorithm is an agglomerative scheme that erases rows and columns in a proximity matrix as old clusters are merged into new ones. The <math>N \times N</math> proximity matrix <it>D</it> contains all distances <it>d</it>(<it>i</it>,<it>j</it>). The clusterings are assigned sequence numbers 0,1,......, (<it>n</it>&nbsp;&amp;minus;&nbsp;1) and <it>L</it>(<it>k</it>) is the level of the kth clustering. A cluster with sequence number <it>m</it> is denoted (<it>m</it>) and the proximity between clusters (<it>r</it>) and (<it>s</it>) is denoted <it>d</it>[(''r''),(''s'')].</p>
<p>

The algorithm is composed of the following steps:</p>
<p>

<list>
<entry level="1" type="number">

 Begin with the disjoint clustering having level <it>L</it>(0) = 0 and sequence number m = 0.</entry>
<entry level="1" type="number">

 Find the least dissimilar pair of clusters in the current clustering, say pair (r), (s), according to <it>d</it>[(''r''),(''s'')] = min <it>d</it>[(''i''),(''j'')] where the minimum is over all pairs of clusters in the current clustering.</entry>
<entry level="1" type="number">

 Increment the sequence number: <it>m</it> = <it>m</it>&nbsp;+&nbsp;1. Merge clusters (<it>r</it>) and (<it>s</it>) into a single cluster to form the next clustering <it>m</it>. Set the level of this clustering to <it>L</it>(<it>m</it>) = <it>d</it>[(''r''),(''s'')]</entry>
<entry level="1" type="number">

 Update the proximity matrix, <it>D</it>, by deleting the rows and columns corresponding to clusters (<it>r</it>) and (<it>s</it>) and adding a row and column corresponding to the newly formed cluster. The proximity between the new cluster, denoted (<it>r</it>,<it>s</it>) and old cluster (<it>k</it>) is defined as <it>d</it>[(''k''), (''r'',''s'')] = min <it>d</it>[(''k''),(''r'')], <it>d</it>[(''k''),(''s'')].</entry>
<entry level="1" type="number">

 If all objects are in one cluster, stop. Else, go to step 2.</entry>
</list>
</p>
<p>

This is essentially the same as <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../776/53776.xml">
Kruskal's algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 for <link xlink:type="simple" xlink:href="../795/41795.xml">
minimum spanning tree</link>s. However, in single linkage clustering, the order in which clusters are formed is important, while for minimum spanning trees what matters is the set of pairs of points that form distances chosen by the algorithm.</p>
<p>

Alternative linkage schemes include <link>
 complete linkage</link> and <link>
 average linkage</link> - implementing a different linkage is simply a matter of using a different formula to calculate inter-cluster distances in steps 2 and 4 of the above algorithm. The formula that should be adjusted has been highlighted using bold text.</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ucl.ac.uk/oncology/MicroCore/HTML_resource/Hier_Linkage.htm">
University College of London - illustrated linkage types</weblink></entry>
</list>
</p>

</sec>
</bdy>
</datum>
</article>
