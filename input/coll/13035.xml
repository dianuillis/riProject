<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:26:25[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Gaussian elimination</title>
<id>13035</id>
<revision>
<id>239997044</id>
<timestamp>2008-09-21T13:43:09Z</timestamp>
<contributor>
<username>Daviddoria</username>
<id>3737125</id>
</contributor>
</revision>
<categories>
<category>Articles with example pseudocode</category>
<category>Numerical linear algebra</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../422/18422.xml">
linear algebra</link>, <b>Gaussian elimination</b> is an efficient <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>  for solving <link xlink:type="simple" xlink:href="../087/113087.xml">
systems of linear equations</link>, to find the <link xlink:type="simple" xlink:href="../561/26561.xml">
rank</link> of a <link xlink:type="simple" xlink:href="../728/19008728.xml">
matrix</link>, and to calculate the inverse of an <link xlink:type="simple" xlink:href="../122/217122.xml">
invertible square matrix</link>.  Gaussian elimination is named after German mathematician and scientist <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../125/6125.xml">
Carl Friedrich Gauss</link></scientist>
</person>
.<p>

<link xlink:type="simple" xlink:href="../031/5897031.xml">
Elementary row operations</link> are used to reduce a matrix to <link xlink:type="simple" xlink:href="../215/330215.xml">
row echelon form</link>. An extension of this algorithm, <link>
Gauss&ndash;Jordan elimination</link>, reduces the matrix further to <link>
reduced row echelon form</link>. Gaussian elimination alone is sufficient for many applications. </p>

<sec>
<st>
 History </st>

<p>

The method of Gaussian elimination appears in Chapter Eight, <it>Rectangular Arrays</it>, of the important Chinese mathematical text <it>Jiuzhang suanshu</it> or <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../207/204207.xml">
The Nine Chapters on the Mathematical Art</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
</it>. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> It was commented on by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../719/202719.xml">
Liu Hui</link></mathematician>
</scientist>
</causal_agent>
</person>
</physical_entity>
 in the 3rd century.</p>
<p>

However, the method was invented in Europe independently by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../125/6125.xml">
Carl Friedrich Gauss</link></scientist>
</person>
 when developing the <link xlink:type="simple" xlink:href="../359/82359.xml">
method of least squares</link> in his 1809 publication <it>Theory of Motion of Heavenly Bodies</it>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

</sec>
<sec>
<st>
 Algorithm overview </st>

<p>

The process of Gaussian elimination has two parts. The first part (Forward Elimination) reduces a given system to either <link xlink:type="simple" xlink:href="../222/374222.xml">
triangular</link> or <link xlink:type="simple" xlink:href="../215/330215.xml">
echelon form</link>, or results in a <link xlink:type="simple" xlink:href="../952/232952.xml">
degenerate</link> equation with no solution, indicating the system has no solution. This is accomplished through the use of <link xlink:type="simple" xlink:href="../031/5897031.xml">
elementary row operations</link>.  The second step uses  to find the solution of the system above.</p>
<p>

Stated equivalently for matrices, the first part reduces a matrix to <link xlink:type="simple" xlink:href="../215/330215.xml">
row echelon form</link> using <link xlink:type="simple" xlink:href="../031/5897031.xml">
elementary row operations</link> while the second reduces it to <link>
reduced row echelon form</link>, or <link>
row canonical form</link>. </p>
<p>

Another point of view, which turns out to be very useful to analyze the algorithm, is that Gaussian elimination computes a <link xlink:type="simple" xlink:href="../873/253873.xml">
matrix decomposition</link>. The three elementary row operations used in the Gaussian elimination (multiplying rows, switching rows, and adding multiples of rows to other rows) amount to multiplying the original matrix with invertible matrices from the left. The first part of the algorithm computes an <link xlink:type="simple" xlink:href="../993/6243993.xml">
LU decomposition</link>, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row-echelon matrix.</p>

</sec>
<sec>
<st>
 Example </st>
<p>

Suppose the goal is to find and describe the solution(s), if any, of the following <link xlink:type="simple" xlink:href="../087/113087.xml">
system of linear equations</link>:</p>
<p>

<indent level="1">

<math>
\begin{align}
2x + y - z &amp; {} = 8 \quad &amp; (L_1) \\
-3x - y + 2z &amp; {} = -11 \quad &amp; (L_2) \\
-2x + y + 2z &amp; {} = -3 \quad  &amp; (L_3)
\end{align}
</math>
</indent>

The algorithm is as follows: eliminate <math>x</math> from all equations below <math>L_1</math>, and then eliminate <math>y</math> from all equations below <math>L_2</math>. This will put the system into <link xlink:type="simple" xlink:href="../222/374222.xml">
triangular form</link>. Then, using back-substitution, each unknown can be solved for.</p>
<p>

In our example, we eliminate <math>x</math> from <math>L_2</math> by adding <math>\begin{matrix}\frac{3}{2}\end{matrix} L_1</math> to <math>L_2</math>, and then we eliminate <math>x</math> from <math>L_3</math> by adding <math>L_1</math> to <math>L_3</math>. Formally:</p>
<p>

<indent level="1">

<math>L_2 + \frac{3}{2}L_1 \rightarrow L_2</math>
</indent>
:<math>L_3 + L_1 \rightarrow L_3</math></p>
<p>

The result is:
<indent level="1">

<math>2x + y - z = 8 \, </math>
</indent>
:<math>\frac{1}{2}y + \frac{1}{2}z = 1 \, </math>
<indent level="1">

<math>2y + z = 5 \, </math>
</indent>

Now we eliminate <math>y</math> from <math>L_3</math> by adding <math>-4L_2</math> to <math>L_3</math>:</p>
<p>

<indent level="1">

<math>L_3 + -4L_2 \rightarrow L_3</math>
</indent>

The result is:</p>
<p>

<indent level="1">

<math>2x + y - z = 8 \, </math>
</indent>
:<math>\frac{1}{2}y + \frac{1}{2}z = 1 \, </math>
<indent level="1">

<math>-z = 1 \, </math>
</indent>

This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. </p>
<p>

The second part, back-substitution, consists of solving for the unknowns in reverse order. Thus, we can easily see that</p>
<p>

<indent level="1">

 <math>z = -1 \quad (L_3)</math>
</indent>

Then, <math>z</math> can be substituted into <math>L_2</math>, which can then be solved easily to obtain</p>
<p>

<indent level="1">

 <math>y = 3 \quad (L_2)</math>
</indent>

Next, <math>z</math> and <math>y</math> can be substituted into <math>L_1</math>, which can be solved to obtain</p>
<p>

<indent level="1">

 <math>x = 2 \quad (L_1)</math>
</indent>

Thus, the system is solved.</p>
<p>

This algorithm works for any system of linear equations. It is possible that the system cannot be reduced to triangular form, yet still have at least one valid solution: for example, if <math>y</math> had not occurred in <math>L_2</math> and <math>L_3</math> after our first step above, the algorithm would have been unable to reduce the system to triangular form. However, it would still have reduced the system to <link xlink:type="simple" xlink:href="../215/330215.xml">
echelon form</link>. In this case, the system does not have a unique solution, as it contains at least one <link>
free variable</link>. The solution set can then be expressed parametrically (that is, in terms of the free variables, so that if values for the free variables are chosen, a solution will be generated).</p>
<p>

In practice, one does not usually deal with the actual systems in terms of equations but instead makes use of the <link xlink:type="simple" xlink:href="../187/3676187.xml">
augmented matrix</link> (which is also suitable for computer manipulations). This, then, is the Gaussian Elimination algorithm applied to the <link xlink:type="simple" xlink:href="../187/3676187.xml">
augmented matrix</link> of the system above, beginning with:</p>
<p>

<indent level="1">

<math>
\begin{bmatrix}
2 &amp; 1 &amp; -1 &amp; 8 \\
-3 &amp; -1 &amp; 2 &amp; -11 \\
-2 &amp; 1 &amp; 2 &amp; -3
\end{bmatrix}
</math>
</indent>

which, at the end of the first part of the algorithm looks like this:</p>
<p>

<indent level="1">

<math>
\begin{bmatrix}
2 &amp; 1 &amp; -1 &amp; 8 \\
0 &amp; \frac{1}{2} &amp; \frac{1}{2} &amp; 1 \\
0 &amp; 0 &amp; -1 &amp; 1
\end{bmatrix}
</math>
</indent>

That is, it is in <link xlink:type="simple" xlink:href="../215/330215.xml">
row echelon form</link>.</p>
<p>

At the end of the algorithm, we are left with</p>
<p>

<indent level="1">

<math>
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 2 \\
0 &amp; 1 &amp; 0 &amp; 3 \\
0 &amp; 0 &amp; 1 &amp; -1
\end{bmatrix}
</math>
</indent>

That is, it is in <link>
reduced row echelon form</link>, or row canonical form.</p>

</sec>
<sec>
<st>
 Other applications </st>

<ss1>
<st>
 Finding the inverse of a matrix </st>
<p>

Suppose <math>A</math> is a <math>n \times n</math> matrix and you need to calculate its <link xlink:type="simple" xlink:href="../122/217122.xml">
inverse</link>. The <math>n \times n</math> <link xlink:type="simple" xlink:href="../718/59718.xml">
identity matrix</link> is augmented to the right of <math>A</math>, forming a <math>n \times 2n</math> matrix (the <link xlink:type="simple" xlink:href="../464/457464.xml">
block matrix</link> <math>B = [A, I]</math>). Through application of elementary row operations and the Gaussian elimination algorithm, the left block of <math>B</math> can be reduced to the identity matrix <math>I</math>, which leaves <math>A^{-1}</math> in the right block of <math>B</math>. </p>
<p>

If the algorithm is unable to reduce <math>A</math> to triangular form, then <math>A</math> is not invertible. </p>
<p>

In practice, inverting a matrix is rarely required. Most of the time, one is really after the solution of a particular system of linear equations.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>

</ss1>
<ss1>
<st>
 The general algorithm to compute ranks and bases  </st>
<p>

The Gaussian elimination algorithm can be applied to any <math>m \times n</math> matrix <math>A</math>. If we get "stuck" in a given column, we move to the next column. In this way, for example, some <math>6 \times 9</math> matrices can be transformed to a matrix that has a reduced row echelon form like
<indent level="1">

<math>
\begin{bmatrix}
1 &amp; * &amp; 0 &amp; 0 &amp; * &amp; * &amp; 0 &amp; * &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; * &amp; * &amp; 0 &amp; * &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; * &amp; * &amp; 0 &amp; * &amp; 0 \\ 
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; * &amp; 0 \\ 
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\ 
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 
\end{bmatrix}
</math>
</indent>

(the *'s are arbitrary entries). This echelon matrix <math>T</math> contains a wealth of information about <math>A</math>: the <link xlink:type="simple" xlink:href="../561/26561.xml">
rank</link> of <math>A</math> is 5 since there are 5 non-zero rows in <math>T</math>; the <link xlink:type="simple" xlink:href="../370/32370.xml">
vector space</link> spanned by the columns of <math>A</math> has a basis consisting of the first, third, fourth, seventh and ninth column of <math>A</math> (the columns of the ones in <math>T</math>), and the *'s tell you how the other columns of <math>A</math> can be written as linear combinations of the basis columns.</p>

</ss1>
</sec>
<sec>
<st>
 Analysis </st>
<p>

Gaussian elimination to solve a system of <it>n</it> equations for <it>n</it> unknowns requires <it>n</it>(<it>n</it>+1) / 2 divisions, (2<it>n</it>3 + 3<it>n</it>2 − 5<it>n</it>)/6 multiplications, and (2<it>n</it>3 + 3<it>n</it>2 − 5<it>n</it>)/6 subtractions,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> for a total of approximately 2<it>n</it>3 / 3 operations. So it has a complexity of <math>\mathcal{O}(n^3)\,</math>.</p>
<p>

This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using <link xlink:type="simple" xlink:href="../237/15237.xml">
iterative method</link>s. Specific methods exist for systems whose coefficients follow a regular pattern (see <link xlink:type="simple" xlink:href="../087/113087.xml">
system of linear equations</link>).</p>
<p>

The Gaussian elimination can be performed over any <link xlink:type="simple" xlink:href="../603/10603.xml">
field</link>.</p>
<p>

Gaussian elimination is <link xlink:type="simple" xlink:href="../807/233807.xml">
numerically stable</link> for <link xlink:type="simple" xlink:href="../748/5262748.xml">
diagonally dominant</link> or <link xlink:type="simple" xlink:href="../326/40326.xml">
positive-definite</link>  matrices. For general matrices, Gaussian elimination is usually considered to be stable in practice if you use <link xlink:type="simple" xlink:href="../503/9811503.xml">
partial pivoting</link> as described below, even though there are examples for which it is unstable.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>

</sec>
<sec>
<st>
 Pseudocode </st>

<p>

As explained above, Gaussian elimination writes a given <it>m</it> &amp;times; <it>n</it> matrix <it>A</it> uniquely as a product of an invertible <it>m</it> &amp;times; <it>m</it> matrix <it>S</it> and a row-echelon matrix <it>T</it>. Here, <it>S</it> is the product of the matrices corresponding to the row operations performed.</p>
<p>

The formal algorithm to compute <math>T</math> from <math>A</math> follows. We write <math>A[i,j]</math> for the entry in row <math>i</math>, column <math>j</math> in matrix <math>A</math>. The transformation is performed "in place", meaning that the original matrix <math>A</math> is lost and successively replaced by <math>T</math>.</p>

<p>

i := 1
j := 1
<b>while</b> (i ≤ m <b>and</b> j ≤ n) <b>do</b>
<it>Find pivot in column j, starting in row i:</it>
maxi := i
<b>for</b> k := i+1 <b>to</b> m <b>do</b>
<b>if</b> abs(A[k,j]) &amp;gt; abs(A[maxi,j]) <b>then</b>
maxi := k
<b>end if</b>
<b>end for</b>
<b>if</b> A[maxi,j] ≠ 0 <b>then</b>
swap rows i and maxi, but do not change the value of i
<it>Now A[i,j] will contain the old value of A[maxi,j].</it>
divide each entry in row i by A[i,j]
<it>Now A[i,j] will have the value 1.</it>
<b>for</b> u := i+1 <b>to</b> m <b>do</b>
subtract A[u,j] * row i from row u
<it>Now A[u,j] will be 0, since A[u,j] - A[i,j] * A[u,j] = A[u,j] - 1 * A[u,j] = 0.</it>
<b>end for</b>
i := i + 1
<b>end if</b>
j := j + 1
<b>end while</b></p>

<p>

This algorithm differs slightly from the one discussed earlier, because before eliminating a variable, it first exchanges rows to move the entry with the largest <link xlink:type="simple" xlink:href="../991/991.xml">
absolute value</link> to the "pivot position". Such "<link xlink:type="simple" xlink:href="../503/9811503.xml">
partial pivoting</link>" improves the numerical stability of the algorithm; some variants are also in use. </p>
<p>

The column currently being transformed is called the pivot column. Proceed from left to right, letting the pivot column be the first column, then the second column, etc. and finally the last column before the vertical line. For each pivot column, do the following two steps before moving on to the next pivot column:
<list>
<entry level="1" type="number">

 Locate the diagonal element in the pivot column. This element is called the pivot. The row containing the pivot is called the pivot row. Divide every element in the pivot row by the pivot to get a new pivot row with a 1 in the pivot position.</entry>
<entry level="1" type="number">

 Get a 0 in each position below the pivot position by subtracting a suitable multiple of the pivot row from each of the rows below it.</entry>
</list>
</p>
<p>

Upon completion of this procedure the augmented matrix will be in <link xlink:type="simple" xlink:href="../215/330215.xml">
row-echelon form</link> and may be solved by back-substitution.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../005/8547005.xml">
Frontal solver</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../689/5134689.xml">
Gauss-Jordan elimination</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Notes </st>
<p>

<reflist>
<entry id="1">
Calinger, pp 234–236</entry>
<entry id="2">
Katz, |18.1.2</entry>
<entry id="3">
Atkinson (1989), p. 514</entry>
<entry id="4">
 <cite id="Reference-Farebrother-1988" style="font-style:normal" class="book">Farebrother, R.W.&#32;(1988). Linear Least Squares Computations, STATISTICS: Textbooks and Monographs.&#32;<link xlink:type="simple" xlink:href="../893/7871893.xml">
Marcel Dekker</link> Inc.,&#32;12. ISBN 0824776615.</cite>&nbsp;</entry>
<entry id="5">
Golub and Van Loan, |3.4.6</entry>
</reflist>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 Atkinson, Kendall A. <it>An Introduction to Numerical Analysis</it>, 2nd edition, John Wiley &amp; Sons, New York, 1989. ISBN 0-471-50023-2.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFCalinger1999" style="font-style:normal">Calinger, Ronald&#32;(1999),&#32;<it>A Contextual History of Mathematics</it>, <link xlink:type="simple" xlink:href="../050/5421050.xml">
Prentice Hall</link>, ISBN 978-0-02-318285-3</cite>&nbsp;.</entry>
<entry level="1" type="bullet">

 Golub, Gene H., and Van Loan, Charles F. <it>Matrix computations</it>, 3rd edition, Johns Hopkins, Baltimore, 1996. ISBN 0-8018-5414-8.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFKatz2004" style="font-style:normal">Katz, Victor J.&#32;(2004),&#32;<it>A History of Mathematics, Brief Version</it>, <link xlink:type="simple" xlink:href="../080/1180080.xml">
Addison-Wesley</link>, ISBN 978-0-321-16193-2</cite>&nbsp;.</entry>
<entry level="1" type="bullet">

 Lipschutz, Seymour, and Lipson, Mark. <it>Schaum's Outlines: Linear Algebra</it>. Tata McGraw-hill edition.Delhi 2001. pp. 69-80.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.math-linux.com/spip.php?article53">
Gaussian elimination</weblink> www.math-linux.com.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www25.brinkster.com/denshade/GaussElimination.html">
Gaussian elimination as java applet</weblink> at some local site. Only takes natural coefficients.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://numericalmethods.eng.usf.edu/topics/gaussian_elimination.html">
Gaussian elimination</weblink> at <weblink xlink:type="simple" xlink:href="http://numericalmethods.eng.usf.edu">
Holistic Numerical Methods Institute</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.hlevkin.com/default.html#numalg">
LinearEquations.c</weblink> Gaussian elimination implemented using C language</entry>
</list>
</p>


</sec>
</bdy>
</article>
