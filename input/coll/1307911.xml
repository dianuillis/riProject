<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:24:24[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Bootstrap aggregating</title>
<id>1307911</id>
<revision>
<id>226460911</id>
<timestamp>2008-07-18T15:23:29Z</timestamp>
<contributor>
<username>Movado73</username>
<id>7285945</id>
</contributor>
</revision>
<categories>
<category>Ensemble learning</category>
<category>Computational statistics</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Bootstrap aggregating</b> (<b>bagging</b>) is a <link xlink:type="simple" xlink:href="../458/774458.xml">
meta-algorithm</link> to improve <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> of <link xlink:type="simple" xlink:href="../426/232426.xml">
classification</link> and <link xlink:type="simple" xlink:href="../568/26568.xml">
regression</link> models in terms of stability and <link xlink:type="simple" xlink:href="../426/232426.xml">
classification</link> accuracy. It also reduces <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link> and helps to avoid <link xlink:type="simple" xlink:href="../332/173332.xml">
overfitting</link>. Although it is usually applied to <link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link> models, it can be used with any type of model. Bagging is a special case of the <link>
model averaging</link> approach.<p>

Given a standard <link xlink:type="simple" xlink:href="../228/1817228.xml">
training set</link> <it>D</it> of size <it>n</it>, bagging generates <it>m</it> new training sets <math>D_i</math> of size <it>n</it>' ≤ <it>n</it>, by <link xlink:type="simple" xlink:href="../361/160361.xml">
sampling</link> examples from <it>D</it> <link xlink:type="simple" xlink:href="../543/23543.xml#xpointer(//*[./st=%22With_finite_support%22])">
uniformly</link> and <link>
with replacement</link>. By sampling with replacement it is likely that some examples will be repeated in 
each <math>D_i</math>. If <it>n</it>'=<it>n</it>, then for large <it>n</it> the set <math>D_i</math> expected to have 63.2% of the unique examples of <it>D</it>, the rest being duplicates. This kind of sample is known as a <link xlink:type="simple" xlink:href="../770/6885770.xml">
bootstrap</link> sample. The <it>m</it> models are fitted using the above <it>m</it> bootstrap samples and combined by averaging the output (for regression) or voting (for classification). </p>
<p>

Since the method averages several predictors, it is not useful for improving linear models.</p>

<sec>
<st>
 Example: Ozone data </st>
<p>

This example is rather artificial, but illustrates the basic principles of bagging.</p>
<p>

Rousseeuw and Leroy (1986) describe a data set concerning ozone levels. The data are available via the <link xlink:type="simple" xlink:href="../495/8495.xml">
classic data sets</link> page. All computations were performed in <link>
 R</link>.</p>
<p>

A scatter plot reveals an apparently non-linear relationship between temperature and ozone. One way to model the relationship is to use a <link xlink:type="simple" xlink:href="../592/4146592.xml">
loess</link> smoother. Such a smoother requires that a span parameter be chosen. In this example, a span of 0.5 was used.</p>
<p>

One hundred <link>
 bootstrap</link> samples of the data were taken, and the <link xlink:type="simple" xlink:href="../592/4146592.xml">
LOESS smoother</link> was fit to each sample. Predictions from these 100 smoothers were then made across the range of the data. The first 10 predicted smooth fits appear as grey lines in the figure below. The lines are clearly very <it>wiggly</it> and they overfit the data - a result of the span being too low.</p>
<p>

The red line on the plot below represents the mean of the 100 smoothers. Clearly, the mean is more stable and there is less <link xlink:type="simple" xlink:href="../332/173332.xml">
overfit</link>. This is the bagged predictor.</p>
<p>

<image width="150px" src="ozone.png">
<caption>

ozone.png
</caption>
</image>
</p>

</sec>
<sec>
<st>
 History </st>

<p>

Bagging (<b>B</b>ootstrap <b>agg</b>regat<b>ing</b>) was proposed by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../283/4909283.xml">
Leo Breiman</link></associate>
</mathematician>
</scientist>
</causal_agent>
</colleague>
</statistician>
</person>
</peer>
</physical_entity>
 in 1994 to improve the classification by combining classifications of randomly generated training sets. See Breiman, 1994. Technical Report No. 421.</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../283/4909283.xml">
Leo Breiman</link></associate>
</mathematician>
</scientist>
</causal_agent>
</colleague>
</statistician>
</person>
</peer>
</physical_entity>
&#32;(1996).&#32;"<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/breiman96bagging.html">
Bagging predictors</weblink>". <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../403/5721403.xml">
Machine Learning</link></periodical>
</it>&#32;<b>24</b>&#32;(2): 123­140. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF00058655">
10.1007/BF00058655</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">S. Kotsiantis, P. Pintelas&#32;(2004).&#32;"<weblink xlink:type="simple" xlink:href="http://www.math.upatras.gr/~esdlab/en/members/kotsiantis/ijci%20paper%20kotsiantis.pdf">
Combining Bagging and Boosting</weblink>". <it><link>
International Journal of Computational Intelligence</link></it>&#32;<b>1</b>&#32;(4): 324–333.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../500/90500.xml">
Boosting</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../612/416612.xml">
Cross validation</link></entry>
</list>
</p>


</sec>
</bdy>
</article>
