<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 01:52:00[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>GoldenGem</title>
<id>13474410</id>
<revision>
<id>233512101</id>
<timestamp>2008-08-22T10:41:33Z</timestamp>
<contributor>
<username>Delaszk</username>
<id>6980814</id>
</contributor>
</revision>
<categories>
<category>Computer system optimization software</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>GoldenGem</b> 
is a <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link> computer program. The default configuration is the standard one, a 
three level perceptron, which was shown simultaneously, but independently, to be a 'universal nonlinear function approximator'
in two articles in the same journal in 1989.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>
<sec>
<st>
Freeware but not open source status</st>
<p>

The program was supported by user registration, and was converted to freeware in 2006 and
2007, first at <link xlink:type="simple" xlink:href="../286/1278286.xml">
download.com</link> and then at the main website. Some software websites continue to post information 
from an earlier pad file relating to a charge for the use of the program, but all links followed will eventually lead to the main site or
download.com. The software is not open source because there is concern whether editors would have adequate technical expertise.</p>

</sec>
<sec>
<st>
Operational modes</st>
<p>

While it can access the end-of-the-day daily stock, index, and bond information provided by
Yahoo, MSN and Google; however a serious user would manage text files of his own data in one of two other formats, either a folder of .csv files or a single .txt file downloaded from other sources such as forexrate.com, or the <central_bank wordnetid="108349916" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../484/4484.xml">
Bank of England</link></central_bank>
, or particular niche commodity prices.
The article on <link xlink:type="simple" xlink:href="../577/112577.xml">
Technical Analysis</link> correctly describes the manner in which neural network software can act
as a bridge between technical analysis and the more highly regarded <link xlink:type="simple" xlink:href="../684/11684.xml">
fundamental analysis</link>.</p>

</sec>
<sec>
<st>
Sources of confusion</st>
<p>

Many users fail to understand the multi-variable nature of the software. A user must provide a set of 
share prices, indices, interest rates, or exchange rates which he already expects may be useful to predict
the share price of interest. This does not require that the input data needs to be correlated with the share price of interest or with itself. The ideal user already understands basic facts and knows how to make estimates and predictions on his own before his first encounter with the program.</p>

</sec>
<sec>
<st>
Training and validation</st>
<p>

Training is accomplished by the use of a logarithmic sensitivity adjustment. Validation is by
a pair of indicator lights. The first indicator light which becomes yellow if both the <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation coefficient</link> and adjusted <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation coefficient</link>
of predicted versus actual change is larger than 0.2 and green if it is larger than 0.5 while the 
second indicator light goes from red to yellow to green as the training input is removed by the
user's control of the sensitivity adjustment. Secondly
visual comparison of the graphs is needed to ensure the correlation coefficient is not large due to isolated coincidental
similarities. Thirdly the user should run a validation data set on an interval in the past.</p>

</sec>
<sec>
<st>
The adjusted correlation coefficient</st>
<p>

The adjusted <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation coefficient</link> is needed because it is possible to obtain a falsely favourable
correlation coefficient during backtesting by a strategy of returning to the known mean value of past
data. A neural network will choose this strategy if it fails to find a relationship. Since 
the known mean value of past data includes time in the relative future during backtesting the
use of the unadjusted correlation coefficient would
incorrectly reward such a strategy, which would not necessarily be profitable
to continue into the future. The adjusted correlation coefficient
is the ordinary correlation coefficient multiplied by the variance of the actual changes 
and divided by the variance of the predicted changes. This becomes small if the neural network
becomes unstable or if it converges upon a strategy of continual sudden return to the past mean value.
The second indicator light shows the minimum of the adjusted and unadjusted correlation coefficients.</p>

</sec>
<sec>
<st>
The transition function</st>
<p>

The transition function is <link xlink:type="simple" xlink:href="../220/374220.xml">
arctan</link> rather than the sometimes used <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../567/56567.xml">
hyperbolic tangent function</link></function>
</mathematical_relation>
.
The reason is that the arctan function is suitable for an analog neural network.</p>

</sec>
<sec>
<st>
Limitations</st>
<p>

The configuration of the program is limited to analyzing the values of a set of variables that change over
time, with the aim of predicting the future value of one of those variables based only on the current value
of all the variables. Therefore it would not be useful for analyzing credit risk at a single point in time,
for example. Also the displays and user interface assume that the data is daily and that no data is provided
on weekends. While this is not an essential feature it would make use of the program too confusing to be used
for intraday data; for example if used for hourly data the tick marks at the bottom of the screen labelled 'weeks' would
refer to five hour intervals, and the slider labelled 'days' being set to 21, in an attempt to exclude
weekends, would set an interval of fifteen hours.</p>
<p>

Whereas the training takes into account the overall relation among the variables throughout
the entire backtesting interval, there is no particular weight attached to, for example, yesterday's values,
or values from last week. A large coincidental change in the values of the variables on the final day of data
would impact upon the prediction. A user can compensate by including what are known as 'stochastics' as 
input variables. The justification for not doing so is that the use of 'stochastics' would degrade
the performance in cases when a change in today's values are genuinely meaningful, and that 
variability in performance due to the dependence on data from only one day should not affect the
overall average performance of the prediction.</p>
<p>

The algorithm is the most widely used and simplest algorithm. Improved algorithms such as <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../821/1448821.xml">
 conjugate gradient</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 may
possibly be superior.</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.goldengem.co.uk">
www.goldengem.co.uk</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
Notes</st>
<p>

<reflist>
<entry id="1">
  K. Funahashi, On the approximate realization of continuous mappings by neural networks, Neural Networks vol 2, 1989</entry>
<entry id="2">
K. Hornik, Multilayer feed-forward networks are universal approximators, Neural Networks, vol 2, 1989</entry>
</reflist>


</p>
</sec>
</bdy>
</article>
