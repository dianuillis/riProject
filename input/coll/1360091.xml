<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:28:20[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Backpropagation</title>
<id>1360091</id>
<revision>
<id>233606899</id>
<timestamp>2008-08-22T20:41:00Z</timestamp>
<contributor>
<username>BOTijo</username>
<id>3729068</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

This article is about the computer algorithm.&#32;&#32;For the biological process, see <link xlink:type="simple" xlink:href="../608/14338608.xml">
Neural backpropagation</link>.&#32;&#32;<p>

<b>Backpropagation</b>, or <b>propagation of error</b>, is a common method of teaching <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link>s how to perform a given task. It was first described by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../764/5693764.xml">
Paul Werbos</link></scientist>
</causal_agent>
</person>
</physical_entity>
 in <link xlink:type="simple" xlink:href="../654/34654.xml">
1974</link>, but it wasn't until <link xlink:type="simple" xlink:href="../761/34761.xml">
1986</link>, through the work of <link xlink:type="simple" xlink:href="../113/2823113.xml">
David E. Rumelhart</link>, <link xlink:type="simple" xlink:href="../174/507174.xml">
Geoffrey E. Hinton</link> and <link>
Ronald J. Williams</link>, that it gained recognition, and it led to a “renaissance” in the field of artificial neural network research.</p>
<p>

It is a <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link> method, and is an implementation of the <link xlink:type="simple" xlink:href="../612/1237612.xml">
Delta rule</link>. It requires a  teacher that knows, or can calculate, the desired output for any given input. It is most useful for <link xlink:type="simple" xlink:href="../759/574759.xml">
feed-forward</link> networks (networks that have no feedback, or simply, that have no connections that loop).  The term is an abbreviation for "backwards propagation of errors".  Backpropagation requires that the <link xlink:type="simple" xlink:href="../835/14179835.xml">
activation function</link> used by the <link xlink:type="simple" xlink:href="../771/349771.xml">
artificial neuron</link>s (or "nodes") is <link xlink:type="simple" xlink:href="../921/7921.xml">
differentiable</link>.</p>

<sec>
<st>
Summary</st>
<p>

Summary of the technique:
<list>
<entry level="1" type="number">

 Present a training sample to the neural network.</entry>
<entry level="1" type="number">

 Compare the network's output to the desired output from that sample. Calculate the error in each output neuron.</entry>
<entry level="1" type="number">

 For each neuron, calculate what the output should have been, and a <it>scaling factor</it>, how much lower or higher the output must be adjusted to match the desired output. This is the local error.</entry>
<entry level="1" type="number">

 Adjust the weights of each neuron to lower the local error.</entry>
<entry level="1" type="number">

 Assign "blame" for the local error to neurons at the previous level, giving greater responsibility to neurons connected by stronger weights.</entry>
<entry level="1" type="number">

 Repeat from step 3 on the neurons at the previous level, using each one's "blame" as its error..</entry>
</list>
</p>

</sec>
<sec>
<st>
Algorithm</st>
<p>

Actual algorithm for a 3-layer network (only one hidden layer):</p>
<p>

Initialize the weights in the network (often randomly)
Do
For each example e in the training set
O = neural-net-output(network, e) ; forward pass
T = teacher output for e
Calculate error (T - O) at the output units
Compute delta_wi for all weights from hidden layer to output layer ; backward pass
Compute delta_wi for all weights from input layer to hidden layer ; backward pass continued
Update the weights in the network
Until all examples classified correctly or stopping criterion satisfied
Return the network</p>
<p>

As the <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>'s name implies, the errors (and therefore the learning) propagate backwards from the output nodes to the inner nodes. So technically speaking, backpropagation is used to calculate the gradient of the error of the network with respect to the network's modifiable weights. This gradient is almost always then used in a simple <link xlink:type="simple" xlink:href="../641/1180641.xml">
stochastic gradient descent</link> algorithm to find weights that minimize the error.  Often the term "backpropagation" is used in a more general sense, to refer to the entire procedure encompassing both the calculation of the gradient and its use in stochastic gradient descent. Backpropagation usually allows quick convergence on satisfactory <link xlink:type="simple" xlink:href="../420/298420.xml">
local minima</link> for error in the kind of networks to which it is suited.</p>
<p>

It is important to note that backpropagation networks are necessarily <link xlink:type="simple" xlink:href="../644/2266644.xml">
multilayer perceptrons</link> (usually with one input, one hidden, and one output layer). In order for the hidden layer to serve any useful function, multilayer networks must have non-linear activation functions for the multiple layers: a multilayer network using only linear activiation functions is equivalent to some single layer, linear network. Non-linear activation functions that are commonly used include the <link xlink:type="simple" xlink:href="../563/84563.xml">
logistic function</link>, the <link xlink:type="simple" xlink:href="../185/6152185.xml">
softmax function</link>, and the <link xlink:type="simple" xlink:href="../552/245552.xml">
gaussian function</link>.</p>
<p>

The backpropagation algorithm for calculating a gradient has been rediscovered a number of times, and is a special case of a more general technique called <link xlink:type="simple" xlink:href="../787/734787.xml">
automatic differentiation</link> in the reverse accumulation mode.</p>
<p>

It is also closely related to the <link>
Gauss–Newton algorithm</link>, and is also part of continuing research in <link xlink:type="simple" xlink:href="../608/14338608.xml">
neural backpropagation</link>.</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 Chapter 7 <weblink xlink:type="simple" xlink:href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf">
The backpropagation algorithm</weblink> of <weblink xlink:type="simple" xlink:href="http://page.mi.fu-berlin.de/rojas/neural/index.html.html">
<it>Neural Networks - A Systematic Introduction''</it></weblink> by <link>
Raúl Rojas</link> (ISBN 978-3540605058)</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://neurondotnet.freehostia.com">
NeuronDotNet - A modular implementation of artificial neural networks in C# along with sample applications</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.codeproject.com/KB/recipes/BP.aspx">
Implementation of BackPropagation in C++</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.codeproject.com/KB/cs/BackPropagationNeuralNet.aspx">
Implementation of BackPropagation in C#</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://ai4r.rubyforge.org/neuralNetworks.html">
Implementation of BackPropagation in Ruby</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.tek271.com/articles/neuralNet/IntoToNeuralNets.html">
Quick explanation of the backpropagation algorithm</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html">
Graphical explanation of the backpropagation algorithm</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.speech.sri.com/people/anand/771/html/node37.html">
Concise explanation of the backpropagation algorithm using math notation</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://fbim.fh-regensburg.de/~saj39122/jfroehl/diplom/e-13-text.html">
Detailed numeric demonstration of how backpropagation algorithms work</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://en.wikiversity.org/wiki/Learning_and_Neural_Networks">
Backpropagation neural network tutorial at the Wikiversity</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
