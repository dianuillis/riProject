<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:28:17[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<plant  confidence="0.8" wordnetid="100017222">
<tree  confidence="0.8" wordnetid="113104059">
<vascular_plant  confidence="0.8" wordnetid="113083586">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<woody_plant  confidence="0.8" wordnetid="113103136">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Random forest</title>
<id>1363880</id>
<revision>
<id>231442633</id>
<timestamp>2008-08-12T12:48:46Z</timestamp>
<contributor>
<username>Randomexpert</username>
<id>7637990</id>
</contributor>
</revision>
<categories>
<category>Decision trees</category>
<category>Statistical randomness</category>
<category>Ensemble learning</category>
<category>Machine learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, a <b>random forest</b>  is a <link xlink:type="simple" xlink:href="../244/1579244.xml">
classifier</link> that consists of many <link xlink:type="simple" xlink:href="../003/577003.xml">
decision trees</link> and outputs the class that is the <link xlink:type="simple" xlink:href="../127/1432127.xml">
mode</link> of the classes output by individual trees.  The algorithm for inducing a random forest was developed by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../283/4909283.xml">
Leo Breiman</link></associate>
</mathematician>
</scientist>
</causal_agent>
</colleague>
</statistician>
</person>
</peer>
</physical_entity>
 and <link>
Adele Cutler</link>, and "Random Forests" is their <link xlink:type="simple" xlink:href="../023/18935023.xml">
trademark</link>. The term came from <b>random decision forests</b> that was first proposed by <link>
Tin Kam Ho</link> of Bell Labs in 1995. The method combines Breiman's "<link xlink:type="simple" xlink:href="../911/1307911.xml">
bagging</link>" idea and Ho's "random subspace method" to construct a collection of decision trees with controlled variations. 
<sec>
<st>
 Learning algorithm </st>
<p>

Each tree is constructed using the following <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>:
<list>
<entry level="1" type="number">

 Let the number of training cases be <it>N</it>, and the number of variables in the classifier be <it>M</it>.  </entry>
<entry level="1" type="number">

 We are told the number <it>m</it> of input variables to be used to determine the decision at a node of the tree; <it>m</it> should be much less than <it>M</it>.</entry>
<entry level="1" type="number">

 Choose a training set for this tree by choosing <it>N</it> times with replacement from all <it>N</it> available training cases (i.e. take a <link xlink:type="simple" xlink:href="../770/6885770.xml">
bootstrap</link> sample).  Use the rest of the cases to estimate the error of the tree, by predicting their classes.</entry>
<entry level="1" type="number">

 For each node of the tree, randomly choose <it>m</it> variables on which to base the decision at that node.  Calculate the best split based on these m variables in the training set.</entry>
<entry level="1" type="number">

 Each tree is fully grown and not <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../075/5462075.xml">
pruned</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (as may be done in constructing a normal tree classifier).</entry>
</list>
</p>

</sec>
<sec>
<st>
 Advantages </st>
<p>

The advantages of random forest are:
<list>
<entry level="1" type="bullet">

 For many data sets, it produces a highly accurate classifier.</entry>
<entry level="1" type="bullet">

 It handles a very large number of input variables.</entry>
<entry level="1" type="bullet">

 It estimates the importance of variables in determining classification.</entry>
<entry level="1" type="bullet">

 It generates an internal unbiased estimate of the generalization error as the forest building progresses.</entry>
<entry level="1" type="bullet">

 It includes a good method for estimating missing data and maintains accuracy when a large proportion of the data are missing.</entry>
<entry level="1" type="bullet">

 It provides an experimental way to detect variable interactions.</entry>
<entry level="1" type="bullet">

 It can balance error in class population unbalanced data sets.</entry>
<entry level="1" type="bullet">

 It computes proximities between cases, useful for <link xlink:type="simple" xlink:href="../172/39172.xml">
clustering</link>, detecting <link xlink:type="simple" xlink:href="../951/160951.xml">
outlier</link>s, and (by scaling) visualizing the data.</entry>
<entry level="1" type="bullet">

 Using the above, it can be extended to unlabeled data, leading to unsupervised clustering, outlier detection and data views. </entry>
<entry level="1" type="bullet">

 Learning is fast.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://cm.bell-labs.com/cm/cs/who/tkh/papers/odt.pdf">
Ho, Tin Kam (1995). "Random Decision Forest". Proc. of the 3rd Int'l Conf. on Document Analysis and Recognition,  Montreal, Canada, August 14-18, 1995, 278-282</weblink> (Preceding Work)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://cm.bell-labs.com/cm/cs/who/tkh/papers/df.pdf">
Ho, Tin Kam (1998). "The Random Subspace Method for Constructing Decision Forests".  IEEE Trans. on Pattern Analysis and Machine Intelligence 20 (8), 832-844</weblink> (Preceding Work)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cis.jhu.edu/publications/papers_in_database/GEMAN/shape.pdf">
Amit, Yali and Geman, Donald (1997) "Shape quantization and recognition with randomized trees". Neural Computation 9, 1545-1588.</weblink> (Preceding work)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ics.uci.edu/~liang/seminars/win05/papers/wald2002-2.pdf">
Breiman, Leo "Looking Inside The Black Box". Wald Lecture II</weblink> (Lecture)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/u0p06167n6173512/fulltext.pdf">
Breiman, Leo (2001).  "Random Forests".  Machine Learning 45 (1), 5-32</weblink> (Original Article)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://stat-www.berkeley.edu/users/breiman/RandomForests/cc_home.htm">
Random Forest classifier description</weblink> (Site of Leo Breiman)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://cran.r-project.org/doc/Rnews/Rnews_2002-3.pdf">
Liaw, Andy &amp; Wiener, Matthew "Classification and Regression by randomForest" R News (2002) Vol. 2/3 p. 18</weblink> (Discussion of the use of the random forest package for <link xlink:type="simple" xlink:href="../707/376707.xml">
R</link>)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://cm.bell-labs.com/cm/cs/who/tkh/papers/compare.pdf">
Ho, Tin Kam (2002). "A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors". Pattern Analysis and Applications 5, p. 102-112</weblink> (Comparison of bagging and random subspace method) </entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007/978-3-540-74469-6_35">
Prinzie, A., Van den Poel, D. (2007). Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB, Dexa 2007, Lecture Notes in Computer Science, 4653, 349-358.</weblink> Generalizing Random Forest framework to other methods. The paper introduces Random MNL and Random NB as two generalizations of Random Forests.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.eswa.2007.01.029">
Prinzie, A., Van den Poel, D. (2008). Random Forests for multiclass classification: Random MultiNomial Logit, Expert Systems with Applications, 34(3), 1721-1732.</weblink> Generalization of Random Forests to choice models like the Multinomial Logit Model (MNL): Random Multinomial Logit.</entry>
</list>
</p>


</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../079/9448079.xml">
Random multinomial logit</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../887/18824887.xml">
Random naive bayes</link></entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</article>
