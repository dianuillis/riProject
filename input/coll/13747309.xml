<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:06:10[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>DBSCAN</title>
<id>13747309</id>
<revision>
<id>239003052</id>
<timestamp>2008-09-17T11:33:47Z</timestamp>
<contributor>
<username>Zorglbot</username>
<id>908518</id>
</contributor>
</revision>
<categories>
<category>Articles with invalid date parameter in template</category>
<category>All articles to be expanded</category>
<category>Articles to be expanded since October 2007</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-notice" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="44px" src="Wiki_letter_w.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=DBSCAN&amp;action=edit">
improve this article or section</weblink> by expanding it.</b> Further information might be found on the  or at . 
<it>(October 2007)''</it></col>
</row>
</table>


<b>DBSCAN</b> (<b>Density-Based Spatial Clustering of Applications with Noise</b>) is a <link xlink:type="simple" xlink:href="../675/669675.xml">
data clustering</link> algorithm proposed by <link>
Martin Ester</link>, <link>
Hans-Peter Kriegel</link>, <link>
JÃ¶rg Sander</link> and <link>
Xiaovei Xui</link> in <link xlink:type="simple" xlink:href="../636/34636.xml">
1996</link>. It is a density based clustering algorithm because it finds a number of clusters starting from the estimated density distribution of corresponding nodes. DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature. The basic DBSCAN algorithm has been used as a base for many other developments. Such as paralellisation by <link>
Domenica Arlia</link> and <link>
Massimo Coppola</link> or an enhancement of the data set background to support <link xlink:type="simple" xlink:href="../043/19058043.xml">
uncertain data</link> presented by <link>
Dirk Habich</link> and <link>
Peter Benjamin Volk</link>.

<sec>
<st>
Steps</st>

<p>

DBScan requires two parameters: epsilon (eps) and minimum points (minPts). It starts with an arbitrary starting point that has not been visited. It then finds all the neighbour points within distance eps of the starting point. </p>
<p>

If the number of neighbors is greater than or equal to minPts, a cluster is formed. The starting point and its neighbors are added to this cluster and the starting point is marked as visited. The algorithm then repeats the evaluation process for all the neighbours recursively.</p>
<p>

If the number of neighbors is less than minPts, the point is marked as noise.</p>
<p>

If a cluster is fully expanded (all points within reach are visited) then the algorithm proceeds to iterate through the remaining unvisited points in the dataset.</p>


</sec>
<sec>
<st>
Pseudocode</st>

<p>

C = 0
for each unvisited point P in dataset D
N = getNeighbors (P, epsilon)
if (sizeof(N)  minPts)
mark P as NOISE
else
++C
mark P as visited
add P to cluster C
recurse (N)</p>

</sec>
<sec>
<st>
Advantages</st>

<p>

<list>
<entry level="1" type="number">

 DBScan does not require you to know the number of clusters in the data a priori. Compare this with <information wordnetid="105816287" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../407/1860407.xml">
k-means</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</datum>
</rule>
</event>
</information>
.</entry>
<entry level="1" type="number">

 DBScan does not have a bias towards a particular cluster shape or size. Compare this with k-means.</entry>
<entry level="1" type="number">

 DBScan is resistant to noise and provides a means of filtering for noise if desired.</entry>
</list>
</p>

</sec>
<sec>
<st>
Disadvantages</st>

<p>

<list>
<entry level="1" type="number">

 DBSCAN can only result in a good clustering as good as its <link>
distance measure</link> is in the function getNeighbors(P,epsilon). The most common distance metric used is the <link xlink:type="simple" xlink:href="../932/53932.xml">
euclidean distance</link> measure. Also any other arbitrary distance measure can be used to influence the behaviour of the resulting cluster. </entry>
<entry level="1" type="number">

 DBScan does not respond well to data sets with varying densities so called hierarchical data sets.</entry>
</list>
</p>
<p>

Compare to other clustering algorithms, such as K-Means, DBSCAN does not scale very well in data volume. If more data points are added to the clustering data set the slower the algorithms gets due to the nature of the getNeighbors(P,epsilon) function.</p>

</sec>
<sec>
<st>
Nearest Neighborhood Detection</st>
<p>

A nearest neighborhood detection takes place in the getNeighbors(P,epsilon) function. For a data point P all data points must be detected that are within the range of epsilon, based on the distance function used in the algorithm. The detection requires that a <link xlink:type="simple" xlink:href="../350/831350.xml">
distance matrix</link> is calculated for the whole data set. The generation of the distance matrix has a complexity of O((n2-n)/2) since only an upper matrix is needed. Within the distance matrix the nearest neighbors can be detected by selecting a tuple with minimums functions over the rows and columns. Research has been pushing the neighborhood detection into traditional databases to enhance the speed of the detection. Databases solve the neighborhood problem with <link xlink:type="simple" xlink:href="../270/15270.xml">
indexes</link> specifically designed for this type of application</p>

</sec>
<sec>
<st>
Parameter Estimation</st>
<p>

Every data mining task has the problem of parameters. Every parameter influences the algorithm in specifc ways. For DBSCAN the parameters epsilon and MinPnts are needed. The parameters must be specified by the user of the algorithms since other data sets and other questions require different parameters. An initial value for epsilon can be determined by a <link>
k-distance graph</link>. As a thumbrule k is determined with D (number of dimensions in the data set): k=D+1.</p>
<p>

Although this parameter estimation gives a sufficient initial parameter set the resulting clustering can turn out to be not the expected partitioning. Therefore research has been performed on incrementally optimizing the parameters against a specific target value. </p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal"> "A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise".&#32;<it>Proceedings of 2nd International Conference on KDD</it>, AAAI Press. Retrieved on <link>
2007-10-15</link>.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal"> "Experiments in Parallel Clustering with DBSCAN".&#32;<it>Euro-Par 2001: Parallel Processing: 7th International Euro-Par Conference Manchester, UK August 28-31, 2001, Proceedings</it>, Springer Berlin. Retrieved on <link>
2004-02-19</link>.</cite>&nbsp;</entry>
</list>
</p>


</sec>
</bdy>
</article>
