<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:18:24[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Maximum likelihood</title>
<id>140806</id>
<revision>
<id>243890441</id>
<timestamp>2008-10-08T14:04:53Z</timestamp>
<contributor>
<username>Davyzhu</username>
<id>4891636</id>
</contributor>
</revision>
<categories>
<category>Estimation theory</category>
<category>Statistics articles linked to the portal</category>
<category>Statistics articles with navigational template</category>
</categories>
</header>
<bdy>

<b>Maximum likelihood estimation</b> (<b>MLE</b>) is a popular <link xlink:type="simple" xlink:href="../685/26685.xml">
statistical</link> method used for fitting a mathematical model to some data. The modeling of real world data using estimation by maximum likelihood offers a way of tuning the free parameters of the model to provide a good fit.<p>

The method was pioneered by <link xlink:type="simple" xlink:href="../154/195154.xml">
geneticist</link> and <link xlink:type="simple" xlink:href="../661/48661.xml">
statistician</link> <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../824/140824.xml">
Sir R. A. Fisher</link></scientist>
</person>
 between 1912 and 1922. It has widespread applications in various fields, including:
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../904/17904.xml">
linear model</link>s and <link xlink:type="simple" xlink:href="../122/747122.xml">
generalized linear model</link>s;</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../322/30322.xml">
communication systems</link>;</entry>
<entry level="1" type="bullet">

 exploratory and confirmatory factor analysis;</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../748/2007748.xml">
structural equation modeling</link>;</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../982/24982.xml">
psychometrics</link> and <link xlink:type="simple" xlink:href="../390/10390.xml">
econometrics</link>;</entry>
<entry level="1" type="bullet">

 time-delay of arrival (TDOA) in acoustic or electromagnetic detection;</entry>
<entry level="1" type="bullet">

 data modeling in nuclear and particle physics;</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../130/3986130.xml">
computational phylogenetics</link>;</entry>
<entry level="1" type="bullet">

 origin/destination and path-choice modeling in transport networks;</entry>
<entry level="1" type="bullet">

 many situations in the context of <link xlink:type="simple" xlink:href="../284/30284.xml">
hypothesis testing</link> and <link xlink:type="simple" xlink:href="../911/280911.xml">
confidence interval</link> formation.</entry>
</list>
</p>
<p>

The method of maximum likelihood corresponds to many well-known estimation methods in statistics. For example, suppose you are interested in the heights of Americans.  You have a sample of some number of Americans, but not the entire population, and record their heights.  Further, you are willing to assume that heights are <link xlink:type="simple" xlink:href="../462/21462.xml">
normally distributed</link> with some unknown <link xlink:type="simple" xlink:href="../192/19192.xml">
mean</link> and <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link>. The sample mean is then the maximum likelihood estimator of the population mean, and the sample variance is a close approximation to the maximum likelihood estimator of the population variance (see examples below).</p>
<p>

For a fixed set of data and underlying probability model, maximum likelihood picks the values of the model parameters that make the data "more likely" than any other values of the parameters would make them. Maximum likelihood estimation gives a unique and easy way to determine solution in the case of the <link xlink:type="simple" xlink:href="../462/21462.xml">
normal distribution</link> and many other problems, although in very complex problems this may not be the case.  If a <link xlink:type="simple" xlink:href="../835/5509835.xml">
uniform</link> <link xlink:type="simple" xlink:href="../877/472877.xml">
prior distribution</link> is assumed over the parameters, the maximum likelihood estimate coincides with the <link xlink:type="simple" xlink:href="../433/1792433.xml">
most probable</link> values thereof. </p>

<sec>
<st>
Prerequisites</st>

<p>

The following discussion assumes that readers are familiar with basic notions in <link xlink:type="simple" xlink:href="../542/23542.xml">
probability theory</link> such as <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link>s, <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link>s, <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>s and <link xlink:type="simple" xlink:href="../653/9653.xml">
expectation</link>. It also assumes they are familiar with standard basic techniques of maximizing <link xlink:type="simple" xlink:href="../122/6122.xml">
continuous</link> <link xlink:type="simple" xlink:href="../491/19725491.xml">
real-valued</link> <link xlink:type="simple" xlink:href="../427/185427.xml">
function</link>s, such as using <link xlink:type="simple" xlink:href="../335/61335.xml">
differentiation</link> to find a function's <link xlink:type="simple" xlink:href="../420/298420.xml">
maxima</link>.</p>

</sec>
<sec>
<st>
 Principles </st>

<p>

Consider a family <math>D_\theta</math> of probability distributions parameterized by an unknown parameter <math>\theta</math> (which could be vector-valued), associated with either a known <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link> (continuous distribution) or a known <link xlink:type="simple" xlink:href="../725/154725.xml">
probability mass function</link> (discrete distribution), denoted as <math>f_\theta</math>. We draw a sample <math>x_1,x_2,\dots,x_n</math> of <it>n</it> values from this distribution, and then using <math>f_\theta</math> we compute the (multivariate) probability density associated with our observed data, <math> f_\theta(x_1,\dots,x_n).\,\!</math></p>
<p>

As a function of &amp;theta; with <it>x</it>1, ..., <it>xn</it> fixed, this is the <link xlink:type="simple" xlink:href="../968/44968.xml">
likelihood function</link></p>
<p>

<indent level="1">

<math> \mathcal{L}(\theta) = f_{\theta}(x_1,\dots,x_n).\,\!</math>
</indent>

The method of maximum likelihood estimates &amp;theta; by finding the value of &amp;theta; that maximizes <math>\mathcal{L}(\theta)</math>. This is the <b>maximum likelihood estimator</b> (<b>MLE</b>) of &amp;theta;:</p>
<p>

<indent level="1">

<math>\widehat{\theta} = \underset{\theta}{\operatorname{arg\ max}}\ \mathcal{L}(\theta).</math>
</indent>

From a simple point of view, the outcome of a maximum likelihood analysis is the maximum likelihood estimate. This can be supplemented by an approximation for the <link xlink:type="simple" xlink:href="../752/191752.xml">
covariance matrix</link> of the MLE, where this approximation is derived from the <link xlink:type="simple" xlink:href="../968/44968.xml">
likelihood function</link>. A more complete outcome from a maximum likelihood analysis would be the likelihood function itself, which can be used to construct improved versions of <link xlink:type="simple" xlink:href="../911/280911.xml">
confidence interval</link>s compared to those obtained from the approximate variance matrix. See also <link xlink:type="simple" xlink:href="../035/45035.xml">
Likelihood Ratio Test</link></p>
<p>

Commonly, one assumes that the data drawn from a particular distribution are <link xlink:type="simple" xlink:href="../067/453067.xml">
independent, identically distributed</link> (iid) with unknown parameters. This considerably simplifies the problem because the likelihood can then be written as a product of <it>n</it> univariate probability densities:</p>
<p>

<indent level="1">

<math>\mathcal{L}(\theta) = \prod_{i=1}^n f_{\theta}(x_i)</math>
</indent>

and since maxima are unaffected by monotone transformations, one can take the logarithm of this expression to turn it into a sum:</p>
<p>

<indent level="1">

<math>\mathcal{L}^*(\theta) = \sum_{i=1}^n \log f_{\theta}(x_i).</math>
</indent>

The maximum of this expression can then be found numerically using various <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> algorithms.</p>
<p>

This contrasts with seeking an <link xlink:type="simple" xlink:href="../479/8450479.xml">
unbiased estimator</link> of &amp;theta;, which may not necessarily yield the MLE but which will yield a value that (on average) will neither tend to over-estimate nor under-estimate the true value of &amp;theta;.</p>
<p>

Note that the maximum likelihood estimator may not be unique, or indeed may not even exist.</p>

</sec>
<sec>
<st>
Properties</st>


<ss1>
<st>
 Functional invariance </st>
<p>

The maximum likelihood estimator selects the parameter value which gives the observed data the largest possible probability (or probability density, in the continuous case). If the parameter consists of a number of components, then we define their separate maximum likelihood estimators, as the corresponding component of the MLE of the complete parameter. Consistent with this, if <math>\widehat{\theta}</math> is the MLE for <it>&amp;theta;</it>, and if <it>g</it> is any function of <it>&amp;theta;</it>, then the MLE for <it>&amp;alpha;</it> = <it>g</it>(<it>&amp;theta;</it>) is by definition</p>
<p>

<indent level="1">

<math>\widehat{\alpha} = g(\widehat{\theta}).\,\!</math>
</indent>

It maximizes the so-called profile likelihood:</p>
<p>

<indent level="1">

<math>\bar{L}(\alpha) = \sup_{\theta: \alpha = g(\theta)} L(\theta).</math>
</indent>

</p>
</ss1>
<ss1>
<st>
Bias</st>
<p>

For small numbers of samples, the <link xlink:type="simple" xlink:href="../479/8450479.xml">
bias</link> of maximum-likelihood estimators can be substantial. Consider a case where <it>n</it> tickets numbered from 1 to <it>n</it> are placed in a box and one is selected at random (<it>see <link xlink:type="simple" xlink:href="../835/5509835.xml">
uniform distribution</link></it>).  If <it>n</it> is unknown, then the maximum-likelihood estimator of <it>n</it> is the number on the drawn ticket. The <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> of the number on the drawn ticket, and therefore the expected value of <it>n</it>, is (<it>n</it>+1)/2. As a result, the maximum likelihood estimator for <it>n</it> will systematically underestimate <it>n</it> by (n-1)/2. In estimating the highest number <it>n</it>, we can only be certain that it is greater than or equal to the drawn ticket number.</p>

</ss1>
<ss1>
<st>
 Asymptotics </st>

<p>

In many cases, estimation is performed using a set of <link xlink:type="simple" xlink:href="../067/453067.xml">
independent identically distributed</link> measurements. These may correspond to distinct elements from a random <link xlink:type="simple" xlink:href="../586/27586.xml">
sample</link>, repeated observations, etc. In such cases, it is of interest to determine the behavior of a given estimator as the number of measurements increases to infinity, referred to as <it><link xlink:type="simple" xlink:href="../503/3469503.xml">
asymptotic</link> behaviour</it>. </p>
<p>

Under certain (fairly weak) regularity conditions, which are listed below, the MLE exhibits several characteristics which can be interpreted to mean that it is "asymptotically optimal". These characteristics include:</p>
<p>

<list>
<entry level="1" type="bullet">

 The MLE is <b>asymptotically unbiased</b>, i.e., its <link xlink:type="simple" xlink:href="../479/8450479.xml">
bias</link> tends to zero as the number of samples increases to infinity.</entry>
<entry level="1" type="bullet">

 The MLE is <b>asymptotically <link xlink:type="simple" xlink:href="../392/574392.xml">
efficient</link></b>, i.e., it achieves the <link>
Cramér-Rao lower bound</link> when the number of samples tends to infinity. This means that, asymptotically, no unbiased estimator has lower <link xlink:type="simple" xlink:href="../816/201816.xml">
mean squared error</link> than the MLE.</entry>
<entry level="1" type="bullet">

 The MLE is <b><link xlink:type="simple" xlink:href="../043/10043.xml#xpointer(//*[./st=%22Asymptotic_normality%22])">
asymptotically normal</link></b>. As the number of samples increases, the distribution of the MLE tends to the Gaussian distribution with mean <math>\theta</math> and covariance matrix equal to the inverse of the <link xlink:type="simple" xlink:href="../971/598971.xml">
Fisher information</link> matrix.</entry>
</list>
</p>
<p>

Since the Cramér-Rao bound only speaks of unbiased estimators while the maximum likelihood estimator is usually biased, asymptotic efficiency as defined here does not mean anything: perhaps there are other nearly unbiased estimators with much smaller variance. However, it can be shown that among all regular estimators, which are estimators which have an asymptotic distribution which is not dramatically disturbed by small changes in the parameters, the asymptotic distribution of the maximum likelihood estimator is the best possible, i.e., most concentrated. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>
<p>

Some regularity conditions which ensure this behavior are:
<list>
<entry level="1" type="number">

 The first and second derivatives of the log-likelihood function must be defined.</entry>
<entry level="1" type="number">

 The Fisher information matrix must not be zero, and must be continuous as a function of the parameter.</entry>
<entry level="1" type="number">

 The maximum likelihood estimator is consistent.</entry>
</list>
</p>
<p>

By the mathematical meaning of the word asymptotic, asymptotic properties are properties which only approached in the limit of larger and larger samples: they are approximately true when the sample size is large enough. The theory does not tell us how large the sample needs to be in order to obtain a good enough degree of approximation. Fortunately, in practice they often appear to be approximately true, when the sample size is moderately large.  So in practice, inference about the estimated parameters is often based on the asymptotic Gaussian distribution of the MLE. When we do this, the Fisher information matrix is usefully estimated by the <link>
observed information matrix</link>.</p>
<p>

Some cases where the asymptotic behaviour described above does not hold are outlined next.</p>
<p>

<b>Estimate on boundary.</b> Sometimes the maximum likelihood estimate lies on the boundary of the set of possible parameters, or (if the boundary is not, strictly speaking, allowed) the likelihood gets larger and larger as the parameter approaches the boundary. Standard asymptotic theory needs the assumption that the true parameter value lies away from the boundary.  If we have enough data, the maximum likelihood estimate will keep away from the boundary too. But with smaller samples, the estimate can lie on the boundary. In such cases, the asymptotic theory clearly does not give a practically useful approximation. Examples here would be variance-component models, where each component of variance, σ2, must satisfy the constraint σ2 &amp;ge;0.</p>
<p>

<b>Data boundary parameter-dependent.</b> For the theory to apply in a simple way, the set of data values which has positive probability (or positive probability density) should not depend on the unknown parameter. A simple example where such parameter-dependence does hold is the case of estimating θ from a set of independent identically distributed when the common distribution is <link xlink:type="simple" xlink:href="../835/5509835.xml">
uniform</link> on the range (0,θ). For estimation purposes the relevant range of θ is such that θ cannot be less than the largest observation. In this instance the maximum likelihood estimate exists and has some good behaviour, but the asymptotics are not as outlined above.</p>
<p>

<b>Nuisance parameters.</b> For maximum likelihood estimations, a model may have a number of <link xlink:type="simple" xlink:href="../528/979528.xml">
nuisance parameter</link>s. For the asymptotic behaviour outlined to hold, the number of nuisance parameters should not increase with the number of observations (the sample size). A well-known example of this case is where observations occur as pairs, where the observations in each pair have a different (unknown) mean but otherwise the observations are independent and Normally distributed with a common variance. Here for 2<it>N</it> observations, there are <it>N</it>+1 parameters. It is well-known that the maximum likelihood estimate for the variance does not converge to the true value of the variance.</p>
<p>

<b>Increasing information.</b> For the asymptotics to hold in cases where the assumption of <link xlink:type="simple" xlink:href="../067/453067.xml">
independent identically distributed</link> observations does not hold, a basic requirement is that the amount of information in the data increases indefinitely as the sample size increases. Such a requirement may not be met if either there is too much dependence in the data (for example, if new observations are essentially identical to existing observations), or if new independent observations are subject to an increasing observation error.</p>

</ss1>
</sec>
<sec>
<st>
Examples</st>


<ss1>
<st>
Discrete distribution, finite parameter space</st>

<p>

Consider tossing an <link xlink:type="simple" xlink:href="../100/9597100.xml">
unfair coin</link> 80 times (i.e., we sample something like <it>x</it>1=H, <it>x</it>2=T, ..., <it>x</it>80=T, and count the number of HEADS "H" observed). Call the probability of tossing a HEAD <it>p</it>, and the probability of tossing TAILS 1-<it>p</it> (so here <it>p</it> is <it>&amp;theta;</it> above). Suppose we toss 49 HEADS and 31 TAILS, and suppose the coin was taken from a  box containing three coins: one which gives HEADS with probability <it>p</it>=1/3, one which gives HEADS with probability <it>p</it>=1/2 and another which gives HEADS with probability <it>p</it>=2/3. The coins have lost their labels, so we don't know which one it was. Using <b>maximum likelihood estimation</b> we can calculate which coin has the largest likelihood, given the data that we observed. The likelihood function (defined below) takes one of three values:</p>
<p>

<indent level="1">

<math>
\begin{matrix}
\Pr(\mathrm{H} = 49 \mid p=1/3) &amp; = &amp; \binom{80}{49}(1/3)^{49}(1-1/3)^{31} \approx 0.000 \\
&amp;&amp;\\
\Pr(\mathrm{H} = 49 \mid p=1/2) &amp; = &amp; \binom{80}{49}(1/2)^{49}(1-1/2)^{31} \approx 0.012 \\
&amp;&amp;\\
\Pr(\mathrm{H} = 49 \mid p=2/3) &amp; = &amp; \binom{80}{49}(2/3)^{49}(1-2/3)^{31} \approx 0.054 \\
\end{matrix}
</math>
</indent>

We see that the likelihood is maximized when <it>p</it>=2/3, and so this is our <it>maximum likelihood estimate</it> for <it>p</it>.</p>

</ss1>
<ss1>
<st>
Discrete distribution, continuous parameter space</st>

<p>

Now suppose we had only one coin but its <it>p</it> could have been any value 0 &amp;le; <it>p</it> &amp;le; 1. We must maximize the likelihood function:</p>
<p>

<indent level="1">

<math>
L(\theta) = f_D(\mathrm{H} = 49 \mid p) = \binom{80}{49} p^{49}(1-p)^{31}
</math>
</indent>

over all possible values 0 &amp;le; <it>p</it> &amp;le; 1.</p>
<p>

One way to maximize this function is by <link xlink:type="simple" xlink:href="../921/7921.xml">
differentiating</link> with respect to <it>p</it> and setting to zero:</p>
<p>

<indent level="1">

<math>
\begin{align}
{0}&amp;{} = \frac{\partial}{\partial p} \left( \binom{80}{49} p^{49}(1-p)^{31} \right) \\
  &amp; {}\propto 49p^{48}(1-p)^{31} - 31p^{49}(1-p)^{30} \\
  &amp; {}= p^{48}(1-p)^{30}\left[ 49(1-p) - 31p \right]  \\
  &amp; {}= p^{48}(1-p)^{30}\left[ 49 - 80p \right]
\end{align}
</math>
</indent>

<image width="200px" src="BinominalLikelihoodGraph.png" type="thumb">
<caption>

Likelihood of different proportion parameter values for a binomial process with <it>t</it> = 3 and <it>n</it> = 10; the ML estimator occurs at the <link xlink:type="simple" xlink:href="../127/1432127.xml">
mode</link> with the peak (maximum) of the curve.
</caption>
</image>
</p>
<p>

which has solutions <it>p</it>=0, <it>p</it>=1, and <it>p</it>=49/80. The solution which maximizes the likelihood is clearly <it>p</it>=49/80 (since <it>p</it>=0 and <it>p</it>=1 result in a likelihood of zero). Thus we say the <it>maximum likelihood estimator</it> for <it>p</it> is 49/80.</p>
<p>

This result is easily generalized by substituting a letter such as <it>t</it> in the place of 49 to represent the observed number of 'successes' of our <link xlink:type="simple" xlink:href="../653/102653.xml">
Bernoulli trial</link>s, and a letter such as <it>n</it> in the place of 80 to represent the number of Bernoulli trials. Exactly the same calculation yields the <it>maximum likelihood estimator</it> <it>t</it>&nbsp;/&nbsp;<it>n</it> for any sequence of <it>n</it> Bernoulli trials resulting in <it>t</it> 'successes'.</p>

</ss1>
<ss1>
<st>
Continuous distribution, continuous parameter space</st>

<p>

For the <link xlink:type="simple" xlink:href="../462/21462.xml">
normal distribution</link> <math>\mathcal{N}(\mu, \sigma^2)</math> which has <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link></p>
<p>

<indent level="1">

<math>f(x\mid \mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\ \sigma\ } 
                               \exp{\left(-\frac {(x-\mu)^2}{2\sigma^2} \right)}, </math>
</indent>

the corresponding <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link> for a sample of <it>n</it> <link xlink:type="simple" xlink:href="../067/453067.xml">
independent identically distributed</link> normal random variables (the likelihood) is</p>
<p>

<indent level="1">

<math>f(x_1,\ldots,x_n \mid \mu,\sigma^2) = \prod_{i=1}^{n} f( x_{i}\mid  \mu, \sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left( -\frac{ \sum_{i=1}^{n}(x_i-\mu)^2}{2\sigma^2}\right),</math>
</indent>

or more conveniently:</p>
<p>

<indent level="1">

<math>f(x_1,\ldots,x_n \mid \mu,\sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left(-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}\right)</math>,
</indent>
where <math> \bar{x} </math> is the <link xlink:type="simple" xlink:href="../612/612.xml">
sample mean</link>.</p>
<p>

This family of distributions has two parameters: <it>&amp;theta;</it>=(<it>&amp;mu;</it>,<it>&amp;sigma;</it>), so we maximize the likelihood, <math>\mathcal{L} (\mu,\sigma) = f(x_1,\ldots,x_n \mid \mu, \sigma)</math>, over both parameters simultaneously, or if possible, individually.  </p>
<p>

Since the <link xlink:type="simple" xlink:href="../476/21476.xml">
logarithm</link> is a <link xlink:type="simple" xlink:href="../122/6122.xml">
continuous</link> <link xlink:type="simple" xlink:href="../260/48260.xml">
strictly increasing</link> function over the <link xlink:type="simple" xlink:href="../991/275991.xml">
range</link> of the likelihood, the values which maximize the likelihood will also maximize its logarithm.  Since maximizing the logarithm often requires simpler algebra, it is the logarithm which is maximized below.  (Note: the log-likelihood is closely related to <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link> and <link xlink:type="simple" xlink:href="../971/598971.xml">
Fisher information</link>.)</p>
<p>

<indent level="1">

<math>
0 = \frac{\partial}{\partial \mu} \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left(-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}\right) \right) </math>
</indent>
</p>

<p>

<indent level="2">

<math> = \frac{\partial}{\partial \mu} \left( \log\left( \frac{1}{2\pi\sigma^2} \right)^{n/2} - \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2} \right)</math>
</indent>
</p>

<p>

<indent level="2">

<math> = 0 - \frac{-2n(\bar{x}-\mu)}{2\sigma^2} </math>
</indent>

which is solved by </p>
<p>

<indent level="1">

<math>\hat\mu = \bar{x} = \sum^{n}_{i=1}x_i/n </math>. 
</indent>

This is indeed the maximum of the function since it is the only turning point in &amp;mu; and the second derivative is strictly less than zero. Its <link xlink:type="simple" xlink:href="../653/9653.xml">
expectation value</link> is equal to the parameter &amp;mu; of the given distribution,</p>
<p>

<indent level="1">

<math> E \left[ \widehat\mu \right] = \mu,</math>
</indent>

which means that the maximum-likelihood estimator <math>\widehat\mu</math> is unbiased. </p>
<p>

Similarly we differentiate the log likelihood with respect to &amp;sigma; and equate to zero:</p>
<p>

<indent level="1">

<math> 0 = \frac{\partial}{\partial \sigma} \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left(-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}\right) \right) </math>
</indent>
</p>

<p>

<indent level="2">

<math> = \frac{\partial}{\partial \sigma} \left( \frac{n}{2}\log\left( \frac{1}{2\pi\sigma^2} \right) - \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2} \right) </math>
</indent>
</p>

<p>

<indent level="2">

<math> = -\frac{n}{\sigma} + \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{\sigma^3} </math>
</indent>

which is solved by </p>
<p>

<indent level="1">

<math>\widehat\sigma^2 = \sum_{i=1}^n(x_i-\widehat{\mu})^2/n</math>.  
</indent>

Inserting <math>\widehat\mu</math> we obtain</p>
<p>

<indent level="1">

<math>\widehat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_{i} - \bar{x})^2 = \frac{1}{n}\sum_{i=1}^n x_i^2
                          -\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n x_i x_j</math>.  
</indent>

To calculate its expected value, it is convenient to rewrite the expression in terms of zero-mean random variables (<link xlink:type="simple" xlink:href="../509/461509.xml">
statistical error</link>)  <math>\delta_i \equiv \mu - x_i</math>. Expressing the estimate in these variables yields</p>
<p>

<indent level="1">

<math>\widehat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (\mu - \delta_i)^2 -\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n (\mu - \delta_i)(\mu - \delta_j)</math>. 
</indent>

Simplifying the expression above, utilizing the facts that <math>E\left[\delta_i\right] = 0 </math> and <math> E[\delta_i^2] = \sigma^2 </math>, allows us to obtain</p>
<p>

<indent level="1">

<math>E \left[ \widehat{\sigma^2}  \right]= \frac{n-1}{n}\sigma^2</math>.  
</indent>

This means that the estimator <math>\widehat\sigma</math> is biased (However, <math>\widehat\sigma</math> is consistent).</p>
<p>

Formally we say that the <it>maximum likelihood estimator</it> for <math>\theta=(\mu,\sigma^2)</math> is:</p>
<p>

<indent level="1">

<math>\widehat{\theta} = \left(\widehat{\mu},\widehat{\sigma}^2\right).</math>
</indent>

In this case the MLEs could be obtained individually.  In general this may not be the case, and the MLEs would have to be obtained simultaneously.</p>

</ss1>
</sec>
<sec>
<st>
 Non-independent variables </st>

<p>

It may be the case that variables are correlated, in which case they are not independent. Two random variables X and Y are only independent if their joint probability density function is the product of the individual probability density functions, i.e.
<indent level="1">

<math>f(x,y)=f(x)f(y)\,</math>
</indent>

Suppose one constructs an order <math>n\,</math> Gaussian vector out of random variables <math>(x_1,\ldots,x_n)\,</math>, where each variable has means given by <math>(\mu_1, \ldots, \mu_n)\,</math>. Furthermore, let the <link xlink:type="simple" xlink:href="../752/191752.xml">
covariance matrix</link> be denoted by <math>\Sigma,</math></p>
<p>

The joint probability density function of these <math>n</math> random variables is then given by:
<indent level="1">

<math>f(x_1,\ldots,x_n)=\frac{1}{2\pi\sqrt{\text{det}(\Sigma)}} \exp\left( -\frac{1}{2} \left[x_1-\mu_1,\ldots,x_n-\mu_n\right]\Sigma^{-1}     \left[x_1-\mu_1,\ldots,x_n-\mu_n\right]^T       \right)</math>
</indent>

In the two variable case, the joint probability density function is given by:
<indent level="1">

<math>f(x,y) = \frac{1}{2\pi \sigma_x \sigma_y \sqrt{1-\rho^2}} \exp\left[ -\frac{1}{2(1-\rho^2)} \left(\frac{(x-\mu_x)^2}{\sigma_x^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y} + \frac{(y-\mu_y)^2}{\sigma_y^2}\right)            \right]</math>
</indent>

In this and other cases where a joint density function exists, the likelihood function is defined as above, under <it>Principles</it>, using this density.</p>

</sec>
<sec>
<st>
See also</st>

<p>

<table style="background:#f9f9f9; font-size:85%; line-height:110%; ">
<row>
<col>
 <image width="75x28px" src="Fisher_iris_versicolor_sepalwidth.svg">
</image>
</col>
<col style="padding:0 0.2em;">
 <b><it>
Statistics&#32;portal</it></b></col>
</row>
</table>
</p>
<p>


<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../459/60459.xml">
Abductive reasoning</link>, a logical technique corresponding to maximum likelihood.</entry>
<entry level="1" type="bullet">

 <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../952/11548952.xml">
Censoring (statistics)</link></kind>
</type>
</category>
</concept>
</idea>
 </entry>
<entry level="1" type="bullet">

 <process wordnetid="105701363" confidence="0.8">
<calculation wordnetid="105802185" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<estimate wordnetid="105803379" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../026/2826026.xml">
Delta method</link></higher_cognitive_process>
</estimate>
</problem_solving>
</thinking>
</calculation>
</process>
, a method for finding the distribution of functions of a maximum likelihood estimator.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../550/1975550.xml">
Generalized method of moments</link>, a method related to maximum likelihood estimation.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../577/27577.xml">
Inferential statistics</link>, for an alternative to the maximum likelihood estimate.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../968/44968.xml">
Likelihood function</link>, a description on what likelihood functions are.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../433/1792433.xml">
Maximum a posteriori (MAP) estimator</link>, for a contrast in the way to calculate estimators when prior knowledge is postulated.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../816/201816.xml">
Mean squared error</link>, a measure of how 'good' an estimator of a distributional parameter is (be it the maximum likelihood estimator or some other estimator).</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../504/2175504.xml">
Method of moments (statistics)</link>, for another popular method for finding parameters of distributions.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../610/15145610.xml">
Method of support</link>, a variation of the maximum likelihood technique.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../533/19456533.xml">
Minimum distance estimation</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../635/17518635.xml">
Quasi-maximum likelihood</link> estimator, a MLE estimator that is misspecified, but still consistent.</entry>
<entry level="1" type="bullet">

 The <link>
Rao–Blackwell theorem</link>, a result which yields a process for finding the best possible unbiased estimator (in the sense of having minimal <link xlink:type="simple" xlink:href="../816/201816.xml">
mean squared error</link>). The MLE is often a good starting place for the process.</entry>
<entry level="1" type="bullet">

 <link>
Sufficient statistic</link>, a function of the data through which the MLE (if it exists and is unique) will depend on the data.</entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<reflist>
<entry id="1">
<weblink xlink:type="simple" xlink:href="http://www.cambridge.org/catalogue/catalogue.asp?isbn=0521784506">
A.W. van der Vaart, Asymptotic Statistics (Cambridge Series in Statistical and Probabilistic Mathematics) (1998)</weblink></entry>
</reflist>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book">Kay, Steven M.&#32;(1993). Fundamentals of Statistical Signal Processing: Estimation Theory.&#32;Prentice Hall,&#32;Ch. 7. ISBN 0-13-345711-7.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book">Lehmann, E. L.;&#32;Casella, G.&#32;(1998). Theory of Point Estimation.&#32;Springer,&#32;2nd ed. ISBN 0-387-98502-6.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 A paper on the history of Maximum Likelihood:  <cite style="font-style:normal">Aldrich, John&#32;(1997).&#32;"<weblink xlink:type="simple" xlink:href="http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1030037906">
R.A. Fisher and the making of maximum likelihood 1912-1922</weblink>". <it>Statistical Science</it>&#32;<b>12</b>&#32;(3): 162–176. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1214%2Fss%2F1030037906">
10.1214/ss/1030037906</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 M. I. Ribeiro, <weblink xlink:type="simple" xlink:href="http://users.isr.ist.utl.pt/~mir/pub/probability.pdf">
Gaussian Probability Density Functions: Properties and Error Characterization</weblink> (Accessed 19 March 2008)</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://statgen.iop.kcl.ac.uk/bgim/mle/sslike_1.html">
Maximum Likelihood Estimation Primer (an excellent tutorial)</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mayin.org/ajayshah/KB/R/documents/mle/mle.html">
Implementing MLE for your own likelihood function using R</weblink></entry>
</list>
</p>
<p>

<table style=";" class="navbox" cellspacing="0">
<row>
<col style="padding:2px;">
<table style="width:100%;background:transparent;color:inherit;;" class="nowraplinks collapsible autocollapse " cellspacing="0">
<row>
<header colspan="2" style=";" class="navbox-title">
<link xlink:type="simple" xlink:href="../685/26685.xml">
Statistics</link></header>
</row>
<row style="height:2px;">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../541/9541.xml">
Design of experiments</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../585/27585.xml">
Population</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../361/160361.xml">
Sampling</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../596/27596.xml">
Stratified sampling</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../262/10306262.xml">
Replication</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../339/1822339.xml">
Blocking</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../839/1776839.xml">
Sample size estimation</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../673/226673.xml">
Null hypothesis</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../892/645892.xml">
Alternative hypothesis</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../877/5657877.xml">
Type I and type II errors</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../695/238695.xml">
Statistical power</link>&nbsp;&amp;bull; <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../276/437276.xml">
Effect size</link></method>
</know-how>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../187/8187.xml">
Descriptive statistics</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<table style="width:100%;;;;" class="nowraplinks  navbox-subgroup" cellspacing="0">
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<structure wordnetid="105726345" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../792/5792.xml">
Continuous  data</link></kind>
</distribution>
</type>
</arrangement>
</category>
</concept>
</idea>
</structure>
</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<table style="width:100%;;;;" class="nowraplinks  navbox-subgroup" cellspacing="0">
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../516/17516.xml">
Location</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../192/19192.xml">
Mean</link> (<link xlink:type="simple" xlink:href="../612/612.xml">
Arithmetic</link>, <link xlink:type="simple" xlink:href="../046/13046.xml">
Geometric</link>, <link xlink:type="simple" xlink:href="../463/14463.xml">
Harmonic</link>)&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../837/18837.xml">
Median</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../127/1432127.xml">
Mode</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../589/27589.xml">
Dispersion</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../588/27588.xml">
Range</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../590/27590.xml">
Standard deviation</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../687/1012687.xml">
Coefficient of variation</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../907/354907.xml">
Percentile</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../684/368684.xml">
Moments</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../344/32344.xml">
Variance</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../212/28212.xml">
Skewness</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../848/16848.xml">
Kurtosis</link></col>
</row>
</table>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../188/8188.xml">
Categorical data</link></kind>
</type>
</category>
</concept>
</idea>
</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../019/4839019.xml">
Frequency</link>&nbsp;&amp;bull; <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../515/935515.xml">
Contingency table</link></datum>
</information>
</col>
</row>
</table>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../577/27577.xml">
Inferential statistics</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian inference</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../284/30284.xml">
Hypothesis testing</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../995/160995.xml">
Significance</link>&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../213/332213.xml">
Z-test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../080/536080.xml">
Student's t-test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<information wordnetid="105816287" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../680/226680.xml">
Chi-square test</link></higher_cognitive_process>
</trial>
</datum>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</information>
</process>
&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../976/318976.xml">
F-test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../994/554994.xml">
P-value</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../381/160381.xml">
Interval estimation</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../806/140806.xml">
Maximum likelihood</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../533/19456533.xml">
Minimum distance</link>&nbsp;&amp;bull; <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../329/62329.xml">
Meta-analysis</link></method>
</know-how>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../911/280911.xml">
Confidence interval</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../259/419259.xml">
Survival analysis</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../178/4649178.xml">
Survival function</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../650/3168650.xml">
Kaplan-Meier</link>&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../026/11871026.xml">
Logrank test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../960/1336960.xml">
Failure rate</link>&nbsp;&amp;bull; <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../267/5352267.xml">
Proportional hazards models</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../057/157057.xml">
Correlation</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../999/3105999.xml">
Confounding variable</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../708/221708.xml">
Pearson product-moment correlation coefficient</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../627/3316627.xml">
Rank correlation</link> (<process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../623/235623.xml">
Spearman's rank correlation coefficient</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
, <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../830/7287830.xml">
Kendall tau rank correlation coefficient</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
)</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../904/17904.xml">
Linear model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../698/877698.xml">
General linear model</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../122/747122.xml">
Generalized linear model</link>&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../634/634.xml">
Analysis of variance</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../926/404926.xml">
Analysis of covariance</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../997/826997.xml">
Regression analysis</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../903/17903.xml">
Linear regression</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../012/1045012.xml">
Nonlinear regression</link>&nbsp;&amp;bull; <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../631/226631.xml">
Logistic regression</link></datum>
</information>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../463/15934463.xml">
Statistical graphics</link></visual_communication>
</chart>
</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../311/393311.xml">
Bar chart</link>&nbsp;&amp;bull; <chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../166/14306166.xml">
Biplot</link></visual_communication>
</chart>
&nbsp;&amp;bull; <chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../960/160960.xml">
Box plot</link></visual_communication>
</chart>
&nbsp;&amp;bull; <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<tool wordnetid="104451818" confidence="0.8">
<chart wordnetid="106999802" confidence="0.8">
<implement wordnetid="103563967" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../754/435754.xml">
Control chart</link></visual_communication>
</implement>
</chart>
</tool>
</instrumentality>
</artifact>
&nbsp;&amp;bull; <know-how wordnetid="105616786" confidence="0.8">
<chart wordnetid="106999802" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../359/11394359.xml">
Forest plot</link></visual_communication>
</method>
</chart>
</know-how>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../266/13266.xml">
Histogram</link>&nbsp;&amp;bull; <chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../859/4031859.xml">
Q-Q plot</link></visual_communication>
</chart>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../749/6392749.xml">
Run chart</link>&nbsp;&amp;bull; <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<tool wordnetid="104451818" confidence="0.8">
<chart wordnetid="106999802" confidence="0.8">
<implement wordnetid="103563967" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../544/412544.xml">
Scatterplot</link></visual_communication>
</implement>
</chart>
</tool>
</instrumentality>
</artifact>
&nbsp;&amp;bull; <chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../649/977649.xml">
Stemplot</link></visual_communication>
</chart>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../442/14986442.xml">
History</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../442/14986442.xml">
History of statistics</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../208/19095208.xml">
Founders of statistics</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../692/19373692.xml">
Timeline of probability and statistics</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Publications</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../827/18753827.xml">
Journals in statistics</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../879/708879.xml">
Important publications</link></col>
</row>
<row style="height:2px;">

</row>
<row>
<col colspan="2" style=";" class="navbox-abovebelow">
<b>
Statistics|Category</b>&nbsp;&amp;bull; <b>
Portal</b>&nbsp;&amp;bull; <b><link xlink:type="simple" xlink:href="../457/191457.xml">
List of topics</link></b></col>
</row>
</table>
</col>
</row>
</table>
</p>


</sec>
</bdy>
</article>
