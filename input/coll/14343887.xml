<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:26:10[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Precision and recall</title>
<id>14343887</id>
<revision>
<id>241785562</id>
<timestamp>2008-09-29T15:26:26Z</timestamp>
<contributor>
<username>Zundark</username>
<id>70</id>
</contributor>
</revision>
<categories>
<category>information retrieval</category>
<category>All articles to be merged</category>
<category>Articles to be merged&amp;#32;since June 2008</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-move" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Mergefrom.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 It has been suggested that  and  be  into this article or section. ()</col>
</row>
</table>

<p>

<image location="right" width="150px" src="recall-precision.svg" type="thumb">
<caption>

recall and precision depend on the outcome of a query (oval) and its relation to all relevant documents (left) and the non-relevant documents (right). The more correct results (green), the better
</caption>
</image>

<b>Precision</b> and <b>Recall</b> are two widely used measures for evaluating the quality of results in domains such as <link xlink:type="simple" xlink:href="../271/15271.xml">
Information Retrieval</link> and  <link xlink:type="simple" xlink:href="../244/1579244.xml">
statistical classification</link>.</p>
<p>

<b>Precision</b> can be seen as a measure of exactness or fidelity, whereas <b>Recall</b> is a measure of completeness.</p>
<p>

In an <link xlink:type="simple" xlink:href="../271/15271.xml">
Information Retrieval</link> scenario, <b>Precision</b> is defined as the <it>number of relevant documents</it> retrieved by a search <it>divided by the total number of documents retrieved</it> by that search, and <b>Recall</b> is defined as the <it>number of relevant documents</it> retrieved by a search <it>divided by the total number of existing relevant documents</it> (which should have been retrieved).</p>
<p>

In a <link xlink:type="simple" xlink:href="../244/1579244.xml">
statistical classification</link> task, the <b>Precision</b> for a class is the <it>number of </it>'true positives<b><it> (i.e. the </it></b><b>number of items correctly labeled as belonging to the class<it>) </it></b><b>divided by the total number of elements labeled as belonging to the class<it> (i.e. the sum of true positives and </it></b><it><link xlink:type="simple" xlink:href="../877/5657877.xml">
false positives</link></it>', which are items incorrectly labeled as belonging to the class). <b>Recall</b> in this context is defined as the <it>number of true positives</it> <it>divided by the total number of elements that actually belong to the class</it> (i.e. the sum of true positives and <b><link xlink:type="simple" xlink:href="../877/5657877.xml">
false negatives</link></b>, which are items which were not labeled as belonging to that class but should have been).</p>
<p>

In Information Retrieval, a perfect <b>Precision</b> score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect <b>Recall</b> score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).</p>
<p>

In a classification task, a Precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a Recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).</p>
<p>

Often, there is an inverse relationship between Precision and Recall, where it is possible to increase one at the cost of reducing the other. For example, an information retrieval system (such as a <link xlink:type="simple" xlink:href="../023/4059023.xml">
search engine</link>) can often increase its Recall by retrieving more  documents, at the cost of increasing number of irrelevant documents retrieved (decreasing Precision).
Similarly, a classification system for deciding whether or not, say, a fruit is an orange, can achieve high Precision by only classifying fruits with the exact right shape and color as oranges, but at the cost of low Recall due to the number of <it>false negatives</it> from oranges that did not quite match the specification.</p>
<p>

Usually, Precision and Recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. <it>precision at a recall level of 0.75</it>) or both are combined into a single measure, such as the <b>F-measure</b>, which is the <it>weighted harmonic mean of precision and recall</it> (see <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22F-measure%22])">
below</link>).</p>

<sec>
<st>
 Definition(Information Retrieval context) </st>

<p>

In <link xlink:type="simple" xlink:href="../271/15271.xml">
Information Retrieval</link> contexts, Precision and Recall are defined in terms of a set of <b>retrieved documents</b> (e.g. the list of documents produced by a <link xlink:type="simple" xlink:href="../023/4059023.xml">
web search engine</link> for a query) and a set of <b>relevant documents</b> (e.g. the list of all documents on the internet that are relevant for a certain topic).</p>
<p>

<math>\mbox{Recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{documents retrieved}\}|}{|\{\mbox{relevant documents}\}|} </math></p>
<p>

<math>\mbox{Precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{documents retrieved}\}|}{|\{\mbox{documents retrieved}\}|} </math></p>

</sec>
<sec>
<st>
 Definition (classification context) </st>
<p>

In the context of classification tasks, the terms <b>true positives</b>, <b>true negatives</b>, <b>false positives</b> and <b>false negatives</b> (see also <link xlink:type="simple" xlink:href="../877/5657877.xml">
Type I and type II errors</link>) are used to compare the given classification of an item (the class label assigned to the item by a classifier) with the desired correct classification (the class the item actually belongs to). This is illustrated by the table below:</p>
<p>

<table style="text-align: center; background: #FFFFFF;" align="center" border="0">



<header colspan="2" style="background: #ddffdd;">
correct result / classification</header>
<row>


<header style="background: #ddffdd;">
&nbsp;E1&nbsp;</header>
<header style="background: #ddffdd;">
&nbsp;E2&nbsp;</header>
</row>
<row >
</row>
<header style="background: #ffdddd;" rowspan="2">
obtained  result / classification</header>
<header style="background: #ffdddd;" width="100">
E1</header>
<row>
<col>
<b>tp</b>  (true positive)</col>
<col>
<b>fp</b>  (false positive)</col>
</row>
<row bgcolor="#EFEFEF">
<header style="background: #ffdddd;">
E2</header>
<col>
<b>fn</b>  (false negative)</col>
<col>
<b>tn</b>  (true negative)</col>
</row>
</table>
</p>
<p>

Precision and Recall are then defined as </p>
<p>

<math>\mbox{Recall}=\frac{tp}{tp+fn} </math></p>
<p>

<math>\mbox{Precision}=\frac{tp}{tp+fp} </math></p>

</sec>
<sec>
<st>
 Probabilistic Interpretation </st>

<p>

It is possible to interpret Precision and Recall not as ratios but as probabilities:</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Recall</b> is the probability that a (randomly selected) relevant document is retrieved in a search.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Precision</b> is the probability that a (randomly selected) retrieved document is relevant.</entry>
</list>
</p>

</sec>
<sec>
<st>
 F-measure </st>

<p>

A popular measure that combines Precision and Recall is the weighted <link xlink:type="simple" xlink:href="../463/14463.xml">
harmonic mean</link> of precision and recall, the traditional F-measure or balanced <link xlink:type="simple" xlink:href="../785/4011785.xml">
F-score</link>:</p>
<p>

<indent level="1">

<math>F = 2 \cdot (\mathrm{precision} \cdot \mathrm{recall}) / (\mathrm{precision} + \mathrm{recall}).\,</math>
</indent>

This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.</p>
<p>

It is a special case of the general <math>F_\beta</math> measure (for non-negative real values of <math>\beta</math>):</p>
<p>

<indent level="1">

<math>F_\beta = (1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall}) / (\beta^2 \cdot \mathrm{precision} + \mathrm{recall}).\,</math>
</indent>

Two other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.</p>
<p>

The F-measure was derived by van Rijsbergen (1979) so that <math>F_\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches Î² times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1-(1/(\alpha/P + (1-\alpha)/R))</math>.  Their relationship is <math>F_\beta = 1 - E</math> where <math>\alpha=1/(\beta^2+1)</math>.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../271/15271.xml">
Information retrieval</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../393/205393.xml">
Binary classification</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Sources </st>
<p>

<list>
<entry level="1" type="bullet">

 Makhoul, John; Francis Kubala; Richard Schwartz; Ralph Weischedel: <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/makhoul99performance.html">
<it>Performance measures for information extraction</it>.</weblink> In: <it>Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999</it>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Baeza-Yates, R.; Ribeiro-Neto, B. (1999). <it>Modern Information Retrieval</it>. New York: ACM Press, Addison-Wesley. Seiten 75 ff. ISBN 0-201-39829-X</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 van Rijsbergen, C.V.: <it>Information Retrieval</it>. London; Boston. Butterworth, 2nd Edition 1979. ISBN 0-408-70929-4</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dcs.gla.ac.uk/Keith/Preface.html">
Information Retrieval â C. J. van Rijsbergen 1979</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
