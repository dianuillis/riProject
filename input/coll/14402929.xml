<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:29:52[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Generalized Hebbian Algorithm</title>
<id>14402929</id>
<revision>
<id>218359690</id>
<timestamp>2008-06-10T08:32:00Z</timestamp>
<contributor>
<username>DOI bot</username>
<id>6652755</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
</categories>
</header>
<bdy>

The <b>Generalized Hebbian Algorithm</b> (<b>GHA</b>), also known in the literature as <b>Sanger's rule</b>, is a linear <link xlink:type="simple" xlink:href="../759/574759.xml">
feedforward</link> <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link> model for unsupervised learning with applications primarily in <link xlink:type="simple" xlink:href="../340/76340.xml">
principal components analysis</link>. First defined in 1989<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>, it is similar to <link xlink:type="simple" xlink:href="../564/9617564.xml">
Oja's rule</link> in its formulation and stability, except it can be applied to networks with multiple outputs.
<sec>
<st>
Theory</st>
<p>

GHA combines Oja's rule with the <link xlink:type="simple" xlink:href="../361/82361.xml">
Gram-Schmidt process</link> to produce a learning rule of the form</p>
<p>

<indent level="1">

<math>\Delta w_{ij} = \eta\left(y_j x_i - y_j \sum_{k=1}^j w_{ik} y_k \right)</math>,
</indent>

where <math>w_{ij}</math> defines the <link xlink:type="simple" xlink:href="../160/14405160.xml">
synaptic weight</link> or connection strength between the <math>i</math>th input and <math>j</math>th output neurons, <math>x</math> and <math>y</math> are the input and output vectors, respectively, and <math>\eta</math> is the <it>learning rate</it> parameter.</p>

<ss1>
<st>
Derivation</st>
<p>

In matrix form, Oja's rule can be written</p>
<p>

<indent level="1">

<math>\frac{d w(t)}{d t}=w(t) Q - \textrm{diag} (w(t) Q w(t)^T) w(t)</math>,
</indent>

and the Gram-Schmidt algorithm is</p>
<p>

<indent level="1">

<math>\,\Delta w(t) = -\textrm{lower} [w(t) w(t)^T] w(t)</math>,
</indent>

where <math>w(t)</math> is any matrix, in this case representing synaptic weights, <math>Q = \eta \textbf{x} \textbf{x}^T</math> is the autocorrelation matrix, simply the outer product of inputs, <math>\textrm{diag}</math> is the function that <link xlink:type="simple" xlink:href="../672/59672.xml">
diagonalizes</link> a matrix, and <math>\textrm{lower}</math> is the function that sets all matrix elements on or above the diagonal equal to 0. We can combine these equations to get our original rule in matrix form,</p>
<p>

<indent level="1">

<math>\Delta w(t) = \eta(t) \left(\textbf{y}(t) \textbf{x}(t)^T - \textrm{LT}[\textbf{y}(t)\textbf{y}(t)^T] w(t)\right)</math>,
</indent>

where the function <math>\textrm{LT}</math> sets all matrix elements above the diagonal equal to 0, and note that our output <math>\textbf{y}(t)= w(t) \textbf{x}(t)</math> is a linear neuron<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.</p>

</ss1>
<ss1>
<st>
Stability and PCA</st>
<p>

<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>


</ss1>
</sec>
<sec>
<st>
Applications</st>
<p>

GHA is used in applications where a <link xlink:type="simple" xlink:href="../996/76996.xml">
self-organizing map</link> is necessary, or where a feature or principal components analysis can be used. Examples of such cases include <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> and speech and image processing.</p>
<p>

Its importance comes from the fact that learning is a single-layer process--that is, a synaptic weight changes only depending on the response of the inputs and outputs of that layer, thus avoiding the multi-layer dependence associated with the <link xlink:type="simple" xlink:href="../091/1360091.xml">
backpropagation</link> algorithm. It also has a simple and predictable trade-off between learning speed and accuracy of convergence as set by the learning rate parameter <math>\eta</math><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../084/404084.xml">
Hebbian learning</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../564/9617564.xml">
Oja's rule</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../492/253492.xml">
Factor analysis</link></entry>
<entry level="1" type="bullet">

<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<component wordnetid="105868954" confidence="0.8">
<part wordnetid="105867413" confidence="0.8">
<link xlink:type="simple" xlink:href="../340/76340.xml">
Principal components analysis</link></part>
</component>
</concept>
</idea>
</entry>
<entry level="1" type="bullet">

<link>
PCA network</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal"><link>
Sanger, Terence D.</link>&#32;(1989).&#32;"<weblink xlink:type="simple" xlink:href="http://ece-classweb.ucsd.edu/winter06/ece173/documents/Sanger%201989%20--%20Optimal%20Unsupervised%20Learning%20in%20a%20Single-layer%20Linear%20FeedforwardNN.pdf">
Optimal unsupervised learning in a single-layer linear feedforward neural network</weblink>". <it>Neural Networks</it>&#32;<b>2</b>&#32;(6): 459–473. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2F0893-6080%2889%2990044-0">
10.1016/0893-6080(89)90044-0</weblink>. Retrieved on <link>
2007-11-24</link>.</cite>&nbsp;</entry>
<entry id="2">
 <cite id="Reference-Haykin-1998" style="font-style:normal" class="book"><link>
Haykin, Simon</link>&#32;(1998). Neural Networks: A Comprehensive Foundation, 2,&#32;Prentice Hall. ISBN 0132733501.</cite>&nbsp;</entry>
<entry id="3">
 <cite style="font-style:normal"><link>
Oja, Erkki</link>&#32;(November 1982).&#32;"<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/u9u6120r003825u1/">
Simplified neuron model as a principal component analyzer</weblink>". <it>Journal of Mathematical Biology</it>&#32;<b>15</b>&#32;(3): 267–273. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF00275687">
10.1007/BF00275687</weblink>. BF00275687. Retrieved on <link>
2007-11-22</link>.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
</bdy>
</article>
