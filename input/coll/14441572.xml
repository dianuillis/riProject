<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:21:16[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Kadir Brady saliency detector</title>
<id>14441572</id>
<revision>
<id>242565044</id>
<timestamp>2008-10-02T19:30:06Z</timestamp>
<contributor>
<username>Queenmomcat</username>
<id>6687298</id>
</contributor>
</revision>
<categories>
<category>All articles needing copy edit</category>
<category>Wikify from November 2007</category>
<category>Computer vision</category>
<category>Wikipedia articles needing copy edit from March 2008</category>
<category>Articles with invalid date parameter in template</category>
<category>Feature detection</category>
<category>All pages needing to be wikified</category>
<category>Image processing</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Wikitext.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>Please  this article or section.</b>
Help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Kadir_Brady_saliency_detector&amp;action=edit">
improve this article</weblink> by adding  . <it>(November 2007)''</it></col>
</row>
</table>


<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="30px" src="Acap.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>This article or section needs  for  grammar, style, cohesion, tone or spelling.</b>
You can assist by <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Kadir_Brady_saliency_detector&amp;action=edit">
editing it</weblink> now. A how-to is available.<it>&nbsp;(March 2008)''</it></col>
</row>
</table>

</p>
<p>

The Kadir Brady saliency detector is designed to detect representative region in images. It performs well in the context of object class recognition.  It was invented by http://www.robots.ox.ac.uk/~timork/ Timor Kadir] and <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~jmb/home.html">
Michael Brady</weblink> in 2001<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.
Later in 2004, the affine invariant version was invented by Kadir, T., 
<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~az/">
Zisserman, A.</weblink> and M. Brady. 
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFA._Baumberg2000%22])">
A. Baumberg 2000</link>)</cite>.</p>

<sec>
<st>
Introduction</st>
<p>

Many <link xlink:type="simple" xlink:href="../596/6596.xml">
computer vision</link> and <link xlink:type="simple" xlink:href="../332/76332.xml">
image processing</link> applications work directly with the features extracted from an image, rather than the raw image, for example, for computing image  correspondences [2, 17, 19,20, 22], or for learning object categories [1, 3, 4, 23]. Depending on the applications, different characteristics are preferred. However, there are three broad classes of image change under which good performance may be required:</p>

<p>

<list>
<entry level="1" type="number">

Global transformation: Features should be repeatable across the expected class of global image transformations. These include both geometric and photometric transformations that arise due to changes in the imaging conditions. For example, region detection should be covariant with viewpoint as illustrated in Figure 1. In short, we require the segmentation to commute with viewpoint change. This property will be evaluated on the repeatability and accuracy of localization and region estimation.</entry>
<entry level="1" type="number">

Local perturbations: Features should be insensitive to classes of semi-local image disturbances. For example, a feature responding to the eye of a human face should be unaﬀected by any motion of the mouth. A second class of disturbance is where a region neighbours a foreground/background boundary. The detector can be required to detect the foreground region despite changes in the background.</entry>
<entry level="1" type="number">

Intra-class variations: Features should capture corresponding object parts under intra-class variations in objects. For example, the headlight of a car for different brands of car (imaged from the same viewpoint).</entry>
</list>
</p>
<p>

All <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../905/14784905.xml">
Feature detection</link></concept>
</idea>
 algorithms are trying to detect regions which is stable under three types of image change described above. 
Instead of finding corner[7 21], or blob[17 22], or any specific shape of regions, 
Kadir brady saliency detector looks for regions which are locally complex, and globally discriminative.
Such regions usually corresponds to region more stable under these types of image change.</p>

</sec>
<sec>
<st>
 Information theoretic saliency</st>
<p>

In the field of <link xlink:type="simple" xlink:href="../773/14773.xml">
Information theory</link>, Shannon entropy is defined to quantify the complexity of a distribution <it>p</it> as <math>p \log p \,</math>. Therefore, higher entropy means <it>p</it> is more complex, hence more unpredictable.</p>

<p>

To measure the complexity of an image region <math>\{x,R\}</math> around point <math>x</math> with shape <math>R</math>,
a descriptor <math>D</math> that takes on values <math>{d_1 , . . . , d_r }</math>
(e.g. in an 8 bit grey level image, D would range from 0 to 255 for each pixel) is defined
so that <math>P_{D}(d_i,x,R)</math>, the probability of descriptor value <math>d_i</math>
occurs in region <math>\{x,R\}</math> can be computed.
Further, the entropy of image region <math>R_x</math> can compute as
<indent level="1">

<math> H_{D}(x,R) = -\sum_{i \in (1...r)} P_{D}(d_i,x,R) \log P_{D}(d_i,x,R).</math>                   
</indent>
Using this entropy equation we can further calculate <math>H_{D}(x,R)</math> for every point <math>x</math>
and region shape <math>R</math>. As shown in Figure 2 (a), more complex region, like eye region, has more complex distribtion, hence higher entropy.</p>
<p>

<math> H_{D}(x,R)</math> is a good measure for local complexity. 
However, entropy only measures the statistic of local attribute. 
It doesn't measure the spatial arrangement of the local attribute.
For example, figure 2 (b) shows three permutations of pixels of the eye region which have the same entropy as
the eye region. However, these four regions are not equally discriminative under scale change as shown in figure 2 (b). This observation is used to define measure on discriminative in subsections.</p>
<p>

The following subsections will discuss different methods to select regions with high local complexity and more discriminative across different region.</p>

<ss1>
<st>
Similarity invariant saliency</st>

<p>

The first version of Kadir brady saliency detector[10] only finds Salient regions
invariant under similarity transformation. The algorithm finds circle regions with different scales.
In other words, given <math>H_{D}(x,s)</math>, where s is the scale parameter of a circle region <math>R</math>,
the algorithm selects a set of circle region,<math>\{x_i,s_i;i=1...N\}</math>.</p>
<p>

The method consists of three steps: 
<list>
<entry level="1" type="bullet">

Calculation of Shannon entropy of local image attributes for each x over a range of scales — <math>H_{D}(x,s) = -\sum_{i \in (1...r)} P_{D}(d_i,x,s) \log P_{D}(d_i,x,s)</math>; </entry>
<entry level="1" type="bullet">

Select scales at which the entropy over scale function exhibits a peak — <math>s_p</math> ;</entry>
<entry level="1" type="bullet">

Calculate the magnitude change of the PDF as a function of scale at each peak — <math>W_D(x,s) = \sum_{i \in (1...r)} |\frac{\part}{\part s}P_{D,}(d_i,x,s)| </math> (s).</entry>
</list>

The ﬁnal saliency <math>Y_D(x,s_p)</math> is the product of <math>H_D(x,s_p)</math> and <math>W_D(x,s_p)</math>.</p>
<p>

For each x the method picks a scale <math>s_p</math> and calculates salient score <math>Y_D(x,s_p)</math>.
By comparing <math>Y_D(x,s_p)</math> of different points <math>x</math> the detector can rank
the saliency of points and pick the most representative ones. 
For example, in Figure 3, blue circle region has higher saliency than red circle.</p>

</ss1>
<ss1>
<st>
 Affine invariant saliency </st>

<p>

Previous method is invariant to the similarity group of geometric transformations 
and to photometric shifts. However, as mentioned in the opening remarks,
the ideal detector should detect region invariant up to viewpoint change.
There are several detector  can detect affine invariant region which is a better approximation 
of viewpoint change than similarity transformation.</p>
<p>

To detect affine invariant region, the detector need to detect ellipse as in figure 4.
<math>R</math> now is parameterized by three parameter (s, "ρ", "θ"),
where "ρ" is the axis ratio and "θ" the orientation of the ellipse.</p>
<p>

This modification increases the search space of the previous algorithm from a scale to a set of parameters.
Therefore the complexity of the affine invariant saliency detector increases.
In practice the affine invariant saliency detector starts with the set of points and scales generate from the Similarity invariant saliency detector, then iteratively approximates the suboptimal parameters.</p>

</ss1>
<ss1>
<st>
 Comparison </st>
<p>

Although similarity invariant saliency detector is faster than Affine invariant saliency detector,
it also has the drawback of favoring isotropic structure since the discriminative measure <math>W_D</math> is measured over isotropic scale.
To summarize, Affine invariant saliency detector is invariant to affine transformation and able to detect more generate salient regions.</p>
<p>

Figure 5 and 6 shows the comparison image output from both Similarity invariant saliency detector and invariant saliency detector.</p>



</ss1>
<ss1>
<st>
Salient Volumn</st>
<p>

It is intuitive to pick points from higher salient score directly and stop when a certain number of threshold on number of points or salient score is satisfied. Natural images contain noise and motion blur, both of them act as randomisers and generally increase entropy, affecting previously low entropy values more than high entropy values. </p>
<p>

A more robust method would be to pick regions rather than points in entropy space. Although the individual pixels within a salient region may be affected at any given instant by the noise, it is unlikely to affect all of them in such a way that the region as a whole becomes non-salient.</p>
<p>

It is also necessary to analyze the whole saliency space such that each salient feature is  represented. A global threshold approach would result in highly salient features in one part of the
image dominating the rest. A local threshold approach would require the setting of another scale parameter.</p>
<p>

A simple clustering algorithm meets these two requirements are used at the end of the algorithm. 
It works by selecting highly salient points that have local support - that is, nearby points with similar saliency and scale. Each region must be sufficiently distant from all others (in R3 ) to qualify as a separate entity. For robustness, we use a representation that includes all of the points in a selected region.
The method works as follows:
<list>
<entry level="1" type="number">

Apply a global threshold.</entry>
<entry level="1" type="number">

Choose the highest salient point in saliency-space (Y).</entry>
<entry level="1" type="number">

Find the K nearest neighbours (K is a pre-set constant).</entry>
<entry level="1" type="number">

Test the support of these using variance of the centre points.</entry>
<entry level="1" type="number">

Find distance, D, in R3 from salient regions already clustered.</entry>
<entry level="1" type="number">

Accept, if D &amp;gt; scalemean of the region and if suﬃciently clustered (variance is less than pre-set threshold Vth ).</entry>
<entry level="1" type="number">

Store as the mean scale and spatial location of K points.</entry>
<entry level="1" type="number">

Repeat from step 2 with next highest salient point.</entry>
</list>

The algorithm is implement as GreedyCluster1.m in matlab by Dr. Timor Kadir which can be download <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~timork/Saliency/AffineScaleSaliency_Public_linux_V1.0.tgz#ScaleSaliency_Public_linux">
here</weblink></p>

</ss1>
</sec>
<sec>
<st>
 Performance evaluation</st>
<p>

In the field of<link xlink:type="simple" xlink:href="../596/6596.xml">
computer vision</link>, different feature detectors have been evaluated by several tests.
The most profound evaluation is published on International Journal of Computer Vision in 2006
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.  
The following subsection discuss the performance of Kadir brady saliency detector 
on a subset of test in the paper.</p>

<ss1>
<st>
Performance under global transformation</st>
<p>

In order to measure the consistency of region detected on the same object or scene across images 
under global transformation, repeatability score, which is first proposed by <weblink xlink:type="simple" xlink:href="http://personal.ee.surrey.ac.uk/Personal/K.Mikolajczyk/Krystian">
Mikolajczyk</weblink> and 
<weblink xlink:type="simple" xlink:href="http://lear.inrialpes.fr/people/schmid/">
Cordelia Schmid</weblink> in [18, 19], is calculated as follow.</p>
<p>

Firstly, overlap error <math>\epsilon</math> of a pair
of corresponding ellipses <math>\mu_a</math> and <math>\mu_b</math> 
each on different images is defined</p>
<p>

<math>\epsilon = 1 - \frac{\mu_a \cap (A^T \mu_b A)}{\mu_a \cup (A^T \mu_b A)}</math></p>
<p>

where A is the locally linearized affine transformation of the homography between the two images,
and <math>\mu_a \cap (A^T \mu_b A)</math> and <math>\mu_a \cup (A^T \mu_b A)</math>
represent the area of intersection and union of the ellipses respectively. 
Notice <math>\mu_a</math> is scaled into a fix scale to take the count of
size variation of different detected region. Only if <math>\epsilon</math> is smaller than certain <math>\epsilon_0</math>, the pair of ellipses are deemed to correspond.</p>
<p>

Then the repeatability score for a given pair of images is computed as the ratio between the number of region-to-region correspondences and the smaller of the number of regions in the pair of images, where only the regions located in the part of the scene present in both images are counted.
In general we would like a detector to have a high repeatability score and a large number of correspondences. </p>
<p>

The specific global transformations tested in the <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/index.html">
test dataset</weblink> are:
<list>
<entry level="1" type="bullet">

Viewpoint change</entry>
<entry level="1" type="bullet">

Zoom+rotation</entry>
<entry level="1" type="bullet">

Image blur</entry>
<entry level="1" type="bullet">

JPEG compression</entry>
<entry level="1" type="bullet">

Light change</entry>
</list>
</p>
<p>

The performance of Kadir Brady saliency detector is inferior to most of other detectors mainly because the number of points detected is usually lower than other detectors.</p>
<p>

The precise procedure is given in the Matlab code from Detector evaluation
<link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Software+implementation%22])">
<list>
<entry level="1" type="number">

Software implementation</entry>
</list>
</link>.</p>

</ss1>
<ss1>
<st>
Performance under intra-class variation and image perturbations</st>
<p>

In the task of object class categorization, the ability of detecting similar regions
given intra-class variation and image perturbations across object instance
is very critical. 
In [cite], Repeatability measure over intra-class variation and image perturbations
is proposed. The following subsection will introduce the definition and discuss the performance.</p>

<ss2>
<st>
Intra-class variation test</st>
<p>

Suppose there are a set of images of the same object class, e.g. motorbikes. A region
detection operator which is unaffected by intra-class variation will reliably select
regions on corresponding parts of all the objects, say the wheels, engine or seat
for motorbikes. </p>
<p>

Repeatability over intra-class variation is measuring the (average) number
of correct correspondences over the set of images, 
where the correct correspondences is established by manual selection.</p>
<p>

A region is matched if it fulﬁls three requirements: 
<list>
<entry level="1" type="bullet">

Its position matches within 10 pixels.</entry>
<entry level="1" type="bullet">

Its scale is within 20%.</entry>
<entry level="1" type="bullet">

Normalised <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link> between the appearances is &amp;gt; 0.4.  </entry>
</list>
</p>
<p>

In detail the average correspondence score S is measured as follows. </p>
<p>

N regions are detected on each image of the M images in the dataset. Then for a particular
reference image i the correspondence score <math>S_i</math> is given by the proportion of
corresponding to detected regions for all the other images in the dataset, i.e.:</p>
<p>

<math>Si =  \frac{Total\ number\ of\ matches}{Total\ number\ of\ detected\ regions}=\frac{N_{M}^{i}}{N (M-1)}</math></p>
<p>

The score<math>S_i</math> is computed for M/2 different selections of the reference image,
and averaged to give S. The score is evaluated as a function of the number of
detected regions N .</p>
<p>

Kadir brady saliency detector gives highest score across three test class, which are motorbike, car, and face.
As illustrate in figure , saliency detector indicates that most detections are near the object. In contrast, other detectors maps show a much more diffuse pattern over the entire area caused by poor localisation and false responses to background clutter.</p>

</ss2>
<ss2>
<st>
Image perturbations test</st>
<p>

In order to test insensitivity to image perturbation the data set is split into
two parts: the ﬁrst contains images with a uniform background and the second,
images with varying degrees of background clutter. If the detector is robust to
background clutter then the average correspondence score S should be similar
for both subsets of images.</p>
<p>

In this test saliency detector also outperforms other detectors duo to three reasons:
<list>
<entry level="1" type="bullet">

Several detection methods blur the image, hence causing a greater degree of similarity between</entry>
</list>

objects and background. 
<list>
<entry level="1" type="bullet">

In most images the objects of interest tend to be in focus while backgrounds are out of focus and hence blurred. Blurred regions tend to exhibit slowly varying statistics which result in a relatively low entropy and inter-scale saliency in the saliency detector. </entry>
<entry level="1" type="bullet">

Other detectors deﬁne saliency with respect to specific properties of the local surface geometry. In contrast, the saliency detector uses a much broader definition.</entry>
</list>
</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
 Conclusion</st>
<p>

Saliency detector is most useful in the task of object recognition, 
whereas several other detector are more useful in the task of computing image correspondences.
However, in the task of 3D object recognition which all three type of image change are combined,
Saliency detector might still be powerful as  mentioned in [xx].</p>

</sec>
<sec>
<st>
 Software implementation</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~timork/salscale.html">
Scale Saliency and Scale Descriptors and download Scale Saliency binaries</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~timork/Saliency/AffineInvariantSaliency.html">
Affine Invariant Scale Saliency and download Affine Invariant Scale Saliency binaries</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/evaluation.html">
Detector evaluation</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
 Scale, Saliency and Image Description. Timor Kadir and Michael Brady. International Journal of Computer Vision. 45 (2):83-105, November 2001</entry>
<entry id="2">
Kadir, T., Zisserman, A. and Brady, M. An affine invariant salient region detector. Proceedings of the 8th European Conference on Computer Vision, Prague, Czech Republic (2004) </entry>
<entry id="3">
A comparison of affine region detectors. K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir and L. Van Gool. International Journal of Computer Vision</entry>
</reflist>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal">A. Baumberg&#32;(2000). "<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/baumberg00reliable.html">
Reliable feature matching across widely separated views</weblink>".&#32;<it>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</it>: pages I:1774--1781.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">J. Matas, O. Chum, M. Urban, and T. Pajdla&#32;(2002). "Robust wide baseline stereo from maximally stable extremal regions".&#32;<it>Proceedings of British Machine Vision Conference</it>: pages 384–393.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">K. Mikolajczyk and C. Schmid&#32;(2002). "An aﬃne invariant interest point detector".&#32;<it>Proceedings of European Conference on Computer Vision</it>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">F. Schaﬀalitzky and A. Zisserman&#32;(2002). "Multi-view matching for unordered image sets, or “How do I organize my holiday snaps?”".&#32;<it>Proceedings of European Conference on Computer Vision</it>: 414–431.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">T. Tuytelaars and L. Van Gool&#32;(2000). "Wide baseline stereo based on local, aﬃnely invariant regions".&#32;<it>Proceedings of British Machine Vision Conference</it>: 412–422.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">S. Agarwal and D. Roth&#32;(2002). "Learning a sparse representation for object detection".&#32;<it>Proceedings of European Conference on Computer Vision</it>: 113–130.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">E. Borenstein and S. Ullman&#32;(2002). "Class-speciﬁc, top-down segmentation".&#32;<it>Proceedings of European Conference on Computer Vision</it>: 109–124.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">R. Fergus, P. Perona, and A. Zisserman&#32;(2003). "Object class recognition by unsupervised scale-invariant learning".&#32;<it>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</it>: II:264–271.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">M. Weber, M. Welling, and P. Perona&#32;(June 20002). "Unsupervised learning of models for recognition".&#32;<it>Proceedings of European Conference on Computer Vision</it>.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>

</sec>
<sec>
<st>
See also</st>


</sec>
</bdy>
</article>
