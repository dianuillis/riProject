<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:17:29[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Parallel computing</title>
<id>145162</id>
<revision>
<id>243750391</id>
<timestamp>2008-10-07T22:06:35Z</timestamp>
<contributor>
<username>Giftlite</username>
<id>37986</id>
</contributor>
</revision>
<categories>
<category>Parallel computing</category>
<category>Featured articles</category>
<category>Concurrent computing</category>
<category>Distributed computing</category>
</categories>
</header>
<bdy>

<image location="right" width="150px" src="Cray_2_Arts_et_Metiers_dsc03940.jpg" type="thumb">
<caption>

The <link xlink:type="simple" xlink:href="../405/1231405.xml">
Cray-2</link> was the world's fastest computer from 1985 to 1989.
</caption>
</image>
<p>

<b>Parallel computing</b> is a form of <link xlink:type="simple" xlink:href="../213/5213.xml">
computation</link> in which many <link xlink:type="simple" xlink:href="../801/3149801.xml">
instructions</link> are carried out simultaneously,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> operating on the principle that large problems can often be divided into smaller ones, which are then solved <link xlink:type="simple" xlink:href="../467/928467.xml">
concurrently</link> ("in parallel"). There are several different forms of parallel computing: <link xlink:type="simple" xlink:href="../148/14229148.xml">
bit-level parallelism</link>, <link xlink:type="simple" xlink:href="../960/245960.xml">
instruction-level parallelism</link>, <link xlink:type="simple" xlink:href="../420/9467420.xml">
data parallelism</link>, and <link xlink:type="simple" xlink:href="../070/9468070.xml">
task parallelism</link>. It has been used for many years, mainly in <link xlink:type="simple" xlink:href="../527/832527.xml">
high-performance computing</link>, but interest in it has grown in recent years due to the physical constraints preventing <link xlink:type="simple" xlink:href="../329/8898329.xml">
frequency scaling</link>. Parallel computing has become the dominant paradigm in <link xlink:type="simple" xlink:href="../509/6509.xml">
computer architecture</link>, mainly in the form of <link xlink:type="simple" xlink:href="../207/3503207.xml">
multicore processor</link>s.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> However, in recent years, <link xlink:type="simple" xlink:href="../978/374978.xml">
power consumption</link> by parallel computers has become a concern.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

Parallel computers can be roughly classified according to the level at which the hardware supports parallelism—with <link xlink:type="simple" xlink:href="../207/3503207.xml">
multi-core</link> and <link xlink:type="simple" xlink:href="../318/50318.xml">
multi-processor</link> computers having multiple processing elements within a single machine, while <link xlink:type="simple" xlink:href="../896/18949896.xml">
clusters</link>, <link xlink:type="simple" xlink:href="../049/584049.xml">
MPPs</link>, and <link xlink:type="simple" xlink:href="../373/49373.xml">
grids</link> use multiple computers to work on the same task. </p>
<p>

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../840/148840.xml">
Parallel computer programs</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 are more difficult to write than sequential ones,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> because concurrency introduces several new classes of potential <link xlink:type="simple" xlink:href="../085/37085.xml">
software bug</link>s, of which <link xlink:type="simple" xlink:href="../661/98661.xml">
race condition</link>s are the most common. <link xlink:type="simple" xlink:href="../652/5652.xml">
Communication</link> and <link xlink:type="simple" xlink:href="../017/4726017.xml">
synchronization</link> between the different subtasks is typically one of the greatest barriers to getting good parallel program performance. The <link xlink:type="simple" xlink:href="../612/1448612.xml">
speed-up</link> of a program as a result of parallelization is given by <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<link xlink:type="simple" xlink:href="../ury/24th_century.xml">
Amdahl's law</link></rule>
</concept>
</idea>
.</p>

<sec>
<st>
Background</st>

<p>

Traditionally, computer software has been written for serial computation. To solve a problem, an <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> is constructed and implemented as a serial stream of instructions. These instructions are executed on a <link xlink:type="simple" xlink:href="../218/5218.xml">
central processing unit</link> on one computer. Only one instruction may execute at a time—after that instruction is finished, the next is executed.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>
<p>

Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>
<p>

<link xlink:type="simple" xlink:href="../329/8898329.xml">
Frequency scaling</link> was the dominant reason for improvements in computer performance  from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. An increase in frequency thus decreases runtime for all <link xlink:type="simple" xlink:href="../697/648697.xml">
computation-bounded</link> programs.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> </p>
<p>

However, <link xlink:type="simple" xlink:href="../978/374978.xml">
power consumption</link> by a chip is given by the equation P = C &amp;times; V2 &amp;times; F, where P is power, C is the <link xlink:type="simple" xlink:href="../711/140711.xml">
capacitance</link> being switched per clock cycle (proportional to the number of transistors whose inputs change), V is <link xlink:type="simple" xlink:href="../549/32549.xml">
voltage</link>, and F is the processor frequency (cycles per second).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref> Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption led ultimately to <link xlink:type="simple" xlink:href="../617/14617.xml">
Intel</link>'s May 2004 cancellation of its <link xlink:type="simple" xlink:href="../524/210524.xml">
Tejas and Jayhawk</link> processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> </p>
<p>

<link xlink:type="simple" xlink:href="../418/39418.xml">
Moore's Law</link> is the empirical observation that transistor density in a microprocessor doubles every 18 to 24&nbsp;months.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref> Despite power consumption issues, and repeated predictions of its end, Moore's law is still in effect. With the end of frequency scaling, these additional transistors (which are no longer used for frequency scaling) can be used to add extra hardware for parallel computing.</p>

<ss1>
<st>
Amdahl's law and Gustafson's law</st>
<p>

<image location="right" width="300px" src="Parallelization_graph.jpg" type="thumbnail">
<caption>

The program runtime and speed-up of a program with suboptimal parallelization. The blue curve illustrates the (linear) speed-up the program would have experienced in the optimal case, while the purple curve indicates the actual (suboptimal) speed-up. By the same token, the yellow curve indicates the runtime the program would have experienced in the optimal case (an <link xlink:type="simple" xlink:href="../107/3107.xml">
asymptote</link> which approaches zero), while the red curve indicates the actual runtime (an asymptote which approaches a value greater-than-zero)
</caption>
</image>
</p>
<p>

Theoretically, the speed-up from parallelization should be linear—doubling the number of processing elements should halve the runtime, and doubling it a second time should again halve the runtime. However, very few parallel algorithms achieve optimal speed-up. Most of them have a near-linear speed-up for small numbers of processing elements, which flattens out into a constant value for large numbers of processing elements. </p>
<p>

The potential speed-up of an algorithm on a parallel computing platform is given by <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<link xlink:type="simple" xlink:href="../ury/24th_century.xml">
Amdahl's law</link></rule>
</concept>
</idea>
, originally formulated by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<physicist wordnetid="110428004" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<pioneer wordnetid="110434725" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<originator wordnetid="110383816" confidence="0.8">
<creator wordnetid="109614315" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../897/247897.xml">
Gene Amdahl</link></associate>
</creator>
</originator>
</scientist>
</causal_agent>
</colleague>
</engineer>
</pioneer>
</person>
</physicist>
</peer>
</physical_entity>
 in the 1960s.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref> It states that a small portion of the program which cannot be parallelized will limit the overall speed-up available from parallelization. Any large mathematical or engineering problem will typically consist of several parallelizable parts and several non-parallelizable (sequential) parts. This relationship is given by the equation: </p>
<p>

<indent level="1">

<math>S = \frac{1}{(1 - P)}</math>
</indent>

where S is the speed-up of the program (as a factor of its original sequential runtime), and P is the fraction that is parallelizable. If the sequential portion of a program is 10% of the runtime, we can get no more than a 10x speed-up, regardless of how many processors are added. This puts an upper limit on the usefulness of adding more parallel execution units. "When a task cannot be partitioned because of sequential constraints, the application of more effort has no effect on the schedule. The bearing of a child takes nine months, no matter how many women are assigned."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref></p>
<p>

<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<link xlink:type="simple" xlink:href="../252/4243252.xml">
Gustafson's law</link></rule>
</concept>
</idea>
 is another law in computer engineering, closely related to Amdahl's law. It can be formulated as: </p>
<p>

<image width="400px" src="Optimizing-different-parts.svg" type="thumb">
<caption>

A graphical representation of <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<link xlink:type="simple" xlink:href="../ury/24th_century.xml">
Amdahl's law</link></rule>
</concept>
</idea>
 Assume that a task has two independent parts, A and B. B takes roughly 25% of the time of the whole computation. With effort, a programmer may be able to make this part five times faster, but this only reduces the time for the whole computation by a little. In contrast, one may need to perform less work to make part&nbsp;A twice as fast. This will make the computation much faster than by optimizing part&nbsp;B, even though B got a greater speed-up, (5x versus 2x).
</caption>
</image>
</p>
<p>

<indent level="1">

<math>\displaystyle S(P) = P - \alpha(P-1)</math>
</indent>

where P is the number of processors, S is the speed-up, and <math>\alpha</math> the non-parallelizable part of the process.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref> Amdahl's law assumes a fixed-problem size and that the size of the sequential section is independent of the number of processors, whereas Gustafson's law does not make these assumptions.</p>

</ss1>
<ss1>
<st>
Dependencies</st>
<p>

Understanding <link xlink:type="simple" xlink:href="../217/4118217.xml">
data dependencies</link> is fundamental in implementing <link xlink:type="simple" xlink:href="../840/148840.xml">
parallel algorithm</link>s. No program can run more quickly than the longest chain of dependent calculations (known as the <link xlink:type="simple" xlink:href="../621/498621.xml">
critical path</link>), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel. </p>
<p>

Let Pi and Pj be two program fragments. Bernstein's conditions<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref> describe when the two are independent and can be executed in parallel. Let Ii be all of the input variables to Pi and Oi the output variables, and likewise for Pj. P i and Pj are independent if they satisfy</p>
<p>

<list>
<entry level="1" type="bullet">

 <math> I_j \cap O_i  =  \emptyset</math></entry>
<entry level="1" type="bullet">

 <math> I_i \cap O_j  =  \emptyset</math></entry>
<entry level="1" type="bullet">

 <math> O_i \cap O_j  = \emptyset.</math></entry>
</list>
</p>
<p>

Violation of the first condition introduces a flow dependency, corresponding to the first statement producing a result used by the second statement. The second condition represents an <link>
anti-dependency</link>, when the first statement overwrites a variable needed by the second expression. The third and final condition, q, is an output dependency. When two variables write to the same location, the final output must have arisen from the second statement.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref></p>
<p>

Consider the following functions, which demonstrate several kinds of dependencies:</p>
<p>

1: function Dep(a, b)
2:    c := a·b
3:    d := 2·c
4: end function</p>
<p>

Operation 3 in Dep(a, b) cannot be executed before (or even in parallel with) operation&nbsp;2, because operation&nbsp;3 uses a result from operation&nbsp;2. It violates condition&nbsp;1, and thus introduces a flow dependency.</p>
<p>

1: function NoDep(a, b)
2:      c := a·b
3:      d := 2·b
4:      e := a+b
5: end function</p>
<p>

In this example, there are no dependencies between the instructions, so they can all be run in parallel.</p>
<p>

Bernstein’s conditions do not allow memory to be shared between different processes. For that, some means of enforcing an ordering between accesses is necessary, such as <link xlink:type="simple" xlink:href="../557/164557.xml">
semaphores</link>, <link xlink:type="simple" xlink:href="../263/4736263.xml">
barriers</link> or some other <link xlink:type="simple" xlink:href="../017/4726017.xml">
synchronization method</link>.</p>

</ss1>
<ss1>
<st>
Race conditions, mutual exclusion, synchronization, and parallel slowdown</st>

<p>

Subtasks in a parallel program are often called <link xlink:type="simple" xlink:href="../303/45303.xml">
threads</link>. Some parallel computer architectures use smaller, lightweight versions of threads known as <link xlink:type="simple" xlink:href="../712/5533712.xml">
fibers</link>, while others use bigger versions known as <link xlink:type="simple" xlink:href="../178/45178.xml">
processes</link>. However, "threads" is generally accepted as a generic term for subtasks. Threads will often need to update some <link xlink:type="simple" xlink:href="../818/32818.xml#xpointer(//*[./st=%22In+computer+programming%22])">
variable</link> that is shared between them. The instructions between the two programs may be <link xlink:type="simple" xlink:href="../495/177495.xml">
interleave</link>d in any order. For example, consider the following program:</p>
<p>

<table class="wikitable">
<row>
<col>
Thread A</col>
<col>
Thread B</col>
</row>
<row>
<col>
1A: Read variable V</col>
<col>
1B: Read variable V</col>
</row>
<row>
<col>
2A: Add 1 to variable V</col>
<col>
2B: Add 1 to variable V</col>
</row>
<row>
<col>
3A Write back to variable V</col>
<col>
3B: Write back to variable V</col>
</row>
</table>
</p>
<p>

If instruction 1B is executed between 1A and 3A, or if instruction 1A is executed between 1B and 3B, the program will produce incorrect data. This is known as a <link xlink:type="simple" xlink:href="../661/98661.xml">
race condition</link>. The programmer must use a <link xlink:type="simple" xlink:href="../593/244593.xml">
lock</link> to provide <link xlink:type="simple" xlink:href="../827/36827.xml">
mutual exclusion</link>. A lock is a programming language construct that allows one thread to take control of a variable and prevent other threads from reading or writing it, until that variable is unlocked. The thread holding the lock is free to execute its <link xlink:type="simple" xlink:href="../312/638312.xml">
critical section</link> (the section of a program that requires exclusive access to some variable), and to unlock the data when it is finished. Therefore, to guarantee correct program execution, the above program can be rewritten to use locks: </p>
<p>

<table class="wikitable">
<row>
<col>
Thread A</col>
<col>
Thread B</col>
</row>
<row>
<col>
1A: Lock variable V</col>
<col>
1B: Lock variable V</col>
</row>
<row>
<col>
2A: Read variable V</col>
<col>
2B: Read variable V</col>
</row>
<row>
<col>
3A: Add 1 to variable V</col>
<col>
3B: Add 1 to variable V</col>
</row>
<row>
<col>
4A Write back to variable V</col>
<col>
4B: Write back to variable V</col>
</row>
<row>
<col>
5A: Unlock variable V</col>
<col>
5B: Unlock variable V</col>
</row>
</table>
</p>
<p>

One thread will successfully lock variable V, while the other thread will be <plant_part wordnetid="113086908" confidence="0.8">
<natural_object wordnetid="100019128" confidence="0.8">
<kernel wordnetid="113137010" confidence="0.8">
<link xlink:type="simple" xlink:href="../798/12332798.xml">
locked out</link></kernel>
</natural_object>
</plant_part>
—unable to proceed until V is unlocked again. This guarantees correct execution of the program. Locks, while necessary to ensure correct program execution, can greatly slow a program.</p>
<p>

Locking multiple variables using <plant_part wordnetid="113086908" confidence="0.8">
<natural_object wordnetid="100019128" confidence="0.8">
<kernel wordnetid="113137010" confidence="0.8">
<link xlink:type="simple" xlink:href="../560/2114560.xml">
non-atomic</link></kernel>
</natural_object>
</plant_part>
 locks introduces the possibility of program <link xlink:type="simple" xlink:href="../181/105181.xml">
deadlock</link>. An atomic lock locks multiple variables all at once. If it cannot lock all of them, it does not lock any of them. If two threads each need to lock the same two variables using non-atomic locks, it is possible that one thread will lock one of them and the second thread will lock the second variable. In such a case, neither thread can complete, and deadlock results.</p>
<p>

Many parallel programs require that their subtasks <link xlink:type="simple" xlink:href="../017/4726017.xml">
act in synchrony</link>. This requires the use of a <link xlink:type="simple" xlink:href="../263/4736263.xml">
barrier</link>. Barriers are typically implemented using a software lock. One class of algorithms, known as <link xlink:type="simple" xlink:href="../864/554864.xml">
lock-free and wait-free algorithms</link>, altogether avoids the use of locks and barriers. However, this approach is generally difficult to implement and requires correctly designed data structures.</p>
<p>

Not all parallelization results in speed-up. Generally, as a task is split up into more and more threads, those threads spend an ever-increasing portion of their time communicating with each other. Eventually, the overhead from communication dominates the time spent solving the problem, and further parallelization (that is, splitting the workload over even more threads) increases rather than decreases the amount of time required to finish. This is known as <link xlink:type="simple" xlink:href="../068/15167068.xml">
parallel slowdown</link>.</p>

</ss1>
<ss1>
<st>
Fine-grained, coarse-grained, and embarrassing parallelism</st>
<p>

Applications are often classified according to how often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it is <link xlink:type="simple" xlink:href="../712/1738712.xml">
embarrassingly parallel</link> if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to parallelize.</p>

</ss1>
<ss1>
<st>
Consistency models</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../522/274522.xml">
Consistency model</link></it>
</indent>

<image upright="upright" location="right" width="150px" src="Leslie_Lamport.jpg" type="thumbnail">
</image>
</p>
<p>

Parallel programming languages and parallel computers must have a <link xlink:type="simple" xlink:href="../522/274522.xml">
consistency model</link> (also known as a memory model). The consistency model defines rules for how operations on <link xlink:type="simple" xlink:href="../300/5300.xml">
computer memory</link> occur and how results are produced. </p>
<p>

One of the first consistency models was <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../671/195671.xml">
Leslie Lamport</link></scientist>
's <link xlink:type="simple" xlink:href="../693/617693.xml">
sequential consistency</link> model. Sequential consistency is the property of a parallel program that its parallel execution produces the same results as a sequential program. Specifically, a program is sequentially consistent if "...&nbsp;the results of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref></p>
<p>

<message wordnetid="106598915" confidence="0.8">
<subject wordnetid="106599788" confidence="0.8">
<link xlink:type="simple" xlink:href="../707/1478707.xml">
Software transactional memory</link></subject>
</message>
 is a common type of consistency model. Software transactional memory borrows from <link xlink:type="simple" xlink:href="../513/8513.xml">
database theory</link> the concept of <link xlink:type="simple" xlink:href="../991/373991.xml">
atomic transactions</link> and applies them to memory accesses.</p>
<p>

Mathematically, these models can be represented in several ways. <language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../192/225192.xml">
Petri net</link></language>
s, which were introduced in Carl Adam Petri's 1962 doctoral thesis, were an early attempt to codify the rules of consistency models. Dataflow theory later built upon these, and <link xlink:type="simple" xlink:href="../032/1325032.xml">
Dataflow architecture</link>s were created to physically implement the ideas of dataflow theory. Beginning in the late 1970s, <link xlink:type="simple" xlink:href="../240/853240.xml">
process calculi</link> such as <link xlink:type="simple" xlink:href="../372/420372.xml">
calculus of communicating systems</link> and <link xlink:type="simple" xlink:href="../370/247370.xml">
communicating sequential processes</link> were developed to permit algebraic reasoning about systems composed of interacting components. More recent additions to the process calculus family, such as the <link xlink:type="simple" xlink:href="../373/420373.xml">
&amp;pi;-calculus</link>, have added the capability for reasoning about dynamic topologies. Logics such as Lamport's <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../113/2214113.xml">
TLA+</link></method>
</know-how>
, and mathematical models such as <language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../131/10815131.xml">
traces</link></language>
 and <knowledge_domain wordnetid="105999266" confidence="0.8">
<discipline wordnetid="105996646" confidence="0.8">
<mathematics wordnetid="106000644" confidence="0.8">
<science wordnetid="105999797" confidence="0.8">
<link xlink:type="simple" xlink:href="../283/2194283.xml">
Actor event diagrams</link></science>
</mathematics>
</discipline>
</knowledge_domain>
, have also been developed to describe the behavior of concurrent systems.</p>

</ss1>
<ss1>
<st>
Flynn's taxonomy</st>

<p>

<person wordnetid="100007846" confidence="0.9638700866880419">
<link xlink:type="simple" xlink:href="../062/7331062.xml">
Michael J. Flynn</link></person>
 created one of the earliest classification systems for parallel (and sequential) computers and programs, now known as <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../349/222349.xml">
Flynn's taxonomy</link></group>
</collection>
</class>
. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, whether or not those instructions were using a single or multiple sets of data.</p>

<p>

<table style="float: right; clear: right; margin: 0 0 1em 1em; text-align: center;" class="wikitable">
<caption>
<b><class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../349/222349.xml">
Flynn's taxonomy</link></group>
</collection>
</class>
</b></caption>
<row>
<header>
&nbsp;</header>
<header>
Single  Instruction</header>
<header>
Multiple  Instruction</header>
</row>
<row>
<header>
Single  Data</header>
<col>
 <link xlink:type="simple" xlink:href="../630/1103630.xml">
SISD</link></col>
<col>
 <link xlink:type="simple" xlink:href="../666/991666.xml">
MISD</link></col>
</row>
<row>
<header>
Multiple  Data</header>
<col>
 <link xlink:type="simple" xlink:href="../359/55359.xml">
SIMD</link></col>
<col>
 <link xlink:type="simple" xlink:href="../139/157139.xml">
MIMD</link></col>
</row>
<row>
<col colspan="3" style="background: #ccd; text-align:right;"></col>
</row>
</table>
</p>
<p>

The single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in <link xlink:type="simple" xlink:href="../324/29324.xml">
signal processing</link> applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures to deal with this were devised (such as <link xlink:type="simple" xlink:href="../517/351517.xml">
systolic array</link>s), few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far the most common type of parallel programs.</p>
<p>

According to <link xlink:type="simple" xlink:href="../239/816239.xml">
David A. Patterson</link> and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../550/437550.xml">
John L. Hennessy</link></scientist>
, "Some machines are hybrids of these categories, of course, but this classic model has survived because it is simple, easy to understand, and gives a good first approximation. It is also—perhaps because of its understandability—the most widely used scheme."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref></p>

</ss1>
</sec>
<sec>
<st>
Types of parallelism</st>

<ss1>
<st>
Bit-level parallelism</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../148/14229148.xml">
Bit-level parallelism</link></it>
</indent>
From the advent of <link xlink:type="simple" xlink:href="../823/32823.xml">
very-large-scale integration</link> (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../344/1613344.xml">
computer word size</link></kind>
</type>
</definite_quantity>
</unit_of_measurement>
</category>
</concept>
</idea>
—the amount of information the processor can execute per cycle.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref> Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word. For example, where an <link xlink:type="simple" xlink:href="../148/45148.xml">
8-bit</link> processor must add two <link xlink:type="simple" xlink:href="../535/64535.xml">
16-bit</link> <link xlink:type="simple" xlink:href="../563/14563.xml">
integer</link>s, the processor must first add the 8&nbsp;lower-order bits from each integer using the standard addition instruction, then add the 8&nbsp;higher-order bits using an add-with-carry instruction and the <link xlink:type="simple" xlink:href="../176/5503176.xml">
carry bit</link> from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction.</p>
<p>

Historically, <link xlink:type="simple" xlink:href="../346/520346.xml">
4-bit</link> microprocessors were replaced with 8-bit, then 16-bit, then 32-bit microprocessors. This trend generally came to an end with the introduction of 32-bit processors, which has been a standard in general-purpose computing for two decades. Not until recently (c. 2003&ndash;2004), with the advent of <link xlink:type="simple" xlink:href="../374/244374.xml">
x86-64</link> architectures, have <link xlink:type="simple" xlink:href="../285/148285.xml">
64-bit</link> processors become commonplace.</p>

</ss1>
<ss1>
<st>
Instruction-level parallelism</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../960/245960.xml">
Instruction level parallelism</link></it>
</indent>

<image width="300px" src="Fivestagespipeline.png" type="thumb">
<caption>

A canonical five-stage pipeline in a <link xlink:type="simple" xlink:href="../672/3963672.xml">
RISC</link> machine (IF = Instruction Fetch, ID = Instruction Decode, EX = Execute, MEM = Memory access, WB = Register write back)
</caption>
</image>
</p>
<p>

A computer program is, in essence, a stream of instructions executed by a processor. These instructions can be <link xlink:type="simple" xlink:href="../307/1002307.xml">
re-ordered</link> and combined into groups which are then executed in parallel without changing the result of the program. This is known as instruction-level parallelism. Advances in instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2218%22])">18</ref></p>
<p>

Modern processors have multi-stage <link xlink:type="simple" xlink:href="../314/220314.xml">
instruction pipeline</link>s. Each stage in the pipeline corresponds to a different action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to N different instructions at different stages of completion. The canonical example of a pipelined processor is a RISC processor, with five stages: instruction fetch, decode, execute, memory access, and write back. The <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../228/165228.xml">
Pentium 4</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
 processor had a 35-stage pipeline.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2219%22])">19</ref></p>
<p>

<image width="300px" src="Superscalarpipeline.png" type="thumb">
<caption>

A five-stage pipelined superscalar processor, capable of issuing two instructions per cycle. It can have two instructions in each stage of the pipeline, for a total of up to 10&nbsp;instructions (shown in green) being simultaneously executed.
</caption>
</image>
 </p>
<p>

In addition to instruction-level parallelism from pipelining, some processors can issue more than one instruction at a time. These are known as <link xlink:type="simple" xlink:href="../702/51702.xml">
superscalar</link> processors. Instructions can be grouped together only if there is no <link xlink:type="simple" xlink:href="../217/4118217.xml">
data dependency</link> between them. <link xlink:type="simple" xlink:href="../102/4543102.xml">
Scoreboarding</link> and the <link xlink:type="simple" xlink:href="../562/390562.xml">
Tomasulo algorithm</link> (which is similar to scoreboarding but makes use of <link xlink:type="simple" xlink:href="../852/435852.xml">
register renaming</link>) are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism.</p>

</ss1>
<ss1>
<st>
Data parallelism</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../420/9467420.xml">
Data parallelism</link></it>
</indent>

Data parallelism is parallelism inherent in <link xlink:type="simple" xlink:href="../459/45459.xml#xpointer(//*[./st=%22Loops%22])">
program loops</link>, which focuses on distributing the data across different computing nodes to be processed in parallel. "Parallelizing loops often leads to similar (not necessarily identical) operation sequences or functions being performed on elements of a large data structure."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2220%22])">20</ref> Many scientific and engineering applications exhibit data parallelism.</p>
<p>

A loop-carried dependency is the dependence of a loop iteration on the output of one or more previous iterations. Loop-carried dependencies prevent the parallelization of loops. For example, consider the following <link xlink:type="simple" xlink:href="../185/24185.xml">
pseudocode</link> that computes the first few <link xlink:type="simple" xlink:href="../918/10918.xml">
Fibonacci number</link>s: </p>
<p>

1:    PREV2 := 0
2:    PREV1 := 1
3:    CUR := 1
4:    do:
5:       CUR := PREV1 + PREV2
6:       PREV2 := PREV1
7:       PREV1 := CUR
8:    while (CUR  10) </p>
<p>

This loop cannot be parallelized because CUR depends on itself (PREV1) and PREV2, which are computed in each loop iteration. Since each iteration depends on the result of the previous one, they cannot be performed in parallel. As the size of a problem gets bigger, the amount of data-parallelism available usually does as well.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2221%22])">21</ref></p>

</ss1>
<ss1>
<st>
Task parallelism</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../070/9468070.xml">
Task parallelism</link></it>
</indent>
Task parallelism is the characteristic of a parallel program that "entirely different calculations can be performed on either the same or different sets of data".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2220%22])">20</ref> This contrasts with data parallelism, where the same calculation is performed on the same or different sets of data. Task parallelism does not usually scale with the size of a problem.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2221%22])">21</ref></p>

</ss1>
</sec>
<sec>
<st>
Hardware</st>

<ss1>
<st>
Memory and communication</st>
<p>

Main memory in a parallel computer is either <link xlink:type="simple" xlink:href="../653/825653.xml">
shared memory</link> (shared between all processing elements in a single <link xlink:type="simple" xlink:href="../144/507144.xml">
address space</link>), or <link xlink:type="simple" xlink:href="../887/234887.xml">
distributed memory</link> (in which each processing element has its own local address space).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2222%22])">22</ref> Distributed memory refers to the fact that the memory is logically distributed, but often implies that it is physically distributed as well. <link xlink:type="simple" xlink:href="../843/399843.xml">
Distributed shared memory</link> is a combination of the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory. </p>
<p>

<image location="right" width="400px" src="Numa.svg" type="thumbnail">
<caption>

A logical view of a <link xlink:type="simple" xlink:href="../643/40643.xml">
Non-Uniform Memory Access</link> (NUMA) architecture. Processors in one directory can access that directory's memory with less latency than they can access memory in the other directory's memory.
</caption>
</image>
 </p>
<p>

Computer architectures in which each element of main memory can be accessed with equal <link xlink:type="simple" xlink:href="../872/5016872.xml">
latency</link> and <link xlink:type="simple" xlink:href="../827/15612827.xml">
bandwidth</link> are known as <link xlink:type="simple" xlink:href="../277/4346277.xml">
Uniform Memory Access</link> (UMA) systems. Typically, that can be achieved only by a <link xlink:type="simple" xlink:href="../653/825653.xml">
shared memory</link> system, in which the memory is not physically distributed. A system that does not have this property is known as a <link xlink:type="simple" xlink:href="../643/40643.xml">
Non-Uniform Memory Access</link> (NUMA) architecture. Distributed memory systems have non-uniform memory access.  </p>
<p>

Computer systems make use of <link xlink:type="simple" xlink:href="../829/6829.xml">
cache</link>s—small, fast memories located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value in more than one location, with the possibility of incorrect program execution. These computers require a <link xlink:type="simple" xlink:href="../865/176865.xml">
cache coherency</link> system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. <link xlink:type="simple" xlink:href="../289/357289.xml">
Bus snooping</link> is one of the most common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared-memory computer architectures do not scale as well as distributed memory systems do.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2222%22])">22</ref></p>
<p>

Processor–processor and processor–memory communication can be implemented in hardware in several ways, including via shared (either multiported or <message wordnetid="106598915" confidence="0.8">
<protocol wordnetid="106665108" confidence="0.8">
<direction wordnetid="106786629" confidence="0.8">
<rule wordnetid="106652242" confidence="0.8">
<link xlink:type="simple" xlink:href="../389/41389.xml">
multiplexed</link></rule>
</direction>
</protocol>
</message>
) memory, a <link xlink:type="simple" xlink:href="../456/45456.xml">
crossbar switch</link>, a shared <link xlink:type="simple" xlink:href="../631/6631.xml">
bus</link> or an interconnect network of a myriad of <link xlink:type="simple" xlink:href="../413/41413.xml">
topologies</link> including <link xlink:type="simple" xlink:href="../244/28244.xml">
star</link>, <link xlink:type="simple" xlink:href="../001/957001.xml">
ring</link>, <graph wordnetid="107000195" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../560/48560.xml">
tree</link></graph>
, <graph wordnetid="107000195" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../815/6706815.xml">
hypercube</link></graph>
, fat hypercube (a hypercube with more than one processor at a node), or <link xlink:type="simple" xlink:href="../687/912687.xml">
n-dimensional mesh</link>.  </p>
<p>

Parallel computers based on interconnect networks need to have some kind of <link xlink:type="simple" xlink:href="../750/25750.xml">
routing</link> to enable the passing of messages between nodes that are not directly connected. The medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines.</p>

</ss1>
<ss1>
<st>
Classes of parallel computers</st>
<p>

Parallel computers can be roughly classified according to the level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes. These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common. </p>

<ss2>
<st>
Multicore computing</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../207/3503207.xml">
Multi-core (computing)</link></it>
</indent>
A multicore processor is a processor that includes multiple <link xlink:type="simple" xlink:href="../828/9828.xml">
execution unit</link>s ("cores"). These processors differ from superscalar processors, which can issue multiple instructions per cycle from one instruction stream (thread); by contrast, a multicore processor can issue multiple instructions per cycle from multiple instruction streams. Each core in a multicore processor can potentially be superscalar as well—that is, on every cycle, each core can issue multiple instructions from one instruction stream. </p>
<p>

<link xlink:type="simple" xlink:href="../021/315021.xml">
Simultaneous multithreading</link> (of which Intel's <link xlink:type="simple" xlink:href="../443/151443.xml">
HyperThreading</link> is the best known) was an early form of pseudo-multicoreism. A processor capable of simultaneous multithreading has only one execution unit ("core"), but when that execution unit is idling (such as during a <link>
cache miss</link>), it uses that execution unit to process a second thread. Intel's <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../080/3429080.xml">
Core</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
 and <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../988/5048988.xml">
Core 2</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
 processor families are Intel's first true multicore architectures. <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link></company>
's <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../950/803950.xml">
Cell microprocessor</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
, designed for use in the <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../989/26989.xml">
Sony</link></company>
 <link xlink:type="simple" xlink:href="../687/803687.xml">
Playstation 3</link>, is another prominent multicore processor.</p>

</ss2>
<ss2>
<st>
Symmetric multiprocessing</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../318/50318.xml">
Symmetric multiprocessing</link></it>
</indent>
A symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2223%22])">23</ref> <link xlink:type="simple" xlink:href="../507/376507.xml">
Bus contention</link> prevents bus architectures from scaling. As a result, SMPs generally do not comprise more than 32&nbsp;processors.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2224%22])">24</ref> "Because of the small size of the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches, such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2223%22])">23</ref></p>

</ss2>
<ss2>
<st>
Distributed computing</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../501/8501.xml">
Distributed computing</link></it>
</indent>
A distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. Distributed computers are highly scalable. </p>

<ss3>
<st>
Cluster computing</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../896/18949896.xml">
Computer cluster</link></it>
</indent>
<image upright="upright" location="right" width="150px" src="Beowulf.jpg" type="thumbnail">
<caption>

A <link xlink:type="simple" xlink:href="../542/66542.xml">
Beowulf cluster</link>
</caption>
</image>
</p>
<p>

A cluster is a group of loosely coupled computers that work together closely, so that in some respects they can be regarded as a single computer.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2225%22])">25</ref> Clusters are composed of multiple standalone machines connected by a network. While machines in a cluster do not have to be symmetric, <link xlink:type="simple" xlink:href="../118/61118.xml">
load balancing</link> is more difficult if they are not. The most common type of cluster is the <link xlink:type="simple" xlink:href="../542/66542.xml">
Beowulf cluster</link>, which is a cluster implemented on multiple identical <link xlink:type="simple" xlink:href="../273/2425273.xml">
commercial off-the-shelf</link> computers connected with a <link xlink:type="simple" xlink:href="../476/15476.xml">
TCP/IP</link> <link xlink:type="simple" xlink:href="../499/9499.xml">
Ethernet</link> <link xlink:type="simple" xlink:href="../739/17739.xml">
local area network</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2226%22])">26</ref> Beowulf technology was originally developed by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<computer_user wordnetid="109951274" confidence="0.8">
<programmer wordnetid="110481268" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../539/10440539.xml">
Thomas Sterling</link></causal_agent>
</engineer>
</programmer>
</computer_user>
</person>
</physical_entity>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<computer_user wordnetid="109951274" confidence="0.8">
<programmer wordnetid="110481268" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../205/173205.xml">
Donald Becker</link></causal_agent>
</engineer>
</programmer>
</computer_user>
</person>
</physical_entity>
. The vast majority of the <site wordnetid="108651247" confidence="0.8">
<region wordnetid="108630985" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<link xlink:type="simple" xlink:href="../532/11976532.xml">
TOP500</link></geographical_area>
</tract>
</location>
</region>
</site>
 supercomputers are clusters.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2227%22])">27</ref></p>

</ss3>
<ss3>
<st>
Massive parallel processing</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../049/584049.xml">
Massive parallel processing</link></it>
</indent>

A massively parallel processor (MPP) is a single computer with many networked processors. MPPs have many of the same characteristics as clusters, but they are usually larger, typically having "far more" than 100&nbsp;processors.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2228%22])">28</ref> In an MPP, "each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2229%22])">29</ref></p>
<p>

<image upright="upright" location="right" width="150px" src="BlueGeneL_cabinet.jpg" type="thumbnail">
<caption>

A cabinet from <computer wordnetid="103082979" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<mainframe wordnetid="103711711" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<digital_computer wordnetid="103196324" confidence="0.8">
<supercomputer wordnetid="104358117" confidence="0.8">
<link xlink:type="simple" xlink:href="../764/136764.xml">
Blue Gene</link></supercomputer>
</digital_computer>
</machine>
</device>
</mainframe>
</instrumentality>
</artifact>
</computer>
/L, ranked as the second fastest supercomputer in the world according to the 6/2008 <site wordnetid="108651247" confidence="0.8">
<region wordnetid="108630985" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<link xlink:type="simple" xlink:href="../532/11976532.xml">
TOP500</link></geographical_area>
</tract>
</location>
</region>
</site>
 rankings. Blue Gene/L is a massively parallel processor.
</caption>
</image>
</p>
<p>

<link xlink:type="simple" xlink:href="../764/136764.xml">
Blue Gene/L</link>, the second fastest supercomputer in the world according to the TOP500 ranking, is an MPP.</p>

</ss3>
<ss3>
<st>
Grid computing</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../373/49373.xml">
Grid computing</link></it>
</indent>
Grid computing is the most distributed form of parallel computing. It makes use of computers communicating over the <link xlink:type="simple" xlink:href="../539/14539.xml">
Internet</link> to work on a given problem. Because of the low bandwidth and extremely high latency available on the Internet, grid computing typically deals only with embarrassingly parallel problems. <link xlink:type="simple" xlink:href="../522/1336522.xml">
Many grid computing applications</link> have been created, of which <link xlink:type="simple" xlink:href="../768/309768.xml">
SETI@home</link> and <link xlink:type="simple" xlink:href="../102/413102.xml">
Folding@Home</link> are the best-known examples.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2230%22])">30</ref></p>
<p>

Most grid computing applications use <link xlink:type="simple" xlink:href="../183/57183.xml">
middleware</link>, software that sits between the operating system and the application to manage network resources and standardize the software interface. The most common grid computing middleware is the <link xlink:type="simple" xlink:href="../896/346896.xml">
Berkeley Open Infrastructure for Network Computing</link> (BOINC). Often, grid computing software makes use of "spare cycles", performing computations at times when a computer is idling.</p>

</ss3>
</ss2>
<ss2>
<st>
Specialized parallel computers</st>
<p>

Within parallel computing, there are specialized parallel devices that remain niche areas of interest. While not <link xlink:type="simple" xlink:href="../239/519239.xml">
domain-specific</link>, they tend to be applicable to only a few classes of parallel problems. </p>
<p>

<list>
<entry level="1" type="definition">

Reconfigurable computing with field-programmable gate arrays</entry>
</list>

<link xlink:type="simple" xlink:href="../371/188371.xml">
Reconfigurable computing</link> is the use of a <link xlink:type="simple" xlink:href="../969/10969.xml">
field-programmable gate array</link> (FPGA) as a co-processor to a general-purpose computer. An FPGA is, in essence, a computer chip that can rewire itself for a given task.  </p>
<p>

FPGAs can be programmed with <link xlink:type="simple" xlink:href="../554/74554.xml">
hardware description language</link>s such as <language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../410/43410.xml">
VHDL</link></language>
 or <standard wordnetid="107260623" confidence="0.8">
<language wordnetid="106282651" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../863/63863.xml">
Verilog</link></system_of_measurement>
</language>
</standard>
. However, programming in these languages can be tedious. Several vendors have created <link xlink:type="simple" xlink:href="../023/9616023.xml">
C to HDL</link> languages that attempt to emulate the syntax and/or semantics of the <link xlink:type="simple" xlink:href="../021/6021.xml">
C programming language</link>, with which most programmers are familiar. The best known C to HDL languages are <link>
Mitrion-C</link>, <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../879/8715879.xml">
Impulse C</link></programming_language>
, <arrangement wordnetid="107938773" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<array wordnetid="107939382" confidence="0.8">
<link xlink:type="simple" xlink:href="../148/9750148.xml">
DIME-C</link></array>
</group>
</arrangement>
, and <language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../144/4244144.xml">
Handel-C</link></language>
.</p>
<p>

AMD's decision to open its <link xlink:type="simple" xlink:href="../270/143270.xml">
HyperTransport</link> technology to third-party vendors has become the enabling technology for high-performance reconfigurable computing.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2231%22])">31</ref> According to Michael R. D'Amour, CEO of <link>
DRC Computer Corporation</link>, "when we first walked into AMD, they called us 'the <link xlink:type="simple" xlink:href="../630/593630.xml">
socket</link> stealers.' Now they call us their partners."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2231%22])">31</ref></p>
<p>

<list>
<entry level="1" type="definition">

GPGPU with graphics processing units</entry>
</list>
</p>
<p>

<indent level="1">

<it>Main article: <substance wordnetid="100019613" confidence="0.8">
<paper wordnetid="114974264" confidence="0.8">
<card wordnetid="102962545" confidence="0.8">
<part wordnetid="113809207" confidence="0.8">
<material wordnetid="114580897" confidence="0.8">
<link xlink:type="simple" xlink:href="../939/1268939.xml">
GPGPU</link></material>
</part>
</card>
</paper>
</substance>
</it>
</indent>

<image location="right" width="150px" src="NvidiaTesla.jpg" type="thumbnail">
<caption>

Nvidia's <link xlink:type="simple" xlink:href="../598/11894598.xml">
Tesla GPGPU card</link>
</caption>
</image>
</p>
<p>

General-purpose computing on <link xlink:type="simple" xlink:href="../214/390214.xml">
graphics processing unit</link>s (GPGPU) is a fairly recent trend in computer engineering research. GPUs are co-processors that have been heavily optimized for <link xlink:type="simple" xlink:href="../210/18567210.xml">
computer graphics</link> processing.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2232%22])">32</ref> Computer graphics processing is a field dominated by data parallel operations—particularly <link xlink:type="simple" xlink:href="../422/18422.xml">
linear algebra</link> <link xlink:type="simple" xlink:href="../728/19008728.xml">
matrix</link> operations. </p>
<p>

In the early days, GPGPU programs used the normal graphics APIs for executing programs. However, recently several new programming languages and platforms have been built to do general purpose computation on GPUs with both <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../120/39120.xml">
Nvidia</link></company>
 and <link xlink:type="simple" xlink:href="../ury/24th_century.xml">
AMD</link> releasing programming environments with <substance wordnetid="100019613" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<engine wordnetid="103287733" confidence="0.8">
<paper wordnetid="114974264" confidence="0.8">
<motor wordnetid="103789946" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<card wordnetid="102962545" confidence="0.8">
<part wordnetid="113809207" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<material wordnetid="114580897" confidence="0.8">
<link xlink:type="simple" xlink:href="../386/7933386.xml">
CUDA</link></material>
</machine>
</part>
</card>
</device>
</motor>
</paper>
</engine>
</instrumentality>
</artifact>
</substance>
 and <occupation wordnetid="100582388" confidence="0.8">
<application wordnetid="100949134" confidence="0.8">
<technology wordnetid="100949619" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<profession wordnetid="100609953" confidence="0.8">
<use wordnetid="100947128" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../269/9535269.xml">
CTM</link></activity>
</psychological_feature>
</act>
</use>
</profession>
</event>
</technology>
</application>
</occupation>
 respectively. Other GPU programming languages are <link xlink:type="simple" xlink:href="../936/3421936.xml">
BrookGPU</link>, <link xlink:type="simple" xlink:href="../199/11647199.xml">
PeakStream</link>, and <link xlink:type="simple" xlink:href="../503/9857503.xml">
RapidMind</link>. Nvidia has also released specific products for computation in their <link xlink:type="simple" xlink:href="../598/11894598.xml">
Tesla series</link>. </p>
<p>

<list>
<entry level="1" type="definition">

Application-specific integrated circuits</entry>
</list>
</p>
<p>

<indent level="1">

<it>Main article: <arrangement wordnetid="107938773" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<array wordnetid="107939382" confidence="0.8">
<link xlink:type="simple" xlink:href="../845/147845.xml">
Application-specific integrated circuit</link></array>
</group>
</arrangement>
</it>
</indent>
Several <arrangement wordnetid="107938773" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<array wordnetid="107939382" confidence="0.8">
<link xlink:type="simple" xlink:href="../845/147845.xml">
Application-specific integrated circuit</link></array>
</group>
</arrangement>
 (ASIC) approaches have been devised for dealing with parallel applications.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2233%22])">33</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2234%22])">34</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2235%22])">35</ref></p>
<p>

Because an ASIC is (by definition) specific to a given application, it can be fully optimized for that application. As a result, for a given application, an ASIC tends to outperform a general-purpose computer. However, ASICs are created by <link xlink:type="simple" xlink:href="../136/3455136.xml">
X-ray lithography</link>. This process requires a mask, which can be extremely expensive. A single mask can cost over a million US dollars.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2236%22])">36</ref> (The smaller the transistors required for the chip, the more expensive the mask will be.) Meanwhile, performance increases in general-purpose computing over time (as described by Moore's Law) tend to wipe out these gains in only one or two chip generations.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2231%22])">31</ref> High initial cost, and the tendency to be overtaken by Moore's-law-driven general-purpose computing, has rendered ASICs unfeasible for most parallel computing applications. However, some have been built. One example is the peta-flop <link xlink:type="simple" xlink:href="../050/10096050.xml">
RIKEN MDGRAPE-3</link> machine which uses custom ASICs for <link xlink:type="simple" xlink:href="../608/198608.xml">
molecular dynamics</link> simulation.</p>
<p>

<list>
<entry level="1" type="definition">

Vector processors</entry>
</list>
</p>
<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../205/58205.xml">
Vector processor</link></it>
</indent>
<image location="right" width="150px" src="Cray-1-p1010221.jpg" type="thumbnail">
<caption>

The <link xlink:type="simple" xlink:href="../171/37171.xml">
Cray-1</link> is the most famous vector processor.
</caption>
</image>
</p>
<p>

A vector processor is a CPU or computer system that can execute the same instruction on large sets of data. "Vector processors have high-level operations that work on linear arrays of numbers or vectors. An example vector operation is <it>A</it> = <it>B</it> &amp;times; <it>C</it>, where <it>A</it>, <it>B</it>, and <it>C</it> are each 64-element vectors of 64-bit <link xlink:type="simple" xlink:href="../376/11376.xml">
floating-point</link> numbers."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2237%22])">37</ref> They are closely related to Flynn's SIMD classification.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2237%22])">37</ref></p>
<p>

<company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../315/217315.xml">
Cray</link></company>
 computers became famous for their vector-processing computers in the 1970s and 1980s. However, vector processors—both as CPUs and as full computer systems—have generally disappeared. Modern <link xlink:type="simple" xlink:href="../772/47772.xml">
processor instruction sets</link> do include some vector processing instructions, such as with <link xlink:type="simple" xlink:href="../360/55360.xml">
AltiVec</link> and <message wordnetid="106598915" confidence="0.8">
<direction wordnetid="106786629" confidence="0.8">
<link xlink:type="simple" xlink:href="../365/55365.xml">
Streaming SIMD Extensions</link></direction>
</message>
 (SSE).</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
Software</st>

<ss1>
<st>
Parallel programming languages</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../375/2242375.xml">
parallel programming model</link></it>
</indent>

Concurrent programming languages|Concurrent programming languages, <message wordnetid="106598915" confidence="0.8">
<information wordnetid="106634376" confidence="0.8">
<structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<format wordnetid="106636806" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../421/106421.xml">
libraries</link></room>
</format>
</library>
</area>
</artifact>
</structure>
</information>
</message>
, <link xlink:type="simple" xlink:href="../ury/24th_century.xml">
APIs</link>, and <link xlink:type="simple" xlink:href="../375/2242375.xml">
parallel programming model</link>s have been created for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses <link xlink:type="simple" xlink:href="../867/1324867.xml">
message passing</link>. <standard wordnetid="107260623" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../671/634671.xml">
POSIX Threads</link></system_of_measurement>
</standard>
 and <link xlink:type="simple" xlink:href="../842/381842.xml">
OpenMP</link> are two of most widely used shared memory APIs, whereas <link xlink:type="simple" xlink:href="../466/221466.xml">
Message Passing Interface</link> (MPI) is the most widely used message-passing system API. One concept used in programming parallel programs is the <link xlink:type="simple" xlink:href="../507/2203507.xml">
future concept</link>, where one part of a program promises to deliver a required datum to another part of a program at some future time. </p>

</ss1>
<ss1>
<st>
Automatic parallelization</st>

<p>

<indent level="1">

<it>Main article: <change_of_state wordnetid="100199130" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<change wordnetid="100191142" confidence="0.8">
<improvement wordnetid="100248977" confidence="0.8">
<action wordnetid="100037396" confidence="0.8">
<optimization wordnetid="100260051" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../264/2537264.xml">
Automatic parallelization</link></psychological_feature>
</act>
</optimization>
</action>
</improvement>
</change>
</event>
</change_of_state>
</it>
</indent>
Automatic parallelization of a sequential program by a <link xlink:type="simple" xlink:href="../739/5739.xml">
compiler</link> is the "<link xlink:type="simple" xlink:href="../322/14322.xml#xpointer(//*[./st=%22Casual+metaphor%22])">
holy grail</link>" of parallel computing. Despite decades of work by compiler researchers, automatic parallelization has had only limited success.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2238%22])">38</ref></p>
<p>

Mainstream parallel programming languages remain either <link xlink:type="simple" xlink:href="../332/3095332.xml">
explicitly parallel</link> or (at best) <link xlink:type="simple" xlink:href="../888/3453888.xml">
partially implicit</link>, in which a programmer gives the compiler <link xlink:type="simple" xlink:href="../128/930128.xml">
directives</link> for parallelization. A few fully implicit parallel programming languages exist—<link xlink:type="simple" xlink:href="../406/57406.xml">
SISAL</link>, Parallel <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../630/13630.xml">
Haskell</link></programming_language>
, and (for FPGAs) <link>
Mitrion-C</link>—but these are niche languages that are not widely used.</p>

</ss1>
<ss1>
<st>
Application checkpointing</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../765/416765.xml">
Application checkpointing</link></it>
</indent>
The larger and more complex a computer, the more that can go wrong and the shorter the <link xlink:type="simple" xlink:href="../397/63397.xml">
mean time between failures</link>. <link xlink:type="simple" xlink:href="../765/416765.xml">
Application checkpointing</link> is a technique whereby the computer system takes a "snapshot" of the application—a record of all current resource allocations and variable states, akin to a <link xlink:type="simple" xlink:href="../721/49721.xml">
core dump</link>; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. For an application that may run for months, that is critical. Application checkpointing may be used to facilitate <link xlink:type="simple" xlink:href="../821/9183821.xml">
process migration</link>.</p>

</ss1>
</sec>
<sec>
<st>
Applications</st>
<p>

As parallel computers become larger and faster, it becomes feasible to solve problems that previously took too long to run. Parallel computing is used in a wide range of fields, from <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link> (to do <link xlink:type="simple" xlink:href="../085/52085.xml">
protein folding</link>) to economics (to do simulation in <link xlink:type="simple" xlink:href="../244/250244.xml">
mathematical finance</link>). Common types of problems found in parallel computing applications are:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2239%22])">39</ref> </p>
<p>

<list>
<entry level="1" type="bullet">

 Dense <link xlink:type="simple" xlink:href="../422/18422.xml">
linear algebra</link></entry>
<entry level="1" type="bullet">

 Sparse linear algebra</entry>
<entry level="1" type="bullet">

 Spectral methods (such as <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../702/352702.xml">
Cooley-Tukey Fast Fourier transform</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
)</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../686/4917686.xml">
N-body problems</link> (such as <link xlink:type="simple" xlink:href="../772/9394772.xml">
Barnes-Hut simulation</link>) </entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../917/3771917.xml">
Structured grid</link> problems (such as <link xlink:type="simple" xlink:href="../413/5074413.xml">
Lattice Boltzmann methods</link>), </entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../072/3772072.xml">
Unstructured grid</link> problems (such as found in <link xlink:type="simple" xlink:href="../581/18233581.xml">
finite element analysis</link>)</entry>
<entry level="1" type="bullet">

 <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo simulation</link></method>
</know-how>
</technique>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../371/89371.xml">
Combinational logic</link> (such as <operation wordnetid="100955060" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<attack wordnetid="100972621" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../784/53784.xml">
brute-force cryptographic techniques</link></activity>
</psychological_feature>
</act>
</attack>
</event>
</operation>
) </entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../731/6263731.xml">
Graph traversal</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (such as <link xlink:type="simple" xlink:href="../442/28442.xml">
Sorting algorithm</link>s)</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../297/125297.xml">
Dynamic programming</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../580/456580.xml">
Branch and bound</link> methods</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<system wordnetid="108435388" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<network wordnetid="108434259" confidence="0.8">
<link xlink:type="simple" xlink:href="../298/447298.xml">
Graphical model</link></network>
</group>
</causal_agent>
</worker>
</assistant>
</model>
</person>
</system>
</physical_entity>
s (such as detecting <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../770/98770.xml">
Hidden Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s and constructing <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian network</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s)</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../931/10931.xml">
Finite State Machine</link> simulation</entry>
</list>
</p>

</sec>
<sec>
<st>
History</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../519/386519.xml">
History of computing</link></it>
</indent>

<image location="right" width="150px" src="ILLIAC_4_parallel_computer.jpg" type="thumbnail">
</image>

The origins of true (MIMD) parallelism go back to <chancellor wordnetid="109906986" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../501/795501.xml">
Federico Luigi, Conte Menabrea</link></chancellor>
 and his "Sketch of the <link xlink:type="simple" xlink:href="../271/1271.xml">
Analytic Engine</link> Invented by <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../698/5698.xml">
Charles Babbage</link></scientist>
".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2240%22])">40</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2241%22])">41</ref> <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link></company>
 introduced the 704 in 1954, through a project in which <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<physicist wordnetid="110428004" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<pioneer wordnetid="110434725" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<originator wordnetid="110383816" confidence="0.8">
<creator wordnetid="109614315" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../897/247897.xml">
Gene Amdahl</link></associate>
</creator>
</originator>
</scientist>
</causal_agent>
</colleague>
</engineer>
</pioneer>
</person>
</physicist>
</peer>
</physical_entity>
 was one of the principal architects. It became the first commercially available computer to use fully automatic <link xlink:type="simple" xlink:href="../376/11376.xml">
floating point</link> arithmetic commands.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2242%22])">42</ref> In 1958, IBM researchers <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../915/301915.xml">
John Cocke</link></scientist>
</person>
 and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2243%22])">43</ref> <company wordnetid="108058098" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../524/4524.xml">
Burroughs Corporation</link></institution>
</company>
 introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a <link xlink:type="simple" xlink:href="../456/45456.xml">
crossbar switch</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2244%22])">44</ref> In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2243%22])">43</ref> It was during this debate that Amdahl's Law was coined to define the limit of speed-up due to parallelism. </p>
<p>

In 1969, US company <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../721/225721.xml">
Honeywell</link></company>
 introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2243%22])">43</ref> <link xlink:type="simple" xlink:href="../135/1547135.xml">
C.mmp</link>, a 1970s multi-processor project at <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../093/48093.xml">
Carnegie Mellon University</link></university>
, was "among the first multiprocessors with more than a few processors".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2241%22])">41</ref> "The first bus-connected multi-processor with snooping caches was the <link>
Synapse N+1</link> in 1984."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2241%22])">41</ref> </p>
<p>

SIMD parallel computers can be traced back to the 1970s. The motivation behind early SIMD computers was to amortize the <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<electrical_device wordnetid="103269401" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<circuit wordnetid="103033362" confidence="0.8">
<link xlink:type="simple" xlink:href="../395/1275395.xml">
gate delay</link></circuit>
</device>
</electrical_device>
</instrumentality>
</artifact>
 of the processor's <link xlink:type="simple" xlink:href="../557/6557.xml">
control unit</link> over multiple instructions.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2245%22])">45</ref> In 1964, Slotnick had proposed building a massively parallel computer for the <lab wordnetid="103629986" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../039/39039.xml">
Lawrence Livermore National Laboratory</link></lab>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2243%22])">43</ref> His design was funded by the <link xlink:type="simple" xlink:href="../090/32090.xml">
US Air Force</link>, which was the earliest SIMD parallel-computing effort, <computer wordnetid="103082979" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<link xlink:type="simple" xlink:href="../963/195963.xml">
ILLIAC IV</link></machine>
</device>
</instrumentality>
</artifact>
</computer>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2243%22])">43</ref> The key to its design was a fairly high parallelism, with up to 256&nbsp;processors, which allowed the machine to work on large datasets in what would later be known as <link xlink:type="simple" xlink:href="../205/58205.xml">
vector processing</link>. However, ILLIAC IV was called "the most infamous of Supercomputers", because the project was only one fourth completed, but took 11&nbsp;years and cost almost four times the original estimate.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2246%22])">46</ref> When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the <link xlink:type="simple" xlink:href="../171/37171.xml">
Cray-1</link>.</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
Almasi, G.S. and A. Gottlieb (1989). <weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=1011116.1011127">
<it>Highly Parallel Computing''</it></weblink>. Benjamin-Cummings publishers, Redwood City, CA.</entry>
<entry id="2">
Asanovic, Krste et al. (December 18, 2006). <weblink xlink:type="simple" xlink:href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf">
"The Landscape of Parallel Computing Research: A View from Berkeley"</weblink> (PDF). University of California, Berkeley. Technical Report No. UCB/EECS-2006-183.  "Old [conventional wisdom]: Increasing clock frequency is the primary method of improving processor performance. New [conventional wisdom]: Increasing parallelism is the primary method of improving processor performance&nbsp;... Even representatives from Intel, a company generally associated with the 'higher clock-speed is better' position, warned that traditional approaches to maximizing performance through maximizing clock speed have been pushed to their limit."</entry>
<entry id="3">
Asanovic et al. Old [conventional wisdom]: Power is free, but transistors are expensive. New [conventional wisdom] is [that] power is expensive, but transistors are "free".</entry>
<entry id="4">
<link xlink:type="simple" xlink:href="../239/816239.xml">
Patterson, David A.</link> and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../550/437550.xml">
John L. Hennessy</link></scientist>
 (1998). <it>Computer Organization and Design</it>, Second Edition, Morgan Kaufmann Publishers, p.&nbsp;715. ISBN 1558604286. </entry>
<entry id="5">
Barney, Blaise.&#32;"<weblink xlink:type="simple" xlink:href="http://www.llnl.gov/computing/tutorials/parallel_comp/">
Introduction to Parallel Computing</weblink>".&#32;  Lawrence Livermore National Laboratory.&#32;Retrieved on <link>
2007-11-09</link>.</entry>
<entry id="6">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../550/437550.xml">
Hennessy, John L.</link></scientist>
 and <link xlink:type="simple" xlink:href="../239/816239.xml">
David A. Patterson</link> (2002). <it>Computer Architecture: A Quantitative Approach</it>. 3rd edition, Morgan Kaufmann, p.&nbsp;43. ISBN 1558607242.</entry>
<entry id="7">
Rabaey, J. M. (1996). <it>Digital Integrated Circuits</it>. Prentice Hall, p.&nbsp;235. ISBN 0131786091.</entry>
<entry id="8">
Flynn, Laurie J. <weblink xlink:type="simple" xlink:href="http://www.nytimes.com/2004/05/08/business/08chip.html?ex=1399348800&amp;en=98cc44ca97b1a562&amp;ei=5007">
"Intel Halts Development of 2 New Microprocessors"</weblink>. <it>The New York Times</it>, May 8, 2004.  Retrieved on April 22, 2008.</entry>
<entry id="9">
Moore, Gordon E.&#32;(1965).&#32;"<weblink xlink:type="simple" xlink:href="ftp://download.intel.com/museum/Moores_Law/Articles-Press_Releases/Gordon_Moore_1965_Article.pdf">
Cramming more components onto integrated circuits</weblink>"&#32;(PDF).&#32;<it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<magazine wordnetid="106595351" confidence="0.8">
<print_media wordnetid="106263609" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<press wordnetid="106263369" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<medium wordnetid="106254669" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../548/1730548.xml">
Electronics Magazine</link></publication>
</medium>
</instrumentality>
</artifact>
</press>
</creation>
</print_media>
</magazine>
</product>
</work>
</it>&#32;4.&#32;Retrieved on <link>
2006-11-11</link>.
</entry>
<entry id="10">
Amdahl, G. (April 1967) "The validity of the single processor approach to achieving large-scale computing capabilities". In <it>Proceedings of AFIPS Spring Joint Computer Conference</it>,  Atlantic City, N.J., AFIPS Press, pp.&nbsp;483–85.</entry>
<entry id="11">
 <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../652/11652.xml">
Brooks, Frederick P. Jr.</link></scientist>
</person>
 . Chapter 2 – The Mythical Man Month. ISBN 0201835959</entry>
<entry id="12">
<weblink xlink:type="simple" xlink:href="http://www.scl.ameslab.gov/Publications/Gus/AmdahlsLaw/Amdahls.html">
Reevaluating Amdahl's Law</weblink> (1988). <work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<periodical wordnetid="106593296" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../161/291161.xml">
Communications of the ACM</link></publication>
</periodical>
</artifact>
</creation>
</product>
</work>
 31(5), pp.&nbsp;532–33.</entry>
<entry id="13">
 Bernstein, A. J. (October 1966). "Program Analysis for Parallel Processing,' IEEE Trans. on Electronic Computers". EC-15, pp.&nbsp;757–62.</entry>
<entry id="14">
 Roosta, Seyed H. (2000). "Parallel processing and parallel algorithms: theory and computation". Springer, p.&nbsp;114. ISBN 0387987169.</entry>
<entry id="15">
Lamport, Leslie (September 1979). "How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs", IEEE Transactions on Computers, C-28,9, pp.&nbsp;690–91.</entry>
<entry id="17">
Culler, David E.; Jaswinder Pal Singh and Anoop Gupta (1999). <it>Parallel Computer Architecture - A Hardware/Software Approach</it>. Morgan Kaufmann Publishers, p.&nbsp;15. ISBN 1558603433.</entry>
<entry id="16">
Patterson and Hennessy, p.&nbsp;748.</entry>
<entry id="19">
<physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../129/4388129.xml">
Patt, Yale</link></associate>
</educator>
</professional>
</adult>
</academician>
</causal_agent>
</colleague>
</engineer>
</person>
</peer>
</physical_entity>
 (April 2004).  "<weblink xlink:type="simple" xlink:href="http://users.ece.utexas.edu/~patt/Videos/talk_videos/cmu_04-29-04.wmv">
The Microprocessor Ten Years From Now: What Are The Challenges, How Do We Meet Them?</weblink> (wmv). Distinguished Lecturer talk at <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../093/48093.xml">
Carnegie Mellon University</link></university>
. Retrieved on November 7, 2007.</entry>
<entry id="18">
Culler et al. p.&nbsp;15.</entry>
<entry id="21">
Culler et al. p.&nbsp;125.</entry>
<entry id="20">
Culler et al. p.&nbsp;124.</entry>
<entry id="23">
Hennessy and Patterson, p.&nbsp;549.</entry>
<entry id="22">
Patterson and Hennessy, p.&nbsp;713.</entry>
<entry id="25">
<weblink xlink:type="simple" xlink:href="http://www.webopedia.com/TERM/c/clustering.html">
What is clustering?</weblink> Webopedia computer dictionary. Retrieved on November 7, 2007.</entry>
<entry id="24">
Patterson and Hennessy, p.&nbsp;714.</entry>
<entry id="27">
<weblink xlink:type="simple" xlink:href="http://www.top500.org/stats/list/29/archtype">
Architecture share for 06/2007</weblink>. TOP500 Supercomputing Sites. Clusters make up 74.60% of the machines on the list. Retrieved on November 7, 2007.</entry>
<entry id="26">
<weblink xlink:type="simple" xlink:href="http://www.pcmag.com/encyclopedia_term/0,2542,t=Beowulf&amp;i=38548,00.asp">
Beowulf definition.</weblink> <it>PC Magazine</it>. Retrieved on November 7, 2007.</entry>
<entry id="29">
<weblink xlink:type="simple" xlink:href="http://www.pcmag.com/encyclopedia_term/0,,t=mpp&amp;i=47310,00.asp">
MPP Definition.</weblink> <it>PC Magazine</it>. Retrieved on November 7, 2007.</entry>
<entry id="28">
Hennessy and Patterson, p.&nbsp;537.</entry>
<entry id="31">
D'Amour, Michael R., CEO <link>
DRC Computer Corporation</link>. "Standard Reconfigurable Computing". Invited speaker at the University of Delaware, February 28, 2007.</entry>
<entry id="30">
Kirkpatrick, Scott (January 31, 2003). "Computer Science: Rough Times Ahead".  <it>Science</it>, Vol. 299. No. 5607, pp. 668 - 669. DOI: 10.1126/science.1081623</entry>
<entry id="34">
Shimokawa, Y.; Y. Fuwa and N. Aramaki (18–21 November 1991). <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=170708">
A parallel ASIC VLSI neurocomputer for a large number of neurons and billion connections per second speed.</weblink> IEEE International Joint Conference on Neural Networks. <b>3:</b> pp.&nbsp;2162–67.</entry>
<entry id="35">
Acken,  K.P.; M.J. Irwin, R.M. Owens (July 1998). <weblink xlink:type="simple" xlink:href="http://www.ingentaconnect.com/content/klu/vlsi/1998/00000019/00000002/00167697?crawler=true">
"A Parallel ASIC Architecture for Efficient Fractal Image Coding".</weblink>  <it>The Journal of VLSI Signal Processing</it>, <b>19</b>(2):97–113(17)</entry>
<entry id="32">
Boggan, Sha'Kia and Daniel M. Pressel (August 2007). <weblink xlink:type="simple" xlink:href="http://www.arl.army.mil/arlreports/2007/ARL-SR-154.pdf">
GPUs: An Emerging Platform for General-Purpose Computation</weblink> (PDF). ARL-SR-154, U.S. Army Research Lab. Retrieved on November 7, 2007.</entry>
<entry id="33">
Maslennikov, Oleg (2002). <weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/jjrdrb0lelyeu3e9/">
"Systematic Generation of Executing Programs for Processor Elements in Parallel ASIC or FPGA-Based Systems and Their Transformation into VHDL-Descriptions of Processor Element Control Units".</weblink> <it>Lecture Notes in Computer Science</it>, <b>2328/2002:</b> p.&nbsp;272.</entry>
<entry id="38">
Shen, John Paul and Mikko H. Lipasti (2005). <it>Modern Processor Design: Fundamentals of Superscalar Processors</it>. McGraw-Hill Professional. p.&nbsp;561. ISBN 0070570647. "However, the holy grail of such research - automated parallelization of serial programs - has yet to materialize. While automated parallelization of certain classes of algorithms has been demonstrated, such success has largely been limited to scientific and numeric applications with predictable flow control (e.g., nested loop structures with statically determined iteration counts) and statically analyzable memory access patterns. (e.g., walks over large multidimensional arrays of float-point data)."</entry>
<entry id="39">
Asanovic, Krste, et al. (December 18, 2006). <weblink xlink:type="simple" xlink:href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf">
The Landscape of Parallel Computing Research: A View from Berkeley</weblink> (PDF). University of California, Berkeley. Technical Report No. UCB/EECS-2006-183. See table on pages 17-19.</entry>
<entry id="36">
Kahng, Andrew B. (June 21, 2004) "<weblink xlink:type="simple" xlink:href="http://www.future-fab.com/documents.asp?grID=353&amp;d_ID=2596">
Scoping the Problem of DFM in the Semiconductor Industry</weblink>." University of California, San Diego. "Future design for manufacturing (DFM) technology must reduce design [non-recoverable expenditure] cost and directly address manufacturing [non-recoverable expenditures] – the cost of a mask set and probe card – which is well over $1&nbsp;million at the 90&nbsp;nm technology node and creates a significant damper on semiconductor-based innovation."</entry>
<entry id="37">
Patterson and Hennessy, p.&nbsp;751.</entry>
<entry id="42">
da Cruz, Frank&#32;(2003).&#32;"<weblink xlink:type="simple" xlink:href="http://www.columbia.edu/acis/history/704.html">
Columbia University Computing History: The IBM 704</weblink>".&#32;  Columbia University.&#32;Retrieved on <link>
2008-01-08</link>.</entry>
<entry id="43">
Wilson, Gregory V&#32;(1994).&#32;"<weblink xlink:type="simple" xlink:href="http://ei.cs.vt.edu/~history/Parallel.html">
The History of the Development of Parallel Computing</weblink>".&#32;  Virginia Tech/Norfolk State University, Interactive Learning with a Digital Library in Computer Science.&#32;Retrieved on <link>
2008-01-08</link>.</entry>
<entry id="40">
<chancellor wordnetid="109906986" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../501/795501.xml">
Menabrea, L. F.</link></chancellor>
 (1842).  <weblink xlink:type="simple" xlink:href="http://www.fourmilab.ch/babbage/sketch.html">
Sketch of the Analytic Engine Invented by Charles Babbage</weblink>. Bibliothèque Universelle de Genève. Retrieved on November 7, 2007.</entry>
<entry id="41">
Patterson and Hennessy, p.&nbsp;753.</entry>
<entry id="46">
Patterson and Hennessy, pp.&nbsp;749–50: "Although successful in pushing several technologies useful in later projects, the ILLIAC IV failed as a computer. Costs escalated from the $8&nbsp;million estimated in 1966 to $31&nbsp;million by 1972, despite the construction of only a quarter of the planned machine&nbsp;... It was perhaps the most infamous of supercomputers. The project started in 1965 and ran its first real application in 1976."</entry>
<entry id="44">
Anthes, Gry&#32;(<link>
2001-11-19</link>).&#32;"<weblink xlink:type="simple" xlink:href="http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;articleId=65878">
The Power of Parallelism</weblink>".&#32;<it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<magazine wordnetid="106595351" confidence="0.8">
<print_media wordnetid="106263609" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<press wordnetid="106263369" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<medium wordnetid="106254669" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../497/1861497.xml">
Computerworld</link></publication>
</medium>
</instrumentality>
</artifact>
</press>
</creation>
</print_media>
</magazine>
</product>
</work>
</it>.&#32;Retrieved on <link>
2008-01-08</link>.</entry>
<entry id="45">
Patterson and Hennessy, p.&nbsp;749.</entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>


<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dmoz.org/Computers/Parallel_Computing//">
Parallel computing</weblink> at the <work wordnetid="100575741" confidence="0.8">
<possession wordnetid="100032613" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<company wordnetid="108058098" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<property wordnetid="113244109" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<subsidiary_company wordnetid="108003935" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../501/18949501.xml">
Open Directory Project</link></institution>
</subsidiary_company>
</activity>
</psychological_feature>
</act>
</property>
</undertaking>
</company>
</event>
</possession>
</work>
</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.llnl.gov/computing/tutorials/parallel_comp/">
Lawrence Livermore National Laboratory: Introduction to Parallel Computing</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www-unix.mcs.anl.gov/dbpp/">
Designing and Building Parallel Programs, by Ian Foster</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://wotug.ukc.ac.uk/parallel/">
Internet Parallel Computing Archive</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://dsonline.computer.org/portal/site/dsonline/index.jsp?pageID=dso_level1_home&amp;path=dsonline/topics/parallel&amp;file=index.xml&amp;xsl=generic.xsl">
Parallel processing topic area at IEEE Distributed Computing Online</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.new-npac.org/projects/cdroms/cewes-1998-05/copywrite/pcw/book.html">
Parallel Computing Works Free On-line Book</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://ark.cdlib.org/ark:/13030/ft0f59n73z/">
Frontiers of Supercomputing Free On-line Book Covering topics like algorithms and industrial applications</weblink></entry>
</list>
</p>

<p>

<table style=";" class="navbox" cellspacing="0">
<row>
<col style="padding:2px;">
<table style="width:100%;background:transparent;color:inherit;;" class="nowraplinks collapsible autocollapse " cellspacing="0">
<row>
<header colspan="2" style=";" class="navbox-title">
<link xlink:type="simple" xlink:href="../162/145162.xml">
Parallel computing</link>topics</header>
</row>
<row style="height:2px;">

</row>
<row>
<col style=";;" class="navbox-group">
General</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<condition wordnetid="113920835" confidence="0.8">
<state wordnetid="100024720" confidence="0.8">
<problem wordnetid="114410605" confidence="0.8">
<difficulty wordnetid="114408086" confidence="0.8">
<link xlink:type="simple" xlink:href="../527/832527.xml">
High-performance computing</link></difficulty>
</problem>
</state>
</condition>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Parallelism</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../148/14229148.xml">
Bit-level parallelism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../960/245960.xml">
Instruction level parallelism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../420/9467420.xml">
Data parallelism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../070/9468070.xml">
Task parallelism</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Threads</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../877/313877.xml">
Superthreading</link>&nbsp;·  <link xlink:type="simple" xlink:href="../443/151443.xml">
Hyperthreading</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Theory</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../612/1448612.xml">
Speedup</link>&nbsp;·  <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<link xlink:type="simple" xlink:href="../ury/24th_century.xml">
Amdahl's law</link></rule>
</concept>
</idea>
&nbsp;·  <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../349/222349.xml">
Flynn's taxonomy</link></group>
</collection>
</class>
 (<link xlink:type="simple" xlink:href="../630/1103630.xml">
SISD</link>&nbsp;&amp;bull;  <link xlink:type="simple" xlink:href="../359/55359.xml">
SIMD</link>&nbsp;&amp;bull;  <link xlink:type="simple" xlink:href="../666/991666.xml">
MISD</link>&nbsp;&amp;bull;  <link xlink:type="simple" xlink:href="../139/157139.xml">
MIMD</link>)&nbsp;·  <link xlink:type="simple" xlink:href="../721/3505721.xml">
Cost efficiency</link>&nbsp;·  <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<link xlink:type="simple" xlink:href="../252/4243252.xml">
Gustafson's law</link></rule>
</concept>
</idea>
&nbsp;·  <link xlink:type="simple" xlink:href="../042/9453042.xml">
Karp-Flatt metric</link>&nbsp;·  <link xlink:type="simple" xlink:href="../068/15167068.xml">
Parallel slowdown</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Elements</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../178/45178.xml">
Process</link>&nbsp;·  <link xlink:type="simple" xlink:href="../303/45303.xml">
Thread</link>&nbsp;·  <link xlink:type="simple" xlink:href="../712/5533712.xml">
Fiber</link>&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../675/956675.xml">
Parallel Random Access Machine</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Coordination</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../020/64020.xml">
Multiprocessing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../679/10520679.xml">
Multithreading</link>&nbsp;·  <link xlink:type="simple" xlink:href="../857/6857.xml">
Multitasking</link>&nbsp;·  <link xlink:type="simple" xlink:href="../818/399818.xml">
Memory coherency</link>&nbsp;·  <link xlink:type="simple" xlink:href="../865/176865.xml">
Cache coherency</link>&nbsp;·  <link xlink:type="simple" xlink:href="../263/4736263.xml">
Barrier</link>&nbsp;·  <link xlink:type="simple" xlink:href="../017/4726017.xml">
Synchronization</link>&nbsp;·  <link xlink:type="simple" xlink:href="../501/8501.xml">
Distributed computing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../373/49373.xml">
Grid computing</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../311/5311.xml">
Programming</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<paradigm wordnetid="113804375" confidence="0.8">
<linguistic_relation wordnetid="113797142" confidence="0.8">
<inflection wordnetid="113803782" confidence="0.8">
<grammatical_relation wordnetid="113796779" confidence="0.8">
<link xlink:type="simple" xlink:href="../375/2242375.xml">
Programming model</link></grammatical_relation>
</inflection>
</linguistic_relation>
</paradigm>
&nbsp;·  <link xlink:type="simple" xlink:href="../888/3453888.xml">
Implicit parallelism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../332/3095332.xml">
Explicit parallelism</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../310/5310.xml">
Hardware</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../896/18949896.xml">
Computer cluster</link>&nbsp;·  <link xlink:type="simple" xlink:href="../542/66542.xml">
Beowulf</link>&nbsp;·  <link xlink:type="simple" xlink:href="../318/50318.xml">
Symmetric multiprocessing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../643/40643.xml">
Non-Uniform Memory Access</link>&nbsp;·  <link xlink:type="simple" xlink:href="../307/910307.xml">
Cache only memory architecture</link>&nbsp;·  <link xlink:type="simple" xlink:href="../506/2576506.xml">
Asymmetric multiprocessing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../021/315021.xml">
Simultaneous multithreading</link>&nbsp;·  <link xlink:type="simple" xlink:href="../653/825653.xml">
Shared memory</link>&nbsp;·  <link xlink:type="simple" xlink:href="../887/234887.xml">
Distributed memory</link>&nbsp;·  <link xlink:type="simple" xlink:href="../049/584049.xml">
Massive parallel processing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../702/51702.xml">
Superscalar processing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../205/58205.xml">
Vector processing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../153/37153.xml">
Supercomputer</link>&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<paradigm wordnetid="113804375" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<linguistic_relation wordnetid="113797142" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<inflection wordnetid="113803782" confidence="0.8">
<grammatical_relation wordnetid="113796779" confidence="0.8">
<link xlink:type="simple" xlink:href="../727/2786727.xml">
Stream processing</link></grammatical_relation>
</inflection>
</causal_agent>
</linguistic_relation>
</worker>
</paradigm>
</assistant>
</model>
</person>
</physical_entity>
&nbsp;·  <substance wordnetid="100019613" confidence="0.8">
<paper wordnetid="114974264" confidence="0.8">
<card wordnetid="102962545" confidence="0.8">
<part wordnetid="113809207" confidence="0.8">
<material wordnetid="114580897" confidence="0.8">
<link xlink:type="simple" xlink:href="../939/1268939.xml">
GPGPU</link></material>
</part>
</card>
</paper>
</substance>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../309/5309.xml">
Software</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../843/399843.xml">
Distributed shared memory</link> &nbsp;·  <link xlink:type="simple" xlink:href="../765/416765.xml">
Application checkpointing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../129/2738129.xml">
Warewulf</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../ury/24th_century.xml">
API</link>s</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<standard wordnetid="107260623" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../671/634671.xml">
POSIX Threads</link></system_of_measurement>
</standard>
&nbsp;·  <link xlink:type="simple" xlink:href="../842/381842.xml">
OpenMP</link>&nbsp;·  <link xlink:type="simple" xlink:href="../466/221466.xml">
Message Passing Interface (MPI)</link>&nbsp;·  <link xlink:type="simple" xlink:href="../616/1057616.xml">
UPC</link>&nbsp;·  <link xlink:type="simple" xlink:href="../077/11625077.xml">
Intel Threading Building Blocks</link>&nbsp;·  <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../324/711324.xml#xpointer(//*[./st=%22Multithreading+=E2=80=93+Boost.Thread%22])">
Boost.Thread</link></room>
</library>
</area>
</artifact>
</structure>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Problems</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../712/1738712.xml">
Embarrassingly parallel</link></instrumentality>
</artifact>
</system>
&nbsp;·  <condition wordnetid="113920835" confidence="0.8">
<state wordnetid="100024720" confidence="0.8">
<problem wordnetid="114410605" confidence="0.8">
<difficulty wordnetid="114408086" confidence="0.8">
<link xlink:type="simple" xlink:href="../754/439754.xml">
Grand Challenge</link></difficulty>
</problem>
</state>
</condition>
&nbsp;·  <plant_part wordnetid="113086908" confidence="0.8">
<natural_object wordnetid="100019128" confidence="0.8">
<kernel wordnetid="113137010" confidence="0.8">
<link xlink:type="simple" xlink:href="../798/12332798.xml">
Software lockout</link></kernel>
</natural_object>
</plant_part>
</col>
</row>
</table>
</col>
</row>
</table>
</p>

<p>

<image width="150px" src="">

</image>

</p>


</sec>
</bdy>
</article>
