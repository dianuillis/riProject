<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:36:52[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Bag of words model in computer vision</title>
<id>14647485</id>
<revision>
<id>243131489</id>
<timestamp>2008-10-05T08:23:23Z</timestamp>
<contributor>
<username>Wwoods</username>
<id>43844</id>
</contributor>
</revision>
<categories>
<category>Wikipedia articles needing clarification</category>
<category>Object recognition and categorization</category>
<category>Uncategorized pages needing expert attention</category>
<category>Pages needing expert attention</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40px" src="Ambox_?.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>All or part of this article may be .</b>
Please help . Suggestions may be on the . <it>(December 2007)</it></col>
</row>
</table>



<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-content" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_content.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>This article or section is in need of attention from an expert on the subject.</b>
Please help recruit one or <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Bag_of_words_model_in_computer_vision&amp;action=edit">
improve this article</weblink> yourself. See the  for details.Please consider using  to associate this request with a </col>
</row>
</table>


For the "bag of words model" in <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link> (NLP) or <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link>, see <link xlink:type="simple" xlink:href="../441/14003441.xml">
Bag of words model</link>.</p>
<p>

This is an article introducing the "Bag of words model" (<b>BoW</b>) in <link xlink:type="simple" xlink:href="../596/6596.xml">
computer vision</link>, especially for <link>
object categorization</link>. From now, the "BoW" model refers to the BoW model in computer vision unless explicitly declared.</p>
<p>

Before introducing the BoW model, the BoW in <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link> (NLP) is briefly reviewed. The BoW in NLP is a popular method for representing documents, which ignores the word orders. For example, "a good book" and "book good a" are the same under this model. The BoW model allows a dictionary-based modeling, and each document looks like a "bag" (thus the order is not considered), which contains some words from the dictionary. Computer vision researchers uses a similar idea for image representation (Here an image may refer to a particular object, such as an image of a car). For example, an image can be treated as a document, and features extracted from the image are considered as the "words" (Usually some manipulations are needed, which are described below). The BoW representation serves as the basic element for further processing, such as <link>
object categorization</link>.</p>

<sec>
<st>
Representation based on the BoW model</st>

<ss1>
<st>
Text Document Representation based on the BoW model</st>
<p>

The text document representation based on the BoW model in NLP is reviewed first. Here are two simple text documents: 
<list>
<entry level="1" type="bullet">

 John likes to watch movies. Mary likes too. </entry>
<entry level="1" type="bullet">

 John also likes to watch football games.</entry>
</list>

Based on these two text documents, a dictionary is constructed as:
<list>
<entry level="1" type="bullet">

 dictionary={1:"John", 2:"likes", 3:"to", 4:"watch", 5:"movies", 6:"also", 7:"football", 8:"games", 9:"Mary", 10:"too"},</entry>
</list>

which has 10 distinct words. And using the indexes of the dictionary, each document is represented by a 10-entry vector:
<list>
<entry level="1" type="bullet">

 [1, 2, 1, 1, 1, 0, 0, 0, 1, 1]</entry>
<entry level="1" type="bullet">

 [1, 1, 1, 1, 0, 1, 1, 1, 0, 0],</entry>
</list>

where each entry of the vectors refers to count of the corresponding entry in the dictionary (This is also the <link xlink:type="simple" xlink:href="../266/13266.xml">
histogram</link> representation). As we can see, this vector representation does not preserve the order of the words in the original sentences. This kind of representation has several successful applications, for example <link xlink:type="simple" xlink:href="../351/4605351.xml">
latent Dirichlet allocation</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

</ss1>
<ss1>
<st>
Image Representation based on the BoW model</st>

<p>

Figure 1 shows the basic of idea of the BoW model. To represent an image using BoW model, an image can be treated as a document. Similarly, "words" in images need to be defined too. However, "word" in images is not the off-the-shelf thing like the word in text documents. To achieve this, it usually includes following three steps: feature detection, feature description and codebook generation.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> A definition of the BoW model can be the "histogram representation based on independent features".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>
</p>
<ss2>
<st>
Feature Detection</st>
<p>

Given an image, feature detection is to extract several local patches (or regions), which are considered as candidates for basic elements, "words".  
</p>
<ss3>
<st>
Regular Grid</st>

<p>

Figure 2 is an example of the regular grid method for feature detection. Regular grid<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> is probably the most simple yet effective method for feature detection. In this method, the image is evenly segmented by some horizontal and vertical lines and some local patches are obtained. This method shows very promising results for natural scene categorization.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> The limitation of this method is that it uses little information of an image itself.</p>

</ss3>
<ss3>
<st>
Interest Point Detector</st>

<p>

Interest point detectors try to detect salient patches, such as edges, corners and blobs in an image. These salient patches are considered more important than other patches, such as the regions attracting human attentions, which might be more useful for object categorization. Some famous detectors are <link>
Harris corner</link> detector,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> Loweâ€™s DoG (<happening wordnetid="107283608" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<movement wordnetid="107309781" confidence="0.8">
<wave wordnetid="107352190" confidence="0.8">
<ripple wordnetid="107344663" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../943/3334943.xml">
Difference of Gaussians</link></psychological_feature>
</ripple>
</wave>
</movement>
</event>
</happening>
) detector<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> and <link xlink:type="simple" xlink:href="../572/14441572.xml">
Kadir Brady saliency detector</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref> Figure 3 shows a result of Harris corner detector.</p>

</ss3>
<ss3>
<st>
Other Methods</st>
<p>

In addition, researchers also use random sampling<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref> and segmentation methods<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref> (such as <link>
Normalized Cut</link>) for feature detection.</p>

</ss3>
</ss2>
<ss2>
<st>
Feature Representation</st>
<p>

After feature detection, each image is abstracted by several local patches. Feature representation methods deal with how to represent the patches as numerical vectors. These methods are called feature descriptors. A good descriptor should have the ability to handle intensity, rotation, scale and affine variations to some extent. One of the most famous descriptors is <link xlink:type="simple" xlink:href="../345/1208345.xml">
Scale-invariant feature transform</link> (SIFT). SIFT converts each patch to 128-dimensional vector. After this step, each image is a collection of vectors of the same dimension (128 for SIFT), where the order of different vectors is of no importance.</p>

</ss2>
<ss2>
<st>
Codebook Generation</st>


<p>

The final step for the BoW model is to convert vector represented patches to "codewords" (analogy to words in text documents), which also produces a "codebook" (analogy to a word dictionary). A codeword can be considered as a representative of several similar patches. One simple method is performing <link xlink:type="simple" xlink:href="../407/1860407.xml">
K-means clustering</link> over all the vectors.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref> Codewords are then defined as the centers of the learned clusters. The number of the clusters is the codebook size (analogy to the size of the word dictionary). </p>
<p>

Thus, each patch in an image is mapped to a certain codeword through the clustering process and the image can be represented by the <link xlink:type="simple" xlink:href="../266/13266.xml">
histogram</link> (see Figure 4) of the codewords. Figure 5 shows several examples of codewords mapped back to image patches.</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
Learning and Recognition based on the BoW model</st>
<p>

Till now, an image is represented based on a BoW model. Computer vision researchers have developed several learning methods to leverage the BoW model for image related task, such as <link>
object categorization</link>. These methods can roughly be divided into two categories, generative and discriminative models. For multiple label categorization problem, the <link xlink:type="simple" xlink:href="../558/847558.xml">
confusion matrix</link> can be used as an evaluation metric.</p>

<ss1>
<st>
Generative Models</st>
<p>

Here are some notations for this section. Suppose the size of codebook is <math>V</math>.
<list>
<entry level="1" type="bullet">

 <math>w</math>: each patch <math>w</math> is a V-dimensional vector that has a single component that equals to one and all other components equal to zero (For K-means clustering setting, the single component equal one indicates the cluster that <math>w</math> belongs to). The <math>v</math>th codeword in the codebook can be represented as <math>w^v=1</math> and <math>w^u = 0</math> for <math>u\neq v</math>. </entry>
<entry level="1" type="bullet">

 <math>\mathbf{w}</math>: each image is represented by <math>\mathbf{w}=[w_1, w_2, \cdots, w_N]</math>, all the patches in an image. </entry>
<entry level="1" type="bullet">

 <math>d_j</math>: the <math>j</math>th image in an image collection.</entry>
<entry level="1" type="bullet">

 <math>c</math>: category of the image.</entry>
<entry level="1" type="bullet">

 <math>z</math>: theme or topic of the patch.</entry>
<entry level="1" type="bullet">

 <math>\pi</math>: mixture proportion.</entry>
</list>

Since the BoW model is an analogy to the BoW model in NLP, generative models developed in text domains can also be adapted in computer vision. Simple NaÃ¯ve Bayes model and hierarchical Bayesian models are discussed.
</p>
<ss2>
<st>
NaÃ¯ve Bayes</st>

<p>

The simplest one is <link>
NaÃ¯ve Bayes</link> classifier.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref> Using the language of <link xlink:type="simple" xlink:href="../298/447298.xml">
graphical models</link>, NaÃ¯ve Bayes classifier is shown as Figure 6. The basic idea (or assumption) of this model is that each category has its own distribution over the codebook, which are assumed quite different from others. Take a face category and a car category for an example. The face category may emphasize the codewords which represent "nose", "eye" and "mouth", while the car category may emphasize the codewords which represent "wheel" and "corner". Given a collection training examples, the classifier learns different distributions for different categories. The categorization decision is made by 
<list>
<entry level="1" type="bullet">

 <math>c^*=\arg \max_c p(c|\mathbf{w}) = \arg \max_c p(c)p(\mathbf{w}|c)=\arg \max_c p(c)\prod_{n=1}^Np(w_n|c)</math></entry>
</list>
</p>
<p>

Since NaÃ¯ve Bayes classifier is simple yet effective, it is usually used as a baseline method for comparison.</p>

</ss2>
<ss2>
<st>
Hierarchical Bayesian Models</st>

<p>

The basic assumption of NaÃ¯ve Bayes model does not hold sometimes. For example, a natural scene image (Figure 7) may contain several different themes.
<information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../675/2088675.xml">
Probabilistic latent semantic analysis</link></datum>
</information>
 (pLSA)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref>  and <link xlink:type="simple" xlink:href="../351/4605351.xml">
latent Dirichlet allocation</link> (LDA)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> are two popular topic models from text domains to tackle the similar multiple "theme" problem. Take LDA for an example. To model natural scene images using LDA, an analogy is made like this (Figure 9): 
<list>
<entry level="1" type="bullet">

 the image category is mapped to the document category; </entry>
<entry level="1" type="bullet">

 the mixture proportion of themes maps the mixture proportion of topics; </entry>
<entry level="1" type="bullet">

 the theme index is mapped to topic index; </entry>
<entry level="1" type="bullet">

 the codeword is mapped to the word. </entry>
</list>

This method shows very promising results in natural scene categorization on <weblink xlink:type="simple" xlink:href="http://vision.cs.princeton.edu/resources_links.html">
13 Natural Scene Categories</weblink>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

</ss2>
</ss1>
<ss1>
<st>
Discriminative Models</st>

<p>

Since images are represented based on the BoW model, any discriminative model suitable for text document categorization can be tried, such as <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link> (SVM)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref> and <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref> <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../912/303912.xml">
Kernel trick</link></method>
</know-how>
 is also applicable when kernel based classifier is used, such as SVM. Pyramid match kernel is newly developed one based on the BoW model. The local feature approach of using BoW model representation learnt by machine learning classifiers with different kernels (e.g., EMD-kernel and <math>X^2</math> kernel) has been vastly tested in the area of texture and object recognition.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref> . Very promising results on a number of datasets have been reported. 
This approach<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref> has achieved very impressive result in the <weblink xlink:type="simple" xlink:href="http://www.pascal-network.org/challenges/VOC/">
the PASCAL Visual Object Classes Challenge</weblink> 
</p>
<ss2>
<st>
Pyramid Match Kernel</st>
<p>

Pyramid match kernel<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref> is a fast kernel function (satisfying <link xlink:type="simple" xlink:href="../343/2182343.xml">
Mercer's condition</link>) which maps the BoW features to multi-resolution histograms. One of the advantages of the multi-resolution histograms is the ability to capture the co-occurring features. The pyramid match kernel builds the multi-resolution histogram by binning data points into discrete regions of increasing larger size. Thus, points do not match at high resolutions have the chance to match at low resolutions (Figure 9). Pyramid match kernel performs approximate similarity match, without explicit search, and the computation time is only linear in the number of features. Compared with other kernel approaches, pyramid match kernel is much faster, yet provides competitively accurate results. Pyramid match kernel was applied to <weblink xlink:type="simple" xlink:href="http://www.mis.informatik.tu-darmstadt.de/Research/Projects/categorization/eth80-db.html">
ETH-80 database</weblink> and <weblink xlink:type="simple" xlink:href="http://vision.cs.princeton.edu/resources_links.html">
Caltech 101 database</weblink> and showed promising results.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref></p>

</ss2>
</ss1>
</sec>
<sec>
<st>
Limitations and recent Developments</st>
<p>

One of notorious disadvantages of BoW is that it ignores the spatial relationships among the patches, which is very important in image representation. Researchers have proposed several methods to incorporate the spatial information. For feature level improvements, correlogram features can capture spatial co-occurrences of features.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2218%22])">18</ref> For generative models, relative positions<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2219%22])">19</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2220%22])">20</ref> of codewords are also taken into account. The hierarchical shape and appearance model for human action<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2221%22])">21</ref> introduces a new part layer (<link xlink:type="simple" xlink:href="../618/15261618.xml">
Constellation model</link>) between the mixture proportion and the BoW features, which captures the spatial relationships among parts in the layer. For discriminative models, spatial pyramid match<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2222%22])">22</ref> performs pyramid matching by partitioning the image into increasingly fine sub-regions and compute histograms of local features inside each sub-region.</p>
<p>

Furthermore, the BoW model has not been extensively tested yet for view point invariance and scale invariance, and the performance is unclear. Also the BoW model for object segmentation and localization is also lack of study.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>

</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">D. Blei, A. Ng, and M. Jordan&#32;(2003).&#32;"<weblink xlink:type="simple" xlink:href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">
Latent Dirichlet allocation</weblink>". <it>Journal of Machine Learning Research</it>&#32;<b>3</b>: 993â€“1022. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1162%2Fjmlr.2003.3.4-5.993">
10.1162/jmlr.2003.3.4-5.993</weblink>.</cite>&nbsp;</entry>
<entry id="2">
 <cite style="font-style:normal">L. Fei-Fei and P. Perona&#32;(2005). "<weblink xlink:type="simple" xlink:href="http://vision.cs.princeton.edu/documents/Fei-FeiPerona2005.pdf">
A Bayesian Hierarchical Model for Learning Natural Scene Categories</weblink>".&#32;<it>Proc. of IEEE Computer Vision and Pattern Recognition</it>: 524-531.</cite>&nbsp;</entry>
<entry id="3">
L. Fei-Fei, R. Fergus, and A. Torralba.&#32;"<weblink xlink:type="simple" xlink:href="http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html">
Recognizing and Learning Object Categories, CVPR 2007 short course</weblink>".
</entry>
<entry id="4">
 <cite style="font-style:normal">J. Vogel and B. Schiele&#32;(2002). "<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/1gafca9vptd2ndy0/">
On Performance Characterization and Optimization for Image Retrieval</weblink>".&#32;<it>Proc. of European Conference on Computer Vision</it>: 51-55.</cite>&nbsp;</entry>
<entry id="5">
 <cite style="font-style:normal">C. Harris and M. Stephens&#32;(1988). "<weblink xlink:type="simple" xlink:href="http://www.csse.uwa.edu.au/~pk/research/matlabfns/Spatial/Docs/Harris/A_Combined_Corner_and_Edge_Detector.pdf">
A combined corner and edge detector</weblink>".&#32;<it>Proc. of the 4th Alvey Vision Conference</it>: 147-151.</cite>&nbsp;</entry>
<entry id="6">
 <cite style="font-style:normal">D. Lowe&#32;(1999). "<weblink xlink:type="simple" xlink:href="http://www.cs.ubc.ca/~lowe/papers/iccv99.pdf">
Object recognition with informative features and linear classification</weblink>".&#32;<it>Proc. of International Conference on Computer Vision</it>: 1150-1157.</cite>&nbsp;</entry>
<entry id="7">
 <cite style="font-style:normal">T. Kadir and M. Brady&#32;(2001).&#32;"<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~timork/Saliency/ijcv_SalScale.pdf">
Scale, saliency and image description</weblink>". <it>International Journal of Computer Vision</it>&#32;<b>45</b>&#32;(2): 83â€“105. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1023%2FA%3A1012460413855">
10.1023/A:1012460413855</weblink>.</cite>&nbsp;</entry>
<entry id="8">
 <cite style="font-style:normal">M. Vidal-Naquet and S. Ullman&#32;(2003). "<weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/iel5/8769/27772/01238356.pdf">
Object recognition with informative features and linear classification</weblink>".&#32;<it>Proc. of IEEE International Conference on Computer Vision</it>: 281-288.</cite>&nbsp;</entry>
<entry id="9">
 <cite style="font-style:normal">Maree, R and   Geurts, P.  and Piater, J. and  Wehenkel, L.&#32;(2005). "<weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1467246&amp;isnumber=31472">
Random subwindows for robust image classification</weblink>".&#32;<it>Proc. of IEEE International Conference on Computer Vision and Pattern Recognition</it>: 34-40.</cite>&nbsp;</entry>
<entry id="10">
 <cite style="font-style:normal">K. Barnard, P. Duygulu, N. de Freitas, F. Forsyth, D. Blei, and M. Jordan&#32;(2003).&#32;"<weblink xlink:type="simple" xlink:href="http://kobus.ca/research/publications/JMLR-03/JMLR-03.pdf">
Matching words and pictures</weblink>". <it>Journal of Machine Learning Research</it>&#32;<b>3</b>: 1107â€“1135. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1162%2F153244303322533214">
10.1162/153244303322533214</weblink>.</cite>&nbsp;</entry>
<entry id="11">
 <cite style="font-style:normal">T. Leung and J. Malik&#32;(2001).&#32;"<weblink xlink:type="simple" xlink:href="http://www.cs.berkeley.edu/~malik/papers/LM-3dtexton.pdf">
Representing and recognizing the visual appearance of materials using three-dimensional textons</weblink>". <it>International Journal of Computer Vision</it>&#32;<b>43</b>&#32;(1): 29â€“44. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1023%2FA%3A1011126920638">
10.1023/A:1011126920638</weblink>.</cite>&nbsp;</entry>
<entry id="12">
 <cite style="font-style:normal">C. Dance, J. Willamowski, L.X. Fan, C. Bray, and G. Csurka&#32;(2004). "<weblink xlink:type="simple" xlink:href="http://www.xrce.xerox.com/Publications/Attachments/2004-010/2004_010.pdf">
Visual categorization with bags of keypoints</weblink>".&#32;<it>Proc. of ECCV International Workshop on Statistical Learning in Computer Vision</it>.</cite>&nbsp;</entry>
<entry id="13">
 <cite style="font-style:normal">T. Hoffman&#32;(1999). "<weblink xlink:type="simple" xlink:href="http://www.cs.brown.edu/~th/papers/Hofmann-UAI99.pdf">
Probabilistic Latent Semantic Analysis</weblink>".&#32;<it>Proc. of the Fifteenth Conference on Uncertainty in Artificial Intelligence</it>.</cite>&nbsp;</entry>
<entry id="14">
 <cite style="font-style:normal">J. Sivic, B. Russell, A. Efros, A. Zisserman, and W. Freeman&#32;(2005). "<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/publications/papers/sivic05b.pdf">
Discovering objects and their location in images</weblink>".&#32;<it>Proc. of International Conference on Computer Vision</it>.</cite>&nbsp;</entry>
<entry id="15">
 <cite style="font-style:normal">T. Serre, L. Wolf, and T. Poggio&#32;(2005). "<weblink xlink:type="simple" xlink:href="http://cbcl.mit.edu/projects/cbcl/publications/ps/serre-PID73457-05.pdf">
Object Recognition with Features Inspired by Visual Cortex</weblink>".&#32;<it>Proc. of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</it>.</cite>&nbsp;</entry>
<entry id="17">
 <cite style="font-style:normal">K. Grauman and T. Darrell&#32;(2005). "<weblink xlink:type="simple" xlink:href="http://www.cs.utexas.edu/~grauman/papers/grauman_darrell_iccv2005.pdf">
The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features</weblink>".&#32;<it>Proc. of IEEE International Conference on Computer Vision</it>.</cite>&nbsp;</entry>
<entry id="16">
 <cite style="font-style:normal">Jianguo Zhang, Marcin MarszaÅ‚ek, Svetlana Lazebnik, Cordelia Schmid&#32;(2001).&#32;"<weblink xlink:type="simple" xlink:href="http://lear.inrialpes.fr/pubs/2007/ZMLS07/ZhangMarszalekLazebnikSchmid-IJCV07-ClassificationStudy.pdf">
Local Features and Kernels for Classification of Texture and Object Categories: a Comprehensive Study</weblink>". <it>International Journal of Computer Vision</it>&#32;<b>73</b>&#32;(2): 213â€“238. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2Fs11263-006-9794-4">
10.1007/s11263-006-9794-4</weblink>.</cite>&nbsp;</entry>
<entry id="19">
 <cite style="font-style:normal">E. Sudderth, A. Torralba, W. Freeman, and A. Willsky&#32;(2005). "<weblink xlink:type="simple" xlink:href="http://ssg.mit.edu/~esuddert/papers/iccv05.pdf">
Learning Hierarchical Models of Scenes, Objects, and Parts</weblink>".&#32;<it>Proc. of International Conference on Computer Vision</it>.</cite>&nbsp;</entry>
<entry id="18">
 <cite style="font-style:normal">S. Savarese, J. Winn, and A. Criminisi&#32;(2006). "<weblink xlink:type="simple" xlink:href="http://johnwinn.org/Publications/papers/Savarese_Winn_Criminisi_Correlatons_CVPR2006.pdf">
Discriminative Object Class Models of Appearance and Shape by Correlatons</weblink>".&#32;<it>Proc. of IEEE Computer Vision and Pattern Recognition</it>.</cite>&nbsp;</entry>
<entry id="21">
 <cite style="font-style:normal">J. C. Niebles and L. Fei-Fei&#32;(2007). "<weblink xlink:type="simple" xlink:href="http://vision.cs.princeton.edu/documents/NieblesFei-Fei_CVPR2007.pdf">
A hierarchical model model of shape and appearance for human action classification</weblink>".&#32;<it>Proc. of IEEE Computer Vision and Pattern Recognition</it>.</cite>&nbsp;</entry>
<entry id="20">
 <cite style="font-style:normal">E. Sudderth, A. Torralba, W. Freeman, and A. Willsky&#32;(2005). "<weblink xlink:type="simple" xlink:href="http://ssg.mit.edu/~esuddert/papers/nips05.pdf">
Describing Visual Scenes using Transformed Dirichlet Processes</weblink>".&#32;<it>Proc. of Neural Information Processing Systems</it>.</cite>&nbsp;</entry>
<entry id="22">
 <cite style="font-style:normal">S. Lazebnik, C. Schmid, and J. Ponce&#32;(2006). "<weblink xlink:type="simple" xlink:href="http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/cvpr06b.pdf">
Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</weblink>".&#32;<it>Proc. of IEEE Conference on Computer Vision and Pattern Recognition</it>.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://people.csail.mit.edu/fergus/iccv2005/bagwords.html">
A demo for two bag-of-words classifiers</weblink> by L. Fei-Fei, R. Fergus, and A. Torralba.</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../436/14662436.xml">
Part-based models</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../852/14681852.xml">
Segmentation based object categorization</link></entry>
</list>
</p>

</sec>
</bdy>
</article>
