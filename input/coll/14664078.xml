<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:37:33[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Harris affine region detector</title>
<id>14664078</id>
<revision>
<id>239075954</id>
<timestamp>2008-09-17T18:24:57Z</timestamp>
<contributor>
<username>Robbot</username>
<id>25261</id>
</contributor>
</revision>
<categories>
<category>Computer vision</category>
<category>Image processing</category>
</categories>
</header>
<bdy>

<image location="none" width="50px" src="Wiktionary-logo-en.svg">
<caption>

Wiktionary
</caption>
</image>

Look up  in <web_site wordnetid="106359193" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../658/156658.xml">
Wiktionary</link></web_site>
, the free dictionary.

<p>

<table style="width: 22em; text-align: left; font-size: 88%; line-height: 1.5em; width:20em;" class="infobox " cellspacing="5">
<row>
<col colspan="2" style="text-align:center; font-size: 125%; font-weight: bold; " class="">
<link xlink:type="simple" xlink:href="../224/1284224.xml">
Feature detection</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
 <image width="200px" src="Corner.png">
</image>
Output of a typical corner detection algorithm</col>
</row>
<row>
<header colspan="2" style="text-align:center; ">
<link xlink:type="simple" xlink:href="../680/331680.xml">
Edge detection</link></header>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../817/476817.xml">
Canny</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../817/476817.xml#xpointer(//*[./st=%22Conclusion%22])">
Canny-Deriche</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../680/331680.xml#xpointer(//*[./st=%22Differential+edge+detection%22])">
Differential</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../116/476116.xml">
Sobel</link></col>
</row>
<row>
<header colspan="2" style="text-align:center; ">
<link xlink:type="simple" xlink:href="../669/7046669.xml">
Interest point detection</link></header>
</row>
<row>
<header colspan="2" style="text-align:center; ">
<link xlink:type="simple" xlink:href="../759/4921759.xml">
Corner detection</link></header>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../759/4921759.xml#xpointer(//*[./st=%22The+Harris+=26+Stephens+/+Plessey+corner+detection+algorithm%22])">
Harris operator</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../759/4921759.xml#xpointer(//*[./st=%22The+Shi+and+Tomasi+corner+detection+algorithm%22])">
Shi and Tomasi</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../759/4921759.xml#xpointer(//*[./st=%22The+level+curve+curvature+approach%22])">
Level curve curvature</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../759/4921759.xml#xpointer(//*[./st=%22The+SUSAN+corner+detector%22])">
SUSAN</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../759/4921759.xml#xpointer(//*[./st=%22The+FAST+feature+detector%22])">
FAST</link></col>
</row>
<row>
<header colspan="2" style="text-align:center; ">
<link xlink:type="simple" xlink:href="../205/6840205.xml">
Blob detection</link></header>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../205/6840205.xml#xpointer(//*[./st=%22The+Laplacian+of+Gaussian%22])">
Laplacian of Gaussian (LoG)</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<happening wordnetid="107283608" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<movement wordnetid="107309781" confidence="0.8">
<wave wordnetid="107352190" confidence="0.8">
<ripple wordnetid="107344663" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../943/3334943.xml">
Difference of Gaussians (DoG)</link></psychological_feature>
</ripple>
</wave>
</movement>
</event>
</happening>
</col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../205/6840205.xml#xpointer(//*[./st=%22The+determinant+of+the+Hessian%22])">
Determinant of Hessian  (DoH)</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../901/14669901.xml">
Maximally stable extremal regions</link></col>
</row>
<row>
<header colspan="2" style="text-align:center; ">
<mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../898/6185898.xml">
Ridge detection</link></function>
</mathematical_relation>
</header>
</row>
<row>
<header colspan="2" style="text-align:center; ">
Affine invariant feature detection</header>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../265/6866265.xml">
Affine shape adaptation</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../078/14664078.xml">
Harris affine</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../110/14664110.xml">
Hessian affine</link></col>
</row>
<row>
<header colspan="2" style="text-align:center; ">
Feature description</header>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../345/1208345.xml">
SIFT</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../697/12341697.xml">
SURF</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../751/12341751.xml">
GLOH</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../795/15895795.xml">
LESH</link></col>
</row>
<row>
<header colspan="2" style="text-align:center; ">
<link xlink:type="simple" xlink:href="../661/1703661.xml">
Scale-space</link></header>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../404/5481404.xml">
Scale-space axioms</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../059/5477059.xml">
Implementation details</link></col>
</row>
<row>
<col colspan="2" style="text-align:center; " class="">
<link xlink:type="simple" xlink:href="../023/15966023.xml">
Pyramids</link></col>
</row>
<row>
<col colspan="2" style="text-align:right;"></col>
</row>
</table>
</p>

<p>

In the fields of <link xlink:type="simple" xlink:href="../596/6596.xml">
computer vision</link> and <link xlink:type="simple" xlink:href="../382/346382.xml">
image analysis</link>, the <b>Harris-affine region detector</b> belongs to the category of <link xlink:type="simple" xlink:href="../905/14784905.xml">
feature detection</link>.  Feature detection is a preprocessing step of several algorithms that rely on identifying characteristic points or <link>
interest points</link> so to make correspondences between images, recognize textures, categorize objects or build panoramas.</p>

<sec>
<st>
 Overview </st>



<p>

The Harris-affine detector can identify similar regions between images that are related through <link xlink:type="simple" xlink:href="../449/38449.xml">
affine transformations</link> and have different illuminations.  These <it>affine-invariant</it> detectors should be capable of identifying similar regions in images taken from different viewpoints that are related by a simple geometric transformation: scaling, rotation and shearing. These detected regions have been called both <it>invariant</it> and <it>covariant</it>.  On one hand, the regions are detected <it>invariant</it> of the image transformation but the regions <it>covariantly</it> change with image transformation .  Do not dwell too much on these two naming conventions; the important thing to understand is that the design of these interest points will make them compatible across images taken from several viewpoints.  Other detectors that are affine-invariant include <link xlink:type="simple" xlink:href="../110/14664110.xml">
Hessian-Affine</link> regions, <link>
Maximally Stable Extremal Regions</link>, <link xlink:type="simple" xlink:href="../572/14441572.xml">
Kadir brady saliency detector</link>, edge-based regions (<link xlink:type="simple" xlink:href="../624/8060624.xml">
EBR</link>) and intensity extrema-based (<link xlink:type="simple" xlink:href="../949/11355949.xml">
IBR</link>) regions.</p>

<p>

Mikolajczyk and Schmid (2002) first described the Harris-Affine detector as it is used today in <weblink xlink:type="simple" xlink:href="http://vasc.ri.cmu.edu/~hebert/04AP/mikolajc_ECCV2002.pdf">
<it>An Affine Invariant Interest Point Detector''</it></weblink><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. Earlier works in this direction include use of affine adapted feature points for matching by Baumberg <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> and the first use of scale invariant feature points by Lindeberg <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>. The <b>Harris-Affine detector</b> relies on the combination of corner points detected thorough <link xlink:type="simple" xlink:href="../759/4921759.xml#xpointer(//*[./st=%22The_Harris_.26_Stephens_.2F_Plessey_corner_detection_algorithm+%22])">
Harris corner detection</link>, multi-scale analysis through <link>
Gaussian scale-space</link> and affine normalization using an iterative <link xlink:type="simple" xlink:href="../265/6866265.xml">
affine shape adaptation</link> algorithm.  The recursive and iterative algorithm follows an iterative approach to detecting these regions:
<list>
<entry level="1" type="number">

 Identify initial region points using scale-invariant <b>Harris-Laplace Detector</b>.</entry>
<entry level="1" type="number">

 For each initial point, normalize the region to be affine invariant using <link xlink:type="simple" xlink:href="../265/6866265.xml">
affine shape adaptation</link>.</entry>
<entry level="1" type="number">

 Iteratively estimate the affine region: selection of proper integration scale, differentiation scale and spatially localize interest points..</entry>
<entry level="1" type="number">

 Update the affine region using these scales and spatial localizations.</entry>
<entry level="1" type="number">

 Repeat step 3 if the stopping criterion is not met.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Algorithm description </st>

<ss1>
<st>
 Harris-Laplace detector (initial region points)</st>
<p>

The Harris-Affine detector relies heavily on both the Harris measure and a Gaussian scale-space representation.  Therefore, a brief examination of both follow.  For a more exhaustive derivations see <link xlink:type="simple" xlink:href="../759/4921759.xml">
corner detection</link> and <link xlink:type="simple" xlink:href="../661/1703661.xml">
Gaussian scale-space</link> or their associated papers.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></p>

<ss2>
<st>
 Harris corner measure </st>
<p>
  
The Harris corner detector algorithm relies on a central principle: at a corner, the image intensity will change largely in multiple directions.  This can alternatively be formulated by examining the changes of intensity due to shifts in a local window.  Around a corner point, the image intensity will change greatly when the window is shifted in an arbitrary direction.  Following this intuition and through a clever decomposition, the Harris detector uses the second moment matrix as the basis of its corner decisions.  (See <link xlink:type="simple" xlink:href="../759/4921759.xml">
corner detection</link> for more complete derivation).  The matrix <math>A</math>, has also been called the autocorrelation matrix and has values closely related to the derivatives of image intensity.</p>
<p>

<indent level="1">

 <math>A(\mathbf{x}) = \sum_{x,y} w(x,y)
\begin{bmatrix}
I_{x}^2(\mathbf{x}) &amp; I_{x}I_{y}(\mathbf{x}) \\
I_{x}I_{y}(\mathbf{x}) &amp; I_{y}^2(\mathbf{x})\\ 
\end{bmatrix}
</math>                                         
</indent>

where <math>I_{x}</math> and <math>I_{y}</math> are the respective derivatives (of pixel intensity) in the <math>x</math> and <math>y</math> direction. The off-diagonal entries are the product of <math>I_{x}</math> and <math>I_{y}</math>, while the diagonal entries are squares of the respective derivatives.  The weighting function <math>w(x,y)</math> can be uniform, but is more typically an isotropic, circular Gaussian, </p>
<p>

<indent level="1">

 <math>w(x,y) = g(x,y,\sigma) = \frac{1}{2\pi \sigma} e^{ \left (-\frac{  x^2 + y^2}{2\sigma} \right )}</math>
</indent>

that  acts to average in a local region while weighting those values near the center more heavily.</p>
<p>

As it turns out, this <math>A</math> matrix describes the shape of the autocorrelation measure as due to shifts in window location.  Thus, if we let <math> \lambda_1 </math> and <math> \lambda_2 </math> be the eigenvalues of <math>A</math>, then these values will provide a quantitative description of the how the autocorrelation measure changes in space: its principal curvatures.  As Harris and Stephens (1988) point out, the <math>A</math> matrix centered on corner points will have two large, positive eigenvalues<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>.  Rather than extracting these eigenvalues using methods like singular value decomposition, the Harris measure based on the trace and determinant is used:</p>
<p>

<indent level="1">

 <math>
R = \det(A) - \alpha \operatorname{trace}^2(A) = \lambda_1 \lambda_2 - \alpha (\lambda_1 + \lambda_2)^2
</math>
</indent>

where <math>\alpha</math> is a constant.  Corner points have large, positive eigenvalues and would thus have a large Harris measure.  Thus, corner points are identified as local maxima of the Harris measure that are above a specified threshold.</p>
<p>

<indent level="1">

 <math>\begin{align}  
\{x_c\} =  \big\{ x_c | R(x_c) &amp;gt; R(x_i), \forall x_i \in W(x_c) \big\}, \\
R(x_c) &amp;gt; t_{threshold}
\end{align}
</math>              
</indent>

where <math> \{x_c\}</math> are the set of all corner points, <math>R(x)</math> is the Harris measure calculated at <math>x</math>, <math>W(x_c)</math> is an 8-neighbor set centered around <math>x_c</math> and <math>t_{threshold}</math> is a specified threshold.
<image location="right" width="250px" src="Eight_point_neighborhood.svg" type="thumb">
<caption>

8-Point neighborhood
</caption>
</image>
</p>

</ss2>
<ss2>
<st>
 Gaussian scale-space </st>
<p>
 
A Gaussian scale-space representation of an image is the set of images that result from convoluting a Gaussian kernel of various sizes with the original image.  In general, the representation can be formulated as: </p>
<p>

<indent level="1">

 <math>
L(\mathbf{x},s) = G(s) \otimes I(\mathbf{x})
</math>
</indent>

where <math>G(s)</math> is an isotropic, circular Gaussian kernel as defined above.  The convolution with a Gaussian kernel smooths the image using a window the size of the kernel.  A larger scale, <math>s</math>, corresponds to a smoother resultant image.  Mikolajczyk and Schmid (2001) point out that derivatives and other measurements must be normalized across scales <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.  A derivative of order <math>m</math>, <math>D_{i_1, ... i_m}</math>, must be normalized by a factor <math>s^m</math> in the following manner:</p>
<p>

<indent level="1">

 <math>
D_{i_1, \dots, i_m}(\mathbf{x},s) = s^m L_{i_1, \dots, i_m}(\mathbf{x},s)
</math>
</indent>

These derivatives, or any arbitrary measure, can be adapted to a scale-space representation by calculating this measure using a set of scales recursively where the <math>nth</math> scale is <math>s_n = k^n s_0 </math>.  See <link xlink:type="simple" xlink:href="../661/1703661.xml">
scale space</link> for a more complete description.</p>

</ss2>
<ss2>
<st>
 Combining Harris detector across Gaussian scale-space </st>
<p>
                                                  
The <b>Harris-Laplace</b> detector combines the traditional 2D Harris corner detector with the idea of a Gaussian scale-space representation in order to create a scale-invariant detector. Harris-corner points are good starting points because they have been shown to have good rotational and illumination invariance in addition to identifying the interesting points of the image<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>.  However, the points are not scale invariant and thus the second-moment matrix must be modified to reflect a scale-invariant property.  Let us denote, <math>M = \mu(\mathbf{x}, \sigma_{\mathit{I}}, \sigma_{\mathit{D}})</math> as the scale adapted second-moment matrix used in the Harris-Laplace detector.</p>
<p>

<indent level="1">

 <math>
M = \mu(\mathbf{x}, \sigma_{\mathit{I}}, \sigma_{\mathit{D}}) = 
\sigma_D^2 g(\sigma_I) \otimes 
\begin{bmatrix}
L_{x}^2(\mathbf{x}, \sigma_{D}) &amp; L_{x}L_{y}(\mathbf{x}, \sigma_{D}) \\
L_{x}L_{y}(\mathbf{x}, \sigma_{D}) &amp; L_{y}^2(\mathbf{x}, \sigma_{D})
\end{bmatrix}
</math><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>
</indent>

where <math>g(\sigma_I)</math> is the Gaussian kernel of scale <math>\sigma_I</math> and <math>\mathbf{x} = (x,y)</math>.  Similar to the Gaussian-scale space, <math>L(\mathbf{x})</math> is the Gaussian-smoothed image.  The <math>\mathbf{\otimes}</math> operator denotes convolution.  <math>L_{x}(\mathbf{x},\sigma_{D})</math> and <math>L_{y}(\mathbf{x}, \sigma_{D})</math> are the derivatives in their respective direction applied to the smoothed image and calculated using a Gaussian kernel with scale <math>\sigma_D</math>.  In terms of our Gaussian scale-space framework, the <math>\sigma_I</math> parameter determines the current scale at which the Harris corner points are detected.</p>
<p>

Building upon this scale-adapted second-moment matrix, the <b>Harris-Laplace</b> detector is a twofold process: applying the Harris corner detector at multiple scales and automatically choosing the <it>characteristic scale</it>.</p>

</ss2>
<ss2>
<st>
 Multi-scale Harris corner points </st>
<p>

The algorithm searches over a fixed number of predefined scales. This set of scales is defined as:</p>
<p>

<indent level="1">

 <math>
{\sigma_1 \dots \sigma_n} = {\sigma_0 \dots k^{n}\sigma_0}
</math>
</indent>

Mikolajczyk and Schmid (2004) use <math>k = 1.4</math>.  For each integration scale, <math>\sigma_I</math>, chosen from this set, the appropriate differentiation scale is chosen to be a constant factor of the integration scale: <math>\sigma_D = s\sigma_I</math>.  Mikolajczyk and Schmid (2004) used <math>s = 0.7</math> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>. Using these scales, the interest points are detected using a Harris measure on the <math> \mu(\mathbf{x}, \sigma_{\mathit{I}}, \sigma_{\mathit{D}})</math> matrix.  The <it>cornerness,</it> like the typical Harris measure, is defined as:</p>
<p>

<indent level="1">

<math>
\mathit{cornerness} = \det(\mu(\mathbf{x}, \sigma_{\mathit{I}}, \sigma_{\mathit{D}})) - \alpha \operatorname{trace}^2(\mu(\mathbf{x}, \sigma_{\mathit{I}}, \sigma_{\mathit{D}}))
</math>     
</indent>

Like the traditional Harris detector, corner points are those local (8 point neighborhood) maxima of the <it>cornerness</it> that are above a specified threshold.</p>

</ss2>
<ss2>
<st>
 Characteristic scale identification </st>
<p>

An iterative algorithm based on Lindeberg (1998) both spatially localizes the corner points and selects the <it>characteristic scale</it> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.  The iterative search has three key steps, that are carried for each point <math>\mathbf{x}</math> that were initially detected at scale <math>\sigma_I</math> by the multi-scale Harris detector (<math>k</math> indicates the <math>kth</math> iteration):</p>
<p>

Choose the scale <math>\sigma_I^{(k+1)}</math> that maximizes the Laplacian-of-Gaussians (LoG) over a predefined range of neighboring scales.  The neighboring scales are typically chosen from a range that is within a <it>two scale-space</it> neighborhood.  That is, if the original points were detected using a scaling factor of <math>1.4</math> between successive scales, a <it>two scale-space</it> neighborhood is the range <math> t \in [0.7, \dots, 1.4] </math>.  Thus the Gaussian scales examined are: <math>\sigma_I^{(k+1)} = t \sigma_I^k</math>.  The LoG measurement is defined as:</p>
<p>

<indent level="1">

 <math>
\det(LoG(\mathbf{x}, \sigma_I)) = \sigma_I^2 \det(L_{xx}(\mathbf{x}, \sigma_I) + L_{yy}(\mathbf{x},\sigma_I))
</math>
</indent>

where <math>L_{xx}</math> and <math>L_{yy}</math> are the second derivatives in their respective directions<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>.  The <math>\sigma_I^2</math> factor (as discussed above in Gaussian scale-space) is used to normalize the LoG across scales and make these measures comparable, thus making a maximum relevant.  Mikolajczyk and Schmid (2001) demonstrate that the LoG measure attains the highest percentage of correctly detected corner points in comparison to other scale-selection measures <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.  The scale which maximizes this LoG measure in the <it>two scale-space</it> neighborhood is deemed the <b><it>characteristic scale,</it></b> <math>\sigma_I^{(k+1)}</math>, and used in subsequent iterations.  If no extrema, or maxima of the LoG is found, this point is discarded from future searches.
Using the characteristic scale, the points are spatially localized.  That is to say, the point <math>\mathbf{x}^{(k+1)}</math> is chosen such that it maximizes the Harris corner measure (<it>cornerness</it> as defined above) within an 8&amp;times;8 local neighborhood.
Stopping criterion: <math>\sigma_I^{(k+1)} == \sigma_I^{(k)}</math> and <math>\mathbf{x}^{(k+1)} == \mathbf{x}^{(k)}</math>.  </p>
<p>

If the stopping criterion is not met, then the algorithm repeats from step 1 using the new <math>k+1</math> points and scale.  When the stopping criterion is met, the found points represent those that maximize the LoG across scales (scale selection) and maximize the Harris corner measure in a local neighborhood (spatial selection).</p>



<p>

It's important to note that although, Harris points may not be localized across scales, they ultimately all converge to the same scale-invariant point.  That is to say, a corner point that might be detected at multiple scales may not be at the same coordinates at each scale.  However, through the selection of characteristic scale and spatial localization, the points will converge <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.</p>
<p>

----</p>

</ss2>
</ss1>
<ss1>
<st>
 Affine invariant points </st>

<ss2>
<st>
 Mathematical theory </st>
<p>

The Harris-Laplace detected points are scale invariant and work well for isotropic 
<image location="none" width="50px" src="Wiktionary-logo-en.svg">
<caption>

Wiktionary
</caption>
</image>

Look up  in <web_site wordnetid="106359193" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../658/156658.xml">
Wiktionary</link></web_site>
, the free dictionary.
regions that are viewed from the same viewing angle.  In order to be invariant to arbitrary affine transformations (and viewpoints), the mathematical framework must be revisited.  The second-moment matrix <math>\mathbf{\mu}</math> is defined more generally for anisotropic regions:</p>
<p>

<indent level="1">

 <math>
\mu (\mathbf{x}, \Sigma_I, \Sigma_D) = \det(\Sigma_D) g(\Sigma_I) * ( \nabla L(\mathbf{x}, \Sigma_D)\nabla L(\mathbf{x},  \Sigma_D)^T)
</math> 
</indent>

where <math>\Sigma_I</math> and <math>\Sigma_D</math> are covariance matrices defining the differentiation and the integration Gaussian kernel scales.  Although this make look significantly different than the second-moment matrix in the Harris-Laplace detector; it is in fact, identical.  The earlier <math>\mu</math> matrix was the 2D-isotropic version in which the covariance matrices <math>\Sigma_I</math> and <math>\Sigma_D</math> were 2x2 identity matrices multiplied by factors <math>\sigma_I</math> and <math>\sigma_D</math>, respectively.  In the new formulation, one can think of Gaussian kernels as a <link>
multivariate Gaussian distributions</link> as opposed to a uniform Gaussian kernel.  A uniform Gaussian kernel can be thought of as an isotropic, circular region.  Simiarly, a more general Gaussian kernel defines an ellipsoid.  In fact, the eigenvectors and eigenvalues of the covariance matrix define the rotation and size of the ellipsoid. Thus we can easily see that this representation allows us to completely define an arbitrary elliptical affine region over which we want to integrate or differentiate.</p>
<p>

The goal of the affine invariant detector is to identify regions in images that are related through affine transformations. We thus consider a point <math>\mathbf{x}_L</math> and the transformed point <math>\mathbf{x}_R = A\mathbf{x}_L</math>, where A is an affine transformation.  In the case of images, both <math>\mathbf{x}_R</math> and <math>\mathbf{x}_L</math> live in <math>R^2</math> space.  The second-moment matrices are related in the following manner <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>:</p>
<p>

<indent level="1">

 <math>\begin{align}
\mu(\mathbf{x}_L,\Sigma_{I,L}, \Sigma_{D,L}) &amp; {} = A^T \mu (\mathbf{x}_R, \Sigma_{I,R}, \Sigma_{D,R}) A \\
M_L &amp; {} = \mu(\mathbf{x}_L,\Sigma_{I,L}, \Sigma_{D,L}) \\
M_R &amp; {} = \mu (\mathbf{x}_R, \Sigma_{I,R}, \Sigma_{D,R}) \\
M_L &amp; {} = A^T M_R A \\
\Sigma_{I,R} &amp; {} = A \Sigma_{I,L} A^T\text{ and }\Sigma_{D,R} = A \Sigma_{D,L} A^T
\end{align}
</math>     
</indent>

where <math>\Sigma_{I,b}</math> and <math>\Sigma_{D,b}</math> are the covariance matrices for the <math>b</math> reference frame.  If we continue with this formulation and enforce that</p>
<p>

<indent level="1">

 <math>\begin{align}
\Sigma_{I,L} = \sigma_I M_L^{-1} \\
\Sigma_{D,L} = \sigma_D M_L^{-1}
\end{align}
</math>
</indent>

where <math>\sigma_I</math> and <math>\sigma_D</math> are scalar factors, one can show that the covariance matrices for the related point are similarly related:</p>
<p>

<indent level="1">

 <math>\begin{align}
\Sigma_{I,R} = \sigma_I M_R^{-1} \\
\Sigma_{D,R} = \sigma_D M_R^{-1}
\end{align}
</math>                    
</indent>

By requiring the covariance matrices to satisfy these conditions, several nice properties arise.  One of these properties is that the square root of the second-moment matrix, <math>M^{\tfrac{1}{2}}</math> will transform the original anisotropic region into isotropic regions that are related simply through a pure rotation matrix <math>R</math>.  These new isotropic regions can be thought of as a normalized reference frame.  The following equations formulate the relation between the normalized points <math>x_R^'</math> and <math>x_L^'</math>:</p>
<p>

<indent level="1">

 <math>\begin{align}
A = M_R^{-\tfrac{1}{2}} R M_L^{\tfrac{1}{2}} \\
x_R^' = M_R^{\tfrac{1}{2}}x_R \\
x_L^' = M_L^{\tfrac{1}{2}}x_L \\
x_L^' = R x_R^'\\
\end{align}            
</math>
</indent>

The rotation matrix can be recovered using gradient methods likes those in the <link xlink:type="simple" xlink:href="../350/1208350.xml">
SIFT</link> descriptor.  As discussed with the Harris detector, the eigenvalues and eigenvectors of the second-moment matrix, <math>M = \mu(\mathbf{x}, \Sigma_I, \Sigma_D)</math> characterize the curvature and shape of the pixel intensities.  That is, the eigenvector associated with the largest eigenvalue indicates the direction of largest change and the eigenvector associated with the smallest eigenvalue defines the direction of least change.  In the 2D case, the eigenvectors and eigenvalues define an ellipse. For an isotropic region, the region should be circular in shape and not elliptical.  This is the case when the eigenvalues have the same magnitude.  Thus a measure of the isotropy around a local region is defined as the following:</p>
<p>

<indent level="1">

 <math>
\mathcal{Q} = \frac{\lambda_\min(M)}{\lambda_\max(M)}
</math>
</indent>

where <math>\lambda</math> denote eigenvalues.  This measure has the range <math>[0 \dots 1]</math>.  A value of <math>1</math> corresponds to perfect isotropy.</p>

</ss2>
<ss2>
<st>
 Iterative algorithm </st>
<p>

Using this mathematical framework, the <b>Harris-Affine</b> detector algorithm iteratively discovers the second-moment matrix that transforms the anisotropic 
<image location="none" width="50px" src="Wiktionary-logo-en.svg">
<caption>

Wiktionary
</caption>
</image>

Look up  in <web_site wordnetid="106359193" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../658/156658.xml">
Wiktionary</link></web_site>
, the free dictionary.
region into a normalized region in which the isotropic measure is sufficiently close to one. The algorithm uses this <it>shape adaptation matrix</it>, <math>U</math>, to transform the image into a normalized reference frame.  In this normalized space, the interest points' parameters (spatial location, integration scale and differentiation scale) are refined using methods similar to the Harris-Laplace detector.  The second-moment matrix is computed in this normalized reference frame and should have an isotropic measure close to one at the final iteration.  At every <math>k</math>th iteration, each interest region is defined by several parameters that the algorithm must discover: the <math>U^{(k)}</math> matrix, position <math>\mathbf{x}^{(k)}</math>, integration scale <math>\sigma_I^{(k)}</math> and differentiation scale <math>\sigma_D^{(k)}</math>.  Because the detector computes the second-moment matrix in the transformed domain, it's convenient to denote this transformed position as <math>\mathbf{x}_w^{(k)}</math> where <math>U^{(k)}\mathbf{x}_w^{(k)} = \mathbf{x^{(k)}}</math>.</p>

<p>

The detector initializes the search space with points detected by the Harris-Laplace detector. </p>
<p>

<math>U^{(0)} = \mathit{identity}</math> and <math>\mathbf{x}^{(0)}</math>, <math>\sigma_D^{(0)}</math>, and <math>\sigma_I^{(0)}</math> are those from the Harris-Laplace detector.
Apply the previous iteration <it>shape adaptation matrix</it>, <math>U^{(k-1)}</math> to generate the normalized reference frame, <math>U^{(k-1)}\mathbf{x}_w^{(k-1)} = \mathbf{x}^{(k-1)}</math>.  For the first iteration, you apply <math>U^{(0)}</math>.
<b>Select the integration scale</b>, <math>\sigma_I^{(k)}</math>, using a method similar to the Harris-Laplace detector.  The scale is chosen as the scale that maximizes the Laplacian of Gaussian (LoG).  The search space of the scales are those within two scale-spaces of the previous iterations scale.</p>
<p>

<indent level="1">

 <math>
\sigma_I^{(k)} = \underset{{\sigma_I = t\sigma_I^{(k-1)}\atop t \in [0.7, \dots, 1.4]}}{\operatorname{argmax}} \, \sigma_I^2 \det(L_{xx}(\mathbf{x}, \sigma_I) + L_{yy}(\mathbf{x},\sigma_I))
</math>          
</indent>

It's important to note that the integration scale in the <math>U-normalized</math> space differs significantly than the non-normalized  space.  Therefore, it is necessary to search for the integration scale as opposed to using the scale in the non-normalized space.
<b>Select the differentiation scale</b>, <math>\sigma_D^{(k)}</math>. In order to reduce the search space and degrees of freedom, the differentiation scale is taken to be related to the integration scale through a constant factor: <math>\sigma_D^{k} = s \sigma_I^{k}</math>.  For obvious reasons, the constant factor is less than one.  Mikolajczyk and Schmid (2001) note that a too small factor will make smoothing (integration) too significant in comparison to differentiation and a factor that's too large will not allow for the integration to average the covariance matrix <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.  It is common to choose <math>s \in [0.5,0.75]</math>.  From this set, the chosen scale will maximize the isotropic measure <math>\mathcal{Q} = \frac{\lambda_{min}(\mu)}{\lambda_{max}(\mu)}</math>.</p>
<p>

<indent level="1">

 <math>
\sigma_D^{(k)} = \underset{\sigma_D = s\sigma_I^{(k)},\; s \in [0.5, \dots, 0.75]}{\operatorname{argmax}} \, \frac{\lambda_\min(\mu(\mathbf{x}_w^{(k)}, \sigma_I^{k}, \sigma_D))}{\lambda_\max(\mu(\mathbf{x}_w^{(k)}, \sigma_I^{k}, \sigma_D))}
</math>
</indent>

where <math>\mu(\mathbf{x}_w^{(k)}, \sigma_I^{k}, \sigma_D)</math> is the second-moment matrix evaluated in the normalized reference frame.  This maximization processes causes the eigenvalues to converge to the same value.
<b>Spatial Localization:</b> Select the point <math>\mathbf{x}_w^{(k)}</math> that maximizes the Harris corner measure (<math>\mathit{cornerness}</math>) within an 8-point neighborhood around the previous <math>\mathbf{x}_w^{(k-1)}</math> point.</p>
<p>

<indent level="1">

 <math>
\mathbf{x}_w^{(k)} = \underset{\mathbf{x}_w \in W(\mathbf{x}_w^{(k-1)})}{\operatorname{argmax}} \,
\det(\mu(\mathbf{x}_w, \sigma_I^{k}, \sigma_D^{(k)})) - \alpha \operatorname{trace}^2(\mu(\mathbf{x}_w, \sigma_I^{k}, \sigma_D^{(k)}))
</math>
</indent>

where <math>\mu</math> is the second-moment matrix as defined above.  The window <math>W(\mathbf{x}_w^{(k-1)})</math> is the set of 8-nearest neighbors of the previous iteration's point in the normalized reference frame.  </p>
<p>

Because our spatial localization was done in the <math>U</math>-normalized reference frame, the newly chosen point must be transformed back to the original reference frame.  This is achieved by transforming a displacement vector and adding this to the previous point:</p>
<p>

<indent level="1">

 <math>
\mathbf{x}^{(k)} = \mathbf{x}^{(k-1)} + U^{(k-1)}\cdot (\mathbf{x}_w^{(k)} - \mathbf{x}_w^{(k-1)})
</math>
</indent>

As mentioned above, the square-root of the second-moment matrix defines the transformation matrix that generates the normalized reference frame.  We thus need to save this matrix: <math>\mu_i^{(k)} = \mu^{-\tfrac{1}{2}}(\mathbf{x}_w^{(k)}, \sigma_I^{(k)}, \sigma_D^{(k)})</math>.  The transformation matrix <math>U</math> is updated: <math>U^{(k)} = \mu_i^{(k)}\cdot U^{(k-1)}</math>.  In order to ensure that the image gets sampled correctly and we are expanding the image in the direction of the least change (smallest eigenvalue), we fix the maximium eigenvalue: <math>\lambda_{max}(U^{(k)}) = 1</math>.  Using this updating method, one can easily see that the final <math>U</math> matrix takes the following form:</p>
<p>

<indent level="1">

 <math>
U = \prod_{k} \mu_i^{(k)} \cdot U^{(0)} = \prod_{k} (\mu^{-\tfrac{1}{2}})^{(k)} \cdot U^{(0)}
</math>
</indent>

If the <b>stopping criterion</b> is not met, continue to the next iteration at step 2.  Because the algorithm iteratively solves for the <math>U-normalization</math> matrix that transforms an anisotropic region into an isotropic region, it makes sense to stop when the isotropic measure, <math>\mathcal{Q} = \frac{\lambda_\max(\mu)}{\lambda_\max(\mu)}</math>, is sufficiently close to its maximum value 1.  <it>Sufficiently close</it> implies the following <b>stopping condition</b>:</p>
<p>

<indent level="1">

 <math>
1 - \frac{\lambda_\max(\mu_i^{(k)})}{\lambda_\max(\mu_i^{(k)})} &amp;lt; \varepsilon_C
</math>
</indent>

Mikolajczyk and Schmid (2004) had good success with <math>\epsilon_C = 0.05 </math>.</p>
<p>

-----</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
 Computation and implementation </st>
<p>

The computational complexity of the Harris-Affine detector is broken into two parts: initial point detection and affine region normalization.  The initial point detection algorithm, Harris-Laplace, has complexity <math>\mathcal{O}(n)</math> where <math>n</math> is the number of pixels in the image.  The affine region normalization algorithm automatically detects the scale and estimates the <it>shape adaptation matrix</it>, <math>U</math>.  This process has complexity <math>\mathcal{O}((m+k)p)</math>, where <math>p</math> is the number of initial points, <math>m</math>  is the size of the search space for the automatic scale selection and <math>k</math> is the number of iterations required to compute the <math>U</math> matrix <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>.</p>
<p>

Some methods exist to reduce the complexity of the algorithm at the expense of accuracy.  One method is to eliminate the search in the differentiation scale step.  Rather than choose a factor <math>s</math> from a set of factors, the sped-up algorithm chooses the scale to be constant across iterations and points: <math>\sigma_D = s \sigma_I,\; s = constant</math>.   Although this reduction in search space might decrease the complexity, this change can severely effect the convergence of the <math>U</math> matrix.
</p>
</sec>
<sec>
<st>
 Analysis </st>

<ss1>
<st>
 Convergence </st>
<p>

One can imagine that this algorithm might identify duplicate interest points at multiple scales.  Because the Harris-affine algorithm looks at each initial point given by the Harris-Laplace detector independently, there is no discrimination between identical points.  In practice, it has been shown that these points will ultimately all converge to the same interest point.  After finishing identifying all interest points, the algorithm accounts for duplicates by comparing the spatial coordinates (<math>\mathbf{x}</math>), the integration scale <math>\sigma_I</math>, the isotropic measure <math>\tfrac{\lambda_\min(U)}{\lambda_\max(U)}</math> and skew <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>.  If these interest point parameters are similar within a specified threshold, then they are labeled duplicates.  The algorithm discards all these duplicate points except for the interest point that's closest to the average of the duplicates.  Typically 30% of the Harris-Affine points are distinct and dissimilar enough to not be discarded <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>.</p>
<p>

Mikolajczyk and Schmid (2004) showed that often the initial points (40%) do not coverage.  The algorithm detects this divergence by stopping the iterative algorithm if the inverse of the isotropic measure is larger than a specified threshold: <math> \tfrac{\lambda_\max(U)}{\lambda_\min(U)} &amp;gt; t_\text{diverge} </math>.  Mikolajczyk and Schmid (2004) use <math>t_{diverge} = 6</math>. Of those that did converge, the typical number of required iterations was 10 <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.</p>

</ss1>
<ss1>
<st>
 Quantitative measure </st>
<p>

Quantitative analysis of affine region detectors take into account both the accuracy of point locations and the overlap of regions across two images.  Mioklajcyzk and Schmid (2004) extend the <b>repeatability measure</b> of  Schmid et al. (1998) as the ratio of point correspondences to minimum detected points of the two images<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>.  </p>
<p>

<indent level="1">

 <math>
R_\text{score} = \frac{C(A,B)}{\min(n_A, n_B)}
</math>
</indent>

where <math>C(A,B)</math> are the number of corresponding points in images <math>A</math> and <math>B</math>. <math>n_B</math> and <math>n_A</math> are the number of detected points in the respective images.  Because each image represents 3D space, it might be the case that the one image contains objects that are not in the second image and thus whose interest points have no chance of corresponding.  In order to make the repeatability measure valid, one remove these points and must only consider points that lie in both images; <math>n_A</math> and <math>n_B</math> only count those points such that <math>x_A = H \cdot x_B </math>.  For a pair of two images related through a <link xlink:type="simple" xlink:href="../962/3051962.xml">
homography</link> matrix <math>H</math>, two points, <math>\mathbf{x_a}</math> and <math>\mathbf{x_b}</math> are said to correspond if:<image location="right" width="250px" src="Overlap_region.svg" type="thumb">
<caption>

Overlap region of two elliptical regions.
</caption>
</image>
</p>

<p>

Error in pixel location is less than 1.5 pixels: <math>\| \mathbf{x_a} - H\cdot \mathbf{x_b} \| &amp;lt; 1.5 </math>
The <b>overlap error</b> of the two affine points (<math>\epsilon_S</math>) must be less than a specified threshold (typically 40%).  For affine regions, this overlap error is the following:</p>
<p>

<indent level="1">

 <math>
\epsilon_S = 1 - \frac{\mu_a \cap (H^T \mu_b H)}{\mu_a \cup (H^T \mu_b H)}
</math>
</indent>

where <math>\mu_a</math> and <math>\mu_b</math> are the recovered elliptical regions whose points satisfy: <math>\mu^T \mathbf{x} \mu = 1 </math>.  Basically, this measure takes a ratio of areas: the area of overlap (intersection) and the total area (union).  Perfect overlap would have a ratio of one and have an <math>\epsilon_S = 0</math>.  Different scales effect the region of overlap and thus must be taken into account by normalizing the area of each region of interest.  Regions with an overlap error as high as 50% are viable detectors to be matched with a good descriptor .</p>
<p>

A second measure, a <b>matching score</b>, more practically assesses the detector's ability to identify matching points between images.  Mikolajczyk and Schmid (2005) use a <link>
SIFT</link> descriptor to identify matching points.  In addition to being the closest points in SIFT-space, two matched points must also have a sufficiently small overlap error (as defined in the repeatability measure).  The <b>matching score</b> is the ratio of the number of matched points and the minimum of the total detected points in each image: </p>
<p>

<indent level="1">

 <math>M_{score} = \frac{M(A,B)}{\min(n_A,n_B)}</math> ,
</indent>

where <math>M(A,B)</math> are the number of matching points and <math>n_B</math> and <math>n_A</math> are the number of detected regions in the respective images.</p>

</ss1>
<ss1>
<st>
 Robustness to affine and other transformations </st>
<p>

Mikolajczyk et al. (2005) have done a thorough analysis of several state-of-the-art affine region detectors: Harris-Affine, <link>
Hessian-Affine</link>, <link xlink:type="simple" xlink:href="../901/14669901.xml">
MSER</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref>, IBR &amp; EBR <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref> and <link>
salient</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref> detectors<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref>.  Mikolajczyk et al. analyzed both structured images and textured images in their evaluation.  Linux binaries of the detectors and their test images are freely available at their <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/index.html">
webpage</weblink>.  A brief summary of the results of Mikolajczyk et al (2005) follow; see <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/vibes_ijcv2004.pdf">
<it>A comparison of affine region detectors''</it></weblink> for a more quantitative analysis.</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Viewpoint Angle Change:</b> The Harris-Affine detector has reasonable (average) robustness to these types of changes.  The detector maintains a repeatability score of above 50% up until a viewpoint angle of above 40 degrees.  The detector tends to detect a high number of repeatable and matchable regions even under a large viewpoint change.</entry>
<entry level="1" type="bullet">

 <b>Scale Change:</b> The Harris-Affine detector remains very consistent under scale changes.  Although the number of points declines considerably at large scale changes (above 2.8), the repeatability (50-60%) and matching scores (25-30%) remain very constant especially with textured images.  This is consistent with the high-performance of the automatic scale selection iterative algorithm. </entry>
<entry level="1" type="bullet">

 <b>Blurred Images:</b> The Harris-Affine detector remains very stable under image blurring.  Because the detector does not rely on image segmentation or region boundaries, the repeatability and matching scores remain constant.</entry>
<entry level="1" type="bullet">

 <b>JPEG Artifacts:</b> The Harris-Affine detector degrades similar to other affine detectors: repeatability and matching scores drop significantly above 80% compression.</entry>
<entry level="1" type="bullet">

 <b>Illumination Changes:</b> The Harris-Affine detector, like other affine detectors, is very robust to illumination changes: repeatability and matching scores remain constant under decreasing light.  This should be expected because the detectors rely heavily on relative intensities (derivatives) and not absolute intensities.</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 General trends </st>
<p>

<list>
<entry level="1" type="bullet">

 Harris-affine region points tend to be small and numerous.  Both the Harris-Affine detector and <link xlink:type="simple" xlink:href="../110/14664110.xml">
Hessian-Affine</link> consistently identify double the number repeatable points as other affine detectors: ~1000 regions for a 800x640 image <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref>. Small regions are less likely to be occluded but have a smaller chance of overlapping neighboring regions. </entry>
<entry level="1" type="bullet">

 The Harris-Affine detector responds well to textured scenes in which there are a lot of corner-like parts.  However, for some structured scenes, like buildings, the Harris-Affine detector performs very well.  This is complementary to MSER that tends to do better with well structured (segmentable) scenes.</entry>
<entry level="1" type="bullet">

 Overall the Harris-Affine detector performs very well, but still behind MSER and Hessian-Affine in all cases but blurred images.</entry>
<entry level="1" type="bullet">

 Harris-Affine and Hessian-Affine detectors are less accurate than others: their repeatability score increases as the overlap threshold is increased.</entry>
<entry level="1" type="bullet">

 The detected affine-invariant regions may still differ in their rotation and illumination. Any descriptor that uses these regions must account for the invariance when using the regions for matching or other comparisons.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 Applications </st>
<p>

<list>
<entry level="1" type="bullet">

 <link>
Content-based image retrieval</link> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref></entry>
<entry level="1" type="bullet">

 Model-based recognition</entry>
<entry level="1" type="bullet">

 Object retrieval in video <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref></entry>
<entry level="1" type="bullet">

 Visual data mining: identifying important objects, characters and scenes in videos <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2218%22])">18</ref></entry>
<entry level="1" type="bullet">

 Object recognition and categorization <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2219%22])">19</ref></entry>
</list>
</p>

</sec>
<sec>
<st>
 Software packages </st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/">
Affine Covariant Features</weblink>: K. Mikolajczyk maintains a web page that contains Linux binaries of the Harris-Affine detector in addition to other detectors and descriptors.  Matlab code is also available that can be used to illustrate and compute the repeatability of various detectors.  Code and images are also available to duplicate the results found in the Mikolajczyk et al. (2005) paper.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<weblink xlink:type="simple" xlink:href="http://vasc.ri.cmu.edu/~hebert/04workshop/presentations/schmid_sicily04.ppt">
http://vasc.ri.cmu.edu/~hebert/04workshop/presentations/schmid_sicily04.ppt</weblink> - Presentation slides from Mikolajczyk et al. on their 2005 paper.</p>
<p>

<weblink xlink:type="simple" xlink:href="http://lear.inrialpes.fr/software">
http://lear.inrialpes.fr/software</weblink> - Cordelia Schmid's Computer Vision Lab</p>
<p>

<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/">
http://www.robots.ox.ac.uk/~vgg/research/affine/</weblink> - Code, test Images, bibliography of Affine Covariant Features maintained by Krystian Mikolajczyk and the <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/">
Visual Geometry Group</weblink> from the Robotics group at the University of Oxford.</p>
<p>

<weblink xlink:type="simple" xlink:href="http://iris.usc.edu/Vision-Notes/bibliography/twod275.html">
http://iris.usc.edu/Vision-Notes/bibliography/twod275.html</weblink> - Bibliography of feature (and blob) detectors maintained by USC Institute for Robotics and Intelligent Systems</p>
<p>

<weblink xlink:type="simple" xlink:href="http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm">
http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm</weblink> - Digital implementation of Laplacian of Gaussian</p>

</sec>
<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../110/14664110.xml">
Hessian-affine</link> </entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../901/14669901.xml">
MSER</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../572/14441572.xml">
Kadir brady saliency detector</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../661/1703661.xml">
Scale-space</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../865/14865.xml">
Isotropy</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../759/4921759.xml">
Corner detection</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../669/7046669.xml">
Interest point detection</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../265/6866265.xml">
Affine shape adaptation</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../596/6596.xml">
Computer vision</link>                      </entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
<weblink xlink:type="simple" xlink:href="http://vasc.ri.cmu.edu/~hebert/04AP/mikolajc_ECCV2002.pdf">
Mikolajcyk, K. and Schmid, C. 2002. An affine invariant interest point detector. In <it>Proceedings of the 8th International Conference on Computer Vision</it>, Vancouver, Canada.</weblink></entry>
<entry id="2">
<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/baumberg00reliable.html">
A. Baumberg (2000). "Reliable feature matching across widely separated views". Proceedings of IEEE Conference on Computer Vision and Pattern Recognition: pages I:1774--1781.</weblink></entry>
<entry id="3">
<weblink xlink:type="simple" xlink:href="http://www.nada.kth.se/cvap/abstracts/cvap198.html">
T. Lindeberg (1998). "Feature detection with automatic scale selection". International Journal of Computer Vision 30 (2): pp 77--116.</weblink></entry>
<entry id="4">
 <weblink xlink:type="simple" xlink:href="http://www.csse.uwa.edu.au/~pk/research/matlabfns/Spatial/Docs/Harris/A_Combined_Corner_and_Edge_Detector.pdf">
C. Harris and M. Stephens (1988). "A combined corner and edge detector". Proceedings of the 4th Alvey Vision Conference: pages 147--151.</weblink></entry>
<entry id="5">
<weblink xlink:type="simple" xlink:href="http://robotics.caltech.edu/readinggroup/vision/mikolajcICCV2001.pdf">
K. Mikola jczyk and C. Schmid. Indexing based on scale invariant interest points. In Proceedings of the 8th International Conference on Computer Vision, Vancouver, Canada, pages 525-531, 2001.</weblink></entry>
<entry id="6">
Schmid, C., Mohr, R., and Bauckhage, C. 2000. Evaluation of interest point detectors. International Journal of Computer Vision, 37(2):151-172.</entry>
<entry id="7">
<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/mikolajczyk_ijcv2004.pdf">
Mikolajczykm K. and Schmid, C. 2004. Scale &amp; affine invariant interest point detectors.  <it>International Journal on Computer Vision</it> 60(1):63-86.</weblink></entry>
<entry id="8">
<weblink xlink:type="simple" xlink:href="http://www.cee.hw.ac.uk/hipr/html/log.html">
Spatial Filters: Laplacian/Laplacian of Gaussian</weblink></entry>
<entry id="9">
<weblink xlink:type="simple" xlink:href="http://www.nada.kth.se/~tony/abstracts/LG94-ECCV.html">
T. Lindeberg and J. Garding (1997). "Shape-adapted smoothing in estimation of 3-{D} depth cues from affine distortions of local 2-{D} structure". Image and Vision Computing 15: pp 415--434.</weblink></entry>
<entry id="10">
C. Schmid, R. Mohr, and C. Bauckhage. Comparing and evaluating interest points.  In <it>International Conference on Computer Vision</it>, pp. 230-135, 1998.</entry>
<entry id="11">
<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/matas_bmvc2002.pdf">
J.Matas, O. Chum, M. Urban, and T. Pajdla, Robust wide baseline stereo from maximally stable extremal regions. In BMVC p. 384-393, 2002.</weblink></entry>
<entry id="12">
<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/tuytelaars_ijcv2004.pdf">
T.Tuytelaars and L. Van Gool, Matching widely separated views based on affine invariant regions . In IJCV 59(1):61-85, 2004.</weblink></entry>
<entry id="13">
<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/kadir04.pdf">
T. Kadir, A. Zisserman, and M. Brady, An affine invariant salient region detector. In ECCV p. 404-416, 2004.</weblink></entry>
<entry id="14">
 <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/vibes_ijcv2004.pdf">
K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir and L. Van Gool, A comparison of affine region detectors. In IJCV 65(1/2):43-72, 2005</weblink></entry>
<entry id="15">
http://staff.science.uva.nl/~gevers/pub/overview.pdf</entry>
<entry id="17">
<weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/publications/papers/sivic03.pdf">
J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In Proceedings of the International Conference on Computer Vision, Nice, France, 2003.</weblink></entry>
<entry id="16">
<weblink xlink:type="simple" xlink:href="http://www.liacs.nl/home/mlew/mir.survey16b.pdf">
R. Datta, J. Li, and J. Z. Wang, Content-based image retrieval - Approaches and trends of the new age, In Proc. Int. Workshop on Multimedia Information Retrieval, pp. 253-262, 2005.IEEE Transactions on Multimedia, vol. 7, no. 1, pp. 127-142, 2005.</weblink></entry>
<entry id="19">
<weblink xlink:type="simple" xlink:href="http://lear.inrialpes.fr/people/triggs/events/iccv03/cdrom/iccv03/0634_dorko.pdf">
G. Dorko and C. Schmid. <it>Selection of scale invariant neighborhoods for object class recognition.</it> In Proceedings of International Conference on Computer Vision, Nice, France, pp. 634-640, 2003.</weblink></entry>
<entry id="18">
 <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~vgg/publications/papers/sivic04b.pdf">
J. Sivic and A. Zisserman. Video data mining using configurations of viewpoint invariant regions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Washington DC, USA, pp. 488-495, 2004.</weblink></entry>
</reflist>
</p>

</sec>
</bdy>
</article>
