<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:38:26[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>3D single object recognition</title>
<id>14669421</id>
<revision>
<id>240732557</id>
<timestamp>2008-09-24T19:39:24Z</timestamp>
<contributor>
<username>KYN</username>
<id>505011</id>
</contributor>
</revision>
<categories>
<category>Orphaned articles from January 2008</category>
<category>Object recognition and categorization</category>
<category>All orphaned articles</category>
</categories>
</header>
<bdy>
<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="44px" src="Wiki_letter_w.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>This article is  as few or no other articles <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Special:Whatlinkshere&amp;target=3D_single_object_recognition&amp;namespace=0">
link to it</weblink>.</b>
Please help  in articles on <weblink xlink:type="simple" xlink:href="http://www.google.com/search?hl=en&amp;as_qdr=all&amp;q=+site%3Aen.wikipedia.org+%223D+single+object+recognition%22">
related topics</weblink>. <it>(January 2008)''</it></col>
</row>
</table>
</p>

<p>

In <link xlink:type="simple" xlink:href="../596/6596.xml">
computer vision</link>, <b>3D single <link xlink:type="simple" xlink:href="../466/14661466.xml">
object recognition</link></b> involves recognizing and determining the pose of user-chosen <link xlink:type="simple" xlink:href="../853/3054853.xml">
3D</link> object in a <link xlink:type="simple" xlink:href="../080/25080.xml">
photograph</link> or <link>
range scan</link>.  Typically, an example of the object to be recognized is presented to a vision system in a controlled environment, and then for an arbitrary input such as a <link xlink:type="simple" xlink:href="../682/28682.xml">
video stream</link>, the system locates the previously presented object.  This can be done either <link xlink:type="simple" xlink:href="../440/41440.xml">
off-line</link>, or in <link xlink:type="simple" xlink:href="../767/25767.xml">
real-time</link>.  The <link xlink:type="simple" xlink:href="../775/775.xml">
algorithms</link> for solving this problem are specialized for locating a single pre-identified object, and can be contrasted with algorithms which operate on general <link xlink:type="simple" xlink:href="../282/6282.xml">
classes</link> of objects, such as <link>
face recognition systems</link> or <link>
3D generic object recognition</link>.  Due to the low cost and ease of acquiring photographs, a significant amount of research has been devoted to 3D object recognition in photographs.</p>

<sec>
<st>
 3D single object recognition in photographs </st>

<p>

The method of recognizing a 3D object depends on the properties of an object.  For simplicity, many existing algorithms have focused on recognizing <link xlink:type="simple" xlink:href="../875/1458875.xml">
rigid</link> objects consisting of a single part, that is, objects whose spatial transformation is a <link xlink:type="simple" xlink:href="../164/652164.xml">
Euclidean motion</link>.  Two general approaches have been taken to the problem: <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link> approaches use low-level image appearance information to locate an object, while feature-based geometric approaches construct a model for the object to be recognized, and match the model against the photograph.  </p>

<ss1>
<st>
 Pattern recognition approaches </st>

<p>

These methods use appearance information gathered from pre-captured or pre-computed projections of an object to match the object in the potentially cluttered scene.  However, they do not take the 3D geometric constraints of the object into consideration during matching, and typically also do not handle occlusion as well as feature-based approaches.  See [Murase and Nayar 1995] and [Selinger and Nelson 1999].</p>

</ss1>
<ss1>
<st>
 Feature-based geometric approaches </st>

<p>

<image location="right" width="322px" src="Feature_example.png" type="thumb">
<caption>

An example of a detected feature in an image.  Blue indicates the center of the feature, the red ellipse indicates the characteristic scale identified by the feature detector, and the green parallelogram is constructed from the coordinates of the ellipse as per [Lowe 2004].
</caption>
</image>
</p>
<p>

Feature-based approaches work well for objects which have distinctive <link xlink:type="simple" xlink:href="../455/3193455.xml">
features</link>.  Thus far, objects which have good <link>
edge feature</link>s or <link xlink:type="simple" xlink:href="../205/6840205.xml">
blob</link> features have been successfully recognized; for example detection algorithms, see <link xlink:type="simple" xlink:href="../078/14664078.xml">
Harris affine region detector</link> and <link xlink:type="simple" xlink:href="../345/1208345.xml">
SIFT</link>, respectively.  Due to lack of the appropriate feature detectors, objects without textured, smooth surfaces cannot currently be handled by this approach.</p>
<p>

Feature-based object recognizers generally work by pre-capturing a number of fixed views of the object to be recognized, extracting features from these views, and then in the recognition process, matching these features to the scene and enforcing geometric constraints.</p>
<p>

As an example of a prototypical system taking this approach, we will present an outline of the method used by [Rothganger et al 2004], with some detail elided.  The method starts by assuming that objects undergo globally rigid transformations.  Because smooth surfaces are locally planar, <link>
affine invariant</link> features are appropriate for matching: the paper <link xlink:type="simple" xlink:href="../905/14784905.xml">
detects</link> ellipse-shaped regions of interest using both edge-like and blob-like features, and as per [Lowe 2004], finds the dominant gradient direction of the ellipse, converts the ellipse into a parallelogram, and takes a <link xlink:type="simple" xlink:href="../350/1208350.xml">
SIFT</link> descriptor on the resulting parallelogram.  Color information is used also to improve discrimination over SIFT features alone.</p>
<p>

<image location="right" width="322px" src="Partial_features_3d.png" type="thumb">
<caption>

Partial models of features, projected into 3D, constructed from nearby views of a teddy-bear.  Taken from [Rothganger et al 2004].
</caption>
</image>
</p>
<p>

Next, given a number of camera views of the object (24 in the paper), the method constructs a 3D model for the object, containing the 3D spatial position and orientation of each feature.  Because the number of views of the object is large, typically each feature is present in several adjacent views.  The center points of such matching features correspond, and detected features are aligned along the dominant gradient direction, so the points at (1, 0) in the local coordinate system of the feature parallelogram also correspond, as do the points (0, 1) in the parallelogram's local coordinates.  Thus for every pair of matching features in nearby views, three point pair correspondences are known.  Given at least two matching features, a multi-view affine <link xlink:type="simple" xlink:href="../671/5386671.xml">
structure from motion</link> algorithm (see [Tomasi and Kanade 1992]) can be used to construct an estimate of points positions (up to an arbitrary affine transformation).  The paper of Rothganger et al therefore selects two adjacent views, uses a <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../270/1089270.xml">
RANSAC</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
-like method to select two corresponding pairs of features, and adds new features to the partial model built by RANSAC so long as they are under an error term.  Thus for any given pair of adjacent views, the algorithm creates a partial model of all features visible in both views.</p>
<p>

<image location="right" width="322px" src="Features_full_3d.png" type="thumb">
<caption>

Final merged model of features for the teddy bear, after Euclidean upgrade.  For recognition, this model is matched against a photograph of the scene using RANSAC.  Taken from [Rothganger et al 2004].
</caption>
</image>
</p>
<p>

To produce a unified model, the paper takes the largest partial model, and incrementally aligns all smaller partial models to it.  Global minimization is used to reduce the error, then a <link>
Euclidean upgrade</link> is used to change the model's feature positions from 3D coordinates unique up to affine transformation to 3D coordinates that are unique up to <link xlink:type="simple" xlink:href="../164/652164.xml">
Euclidean motion</link>.  At the end of this step, one has a model of the target object, consisting of features projected into a common 3D space.</p>
<p>

To recognize an object in an arbitrary input image, the paper detects features, and then uses <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../270/1089270.xml">
RANSAC</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 to find the <link>
affine projection</link> matrix which best fits the unified object model to the 2D scene.  If this RANSAC approach has sufficiently low error, then on success, the algorithm both recognizes the object and gives the object's pose in terms of an affine projection.  Under the assumed conditions, the method typically achieves recognition rates of around 95%.</p>

</ss1>
</sec>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

 Murase, H. and S. K. Nayar: 1995, <it>Visual Learning and Recognition of 3-D Objects from Appearance</it>.  International Journal of Computer Vision 14, 5–24. <weblink xlink:type="simple" xlink:href="http://www.cse.unr.edu/~bebis/MathMethods/PCA/case_study_pca2.pdf">
http://www.cse.unr.edu/~bebis/MathMethods/PCA/case_study_pca2.pdf</weblink></entry>
<entry level="1" type="bullet">

 Selinger, A. and R. Nelson: 1999, <it>A Perceptual Grouping Hierarchy for Appearance-Based 3D Object Recognition.</it> Computer Vision and Image Understanding 76(1), 83–92. <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/282716.html">
http://citeseer.ist.psu.edu/282716.html</weblink></entry>
<entry level="1" type="bullet">

 Rothganger, F; S. Lazebnik, C. Schmid, and J. Ponce: 2004.  <it>3D Object Modeling and Recognition Using Local Affine-Invariant Image Descriptors and Multi-View Spatial Constraints</it>, ICCV. <weblink xlink:type="simple" xlink:href="http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/ijcv04d.pdf">
http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/ijcv04d.pdf</weblink></entry>
<entry level="1" type="bullet">

 Lowe, D.: 2004, <it>Distinctive image features from scale-invariant keypoints.</it> International Journal of Computer Vision. In press. <weblink xlink:type="simple" xlink:href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">
http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf</weblink></entry>
<entry level="1" type="bullet">

 Tomasi, C. and T. Kanade: 1992, <it>Shape and Motion from Image Streams: a Factorization Method.</it> International Journal of Computer Vision 9(2), 137–154. <weblink xlink:type="simple" xlink:href="http://www.cse.huji.ac.il/course/2006/compvis/lectures/tomasiTr92Text.pdf">
http://www.cse.huji.ac.il/course/2006/compvis/lectures/tomasiTr92Text.pdf</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../466/14661466.xml">
Object recognition</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../224/1284224.xml">
Feature detection (computer vision)</link></entry>
<entry level="1" type="bullet">

 <link>
Feature descriptor</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../270/1089270.xml">
RANSAC</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../345/1208345.xml">
SIFT</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../205/6840205.xml">
Blob detection</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../078/14664078.xml">
Harris affine region detector</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../671/5386671.xml">
Structure from motion</link></entry>
</list>
</p>

</sec>
</bdy>
</article>
