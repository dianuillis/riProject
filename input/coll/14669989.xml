<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:38:27[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Robust real-time object detection</title>
<id>14669989</id>
<revision>
<id>239958808</id>
<timestamp>2008-09-21T08:23:00Z</timestamp>
<contributor>
<username>KYN</username>
<id>505011</id>
</contributor>
</revision>
<categories>
<category>Face recognition</category>
<category>Object recognition and categorization</category>
</categories>
</header>
<bdy>

The Viola and Jones Object Detection Framework is the first object detection framework to provide competitive object detection rates in real-time.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> Although it can be trained to detect a variety of object classes, it was motivated primarily by the problem of face detection. The purpose of this article is to introduce the contributions which made this advancement possible.
<sec>
<st>
 Components of the Framework </st>

<p>

<image location="right" width="150px" src="Prm_VJ_fig1_featureTypesWithAlpha.png" type="thumb">
<caption>

Feature types used by Viola and Jones
</caption>
</image>

</p>
<ss1>
<st>
 Feature Types and Evaluation </st>


<p>

The features employed by the detection framework universally involve the sums of image pixels within rectangular areas. As such, they bear some resemblance to Haar basis functions, which have been used previously in the realm of image-based object detection.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> However, since the features used by Viola and Jones all rely on more than one rectangular area, they are generally more complex. The figure at right illustrates the three different types of features used in the framework. The value of any given feature is always simply the sum of the pixels within clear rectangles subtracted from the sum of the pixels within shaded rectangles. As is to be expected, rectangular features of this sort are rather primitive when compared to alternatives such as steerable filters. Although they are sensitive to vertical and horizontal features, their feedback is considerably coarser. However, with the use of an image representation called the <link xlink:type="simple" xlink:href="../333/18696333.xml">
integral image</link>, rectangular features can be evaluated in <it>constant</it> time, which gives them a considerable speed advantage over their more sophisticated relatives. Because each rectangular area in a feature is always adjacent to at least one other rectangle, it follows that any two-rectangle feature can be computed in six array references, any three-rectangle feature in eight, and any four-rectangle feature in just nine.</p>

</ss1>
<ss1>
<st>
 Learning Algorithm </st>

<p>

The speed with which features may be evaluated does not adequately compensate for their number, however. For example, in a standard 24x24 pixel sub-window, there are a total of 45,396 possible features, and it would be prohibitively expensive to evaluate them all. Thus, the object detection framework employs a variant of the learning algorithm <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 to both select the best features and to train classifiers that use them. </p>
<p>

<image location="right" width="150px" src="Prm_VJ_fig4_cascadeWithAlpha.png" type="thumb">
<caption>

Cascade Architecture
</caption>
</image>

</p>
</ss1>
<ss1>
<st>
 Cascade Architecture </st>

<p>

The evaluation of the strong classifiers generated by the learning process can be done quickly, but it isnâ€™t fast enough to run in real-time. For this reason, the strong classifiers are arranged in a cascade in order of complexity, where each successive classifier is trained only on those examples which pass through the preceding classifiers. If at any point in the cascade a classifier rejects the sub-window under inspection, no further processing is performed and the search moves on to the next sub-window (see figure at right). The cascade therefore has the form of a degenerate <link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link>. In the case of faces, the first classifier in the cascade - called the attentional operator - uses only two features to achieve a false negative rate of approximately 0% and a false positive rate of 40%.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> The effect of this single classifier is to reduce by roughly half the number of times the entire cascade is evaluated.</p>
<p>

The cascade architecture has interesting implications for the performance of the individual classifiers. Because the activation of each classifier depends entirely on the behavior of its predecessor, the false positive rate for an entire cascade is:</p>

<p>

<math>F = \prod_{i=1}^K f_i</math></p>

<p>

Similarly, the detection rate is:</p>

<p>

<math>D = \prod_{i=1}^K d_i</math></p>

<p>

Thus, to match the false positive rates typically achieved by other detectors, each classifier can get away with having surprisingly poor performance. For example, for a 32-stage cascade to achieve a false positive rate of <math>10^{-6}</math>, each classifier need only achieve a false positive rate of about 65%. At the same time, however, each classifier needs to be exceptionally capable if it is to achieve adequate detection rates. For example, to achieve a detection rate of about 90%, each classifier in the aforementioned cascade needs to achieve a detection rate of approximately 99.7%.</p>



</ss1>
</sec>
<sec>
<st>
 References </st>
<p>

<reflist>
<entry id="1">
<weblink xlink:type="simple" xlink:href="http://research.microsoft.com/~viola/Pubs/Detect/violaJones_IJCV.pdf">
Viola, Jones: Robust Real-time Object Detection, IJCV 2004</weblink> See pages 1,3.</entry>
<entry id="2">
C. Papageorgiou, M. Oren and T. Poggio. A General Framework for Object Detection. <it>International Conference on Computer Vision</it>, 1998</entry>
<entry id="3">
<weblink xlink:type="simple" xlink:href="http://research.microsoft.com/~viola/Pubs/Detect/violaJones_IJCV.pdf">
Viola, Jones: Robust Real-time Object Detection, IJCV 2004</weblink> See page 11.</entry>
</reflist>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cmucam.org/wiki/viola-jones">
Demo Implementation</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.slideshare.net/wolf/avihu-efrats-viola-and-jones-face-detection-slides/">
Slides Presenting the Framework</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://mathworld.wolfram.com/HaarFunction.html">
Information Regarding Haar Basis Functions</weblink></entry>
</list>
</p>

</sec>
</bdy>
</article>
