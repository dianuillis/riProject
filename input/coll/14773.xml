<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:27:46[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Information theory</title>
<id>14773</id>
<revision>
<id>244385449</id>
<timestamp>2008-10-10T15:23:27Z</timestamp>
<contributor>
<username>The Epopt</username>
<id>30</id>
</contributor>
</revision>
<categories>
<category>Discrete mathematics</category>
<category>Formal sciences</category>
<category>Cybernetics</category>
<category>Communication</category>
<category>Digital Revolution</category>
<category>Information theory</category>
</categories>
</header>
<bdy>

Not to be confused with <link xlink:type="simple" xlink:href="../340/15340.xml">
information technology</link>, <link xlink:type="simple" xlink:href="../354/149354.xml">
information science</link>, or <link xlink:type="simple" xlink:href="../625/4964625.xml">
informatics</link>.<p>

<b>Information theory</b> is a branch of <link xlink:type="simple" xlink:href="../988/18950988.xml">
applied mathematics</link> and <link xlink:type="simple" xlink:href="../531/9531.xml">
electrical engineering</link> involving the quantification of <link xlink:type="simple" xlink:href="../062/18985062.xml">
information</link>.  Historically, information theory was developed to find fundamental limits on compressing and reliably <link xlink:type="simple" xlink:href="../177/5177.xml">
communicating</link> data.  Since its inception it has broadened to find applications in many other areas, including <link xlink:type="simple" xlink:href="../577/27577.xml">
statistical inference</link>, <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link>, <link xlink:type="simple" xlink:href="../432/18934432.xml">
cryptography</link> generally, <link xlink:type="simple" xlink:href="../934/36934.xml">
networks</link> other than communication networks -- as in <link xlink:type="simple" xlink:href="../867/267867.xml">
neurobiology</link>,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> the evolution<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> and function<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> of molecular codes, model selection<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> in ecology, thermal physics,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> <link xlink:type="simple" xlink:href="../220/25220.xml">
quantum computing</link>, plagiarism detection<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> and other forms of <link xlink:type="simple" xlink:href="../954/2720954.xml">
data analysis</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></p>
<p>

A key measure of information in the theory is known as <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link>, which is usually expressed by the average number of bits needed for storage or communication.  Intuitively, entropy quantifies the uncertainty involved when encountering a <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>. For example, a fair coin flip (2 equally likely outcomes) will have less entropy than a roll of a die (6 equally likely outcomes).</p>
<p>

Applications of fundamental topics of information theory include <link xlink:type="simple" xlink:href="../209/18209.xml">
lossless data compression</link> (e.g. <format wordnetid="106636806" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../488/188488.xml">
ZIP files</link></format>
), <link xlink:type="simple" xlink:href="../208/18208.xml">
lossy data compression</link> (e.g. <format wordnetid="106636806" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../673/19673.xml">
MP3</link></format>
s), and <link xlink:type="simple" xlink:href="../204/231204.xml">
channel coding</link> (e.g. for <link xlink:type="simple" xlink:href="../038/41038.xml">
DSL</link> lines).  The field is at the intersection of <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link>, <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>, <link xlink:type="simple" xlink:href="../939/22939.xml">
physics</link>, <link xlink:type="simple" xlink:href="../867/267867.xml">
neurobiology</link>, and <link xlink:type="simple" xlink:href="../531/9531.xml">
electrical engineering</link>. Its impact has been crucial to the success of the <idea wordnetid="105833840" confidence="0.8">
<plan wordnetid="105898568" confidence="0.8">
<link xlink:type="simple" xlink:href="../795/47795.xml">
Voyager</link></plan>
</idea>
 missions to deep space, the invention of the CD, the feasibility of mobile phones, the development of the <link xlink:type="simple" xlink:href="../539/14539.xml">
Internet</link>, the study of <link xlink:type="simple" xlink:href="../526/17526.xml">
linguistics</link> and of human perception, the understanding of <link xlink:type="simple" xlink:href="../650/4650.xml">
black hole</link>s, and numerous other fields. Important sub-fields of information theory are source coding, channel coding, algorithmic complexity theory, algorithmic information theory, and measures of information.</p>

<sec>
<st>
Overview</st>
<p>

The main concepts of information theory can be grasped by considering the most widespread means of human communication: language.  Two important aspects of a good language are as follows:  First, the most common words (e.g., "a", "the", "I") should be shorter than less common words (e.g., "benefit", "generation", "mediocre"), so that sentences will not be too long.  Such a tradeoff in word length is analogous to <link xlink:type="simple" xlink:href="../013/8013.xml">
data compression</link> and is the essential aspect of <link xlink:type="simple" xlink:href="../013/8013.xml">
source coding</link>.  Second, if part of a sentence is unheard or misheard due to noise — e.g., a passing car — the listener should still be able to glean the meaning of the underlying message.  Such robustness is as essential for an electronic communication system as it is for a language; properly building such robustness into communications is done by <link xlink:type="simple" xlink:href="../204/231204.xml">
channel coding</link>. Source coding and channel coding are the fundamental concerns of information theory.</p>
<p>

Note that these concerns have nothing to do with the <it>importance</it> of messages. For example, a platitude such as "Thank you; come again" takes about as long to say or write as the urgent plea, "Call an ambulance!" while clearly the latter is more important and more meaningful. Information theory, however, does not consider message importance or meaning, as these are matters of the quality of data rather than the quantity and readability of data, the latter of which is determined solely by probabilities.</p>
<p>

Information theory is generally considered to have been founded in 1948 by <link xlink:type="simple" xlink:href="../693/5693.xml">
Claude Shannon</link> in his seminal work, "<work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../061/828061.xml">
A Mathematical Theory of Communication</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
."  The central paradigm of classical information theory is the engineering problem of the transmission of information over a noisy channel. The most fundamental results of this theory are Shannon's <link xlink:type="simple" xlink:href="../872/1208872.xml">
source coding theorem</link>, which establishes that, on average, the number of <it>bits</it> needed to represent the result of an uncertain event is given by its <link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link>; and Shannon's <link xlink:type="simple" xlink:href="../289/3474289.xml">
noisy-channel coding theorem</link>, which states that <it>reliable</it> communication is possible over <it>noisy</it> channels provided that the rate of communication is below a certain threshold called the channel capacity. The channel capacity can be approached in practice by using appropriate encoding and decoding systems.</p>
<p>

Information theory is closely associated with a collection of pure and applied disciplines that have been investigated and reduced to engineering practice under a variety of <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../066/2341066.xml">
rubrics</link></method>
</know-how>
 throughout the world over the past half century or more: <link xlink:type="simple" xlink:href="../588/739588.xml">
adaptive system</link>s, <link xlink:type="simple" xlink:href="../216/1126216.xml">
anticipatory system</link>s, <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link>, <link xlink:type="simple" xlink:href="../438/37438.xml">
complex system</link>s, <link xlink:type="simple" xlink:href="../903/62903.xml">
complexity science</link>, <link xlink:type="simple" xlink:href="../904/5904.xml">
cybernetics</link>, <link xlink:type="simple" xlink:href="../625/4964625.xml">
informatics</link>, <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, along with <link xlink:type="simple" xlink:href="../757/659757.xml">
systems science</link>s of many descriptions. Information theory is a broad and deep mathematical theory, with equally broad and deep applications, amongst which is the vital field of <link xlink:type="simple" xlink:href="../869/321869.xml">
coding theory</link>.</p>
<p>

Coding theory is concerned with finding explicit methods, called <it>codes</it>, of increasing the efficiency and reducing the net error rate of data communication over a noisy channel to near the limit that Shannon proved is the maximum possible for that channel. These codes can be roughly subdivided into <link xlink:type="simple" xlink:href="../013/8013.xml">
data compression</link> (source coding) and <link xlink:type="simple" xlink:href="../375/10375.xml">
error-correction</link> (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible. A third class of information theory codes are cryptographic algorithms (both <link xlink:type="simple" xlink:href="../989/838989.xml">
code</link>s and <link xlink:type="simple" xlink:href="../244/5244.xml">
cipher</link>s). Concepts, methods and results from coding theory and information theory are widely used in <link xlink:type="simple" xlink:href="../432/18934432.xml">
cryptography</link> and <link xlink:type="simple" xlink:href="../715/5715.xml">
cryptanalysis</link>. <it>See the article <link xlink:type="simple" xlink:href="../780/3070780.xml">
ban (information)</link> for a historical application.</it></p>
<p>

Information theory is also used in <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link>, <link xlink:type="simple" xlink:href="../276/519276.xml">
intelligence gathering</link>, <link xlink:type="simple" xlink:href="../921/11921.xml">
gambling</link>, <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, and even in <link xlink:type="simple" xlink:href="../962/47962.xml">
musical composition</link>.</p>

</sec>
<sec>
<st>
Historical background</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../452/5642452.xml">
History of information theory</link></it>
</indent>

The landmark event that established the discipline of information theory, and brought it to immediate worldwide attention, was the publication of <link xlink:type="simple" xlink:href="../693/5693.xml">
Claude E. Shannon</link>'s classic paper "<work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../061/828061.xml">
A Mathematical Theory of Communication</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
" in the <it><link xlink:type="simple" xlink:href="../522/2665522.xml">
Bell System Technical Journal</link></it> in July and October of 1948.</p>
<p>

Prior to this paper, limited information theoretic ideas had been developed at Bell Labs, all implicitly assuming events of equal probability.  <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../571/158571.xml">
Harry Nyquist</link></scientist>
's 1924 paper, <it>Certain Factors Affecting Telegraph Speed,</it> contains a theoretical section quantifying "intelligence" and the "line speed" at which it can be transmitted by a communication system, giving the relation <math>W = K \log m</math>, where <it>W</it> is the speed of transmission of intelligence, <it>m</it> is the number of different voltage levels to choose from at each time step, and <it>K</it> is a constant.  <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../525/184525.xml">
Ralph Hartley</link></scientist>
's 1928 paper, <it>Transmission of Information,</it> uses the word <it>information</it> as a measurable quantity, reflecting the receiver's ability to distinguish that one sequence of symbols from any other, thus quantifying information as <math>H = \log S^n = n \log S</math>, where <it>S</it> was the number of possible symbols, and <it>n</it> the number of symbols in a transmission. The natural unit of information was therefore the decimal digit, much later renamed the <link xlink:type="simple" xlink:href="../780/3070780.xml">
hartley</link> in his honour as a unit or scale or measure of information. <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Alan Turing</link></scientist>
</person>
 in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war <link xlink:type="simple" xlink:href="../175/872175.xml">
Enigma</link> ciphers.</p>
<p>

Much of the mathematics behind information theory with events of different probabilities was developed for the field of <link xlink:type="simple" xlink:href="../952/29952.xml">
thermodynamics</link> by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../255/544255.xml">
Ludwig Boltzmann</link></scientist>
</person>
 and <link xlink:type="simple" xlink:href="../332/37332.xml">
J. Willard Gibbs</link>.  Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../519/1385519.xml">
Rolf Landauer</link></scientist>
</person>
 in the 1960s, are explored in <it><link xlink:type="simple" xlink:href="../140/3325140.xml">
Entropy in thermodynamics and information theory</link></it>.</p>
<p>

In Shannon's revolutionary and groundbreaking paper, the work for which had been substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion that
<indent level="1">

"The fundamental problem of communication is that of reproducing at one point, either exactly or approximately, a message selected at another point."
</indent>

With it came the ideas of
<list>
<entry level="1" type="bullet">

 the <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link> and <link xlink:type="simple" xlink:href="../582/1953582.xml">
redundancy</link> of a source, and its relevance through the <link xlink:type="simple" xlink:href="../872/1208872.xml">
source coding theorem</link>;</entry>
<entry level="1" type="bullet">

 the <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>, and the <link xlink:type="simple" xlink:href="../204/231204.xml">
channel capacity</link> of a noisy channel, including the promise of perfect loss-free communication given by the <link xlink:type="simple" xlink:href="../289/3474289.xml">
noisy-channel coding theorem</link>;</entry>
<entry level="1" type="bullet">

 the practical result of the <link>
Shannon–Hartley law</link> for the channel capacity of a Gaussian channel; and of course</entry>
<entry level="1" type="bullet">

 the <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link>—a new way of seeing the most fundamental unit of information</entry>
</list>
</p>

</sec>
<sec>
<st>
Ways of measuring information</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../309/6101309.xml">
Quantities of information</link></it>
</indent>

Information theory is based on <link xlink:type="simple" xlink:href="../542/23542.xml">
probability theory</link> and <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>.  The most important quantities of information are <link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link>, the information in a <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>, and <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>, the amount of information in common between two random variables.  The former quantity indicates how easily message data can be <link xlink:type="simple" xlink:href="../013/8013.xml">
compressed</link> while the latter can be used to find the communication rate across a <link xlink:type="simple" xlink:href="../700/156700.xml">
channel</link>.</p>
<p>

The choice of logarithmic base in the following formulae determines the <link xlink:type="simple" xlink:href="../364/586364.xml">
unit</link> of <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link> that is used.  The most common unit of information is the <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link>, based on the <link xlink:type="simple" xlink:href="../992/249992.xml">
binary logarithm</link>. Other units include the <link xlink:type="simple" xlink:href="../481/3070481.xml">
nat</link>, which is based on the <link xlink:type="simple" xlink:href="../476/21476.xml">
natural logarithm</link>, and the <link xlink:type="simple" xlink:href="../780/3070780.xml">
hartley</link>, which is based on the <link xlink:type="simple" xlink:href="../482/174482.xml">
common logarithm</link>.</p>
<p>

In what follows, an expression of the form <math>p \log p \,</math> is considered by convention to be equal to zero whenever <math>p=0.</math>  This is justified because <math>\lim_{p \rightarrow 0+} p \log p = 0</math> for any logarithmic base.</p>

<ss1>
<st>
Entropy</st>
<p>

<image location="right" width="200px" src="Binary_entropy_plot.svg" type="thumbnail">
<caption>

Entropy of a <link xlink:type="simple" xlink:href="../653/102653.xml">
Bernoulli trial</link> as a function of success probability, often called the <b><link xlink:type="simple" xlink:href="../277/5275277.xml">
binary entropy function</link></b>, <math>H_\mbox{b}(p)</math>.  The entropy is maximized at 1 bit per trial when the two possible outcomes are equally probable, as in an unbiased coin toss.
</caption>
</image>

The <b><link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link></b>, <math>H</math>, of a discrete random variable <math>X</math> is a measure of the amount of <it>uncertainty</it> associated with the value of <math>X</math>.</p>
<p>

Suppose one transmits 1000 bits (0s and 1s).   If these bits are known ahead of transmission (to be a certain value with absolute probability), logic dictates that no information has been transmitted.  If, however, each is equally and independently likely to be 0 or 1, 1000 bits (in the information theoretic sense) have been transmitted.  Between these two extremes, information can be quantified as follows. If <math>\mathbb{X}\,</math> is the set of all messages <math>x</math> that <math>X</math> could be, and <math>p(x)</math> is the probability of <math>X</math> given <math>x</math>, then the entropy of <math>X</math> is defined:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref></p>
<p>

<indent level="1">

<math> H(X) = \mathbb{E}_{X} [I(x)] = -\sum_{x \in \mathbb{X}} p(x) \log p(x).</math>
</indent>

(Here, <math>I(x)</math> is the <link xlink:type="simple" xlink:href="../447/542447.xml">
self-information</link>, which is the entropy contribution of an individual message, and <math>\mathbb{E}_{X}</math> is the <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link>.) An important property of entropy is that it is maximized when all the messages in the message space are equiprobable—i.e., most unpredictable—in which case <math>H(X) = \log |\mathbb{X}|.</math></p>
<p>

The special case of information entropy for a random variable with two outcomes is the <b><link xlink:type="simple" xlink:href="../277/5275277.xml">
binary entropy function</link></b>:</p>
<p>

<indent level="1">

<math>H_\mbox{b}(p) = - p \log p - (1-p)\log (1-p).\,</math>
</indent>

</p>
</ss1>
<ss1>
<st>
Joint entropy</st>
<p>

The <b><link xlink:type="simple" xlink:href="../967/910967.xml">
joint entropy</link></b> of two discrete random variables <math>X</math> and <math>Y</math> is merely the entropy of their pairing: <math>(X, Y)</math>.  This implies that if <math>X</math> and <math>Y</math> are <link xlink:type="simple" xlink:href="../593/27593.xml">
independent</link>, then their joint entropy is the sum of their individual entropies.</p>
<p>

For example, if <math>(X,Y)</math> represents the position of a <link xlink:type="simple" xlink:href="../134/5134.xml">
chess</link> piece &mdash; <math>X</math> the row and <math>Y</math> the column, then the joint entropy of the row of the piece and the column of the piece will be the entropy of the position of the piece.</p>
<p>

<indent level="1">

<math>H(X, Y) = \mathbb{E}_{X,Y} [-\log p(x,y)] = - \sum_{x, y} p(x, y) \log p(x, y) \,</math>
</indent>

Despite similar notation, joint entropy should not be confused with <b><link xlink:type="simple" xlink:href="../250/1735250.xml">
cross entropy</link></b>.</p>

</ss1>
<ss1>
<st>
Conditional entropy (equivocation)</st>
<p>

The <b><link xlink:type="simple" xlink:href="../548/908548.xml">
conditional entropy</link></b> or <b>conditional uncertainty</b> of <math>X</math> given random variable <math>Y</math> (also called the <b>equivocation</b> of <math>X</math> about <math>Y</math>) is the average conditional entropy over <math>Y</math>:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref></p>
<p>

<indent level="1">

<math> H(X|Y) = \mathbb E_Y [H(X|y)] = -\sum_{y \in Y} p(y) \sum_{x \in X} p(x|y) \log p(x|y) = -\sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(y)}.</math>
</indent>

Because entropy can be conditioned on a random variable or on that random variable being a certain value, care should be taken not to confuse these two definitions of conditional entropy, the former of which is in more common use.  A basic property of this form of conditional entropy is that:</p>
<p>

<indent level="1">

 <math> H(X|Y) = H(X,Y) - H(Y) .\,</math>
</indent>

</p>
</ss1>
<ss1>
<st>
Mutual information (transinformation)</st>
<p>

<b><link xlink:type="simple" xlink:href="../282/427282.xml">
Mutual information</link></b> measures the amount of information that can be obtained about one random variable by observing another.  It is important in communication where it can be used to maximize the amount of information shared between sent and received signals.  The mutual information of <math>X</math> relative to <math>Y</math> is given by:</p>
<p>

<indent level="1">

<math>I(X;Y) = \mathbb{E}_{X,Y} [SI(x,y)] = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)\, p(y)}</math>
</indent>
where <math>SI</math> (<it>S</it>pecific mutual <it>I</it>nformation) is the <link xlink:type="simple" xlink:href="../679/6793679.xml">
pointwise mutual information</link>.</p>
<p>

A basic property of the mutual information is that
<indent level="1">

 <math>I(X;Y) = H(X) - H(X|Y).\,</math>
</indent>
That is, knowing <it>Y</it>, we can save an average of <math>I(X; Y)</math> bits in encoding <it>X</it> compared to not knowing <it>Y</it>.</p>
<p>

Mutual information is <link xlink:type="simple" xlink:href="../460/364460.xml">
symmetric</link>:
<indent level="1">

 <math>I(X;Y) = I(Y;X) = H(X) + H(Y) - H(X,Y).\,</math>
</indent>

Mutual information can be expressed as the average <link>
Kullback–Leibler divergence</link> (information gain) of the <link xlink:type="simple" xlink:href="../672/357672.xml">
posterior probability distribution</link> of <it>X</it> given the value of <it>Y</it> to the <link xlink:type="simple" xlink:href="../877/472877.xml">
prior distribution</link> on <it>X</it>:
<indent level="1">

 <math>I(X;Y) = \mathbb E_{p(y)} [D_{\mathrm{KL}}( p(X|Y=y) \| p(X) )].</math>
</indent>
In other words, this is a measure of how much, on the average, the probability distribution on <it>X</it> will change if we are given the value of <it>Y</it>.  This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:
<indent level="1">

 <math>I(X; Y) = D_{\mathrm{KL}}(p(X,Y) \| p(X)p(Y)).</math>
</indent>

Mutual information is closely related to the <link xlink:type="simple" xlink:href="../035/45035.xml">
log-likelihood ratio test</link> in the context of contingency tables and the <link xlink:type="simple" xlink:href="../553/1045553.xml">
multinomial distribution</link> and to <process wordnetid="105701363" confidence="0.8">
<calculation wordnetid="105802185" confidence="0.8">
<information wordnetid="105816287" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<estimate wordnetid="105803379" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../653/226653.xml">
Pearson's χ2 test</link></higher_cognitive_process>
</estimate>
</trial>
</datum>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</information>
</calculation>
</process>
: mutual information can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution.</p>

</ss1>
<ss1>
<st>
Kullback–Leibler divergence (information gain)</st>
<p>

The <b><link>
Kullback–Leibler divergence</link></b> (or <b>information divergence</b>, <b>information gain</b>, or <b>relative entropy</b>) is a way of comparing two distributions: a "true" <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> <it>p(X)</it>, and an arbitrary probability distribution <it>q(X)</it>. If we compress data in a manner that assumes <it>q(X)</it> is the distribution underlying some data, when, in reality, <it>p(X)</it> is the correct distribution, the Kullback–Leibler divergence is the number of average additional bits per datum necessary for compression.  It is thus defined</p>
<p>

<indent level="1">

<math>D_{\mathrm{KL}}(p(X) \| q(X)) = \sum_{x \in X} -p(x) \log {q(x)} \, - \, \left( -p(x) \log {p(x)}\right) = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}.</math>
</indent>

Although it is sometimes used as a 'distance metric', it is not a true <link xlink:type="simple" xlink:href="../467/1561467.xml">
metric</link> since it is not symmetric and does not satisfy the <link xlink:type="simple" xlink:href="../941/53941.xml">
triangle inequality</link> (making it a semi-quasimetric).</p>

</ss1>
<ss1>
<st>
Other quantities</st>
<p>

Other important information theoretic quantities include <link>
Rényi entropy</link> (a generalization of entropy) and <link xlink:type="simple" xlink:href="../168/3504168.xml">
differential entropy</link> (a generalization of quantities of information to continuous distributions.)</p>

</ss1>
</sec>
<sec>
<st>
Coding theory</st>


<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../869/321869.xml">
Coding theory</link></it>
</indent>

<image location="right" width="150px" src="CDSCRATCHES.jpg" type="thumb">
<caption>

A picture showing scratches on the readable surface of a CD-R.  Music and data CDs are coded using error correcting codes and thus can still be read even if they have minor scratches using <link xlink:type="simple" xlink:href="../375/10375.xml">
error detection and correction</link>.
</caption>
</image>
</p>
<p>

<link xlink:type="simple" xlink:href="../869/321869.xml">
Coding theory</link> is one of the most important and direct applications of information theory. It can be subdivided into <link xlink:type="simple" xlink:href="../013/8013.xml">
source coding</link> theory and <link xlink:type="simple" xlink:href="../375/10375.xml">
channel coding</link> theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.</p>
<p>

<list>
<entry level="1" type="bullet">

 Data compression (source coding): There are two formulations for the compression problem:</entry>
<entry level="1" type="number">

<link xlink:type="simple" xlink:href="../209/18209.xml">
lossless data compression</link>: the data must be reconstructed exactly;</entry>
<entry level="1" type="number">

<link xlink:type="simple" xlink:href="../208/18208.xml">
lossy data compression</link>: allocates bits needed to reconstruct the data, within a specified fidelity level measured by a distortion function. This subset of Information theory is called <link>
rate–distortion theory</link>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Error-correcting codes (channel coding): While data compression removes as much <link xlink:type="simple" xlink:href="../582/1953582.xml">
redundancy</link> as possible, an error correcting code adds just the right kind of redundancy (i.e. <link xlink:type="simple" xlink:href="../375/10375.xml">
error correction</link>) needed to transmit the data efficiently and faithfully across a noisy channel.</entry>
</list>
</p>
<p>

This division of coding theory into compression and transmission is justified by the information transmission theorems, or source–channel separation theorems that justify the use of bits as the universal currency for information in many contexts. However, these theorems only hold in the situation where one transmitting user wishes to communicate to one receiving user. In scenarios with more than one transmitter (the multiple-access channel), more than one receiver (the <link>
broadcast channel</link>) or intermediary "helpers" (the <link xlink:type="simple" xlink:href="../309/3593309.xml">
relay channel</link>), or more general <link xlink:type="simple" xlink:href="../592/4122592.xml">
networks</link>, compression followed by transmission may no longer be optimal. <link>
Network information theory</link> refers to these multi-agent communication models.</p>

<ss1>
<st>
Source theory</st>

<p>

Any process that generates successive messages can be considered a <b><link xlink:type="simple" xlink:href="../748/487748.xml">
source</link></b> of information.  A memoryless source is one in which each message is an <link xlink:type="simple" xlink:href="../067/453067.xml">
independent identically-distributed random variable</link>, whereas the properties of <link xlink:type="simple" xlink:href="../986/258986.xml">
ergodicity</link> and <link xlink:type="simple" xlink:href="../898/329898.xml">
stationarity</link> impose more general constraints.  All such sources are <link xlink:type="simple" xlink:href="../895/47895.xml">
stochastic</link>.  These terms are well studied in their own right outside information theory.</p>

<ss2>
<st>
Rate</st>
<p>

Information <link xlink:type="simple" xlink:href="../463/11071463.xml">
<b>rate</b></link> is the average entropy per symbol.  For memoryless sources, this is merely the entropy of each symbol, while, in the case of a stationary stochastic process, it is</p>
<p>

<indent level="1">

<math>r = \lim_{n \to \infty} H(X_n|X_{n-1},X_{n-2},X_{n-3}, \ldots);</math>
</indent>

that is, the conditional entropy of a symbol given all the previous symbols generated.  For the more general case of a process that is not necessarily stationary, the <it>average rate</it> is</p>
<p>

<indent level="1">

<math>r = \lim_{n \to \infty} \frac{1}{n} H(X_1, X_2, \dots X_n);</math>
</indent>

that is, the limit of the joint entropy per symbol.  For stationary sources, these two expressions give the same result.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></p>
<p>

It is common in information theory to speak of the "rate" or "entropy" of a language.  This is appropriate, for example, when the source of information is English prose.  The rate of a source of information is related to its <link xlink:type="simple" xlink:href="../582/1953582.xml">
redundancy</link> and how well it can be <link xlink:type="simple" xlink:href="../013/8013.xml">
compressed</link>, the subject of <b>source coding</b>.</p>

</ss2>
</ss1>
<ss1>
<st>
Channel capacity</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../289/3474289.xml">
Noisy channel coding theorem</link></it>
</indent>

Communications over a channel—such as an <link xlink:type="simple" xlink:href="../499/9499.xml">
ethernet</link> wire—is the primary motivation of information theory.  As anyone who's ever used a telephone (mobile or landline) knows, however, such channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.  How much information can one hope to communicate over a noisy (or otherwise imperfect) channel?</p>
<p>

Consider the communications process over a discrete channel. A simple model of the process is shown below:</p>
<p>

<image location="center" width="500px" src="Comm_Channel.svg">
</image>
</p>
<p>

Here <it>X</it> represents the space of messages transmitted, and <it>Y</it> the space of messages received during a unit time over our channel. Let <math>p(y|x)</math> be the <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probability</link> distribution function of <it>Y</it> given <it>X</it>. We will consider <math>p(y|x)</math> to be an inherent fixed property of our communications channel (representing the nature of the <b><link xlink:type="simple" xlink:href="../932/236932.xml">
noise</link></b> of our channel). Then the joint distribution of <it>X</it> and <it>Y</it> is completely determined by our channel and by our choice of <math>f(x)</math>, the marginal distribution of messages we choose to send over the channel. Under these constraints, we would like to maximize the rate of information, or the <b><link xlink:type="simple" xlink:href="../871/275871.xml">
signal</link></b>, we can communicate over the channel. The appropriate measure for this is the <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>, and this maximum mutual information is called the <b><link xlink:type="simple" xlink:href="../204/231204.xml">
channel capacity</link></b> and is given by:
<indent level="1">

<math> C = \max_{f} I(X;Y).\! </math>
</indent>
This capacity has the following property related to communicating at information rate <it>R</it> (where <it>R</it> is usually bits per symbol).  For any information rate <it>R  C</it> and coding error ε &amp;gt; 0, for large enough <it>N</it>, there exists a code of length <it>N</it> and rate ≥ R and a decoding algorithm, such that the maximal probability of block error is ≤ ε; that is, it is always possible to transmit with arbitrarily small block error.  In addition, for any rate <it>R &amp;gt; C</it>, it is impossible to transmit with arbitrarily small block error.</p>
<p>

<b><link xlink:type="simple" xlink:href="../548/1891548.xml">
Channel coding</link></b> is concerned with finding such nearly optimal <link xlink:type="simple" xlink:href="../375/10375.xml">
codes</link> that can be used to transmit data over a noisy channel with a small coding error at a rate near the channel capacity.</p>

<ss2>
<st>
Channel capacity of particular model channels</st>
<p>

<list>
<entry level="1" type="bullet">

 A continuous-time analog communications channel subject to Gaussian noise — see <link>
Shannon–Hartley theorem</link>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 A <link xlink:type="simple" xlink:href="../361/74361.xml">
binary symmetric channel</link> (BSC) with crossover probability <it>p</it> is a binary input, binary output channel that flips the input bit with probability <it> p</it>. The BSC has a capacity of <math>1 - H_\mbox{b}(p)</math> bits per channel use, where <math>H_\mbox{b}</math> is the <link xlink:type="simple" xlink:href="../277/5275277.xml">
binary entropy function</link>:</entry>
</list>
</p>
<p>

<indent level="2">

<image width="150px" src="Binary_symmetric_channel.svg">
<caption>

Binary symmetric channel.svg
</caption>
</image>

</indent>

<list>
<entry level="1" type="bullet">

 A binary erasure channel (BEC) with erasure probability <it> p </it> is a binary input, ternary output channel. The possible channel outputs are <it>0</it>, <it>1</it>, and a third symbol 'e' called an erasure. The erasure represents complete loss of information about an input bit. The capacity of the BEC is <it>1 - p</it> bits per channel use.</entry>
</list>
</p>
<p>

<indent level="2">

<image width="150px" src="Binary_erasure_channel.svg">
<caption>

Binary erasure channel.svg
</caption>
</image>

</indent>

</p>
</ss2>
</ss1>
</sec>
<sec>
<st>
Applications to other fields</st>

<ss1>
<st>
Intelligence uses and secrecy applications</st>

<p>

Information theoretic concepts apply to <link xlink:type="simple" xlink:href="../432/18934432.xml">
cryptography</link> and <link xlink:type="simple" xlink:href="../715/5715.xml">
cryptanalysis</link>.  <link xlink:type="simple" xlink:href="../272/175272.xml">
Turing</link>'s information unit, the <unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link xlink:type="simple" xlink:href="../780/3070780.xml">
ban</link></definite_quantity>
</unit_of_measurement>
, was used in the <link xlink:type="simple" xlink:href="../748/31748.xml">
Ultra</link> project, breaking the German <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<link xlink:type="simple" xlink:href="../256/9256.xml">
Enigma machine</link></machine>
</device>
</instrumentality>
</artifact>
 code and hastening the <look wordnetid="100877127" confidence="0.8">
<fundamental_quantity wordnetid="113575869" confidence="0.8">
<time_period wordnetid="115113229" confidence="0.8">
<time wordnetid="115122231" confidence="0.8">
<observation wordnetid="100879759" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<sensory_activity wordnetid="100876737" confidence="0.8">
<day wordnetid="115123115" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<sensing wordnetid="100876874" confidence="0.8">
<link xlink:type="simple" xlink:href="../257/215257.xml">
end of WWII in Europe</link></sensing>
</activity>
</psychological_feature>
</act>
</day>
</sensory_activity>
</event>
</observation>
</time>
</time_period>
</fundamental_quantity>
</look>
.  Shannon himself defined an important concept now called the <link xlink:type="simple" xlink:href="../630/71630.xml">
unicity distance</link>. Based on the <link xlink:type="simple" xlink:href="../582/1953582.xml">
redundancy</link> of the <link xlink:type="simple" xlink:href="../935/157935.xml">
plaintext</link>, it attempts to give a minimum amount of <link xlink:type="simple" xlink:href="../294/10294.xml">
ciphertext</link> necessary to ensure unique decipherability.</p>
<p>

Information theory leads us to believe it is much more difficult to keep secrets than it might first appear.  A <link xlink:type="simple" xlink:href="../784/53784.xml">
brute force attack</link> can break systems based on <link xlink:type="simple" xlink:href="../222/24222.xml">
asymmetric key algorithms</link> or on most commonly used methods of <link xlink:type="simple" xlink:href="../042/53042.xml">
symmetric key algorithms</link> (sometimes called secret key algorithms), such as <link xlink:type="simple" xlink:href="../594/4594.xml">
block cipher</link>s.  The security of all such methods currently comes from the assumption that no known attack can break them in a practical amount of time.</p>
<p>

<link xlink:type="simple" xlink:href="../968/677968.xml">
Information theoretic security</link> refers to methods such as the <link xlink:type="simple" xlink:href="../210/22210.xml">
one-time pad</link> that are not vulnerable to such brute force attacks.  In such cases, the positive conditional <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link> between the <link xlink:type="simple" xlink:href="../935/157935.xml">
plaintext</link> and <link xlink:type="simple" xlink:href="../294/10294.xml">
ciphertext</link> (conditioned on the <link xlink:type="simple" xlink:href="../039/53039.xml">
 key</link>) can ensure proper transmission, while the unconditional mutual information between the plaintext and ciphertext remains zero, resulting in absolutely secure communications.  In other words, an eavesdropper would not be able to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key. However, as in any other cryptographic system, care must be used to correctly apply even information-theoretically secure methods; the <work wordnetid="100575741" confidence="0.8">
<sound_property wordnetid="104983122" confidence="0.8">
<ring wordnetid="104981658" confidence="0.8">
<sound wordnetid="104981139" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<property wordnetid="104916342" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../620/173620.xml">
Venona project</link></activity>
</psychological_feature>
</act>
</property>
</undertaking>
</event>
</sound>
</ring>
</sound_property>
</work>
 was able to crack the one-time pads of the <system wordnetid="108435388" confidence="0.8">
<economy wordnetid="108366753" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../779/26779.xml">
Soviet Union</link></group>
</economy>
</system>
 due to their improper reuse of key material.</p>

</ss1>
<ss1>
<st>
Pseudorandom number generation</st>
<p>

<link xlink:type="simple" xlink:href="../524/45524.xml">
Pseudorandom number generator</link>s are widely available in computer language libraries and application programs. They are, almost universally, unsuited to cryptographic use as they do not evade the deterministic nature of modern computer equipment and software. A class of improved random number generators is termed <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../249/182249.xml">
Cryptographically secure pseudorandom number generator</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
s, but even they require external to the software <link xlink:type="simple" xlink:href="../259/2503259.xml">
random seed</link>s to work as intended. These can be obtained via <link xlink:type="simple" xlink:href="../309/9309.xml">
extractor</link>s, if done carefully. The measure of  sufficient randomness in extractors is <link xlink:type="simple" xlink:href="../053/1052053.xml">
min-entropy</link>, a value related to Shannon entropy through <link>
Rényi entropy</link>; Rényi entropy is also used in evaluating randomness in cryptographic systems.  Although related, the distinctions among these measures mean that a <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> with high Shannon entropy is not necessarily satisfactory for use in an extractor and so for cryptography uses.</p>

</ss1>
<ss1>
<st>
Seismic Exploration</st>
<p>

One early commercial application of information theory was in the field seismic oil exploration. Work in this field made it possible to strip off and separate the unwanted noise from the desired seismic signal. Information theory and <link xlink:type="simple" xlink:href="../TN$$/HT$C$_T$yT$N$.xml">
digital signal processing</link> offer a major improvement of resolution and image clarity over previous analog methods.  <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref></p>

</ss1>
<ss1>
<st>
Miscellaneous applications</st>
<p>

Information theory also has applications in <link xlink:type="simple" xlink:href="../853/5642853.xml">
gambling and investing</link>, <link xlink:type="simple" xlink:href="../008/851008.xml">
black holes</link>, <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>, and <link xlink:type="simple" xlink:href="../839/18839.xml">
music</link>.</p>

</ss1>
</sec>
<sec>
<st>
References</st>

<ss1>
<st>
Footnotes</st>


<p>

<reflist>
<entry id="1">
F. Rieke, D. Warland, R Ruyter van Steveninck, W Bialek,  Spikes: Exploring the Neural Code. The MIT press (1997). </entry>
<entry id="2">
cf. Huelsenbeck, J. P., F. Ronquist, R. Nielsen and J. P. Bollback (2001) Bayesian inference of phylogeny and its impact on evolutionary biology, <it>Science</it> <b>294</b>:2310-2314</entry>
<entry id="3">
Rando Allikmets, Wyeth W. Wasserman, Amy Hutchinson, Philip Smallwood, Jeremy Nathans, Peter K. Rogan, <weblink xlink:type="simple" xlink:href="http://www.lecb.ncifcrf.gov/~toms/">
Thomas D. Schneider</weblink>, Michael Dean (1998) Organization of the ABCR gene: analysis of promoter and splice junction sequences, <it>Gene</it> <b>215</b>:1, 111-122</entry>
<entry id="4">
Burnham, K. P. and Anderson D. R. (2002) <it>Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition</it> (Springer Science, New York) ISBN 978-0-387-95364-9.</entry>
<entry id="5">
Jaynes, E. T. (1957) <weblink xlink:type="simple" xlink:href="http://bayes.wustl.edu/">
Information Theory and Statistical Mechanics</weblink>, <it>Phys. Rev.</it> <b>106</b>:620</entry>
<entry id="6">
Charles H. Bennett, Ming Li, and Bin Ma (2003) <weblink xlink:type="simple" xlink:href="http://sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&amp;ARTICLEID_CHAR=08B64096-0772-4904-9D48227D5C9FAC75">
Chain Letters and Evolutionary Histories</weblink>, <it>Scientific American</it> <b>288</b>:6, 76-81</entry>
<entry id="7">
David R. Anderson&#32;(November 1, 2003).&#32;"<weblink xlink:type="simple" xlink:href="http://www.jyu.fi/science/laitokset/bioenv/en/coeevolution/events/itms/why">
Some background on why people in the empirical sciences may want to better understand the information-theoretic methods</weblink>"&#32;(pdf).&#32;Retrieved on <link>
2007-12-30</link>.
</entry>
<entry id="8">
 <cite style="font-style:normal" class="book">Fazlollah M. Reza&#32;(1961, 1994). <weblink xlink:type="simple" xlink:href="http://books.google.com/books?id=RtzpRAiX6OgC&amp;pg=PA8&amp;dq=intitle:%22An+Introduction+to+Information+Theory%22++%22entropy+of+a+simple+source%22&amp;as_brr=0&amp;ei=zP79Ro7UBovqoQK4g_nCCw&amp;sig=j3lPgyYrC3-bvn1Td42TZgTzj0Q">
An Introduction to Information Theory</weblink>.&#32;Dover Publications, Inc., New York. ISBN 0-486-68210-2.</cite>&nbsp;</entry>
<entry id="9">
 <cite style="font-style:normal" class="book">Robert B. Ash&#32;(1965, 1990). <weblink xlink:type="simple" xlink:href="http://books.google.com/books?id=ngZhvUfF0UIC&amp;pg=PA16&amp;dq=intitle:information+intitle:theory+inauthor:ash+conditional+uncertainty&amp;as_brr=0&amp;ei=kKwNR4rbH5mepgKB4d2zBg&amp;sig=YAsiCEVISjJ484R3uGoXpi-a5rI">
Information Theory</weblink>.&#32;Dover Publications, Inc.. ISBN 0-486-66521-6.</cite>&nbsp;</entry>
<entry id="10">
 <cite style="font-style:normal" class="book">Jerry D. Gibson&#32;(1998). <weblink xlink:type="simple" xlink:href="http://books.google.com/books?id=aqQ2Ry6spu0C&amp;pg=PA56&amp;dq=entropy-rate+conditional&amp;as_brr=3&amp;ei=YGDsRtzGGKjupQKa2L2xDw&amp;sig=o0UCtf0xZOf11lPIexPrjOKPgNc#PPA57,M1">
Digital Compression for Multimedia: Principles and Standards</weblink>.&#32;Morgan Kaufmann. ISBN 1558603697.</cite>&nbsp;</entry>
<entry id="11">
The Corporation and Innovation, Haggerty, Patrick, Strategic Management Journal, Vol. 2, 97-118 (1981)</entry>
</reflist>
</p>

</ss1>
<ss1>
<st>
The classic work</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../693/5693.xml">
Shannon, C.E.</link> (1948), "<work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../061/828061.xml">
A Mathematical Theory of Communication</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
", <it>Bell System Technical Journal</it>, 27, pp. 379–423 &amp; 623–656, July &amp; October, 1948. <weblink xlink:type="simple" xlink:href="http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">
PDF.</weblink> <weblink xlink:type="simple" xlink:href="http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html">
Notes and other formats.</weblink></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../255/544255.xml">
Ludwig Boltzmann</link></scientist>
</person>
 formally defined entropy in 1870. Compare: Boltzmann, Ludwig (1896, 1898). Vorlesungen über Gastheorie : 2 Volumes - Leipzig 1895/98 UB: O 5262-6. English version: Lectures on gas theory.  Translated by Stephen G. Brush (1964) Berkeley: University of California Press; (1995) New York: Dover ISBN 0-486-68455-5</entry>
</list>
</p>

</ss1>
<ss1>
<st>
Other journal articles</st>
<p>

<list>
<entry level="1" type="bullet">

 R.V.L. Hartley, <weblink xlink:type="simple" xlink:href="http://www.dotrose.com/etext/90_Miscellaneous/transmission_of_information_1928b.pdf">
"Transmission of Information"</weblink>, <it>Bell System Technical Journal</it>, July 1928</entry>
<entry level="1" type="bullet">

 J. L. Kelly, Jr., "<weblink xlink:type="simple" xlink:href="http://www.arbtrading.com/reports/kelly.pdf">
A New Interpretation of Information Rate</weblink>," <it>Bell System Technical Journal</it>, Vol. 35, July 1956, pp. 917-26</entry>
<entry level="1" type="bullet">

 R. Landauer, <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=615478">
Information is Physical</weblink> <it>Proc. Workshop on Physics and Computation PhysComp'92</it> (IEEE Comp. Sci.Press, Los Alamitos, 1993) pp. 1-4.</entry>
<entry level="1" type="bullet">

 R. Landauer, "<weblink xlink:type="simple" xlink:href="http://www.research.ibm.com/journal/rd/441/landauerii.pdf">
Irreversibility and Heat Generation in the Computing Process</weblink>" <it>IBM J. Res. Develop.</it> Vol. 5, No. 3, 1961</entry>
</list>
</p>

</ss1>
<ss1>
<st>
Textbooks on information theory</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../693/5693.xml">
Claude E. Shannon</link>, Warren Weaver. <it>The Mathematical Theory of Communication.</it> Univ of Illinois Press, 1949. ISBN 0-252-72548-4</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../877/553877.xml">
Robert Gallager</link>. <it>Information Theory and Reliable Communication.</it> New York: John Wiley and Sons, 1968. ISBN 0-471-29048-3</entry>
<entry level="1" type="bullet">

 Robert B. Ash. <it>Information Theory</it>. New York: Interscience, 1965. ISBN 0-470-03445-9. New York: Dover 1990. ISBN 0-486-66521-6</entry>
<entry level="1" type="bullet">

 <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../278/4740278.xml">
Thomas M. Cover</link></scientist>
, Joy A. Thomas. <it>Elements of information theory</it>, 1st Edition.  New York: Wiley-Interscience, 1991. ISBN 0-471-06259-6.</entry>
<entry level="1" type="indent">

2nd Edition. New York: Wiley-Interscience, 2006. ISBN 0-471-24195-4.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../077/13008077.xml">
Imre Csiszar</link>, Janos Korner. <it>Information Theory: Coding Theorems for Discrete Memoryless Systems</it>  Akademiai Kiado: 2nd edition, 1997. ISBN 9630574403</entry>
<entry level="1" type="bullet">

 Raymond W. Yeung.  <it><weblink xlink:type="simple" xlink:href="http://iest2.ie.cuhk.edu.hk/~whyeung/book/">
A First Course in Information Theory</weblink></it> Kluwer Academic/Plenum Publishers, 2002.  ISBN 0-306-46791-7</entry>
<entry level="1" type="bullet">

 David J. C. MacKay. <it><weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">
Information Theory, Inference, and Learning Algorithms</weblink></it> Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1</entry>
<entry level="1" type="bullet">

 Stanford Goldman. <it>Information Theory</it>. New York: Prentice Hall, 1953. New York: Dover 1968 ISBN 0-486-62209-6, 2005 ISBN 0-486-44271-3</entry>
<entry level="1" type="bullet">

 <skilled_worker wordnetid="110605985" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<ambassador wordnetid="109787534" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<official wordnetid="110372373" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<diplomat wordnetid="110013927" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../765/6065765.xml">
Fazlollah Reza</link></scholar>
</diplomat>
</causal_agent>
</academician>
</official>
</alumnus>
</worker>
</ambassador>
</educator>
</professional>
</adult>
</engineer>
</intellectual>
</person>
</physical_entity>
</skilled_worker>
. <it>An Introduction to Information Theory</it>. New York: McGraw-Hill 1961. New York: Dover 1994. ISBN 0-486-68210-2</entry>
<entry level="1" type="bullet">

 Masud Mansuripur. <it>Introduction to Information Theory</it>. New York: Prentice Hall, 1987. ISBN 0-13-484668-0</entry>
<entry level="1" type="bullet">

 Christoph Arndt: <it>Information Measures, Information and its Description in Science and Engineering</it> (Springer Series: Signals and Communication Technology), 2004, ISBN 978-3-540-40855-0, <weblink xlink:type="simple" xlink:href="http://www.springer.com/east/home?SGWID=5-102-22-17328941-0&amp;changeHeader=true&amp;referer=www.springeronline.com&amp;SHORTCUT=www.springer.com/east/3-540-40855-X">
http://www.springer.com/east/home?SGWID=5-102-22-17328941-0&amp;changeHeader=true&amp;referer=www.springeronline.com&amp;SHORTCUT=www.springer.com/east/3-540-40855-X</weblink>;</entry>
</list>
</p>

</ss1>
<ss1>
<st>
Other books</st>
<p>

<list>
<entry level="1" type="bullet">

 Leon Brillouin, <it>Science and Information Theory</it>, Mineola, N.Y.: Dover, [1956, 1962] 2004. ISBN 0-486-43918-6</entry>
<entry level="1" type="bullet">

 A. I. Khinchin, <it>Mathematical Foundations of Information Theory</it>, New York: Dover, 1957. ISBN 0-486-60434-9</entry>
<entry level="1" type="bullet">

 H. S. Leff and A. F. Rex, Editors, <it>Maxwell's Demon: Entropy, Information, Computing</it>, Princeton University Press, Princeton, NJ (1990). ISBN 0-691-08727-X</entry>
<entry level="1" type="bullet">

 Tom Siegfried, <it>The Bit and the Pendulum</it>, Wiley, 2000. ISBN 0-471-32174-5</entry>
<entry level="1" type="bullet">

 Charles Seife, <it>Decoding The Universe</it>, Viking, 2006. ISBN 0-670-03441-X</entry>
<entry level="1" type="bullet">

 Jeremy Campbell, <it>Grammatical Man</it>, Touchstone/Simon &amp; Schuster, 1982, ISBN 0-671-44062-4</entry>
<entry level="1" type="bullet">

 Henri Theil, <it>Economics and Information Theory</it>, Rand McNally &amp; Company - Chicago, 1967.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../810/248810.xml">
Communication theory</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../351/454351.xml#xpointer(//*[./st=%22Information+theory%22])">
List of important publications</link></entry>
<entry level="1" type="bullet">

<division wordnetid="108220714" confidence="0.8">
<administrative_unit wordnetid="108077292" confidence="0.8">
<branch wordnetid="108401248" confidence="0.8">
<link xlink:type="simple" xlink:href="../868/4522868.xml">
Philosophy of information</link></branch>
</administrative_unit>
</division>
</entry>
</list>
</p>

<ss1>
<st>
Applications</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../432/18934432.xml">
Cryptography</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../715/5715.xml">
Cryptanalysis</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../140/3325140.xml">
Entropy in thermodynamics and information theory</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../418/676418.xml">
seismic exploration</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../276/519276.xml">
Intelligence (information gathering)</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../921/11921.xml">
Gambling</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../904/5904.xml">
Cybernetics</link></entry>
</list>
</p>

</ss1>
<ss1>
<st>
History</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../452/5642452.xml">
History of information theory</link></entry>
<entry level="1" type="bullet">

 <record wordnetid="106647206" confidence="0.8">
<chronology wordnetid="106503224" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<timeline wordnetid="106504965" confidence="0.8">
<written_record wordnetid="106502378" confidence="0.8">
<link xlink:type="simple" xlink:href="../938/3475938.xml">
Timeline of information theory</link></written_record>
</timeline>
</evidence>
</indication>
</chronology>
</record>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../693/5693.xml">
Shannon, C.E.</link></entry>
<entry level="1" type="bullet">

 <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../525/184525.xml">
Hartley, R.V.L.</link></scientist>
</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../816/2373816.xml">
Yockey, H.P.</link></scientist>
</causal_agent>
</person>
</physical_entity>
</entry>
</list>
</p>

</ss1>
<ss1>
<st>
Theory</st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../869/321869.xml">
Coding theory</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../013/8013.xml">
Source coding</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../527/1156527.xml">
Detection theory</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../926/1565926.xml">
Estimation theory</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../971/598971.xml">
Fisher information</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov complexity</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../526/5259526.xml">
Information Algebra</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../312/487312.xml">
Information geometry</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../731/5642731.xml">
Information theory and measure theory</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../359/4095359.xml">
Logic of information</link></entry>
<entry level="1" type="bullet">

 <region wordnetid="108630985" confidence="0.8">
<field wordnetid="108569998" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<link xlink:type="simple" xlink:href="../295/3084295.xml">
Network coding</link></geographical_area>
</tract>
</location>
</field>
</region>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../094/659094.xml">
Quantum information science</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../139/4097139.xml">
Semiotic information theory</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../868/4522868.xml">
Philosophy of Information</link></entry>
</list>
</p>


</ss1>
<ss1>
<st>
Concepts</st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../447/542447.xml">
Self-information</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../445/15445.xml">
Information entropy</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../967/910967.xml">
Joint entropy</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../548/908548.xml">
Conditional entropy</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../582/1953582.xml">
Redundancy</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../700/156700.xml">
Channel (communications)</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../748/487748.xml">
Communication source</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../890/4412890.xml">
Receiver (information theory)</link></entry>
<entry level="1" type="bullet">

 <link>
Rényi entropy</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../835/12261835.xml">
Variety</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../282/427282.xml">
Mutual information</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../679/6793679.xml">
Pointwise Mutual Information</link> (PMI)</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../168/3504168.xml">
Differential entropy</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler divergence</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../204/231204.xml">
Channel capacity</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../630/71630.xml">
Unicity distance</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../780/3070780.xml">
ban (information)</link></entry>
<entry level="1" type="bullet">

 <accomplishment wordnetid="100035189" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<action wordnetid="100037396" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<feat wordnetid="100036762" confidence="0.8">
<link xlink:type="simple" xlink:href="../907/174907.xml">
Covert channel</link></feat>
</psychological_feature>
</act>
</action>
</event>
</accomplishment>
</entry>
<entry level="1" type="bullet">

 <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<electrical_device wordnetid="103269401" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<circuit wordnetid="103033362" confidence="0.8">
<link xlink:type="simple" xlink:href="../461/313461.xml">
Encoder</link></circuit>
</device>
</electrical_device>
</instrumentality>
</artifact>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../462/313462.xml">
Decoder</link></entry>
</list>
</p>


</ss1>
</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 Gibbs, M., "Quantum Information Theory", <weblink xlink:type="simple" xlink:href="http://members.aol.com/jmtsgibbs/infothry.htm">
Eprint</weblink></entry>
<entry level="1" type="bullet">

 Schneider, T., "Information Theory Primer", <weblink xlink:type="simple" xlink:href="http://www.lecb.ncifcrf.gov/~toms/paper/primer">
Eprint</weblink></entry>
<entry level="1" type="bullet">

 Srinivasa, S. "A Review on Multivariate Mutual Information" <weblink xlink:type="simple" xlink:href="http://www.nd.edu/~jnl/ee80653/tutorials/sunil.pdf">
PDF</weblink>.</entry>
<entry level="1" type="bullet">

 Challis, J. <weblink xlink:type="simple" xlink:href="http://www.conceptsearching.com/Web/home/technology/lateral-thinking.aspx">
Lateral Thinking in Information Retrieval</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://jchemed.chem.wisc.edu/Journal/Issues/1999/Oct/abs1385.html">
Journal of Chemical Education, <it>Shuffled Cards, Messy Desks, and Disorderly Dorm Rooms - Examples of Entropy Increase? Nonsense!</it> </weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.itsoc.org/index.html">
IEEE Information Theory Society</weblink> and <weblink xlink:type="simple" xlink:href="http://www.itsoc.org/review.html">
the review articles</weblink>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/">
On-line textbook: Information Theory, Inference, and Learning Algorithms</weblink>, by <peer wordnetid="109626238" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<physicist wordnetid="110428004" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<theorist wordnetid="110706812" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../315/2679315.xml">
David MacKay</link></causal_agent>
</academician>
</theorist>
</associate>
</educator>
</professional>
</scientist>
</adult>
</colleague>
</intellectual>
</physicist>
</person>
</physical_entity>
</peer>
 - gives an entertaining and thorough introduction to Shannon theory, including state-of-the-art methods from coding theory, such as <link xlink:type="simple" xlink:href="../545/62545.xml">
arithmetic coding</link>, <link xlink:type="simple" xlink:href="../393/516393.xml">
low-density parity-check code</link>s, and <link xlink:type="simple" xlink:href="../535/497535.xml">
Turbo code</link>s.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.physiol.ox.ac.uk/~jan/infoTheory/MutInfo.htm">
A good tutorial of Information Theory</weblink>.</entry>
</list>
</p>

<p>

<table style=";" class="navbox" cellspacing="0">
<row>
<col style="padding:2px;">
<table style="width:100%;background:transparent;color:inherit;;" class="nowraplinks collapsible autocollapse " cellspacing="0">
<row>
<header colspan="2" style=";" class="navbox-title">
Subfields of and scientists involved in <link xlink:type="simple" xlink:href="../904/5904.xml">
cybernetics</link></header>
</row>
<row style="height:2px;">

</row>
<row>
<col style=";;" class="navbox-group">
Subfields</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;line-height:1.4em;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../724/615724.xml">
Polycontexturality</link>&nbsp;·  <link xlink:type="simple" xlink:href="../462/2073462.xml">
Second-order cybernetics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../288/434288.xml">
Catastrophe theory</link>&nbsp;·  <link xlink:type="simple" xlink:href="../636/263636.xml">
Connectionism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../039/7039.xml">
Control theory</link>&nbsp;·  <social_science wordnetid="106143154" confidence="0.8">
<knowledge_domain wordnetid="105999266" confidence="0.8">
<economics wordnetid="106149484" confidence="0.8">
<discipline wordnetid="105996646" confidence="0.8">
<science wordnetid="105999797" confidence="0.8">
<link xlink:type="simple" xlink:href="../216/446216.xml">
Decision theory</link></science>
</discipline>
</economics>
</knowledge_domain>
</social_science>
&nbsp;·  <link xlink:type="simple" xlink:href="../773/14773.xml">
Information theory</link>&nbsp;·  <link xlink:type="simple" xlink:href="../301/29301.xml">
Semiotics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../227/2748227.xml">
Synergetics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../318/3929318.xml">
Biological cybernetics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../722/551722.xml">
Biosemiotics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../963/1651963.xml">
Biomedical cybernetics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../293/1006293.xml">
Biorobotics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../430/271430.xml">
Computational neuroscience</link>&nbsp;·  <link xlink:type="simple" xlink:href="../980/13980.xml">
Homeostasis</link>&nbsp;·  <link xlink:type="simple" xlink:href="../518/12457518.xml">
Management cybernetics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../506/723506.xml">
Medical cybernetics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../541/13629541.xml">
New Cybernetics</link>&nbsp;·  <region wordnetid="108630985" confidence="0.8">
<field wordnetid="108569998" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<link xlink:type="simple" xlink:href="../924/990924.xml">
Neuro cybernetics</link></geographical_area>
</tract>
</location>
</field>
</region>
&nbsp;·  <link xlink:type="simple" xlink:href="../821/936821.xml">
Sociocybernetics</link>&nbsp;·  <link xlink:type="simple" xlink:href="../436/37436.xml">
Emergence</link>&nbsp;·  <link xlink:type="simple" xlink:href="../164/1164.xml">
Artificial intelligence</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../970/12809970.xml">
Cyberneticist</link>s</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;line-height:1.4em;;;" class="navbox-list navbox-even">
<physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../583/632583.xml">
Igor Aleksander</link></associate>
</educator>
</research_worker>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
&nbsp;· <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<specialist wordnetid="110632576" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<health_professional wordnetid="110165109" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<doctor wordnetid="110020890" confidence="0.8">
<medical_practitioner wordnetid="110305802" confidence="0.8">
<theorist wordnetid="110706812" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<psychiatrist wordnetid="110488016" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../926/298926.xml">
William Ross Ashby</link></research_worker>
</psychiatrist>
</causal_agent>
</theorist>
</medical_practitioner>
</doctor>
</psychologist>
</professional>
</scientist>
</health_professional>
</adult>
</intellectual>
</specialist>
</person>
</physical_entity>
&nbsp;· <link xlink:type="simple" xlink:href="../597/142597.xml">
Anthony Stafford Beer</link>&nbsp;· <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../949/376949.xml">
Claude Bernard</link></scientist>
&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../597/64597.xml">
Ludwig von Bertalanffy</link></scientist>
</causal_agent>
</biologist>
</person>
</physical_entity>
&nbsp;·  <link xlink:type="simple" xlink:href="../931/2360931.xml">
Valentin Braitenberg</link>&nbsp;·  <person wordnetid="100007846" confidence="0.9638700866880419">
<link xlink:type="simple" xlink:href="../525/7254525.xml">
Gordon S. Brown</link></person>
&nbsp;·  <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../411/606411.xml">
Walter Bradford Cannon</link></scientist>
&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../035/105035.xml">
Heinz von Foerster</link></scientist>
</causal_agent>
</person>
</physical_entity>
&nbsp;·  <link>
Charles François</link>&nbsp;·  <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../393/290393.xml">
Jay Wright Forrester</link></scientist>
&nbsp;·  <architect wordnetid="109805475" confidence="0.9173553029164789">
<visionary wordnetid="110756641" confidence="0.9173553029164789">
<person wordnetid="100007846" confidence="0.9508927676800064">
<poet wordnetid="110444194" confidence="0.9173553029164789">
<interior_designer wordnetid="110210648" confidence="0.9173553029164789">
<inventor wordnetid="110214637" confidence="0.9173553029164789">
<writer wordnetid="110794014" confidence="0.9173553029164789">
<link xlink:type="simple" xlink:href="../031/4031.xml">
Buckminster Fuller</link></writer>
</inventor>
</interior_designer>
</poet>
</person>
</visionary>
</architect>
&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../267/1404267.xml">
Ernst von Glasersfeld</link></psychologist>
</scientist>
</causal_agent>
</person>
</physical_entity>
&nbsp;·  <link xlink:type="simple" xlink:href="../054/2291054.xml">
Francis Heylighen</link>&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<physiologist wordnetid="110429965" confidence="0.8">
<link xlink:type="simple" xlink:href="../473/8902473.xml">
Erich von Holst</link></physiologist>
</scientist>
</causal_agent>
</biologist>
</person>
</physical_entity>
&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../902/62902.xml">
Stuart Kauffman</link></associate>
</scholar>
</scientist>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</biologist>
</person>
</peer>
</physical_entity>
&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<physicist wordnetid="110428004" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../424/7304424.xml">
Sergei P. Kurdyumov</link></scientist>
</causal_agent>
</person>
</physicist>
</physical_entity>
&nbsp;·  <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../465/21465.xml">
Niklas Luhmann</link></scientist>
</person>
&nbsp;·  <link xlink:type="simple" xlink:href="../508/44508.xml">
Warren McCulloch</link>&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../061/9638061.xml">
Humberto Maturana</link></psychologist>
</scholar>
</scientist>
</causal_agent>
</intellectual>
</biologist>
</person>
</philosopher>
</physical_entity>
&nbsp;·  <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../041/54041.xml">
Talcott Parsons</link></scientist>
&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../621/4848621.xml">
Gordon Pask</link></educator>
</psychologist>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
&nbsp;·  <link xlink:type="simple" xlink:href="../949/302949.xml">
Walter Pitts</link>&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<anthropologist wordnetid="109796323" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<social_scientist wordnetid="110619642" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../030/813030.xml">
Alfred Radcliffe-Brown</link></scientist>
</causal_agent>
</social_scientist>
</person>
</anthropologist>
</physical_entity>
&nbsp;·  <person wordnetid="100007846" confidence="0.9638700866880419">
<link xlink:type="simple" xlink:href="../924/8532924.xml">
Robert Trappl</link></person>
&nbsp;·  <link xlink:type="simple" xlink:href="../885/2360885.xml">
Valentin Turchin</link>&nbsp;·  <link>
Jakob von Uexküll</link> &nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<convert wordnetid="109962414" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../989/76989.xml">
Francisco Varela</link></research_worker>
</scholar>
</scientist>
</causal_agent>
</convert>
</intellectual>
</biologist>
</person>
</philosopher>
</physical_entity>
&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<biologist wordnetid="109855630" confidence="0.8">
<biochemist wordnetid="109854915" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<chemist wordnetid="109913824" confidence="0.8">
<link xlink:type="simple" xlink:href="../956/355956.xml">
Frederic Vester</link></chemist>
</scientist>
</causal_agent>
</biochemist>
</biologist>
</person>
</physical_entity>
&nbsp;·  <link xlink:type="simple" xlink:href="../147/1191147.xml">
Charles Geoffrey Vickers</link> &nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../528/13588528.xml">
Stuart Umpleby</link></scientist>
</causal_agent>
</person>
</physical_entity>
&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../783/2935783.xml">
John N. Warfield</link></scientist>
</causal_agent>
</engineer>
</person>
</physical_entity>
&nbsp;·  <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9173553029164789">
<link xlink:type="simple" xlink:href="../453/17453.xml">
Kevin Warwick</link></scientist>
</person>
&nbsp;·  <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../185/63185.xml">
Norbert Wiener</link></scientist>
</person>
</col>
</row>
</table>
</col>
</row>
</table>
</p>

<p>

<table style=";" class="navbox" cellspacing="0">
<row>
<col style="padding:2px;">
<table style="width:100%;background:transparent;color:inherit;;" class="nowraplinks collapsible autocollapse " cellspacing="0">
<row>
<header colspan="2" style=";" class="navbox-title">
<link xlink:type="simple" xlink:href="../013/8013.xml">
Data compression</link>methods</header>
</row>
<row style="height:2px;">

</row>
<row>
<col style=";;" class="navbox-group">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../209/18209.xml">
Lossless</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<table style="width:100%;;;;" class="nowraplinks  navbox-subgroup" cellspacing="0">
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../773/14773.xml">
Theory</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../445/15445.xml">
Entropy</link>&nbsp;·  <link xlink:type="simple" xlink:href="../635/1635.xml">
Complexity</link>&nbsp;·  <link xlink:type="simple" xlink:href="../582/1953582.xml">
Redundancy</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../680/46680.xml">
Entropy encoding</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../883/13883.xml">
Huffman</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../824/1053824.xml">
Adaptive Huffman</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <link xlink:type="simple" xlink:href="../545/62545.xml">
Arithmetic</link> (<link>
Shannon-Fano</link>&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../120/46120.xml">
Range</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
)&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../230/147230.xml">
Golomb</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../202/7167202.xml">
Exp-Golomb</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../009/2522009.xml">
Universal</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../761/51761.xml">
Elias</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <amount wordnetid="105107765" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<magnitude wordnetid="105090441" confidence="0.8">
<property wordnetid="104916342" confidence="0.8">
<number wordnetid="105121418" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../063/48063.xml">
Fibonacci</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</number>
</property>
</magnitude>
</rule>
</event>
</amount>
)</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../812/894812.xml">
Dictionary</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../392/26392.xml">
RLE</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;· <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../859/75859.xml">
DEFLATE</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;· <link>
LZ Family</link> (<link xlink:type="simple" xlink:href="../855/75855.xml">
LZ77/78</link>&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../441/2211441.xml">
LZSS</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../854/75854.xml">
LZW</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <link xlink:type="simple" xlink:href="../578/3980578.xml">
LZWL</link>&nbsp;·  <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../891/1108891.xml">
LZO</link></algorithm>
</room>
</activity>
</procedure>
</psychological_feature>
</act>
</library>
</rule>
</event>
</area>
</artifact>
</structure>
&nbsp;·   <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../380/359380.xml">
LZMA</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../430/739430.xml">
LZX</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../443/7783443.xml">
LZJB</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;· <link>
LZT</link>)</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
Others</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../430/14313430.xml">
CTW</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../777/36777.xml">
BWT</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../918/527918.xml">
PPM</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <link xlink:type="simple" xlink:href="../633/11014633.xml">
DMC</link></col>
</row>
</table>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../025/18950025.xml">
Audio</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<table style="width:100%;;;;" class="nowraplinks  navbox-subgroup" cellspacing="0">
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../198/1198.xml">
Theory</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../519/7519.xml">
Convolution</link>&nbsp;·  <link xlink:type="simple" xlink:href="../605/201605.xml">
Sampling</link>&nbsp;·  <link>
 Nyquist–Shannon theorem</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../474/54474.xml">
Audio codec</link>parts</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../682/36682.xml">
LPC</link> (<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../939/6284939.xml">
LAR</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../311/3069311.xml">
LSP</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
)&nbsp;·  <link xlink:type="simple" xlink:href="../096/7191096.xml">
WLPC</link>&nbsp;·  <link xlink:type="simple" xlink:href="../993/1407993.xml">
CELP</link>&nbsp;·  <link xlink:type="simple" xlink:href="../517/1079517.xml">
ACELP</link>&nbsp;·  <link xlink:type="simple" xlink:href="../707/40707.xml">
A-law</link>&nbsp;·  <link>
μ-law</link>&nbsp;·   <link xlink:type="simple" xlink:href="../387/231387.xml">
MDCT</link>&nbsp;·  <mathematical_relation wordnetid="113783581" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<operator wordnetid="113786413" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../247/52247.xml">
Fourier transform</link></function>
</operator>
</concept>
</idea>
</mathematical_relation>
&nbsp;·   <link xlink:type="simple" xlink:href="../448/24448.xml">
Psychoacoustic model</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
Others</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../733/262733.xml">
Dynamic range compression</link>&nbsp;·  <link xlink:type="simple" xlink:href="../618/28618.xml">
Speech compression</link>&nbsp;·  <link xlink:type="simple" xlink:href="../959/4748959.xml">
Sub-band coding</link></col>
</row>
</table>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../469/46469.xml">
Image</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<table style="width:100%;;;;" class="nowraplinks  navbox-subgroup" cellspacing="0">
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
Terms</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../081/6081.xml">
Color space</link>&nbsp;·  <link xlink:type="simple" xlink:href="../665/23665.xml">
Pixel</link>&nbsp;·  <link xlink:type="simple" xlink:href="../944/172944.xml">
Chroma subsampling</link>&nbsp;·  <artifact wordnetid="100021939" confidence="0.8">
<link xlink:type="simple" xlink:href="../669/185669.xml">
Compression artifact</link></artifact>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
Methods</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../392/26392.xml">
RLE</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../022/60022.xml">
Fractal</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../911/50911.xml">
Wavelet</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
&nbsp;·  <link xlink:type="simple" xlink:href="../175/1797175.xml">
EZW</link>&nbsp;·  <link xlink:type="simple" xlink:href="../514/1131514.xml">
SPIHT</link>&nbsp;·  <link xlink:type="simple" xlink:href="../962/59962.xml">
DCT</link>&nbsp;·  <link>
KLT</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
Others</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link xlink:type="simple" xlink:href="../290/272290.xml">
Bit rate</link></definite_quantity>
</unit_of_measurement>
&nbsp;·  <link xlink:type="simple" xlink:href="../011/2932011.xml">
Test images</link>&nbsp;·  <magnitude_relation wordnetid="113815152" confidence="0.8">
<ratio wordnetid="113819207" confidence="0.8">
<link xlink:type="simple" xlink:href="../455/1281455.xml">
PSNR quality measure</link></ratio>
</magnitude_relation>
&nbsp;·  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../012/1092012.xml">
Quantization</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</col>
</row>
</table>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../480/54480.xml">
Video</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<table style="width:100%;;;;" class="nowraplinks  navbox-subgroup" cellspacing="0">
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
Terms</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../441/32441.xml#xpointer(//*[./st=%22Characteristics+of+video+streams%22])">
Video Characteristics</link>&nbsp;·  <message wordnetid="106598915" confidence="0.8">
<information wordnetid="106634376" confidence="0.8">
<format wordnetid="106636806" confidence="0.8">
<link xlink:type="simple" xlink:href="../697/2754697.xml">
Frame</link></format>
</information>
</message>
&nbsp;·  <link xlink:type="simple" xlink:href="../805/736805.xml">
Frame types</link>&nbsp;·  <link xlink:type="simple" xlink:href="../867/2534867.xml">
Video quality</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../475/54475.xml">
Video codec parts</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../035/52035.xml">
Motion compensation</link>&nbsp;·  <link xlink:type="simple" xlink:href="../962/59962.xml">
DCT</link>&nbsp;·  <link xlink:type="simple" xlink:href="../018/317018.xml">
Quantization</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;width:11em;;" class="navbox-group">
Others</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;width:auto;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../475/54475.xml">
Video codecs</link>&nbsp;·  <link xlink:type="simple" xlink:href="../842/192842.xml">
Rate distortion theory</link> (<link xlink:type="simple" xlink:href="../101/297101.xml">
CBR</link>&nbsp;·  <link xlink:type="simple" xlink:href="../011/1934011.xml">
ABR</link>&nbsp;·  <link xlink:type="simple" xlink:href="../826/100826.xml">
VBR</link>)</col>
</row>
</table>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col colspan="2" style="width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<record wordnetid="106647206" confidence="0.8">
<chronology wordnetid="106503224" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<timeline wordnetid="106504965" confidence="0.8">
<written_record wordnetid="106502378" confidence="0.8">
<link xlink:type="simple" xlink:href="../938/3475938.xml">
Timeline of information theory, data compression, and error-correcting codes</link></written_record>
</timeline>
</evidence>
</indication>
</chronology>
</record>
</col>
</row>
<row style="height:2px;">

</row>
<row>
<col colspan="2" style=";" class="navbox-abovebelow">
See  for formats and for codecs</col>
</row>
</table>
</col>
</row>
</table>
</p>



</sec>
</bdy>
</article>
