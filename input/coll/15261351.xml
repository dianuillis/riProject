<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:55:02[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<entity  confidence="0.9511911446218017" wordnetid="100001740">
<header>
<title>Caltech 101</title>
<id>15261351</id>
<revision>
<id>218726562</id>
<timestamp>2008-06-11T22:07:04Z</timestamp>
<contributor>
<username>Lightbot</username>
<id>7178666</id>
</contributor>
</revision>
<categories>
<category>Datasets in computer vision</category>
</categories>
</header>
<bdy>

<b>Caltech 101</b> is a <link xlink:type="simple" xlink:href="../495/8495.xml">
dataset</link> of <link xlink:type="simple" xlink:href="../726/503726.xml">
digital images</link> created in September, 2003, compiled by <link>
Fei-Fei Li</link>, <link>
Marco Andreetto</link>, and <link>
Marc 'Aurelio Ranzato</link> at the <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../786/5786.xml">
California Institute of Technology</link></university>
.  It is intended to facilitate <link xlink:type="simple" xlink:href="../596/6596.xml">
Computer Vision</link> <link xlink:type="simple" xlink:href="../524/25524.xml">
research</link> and techniques.  It is most applicable to techniques interested in <link xlink:type="simple" xlink:href="../543/1208543.xml">
recognition</link>, <link xlink:type="simple" xlink:href="../426/232426.xml">
classification</link>, and <link xlink:type="simple" xlink:href="../717/72717.xml">
categorization</link>.  Caltech 101 contains a total of 9146 images, split between 101 distinct object (including <link xlink:type="simple" xlink:href="../234/53234.xml">
face</link>s, <link xlink:type="simple" xlink:href="../883/60883.xml">
watches</link>, <link xlink:type="simple" xlink:href="../594/2594.xml">
ants</link>, <link xlink:type="simple" xlink:href="../034/23034.xml">
pianos</link>, etc.) and a background category (for a total of 102 <link xlink:type="simple" xlink:href="../425/144425.xml">
categories</link>). Provided with the images are a set of <link xlink:type="simple" xlink:href="../684/624684.xml">
annotations</link> describing the outlines of each image, along with a <link xlink:type="simple" xlink:href="../412/20412.xml">
Matlab</link> <link xlink:type="simple" xlink:href="../524/29524.xml">
script</link> for viewing.
<sec>
<st>
Purpose</st>
<p>

Most Computer Vision and <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine Learning</link> algorithms function by training on a large set of example inputs.
To work effectively, most of these techniques require a large and varied set of training data.  For example, the relatively well known real time face detection method used by <link>
Paul Viola</link> and <link>
Micheal J. Jones</link> was trained on 4916 hand labeled faces <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.
However, acquiring a large volume of appropriate and usable images is often difficult.  Furthermore, cropping and resizing many images, as well as marking point of interest by hand, is a tedious and time intensive task.</p>
<p>

Historically, most datasets used in computer vision research have been tailored to the specific needs of the project being worked on.  
<image width="150px" src="Caltech101vs256.gif">
<caption>

 Caltech 101 vs Caltech 256 on same algorithms
</caption>
</image>

A large problem in comparing different computer vision techniques is the fact that most groups are using their own datasets.  Each of these datasets may have different properties that make reported results from different methods harder to compare directly.  For example, differences in image size, image quality, relative location of objects within the images, and level of occlusion and clutter present can lead to varying results.  </p>
<p>

The Caltech 101 dataset aims to alleviate many of these common problems.
<list>
<entry level="1" type="bullet">

The work of collecting a large set of images, and cropping and resizing them appropriately has been taken care of.</entry>
<entry level="1" type="bullet">

A large number of different categories are represented, which benefits both single, and multi class recognition algorithms.</entry>
<entry level="1" type="bullet">

Detailed object outlines have been marked for each image.</entry>
<entry level="1" type="bullet">

By being released for general use, the Caltech 101 acts as a common standard by which to compare different algorithms without bias due to different datasets.</entry>
</list>
</p>
<p>

However, a recent study <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> demonstrates that tests based on uncontrolled natural images (like the Caltech 101 dataset) can be seriously misleading, potentially guiding progress in the wrong direction.</p>

</sec>
<sec>
<st>
The Dataset</st>

<ss1>
<st>
Images</st>
<p>

<image width="150px" src="Caltech101.gif">
<caption>

 Caltech 101 images
</caption>
</image>

The Caltech 101 dataset consists of a total of 9146 images, split between 101 different object categories, as well as an additional background/clutter category.</p>
<p>

Each object category contains between 40 and 800 images on average.  Common and popular categories such as faces tend to have a larger number of images than less used categories.
Each image is about 300x200 pixels in dimension.
Images of oriented objects such as <link xlink:type="simple" xlink:href="../770/46770.xml">
airplanes</link> and <link xlink:type="simple" xlink:href="../876/19876.xml">
motorcycles</link> were mirrored to be left-right aligned, and vertically oriented structures such as buildings were rotated to be off axis.</p>

</ss1>
<ss1>
<st>
Annotations</st>

<p>

As a supplement to the images, a set of annotations are provided for each image.  Each set of annotations contains two pieces of information.</p>
<p>

The general bounding box in which the object is located, and a detailed human specified outline enclosing the object.
A Matlab script is provided along with the annotations that will load an image and its corresponding annotation file and display them as a Matlab figure.</p>
<p>

<image width="150px" src="Caltech101_croc_annotated.jpg">
<caption>

 Crocodile image with annotations.
</caption>
</image>
</p>
<p>

The bounding box is yellow and the outline is red.</p>

</ss1>
</sec>
<sec>
<st>
Uses</st>

<p>

The Caltech 101 dataset has been used to train and test several Computer Vision recognition and classification algorithms.
The first paper to make use of Caltech 101 was an incremental Bayesian approach to <link>
one shot learning</link> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.  One shot learning is an attempt to learn a class of object using only a few examples, by building off of prior knowledge of many other classes.</p>
<p>

The Caltech 101 images, along with the annotations, were used for another one shot learning paper at Caltech.</p>
<p>

L. Fei-Fei, R. Fergus and P. Perona. One-Shot learning of object categories <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></p>
<p>

Other Computer Vision papers that report using the Caltech 101 dataset:
<list>
<entry level="1" type="bullet">

Shape Matching and Object Recognition using Low Distortion Correspondence. Alexander C. Berg, Tamara L. Berg, Jitendra Malik. CVPR 2005</entry>
<entry level="1" type="bullet">

The Pyramid Match Kernel:Discriminative Classification with Sets of Image Features. K. Grauman and T. Darrell. International Conference on Computer Vision (ICCV), 2005 <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></entry>
<entry level="1" type="bullet">

Combining Generative Models and Fisher Kernels for Object Class Recognition  Holub, AD. Welling, M. Perona, P. International Conference on Computer Vision (ICCV), 2005 <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> </entry>
<entry level="1" type="bullet">

Object Recognition with Features Inspired by Visual Cortex. T. Serre, L. Wolf and T. Poggio. Proceedings of 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005), IEEE Computer Society Press, San Diego, June 2005.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></entry>
<entry level="1" type="bullet">

SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition. Hao Zhang, Alex Berg, Michael Maire, Jitendra Malik. CVPR, 2006<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref></entry>
<entry level="1" type="bullet">

Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. CVPR, 2006<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref></entry>
<entry level="1" type="bullet">

 Empirical study of multi-scale filter banks for object categorization, M.J. Mar韓-Jim閚ez, and N. P閞ez de la Blanca. December 2005<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></entry>
<entry level="1" type="bullet">

Multiclass Object Recognition with Sparse, Localized Features, Jim Mutch and David G. Lowe. , pg. 11-18, CVPR 2006, IEEE Computer Society Press, New York, June 2006<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref></entry>
<entry level="1" type="bullet">

Using Dependent Regions or Object Categorization in a Generative Framework, G. Wang, Y. Zhang, and L. Fei-Fei. IEEE Comp. Vis. Patt. Recog. 2006<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref></entry>
</list>
</p>

</sec>
<sec>
<st>
Analysis and Comparison</st>

<ss1>
<st>
Advantages</st>
<p>

Caltech 101 has several advantages over other similar datasets:
<list>
<entry level="1" type="bullet">

Uniform size and presentation.</entry>
</list>

Almost all the images within each category are uniform in image size and in the relative position of interest objects.  This means that, in general, users who wish to use the Caltech 101 dataset do not need to spend and extra time cropping and scaling the images before they can be used.
<list>
<entry level="1" type="bullet">

Low level of clutter/occlusion:</entry>
</list>

Algorithms concerned with recognition usually function by storing features unique to the object that is to be recognized.  However, the majority of images taken have varying degrees of background clutter.  Algorithms trained on cluttered images can potentially build incorrect 
<list>
<entry level="1" type="bullet">

Detailed Annotations:</entry>
</list>

The detailed annotations of object outlines is another advantage to using the dataset.  </p>

</ss1>
<ss1>
<st>
Weaknesses</st>
<p>

There are several weaknesses to the Caltech 101 dataset <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref>.  Some of them are conscious trade-offs for the advantages it provides, and some are simply limitations of the dataset itself.
<list>
<entry level="1" type="bullet">

Limited number of categories:</entry>
</list>

There are approximately 10,000 different categories of objects.  The Caltech 101 dataset represents only a small fraction of these.
<list>
<entry level="1" type="bullet">

Some categories contain few images:</entry>
</list>

Certain categories are not represented as well as others, containing as few as 31 images.
This means that <math>\mathrm{N}_{\mathrm{train}} \le 30</math>.  The number of images used for training must be less than or equal to 30, which is not sufficient for all purposes.
<list>
<entry level="1" type="bullet">

Can be too easy:</entry>
</list>

Images are very uniform in presentation, left right aligned, and usually not occluded.  As a result, the images are not always representative of practical inputs that the algorithm being trained might be expected to see.  Under practical conditions, there is usually more clutter, occlusion, and variance in relative position and orientation of interest objects.
<list>
<entry level="1" type="bullet">

Aliasing and artifacts due to manipulation:</entry>
</list>

Some images have been rotated and scaled from their original orientation, and suffer from some amount of <link xlink:type="simple" xlink:href="../071/397071.xml">
artifacts</link> or <link xlink:type="simple" xlink:href="../474/151474.xml">
aliasing</link>.</p>

</ss1>
<ss1>
<st>
Other Datasets</st>
<p>

<list>
<entry level="1" type="bullet">

<link>
Caltech 256</link> is another image dataset created at the California Institute of technology in 2007, a successor to Caltech 101.  It is intended to address some of the weaknesses inherent to Caltech 101.  Overall, it is a more difficult dataset than Caltech 101 (but it suffers from the same problems <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>)</entry>
<entry level="2" type="bullet">

30,607 images, covering a larger number of categories.</entry>
<entry level="2" type="bullet">

Minimum number of image per category raised to 80.</entry>
<entry level="2" type="bullet">

Images not left-right aligned.</entry>
<entry level="2" type="bullet">

More variation in image presentation.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../989/14622989.xml">
LabelMe</link> is an open, dynamic dataset created at <link xlink:type="simple" xlink:href="../274/434274.xml">
MIT Computer Science and Artificial Intelligence Laboratory</link> (CSAIL).  LabelMe takes a different approach to the problem of creating a large image dataset, with different trade-offs.</entry>
<entry level="2" type="bullet">

106,739 images, 41,724 annotated images, and 203,363 labeled objects.</entry>
<entry level="2" type="bullet">

Users may add images to the dataset by upload, and add labels or annotations to existing images.</entry>
<entry level="2" type="bullet">

Due to its open nature, LabelMe has many more images covering a much wider scope than Caltech 101.  However, since each person decides what images to upload, and how to label and annotate each image, there can be a lack of consistency between images.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
 P. Viola and M. J. Jones, Robust Real-Time Object Detection, , IJCV 2004</entry>
<entry id="2">
<weblink xlink:type="simple" xlink:href="http://compbiol.plosjournals.org/perlserv/?request=get-document&amp;doi=10.1371/journal.pcbi.0040027">
| Why is Real-World Visual Object Recognition Hard?  Pinto N, Cox DD, DiCarlo JJ PLoS Computational Biology Vol. 4, No. 1, e27 doi:10.1371/journal.pcbi.0040027</weblink></entry>
<entry id="3">
<weblink xlink:type="simple" xlink:href="http://www.vision.caltech.edu/feifeili/Fei-Fei_GMBV04.pdf">
|L. Fei-Fei, R. Fergus and P. Perona. Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories. IEEE. CVPR 2004, Workshop on Generative-Model Based Vision. 2004</weblink></entry>
<entry id="4">
 <weblink xlink:type="simple" xlink:href="http://vision.cs.princeton.edu/documents/Fei-FeiFergusPerona2006.pdf">
| L. Fei-Fei, R. Fergus and P. Perona. One-Shot learning of object categories. IEEE Trans. Pattern Analysis and Machine Intelligence, Vol28(4), 594 - 611, 2006.</weblink></entry>
<entry id="5">
<weblink xlink:type="simple" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/grauman_darrell_iccv05.pdf">
| The Pyramid Match Kernel:Discriminative Classification with Sets of Image Features. K. Grauman and T. Darrell. International Conference on Computer Vision (ICCV), 2005</weblink></entry>
<entry id="6">
<weblink xlink:type="simple" xlink:href="http://www.its.caltech.edu/%7Eholub/publications.htm">
| Combining Generative Models and Fisher Kernels for Object Class Recognition  Holub, AD. Welling, M. Perona, P. International Conference on Computer Vision (ICCV), 2005</weblink></entry>
<entry id="7">
<weblink xlink:type="simple" xlink:href="http://web.mit.edu/serre/www/publications/serre_etal-CVPR05.pdf">
| Object Recognition with Features Inspired by Visual Cortex. T. Serre, L. Wolf and T. Poggio. Proceedings of 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005), IEEE Computer Society Press, San Diego, June 2005</weblink></entry>
<entry id="8">
<weblink xlink:type="simple" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/nhz_cvpr06.pdf">
| SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition. Hao Zhang, Alex Berg, Michael Maire, Jitendra Malik. CVPR, 2006</weblink></entry>
<entry id="9">
<weblink xlink:type="simple" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/cvpr06b_lana.pdf">
| Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. CVPR, 2006</weblink></entry>
<entry id="10">
<weblink xlink:type="simple" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/mjmarinVIP121505.pdf">
| Empirical study of multi-scale filter banks for object categorization, M.J. Mar韓-Jim閚ez, and N. P閞ez de la Blanca. December 2005</weblink></entry>
<entry id="11">
<weblink xlink:type="simple" xlink:href="http://www.mit.edu/~jmutch/papers/cvpr2006_mutch_lowe.pdf">
| Multiclass Object Recognition with Sparse, Localized Features, Jim Mutch and David G. Lowe. , pg. 11-18, CVPR 2006, IEEE Computer Society Press, New York, June 2006</weblink></entry>
<entry id="12">
<weblink xlink:type="simple" xlink:href="http://vision.cs.princeton.edu/documents/WangZhangFei-Fei_CVPR2006.pdf">
| Using Dependent Regions or Object Categorization in a Generative Framework, G. Wang, Y. Zhang, and L. Fei-Fei. IEEE Comp. Vis. Patt. Recog. 2006</weblink></entry>
<entry id="13">
<weblink xlink:type="simple" xlink:href="http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/sicily06c.pdf">
| Dataset Issues in Object Recognition. J. Ponce, T. L. Berg, M. Everingham, D. A. Forsyth, M. Hebert, S. Lazebnik, M. Marszalek, C. Schmid, B. C. Russell, A. Torralba, C. K. I. Williams, J. Zhang, and A. Zisserman. Toward Category-Level Object Recognition, Springer-Verlag Lecture Notes in Computer Science. J. Ponce, M. Hebert, C. Schmid, and A. Zisserman (eds.), 2006</weblink></entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 http://www.vision.caltech.edu/Image_Datasets/Caltech101/ -Caltech 101 Homepage (Includes download)</entry>
<entry level="1" type="bullet">

 http://www.vision.caltech.edu/Image_Datasets/Caltech256/ -Caltech 256 Homepage (Includes download)</entry>
<entry level="1" type="bullet">

 http://labelme.csail.mit.edu/ -LabelMe Homepage</entry>
</list>
</p>

</sec>
</bdy>
</entity>
</article>
