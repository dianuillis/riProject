<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 02:55:28[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Shape context</title>
<id>15281107</id>
<revision>
<id>241010386</id>
<timestamp>2008-09-25T22:54:22Z</timestamp>
<contributor>
<username>SmackBot</username>
<id>433328</id>
</contributor>
</revision>
<categories>
<category>Computer vision</category>
<category>Feature detection</category>
</categories>
</header>
<bdy>

<b>Shape Context</b> is the term given by <link>
Serge Belongie</link> and <link>
Jitendra Malik</link> to the feature descriptor they first proposed in their paper "Matching with Shape Contexts" in <link xlink:type="simple" xlink:href="../548/34548.xml">
2000</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. Shape context can be used in <link xlink:type="simple" xlink:href="../466/14661466.xml">
object recognition</link>.
<sec>
<st>
Theory</st>
<p>

The shape context is intended to be a way of describing shapes that allows for measuring shape similarity and the recovering of point correspondences <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. The basic idea is to pick <math>n</math> points on the contours of a shape. For each point <math>p_i</math> on the shape, consider the <math>n - 1</math> vectors obtained by connecting <math>p_i</math> to all other points. The set of all these vectors is a rich description of the shape localized at that point but is far too detailed. The key idea is that the distribution over relative positions is a robust, compact, and highly discriminative descriptor. So, for the point <math>p_i</math>, the coarse histogram of the relative coordinates of the remaining <math>n - 1</math> points,</p>
<p>

<math>h_i(k) = \#\{q \ne p_i  :  (q - p_i) \in \mbox{bin}(k)\}</math></p>
<p>

is defined to be the shape context of <math>p_i</math>. The bins are normally taken to be uniform in log-polar space. The fact that the shape context is a rich and discriminative descriptor can be seen in the figure below, in which the shape contexts of two different versions of the letter "A" are shown. </p>
<p>

<image width="400px" src="shapecontext.jpg">
</image>
</p>
<p>

(a) and (b) are the sampled edge points of the two shapes. (c) is the diagram of the log-polar bins used to compute the shape context. (d) is the shape context for the circle, (e) is that for the diamond, and (f) is that for the triangle. As can be seen, since (d) and (e) are the shape contexts for two closely related points, they are quite similar, while the shape context in (f) is very different.</p>
<p>

Now in order for a feature descriptor to be useful, it needs to have certain invariances. In particular it needs to be invariant to translation, scale, small perturbations, and depending on application rotation. Translational invariance come naturally to shape context. Scale invariance is obtained by normalizing all radial distances by the mean distance <math>\alpha</math> between all the point pairs in the shape <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> although the median distance can also be used<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>. Shape contexts are empirically demonstrated to be robust to deformations, noise, and outliers<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> using synthetic point set matching experiments<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.</p>
<p>

One can provide complete rotation invariance in shape contexts. One way is to measure angles at each point relative to the direction of the tangent at that point (since the points are chosen on edges). This results in a completely rotationally invariant descriptor. But of course this is not always desired since some local features lose their discriminative power if not measured relative to the same frame. Many applications in fact forbid rotation invariance e.g. distinguishing a "6" from a "9".</p>

</sec>
<sec>
<st>
Use in Shape Matching</st>
<p>

A complete system that uses shape contexts for shape matching consists of the following steps (which will be covered in more detail in the <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Details+of+Implementation%22])">
<list>
<entry level="1" type="number">

Details of Implementation</entry>
</list>
</link> section): </p>
<p>

<list>
<entry level="1" type="number">

 Finding a list of points on shape edges</entry>
<entry level="1" type="number">

 Computing the shape context of each point found in step 1</entry>
<entry level="1" type="number">

 Calculating the cost of matching each point in the first shape to each point of the second shape</entry>
<entry level="1" type="number">

 Find the one-to-one matching that minimizes the total cost of matching. This is an instance of the <link xlink:type="simple" xlink:href="../592/140592.xml">
assignment problem</link>.</entry>
<entry level="1" type="number">

 Find a transformation (e.g. <link xlink:type="simple" xlink:href="../449/38449.xml">
Affine</link>, <link xlink:type="simple" xlink:href="../546/8136546.xml">
Thin plate spline</link>, etc) that maps one shape to the other (essentially aligning the two shapes)</entry>
<entry level="1" type="number">

 Calculate the "shape distance" between the two shapes as a weighted sum of the shape context distance, image appearance distance, and bending energy (a measure of how much transformation is required to bring the two shapes into alignment)</entry>
</list>
</p>
<p>

Now that the shape distance has been calculated, the distance can be used in a <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../022/7309022.xml">
nearest-neighbor classifier</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 for a number of different object recognition problems.</p>

</sec>
<sec>
<st>
Details of Implementation</st>

<ss1>
<st>
Step 1: Finding a list of points on shape edges</st>
<p>

The approach essentially assumes that the shape of an object is essentially captured by a finite subset of the points on the internal or external contours on the object. These can be simply obtained using the <link xlink:type="simple" xlink:href="../817/476817.xml">
Canny edge detector</link> and picking a random set of points from the edges. Note that these points need not and in general do not correspond to key-points such as maxima of curvature or <link xlink:type="simple" xlink:href="../845/379845.xml">
inflection point</link>s. It is preferable to sample the shape with roughly uniform spacing, though it is not critical <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.</p>

</ss1>
<ss1>
<st>
Step 2: Computing the shape context</st>
<p>

This step is described in detail in the <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Theory%22])">
Theory section</link>.</p>

</ss1>
<ss1>
<st>
Step 3: Computing the cost matrix</st>
<p>

Consider two points <math>p</math> and <math>q</math> that have normalized K-bin histograms (i.e. shape contexts) <math>g(k)</math> and <math>h(k)</math>. As shape contexts are distributions represented as histograms, it is natural to use the <math>\chi^2</math> test statistic as the "shape context cost" of matching the two points:</p>
<p>

<math>C_S = \frac{1}{2}\sum_{k=1}^K \frac{[g(k) - h(k)]^2}{g(k) + h(k)}</math></p>
<p>

The values of this range from 0 to 1 <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.
In addition to the shape context cost, an extra cost based on the appearance can be added. For instance, it could be a measure of tangent angle dissimilarity (particularly useful in digit recognition):</p>
<p>

<math>C_A = \frac{1}{2}\begin{Vmatrix}
 \dbinom{\cos(\theta_1)}{\sin(\theta_1)} - \dbinom{\cos(\theta_2)}{\sin(\theta_2)}
\end{Vmatrix}</math></p>
<p>

This is half the length of the chord in unit circle between the unit vectors with angles <math>\theta_1</math> and <math>\theta_2</math>. Its values also range from 0 to 1. Now the total cost of matching the two points could be a weighted-sum of the two costs:</p>
<p>

<math>C = (1 - \beta)C_S + \beta C_A\!\,</math></p>
<p>

Now for each point <math>p_i</math> on the first shape and a point <math>q_j</math> on the second shape, calculate the cost as described and call it <math>C_{i,j}</math>. This is the cost matrix.</p>

</ss1>
<ss1>
<st>
Step 4: Finding the matching that minimizes total cost</st>
<p>

<image location="right" width="100px" src="matching.JPG" type="thumb">
<caption>

Results of Matching
</caption>
</image>

Now, a one-to-one matching <math>\pi (i)\!</math> that matches each point <math>p_i</math> on shape 1 and <math>q_j</math> on shape 2 that minimizes the total cost of matching,</p>
<p>

<math>H(\pi) = \sum_i C\left (p_i,q_{\pi (i)} \right )</math></p>
<p>

is needed. This can be done in <math>O(N^3)</math> time using the <link xlink:type="simple" xlink:href="../001/2609001.xml">
Hungarian method</link>, although there are more efficient algorithms<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>.
To have robust handling of outliers, one can add "dummy" nodes that have a constant but reasonably large cost of matching to the cost matrix. This would cause the matching algorithm to match outliers to a "dummy" if there is no real match.</p>

</ss1>
<ss1>
<st>
Step 5: Modeling Transformation</st>
<p>

Given the set of correspondences between a finite set of points on the two shapes, a transformation <math>T : \Bbb{R}^2 \to \Bbb{R}^2</math> can be estimated to map any point from one shape to the other. There are several choices for this transformation, described below.
</p>
<ss2>
<st>
Affine</st>
<p>

The <link xlink:type="simple" xlink:href="../449/38449.xml">
affine model</link> is a standard choice: <math>T(p) = Ap + o\!</math>. The <link xlink:type="simple" xlink:href="../359/82359.xml">
least squares</link> solution for the matrix <math>A</math> and the translational offset vector <math>o</math> is obtained by:</p>
<p>

<math>o = \frac{1}{n}\sum_{i=1}^n \left (p_i - q_{\pi(i)} \right ),
      A = (Q^+ P)^t</math></p>
<p>

Where <math>P = \begin{pmatrix}
             1 &amp; p_{11} &amp; p_{12} \\
             \vdots &amp; \vdots &amp; \vdots \\
             1 &amp; p_{n1} &amp; p_{n2}
\end{pmatrix} </math> with a similar expression for <math>Q\!</math>. <math>Q^+\!</math> is the <link xlink:type="simple" xlink:href="../258/331258.xml">
pseudoinverse</link> of <math>Q\!</math>.
</p>
</ss2>
<ss2>
<st>
Thin Plate Spline</st>
<p>

The <link xlink:type="simple" xlink:href="../546/8136546.xml">
thin plate spline (TPS)</link> model is the most widely used model for transformations when working with shape contexts. A 2D transformation can be separated into two TPS function to model a coordinate transform:
<math> T(x,y) = \left (f_x(x,y),f_y(x,y)\right )</math>
where each of the <math>f_x\!</math> and <math>f_y\!</math> have  the form:
<math> f(x,y) = a_1 + a_xx + a_yy + \sum_{i=1}^n\omega_iU\left (\begin{Vmatrix}
(x_i,y_i) - (x,y) \end{Vmatrix} \right ),</math></p>
<p>

and the kernel function <math>U(r)\!</math> is defined by <math>U(r) = r^2\log r^2\!</math>. The exact details of how to solve for the parameters can be found elsewhere<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> but it essentially involves solving a <link xlink:type="simple" xlink:href="../087/113087.xml">
linear system of equations</link>. The bending energy (a measure of how much transformation is needed to align the points) will also be easily obtained.
</p>
</ss2>
<ss2>
<st>
Regularized TPS</st>
<p>

The TPS formulation above has exact matching requirement for the pairs of points on the two shapes. For noisy data, it is best to relax this exact requirement. If we let <math>v_i</math> denote the target function values at corresponding locations <math>p_i = (x_i,y_i)</math> (Note that for <math>f_x</math>, <math>v_i</math> would <math>x'</math> the x-coordinate of the point corresponding to <math>p_i</math> and for <math>f_y</math> it would be the y-coordinate, <math>y'</math>), relaxing the requirement amounts to minimizing
<math> H[f] = \sum_{i=1}^n(v_i - f(x_i,y_i))^2 + \lambda I_f</math> where <math>I_f\!</math> is the bending energy and <math>\lambda\!</math> is called the regularization parameter. This <math>f</math> that minimizes <math>H[f]</math> can be found in a fairly straightforward way<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>. If one uses normalize coordinates for <math>(x_i,y_i)\mbox{ and } (x'_i,y'_i)</math>, then scale invariance is kept. However, if one uses the original non-normalized coordinates, then the regularization parameter needs to be normalized.</p>
<p>

Note that in many cases, regardless of the transformation used, the initial estimate of the correspondences contains some errors which could reduce the quality of the transformation. If we iterate the steps of finding correspondences and estimating transformations (i.e. repeating steps 2-5 with the newly transformed shape) we can overcome this problem. Typically, three iterations are all that is needed to obtain reasonable results.</p>

</ss2>
</ss1>
<ss1>
<st>
Step 6: Computing the shape distance</st>
<p>

Now, a shape distance between two shapes <math>P\!</math> and <math>Q\!</math>. This distance is going to be a weighted sum of three potential terms:</p>
<p>

<b>Shape context distance</b>: this is the symmetric sum of shape context matching costs over best matching points: 
<math>D_{sc}(P,Q) = \frac{1}{n}\sum_{p \in P} \arg \underset{q \in Q}{\min} C(p,T(q)) + \frac{1}{m}\sum_{q \in Q} \arg \underset{p \in P}{\min} C(p,T(q))</math></p>
<p>

Where <math>T(.)</math> is the estimated TPS transform that maps the points in <math>Q</math> to those in <math>P</math>.</p>
<p>

<b>Appearance cost</b>: After establishing image correspondences and properly warping one image to match the other, one can define an appearance cost as the sum of squared brightness differences in <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<filter wordnetid="103339643" confidence="0.8">
<link xlink:type="simple" xlink:href="../389/5842389.xml">
Gaussian windows</link></filter>
</device>
</instrumentality>
</artifact>
 around corresponding image points:
<math>D_{ac}(P,Q) = \frac{1}{n}\sum_{i=1}^n\sum_{\Delta \in Z^2} G(\Delta)\left [I_P(p_i + \Delta) - I_Q(T(q_{\pi(i)}) + \Delta)\right ]^2</math></p>
<p>

where <math>I_P\!</math> and <math>I_Q\!</math> are the gray-level images (<math>I_Q\!</math> is the image after warping) and <math>G\!</math> is a Gaussian windowing function.</p>
<p>

<b>Transformation cost</b>: The final cost <math>D_{be}(P,Q)\!\,</math> measures how much transformation is necessary to bring the two images into alignment. In the case of TPS, it is assigned to be the bending energy.</p>
<p>

Now that we have a way of calculating the distance between two shapes, we can use a <link xlink:type="simple" xlink:href="../388/1775388.xml">
nearest neighbor</link> <link xlink:type="simple" xlink:href="../224/1579224.xml">
classifier</link> (k-NN) with distance defined as the shape distance calculated here. The results of applying this to different situations is given in the following section.</p>

</ss1>
</sec>
<sec>
<st>
Results</st>

<ss1>
<st>
Digit Recognition</st>
<p>

The authors <link>
Serge Belongie</link> and <link>
Jitendra Malik</link> tested their approach on the <weblink xlink:type="simple" xlink:href="http://yann.lecun.com/exdb/mnist/">
MNIST dataset of handwritten digits</weblink>. Currently, more than 50 algorithms have been tested on the database. The database has a training set of 60,000 examples, and a test set of 10,000 examples. The error rate for this approach was 0.63% using 20,000 training examples and 3-NN. At the time of publication, this error rate was the lowest. Currently, the lowest error rate is 0.39%.
</p>
</ss1>
<ss1>
<st>
Silhouette Similarity-based Retrieval</st>
<p>

The authors experimented with the MPEG-7 shape silhouette database, performing Core Experiment CE-Shape-1 part B, which measures performance of similarity-based retrieval<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref>. The database has 70 shape categories and 20 images per shape category. Performance of a retrieval scheme is tested by using each image as a query and counting the number of correct images in the top 40 matches. For this experiment, the authors increased the amount of points sampled from each shape. Also, since the shapes in the database sometimes were rotated or flipped, the authors took defined the distance between a reference shape and query shape to be minimum shape distance between the query shape and either the unchanged reference, the vertically flipped, or the reference horizontally flipped <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>. With these changes, they obtained a retrieval rate of 76.45%, which by <link xlink:type="simple" xlink:href="../502/35502.xml">
2002</link> was the best.</p>

</ss1>
<ss1>
<st>
3D Object Recognition</st>
<p>

The next experiment performed on shape contexts involved the 20 common household objects in the <weblink xlink:type="simple" xlink:href="http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php">
Columbia Object Image Library (COIL-20)</weblink>. Each object has 72 views in the database. In the experiment, the method was trained on a number of equally spaced views for each object and the remaining views were used for testing. A 1-NN classifier was used. The results are shown to the right. The authors also developed an <it>editing</it> algorithm based on shape context similarity and <link xlink:type="simple" xlink:href="../095/6406095.xml">
k-medoid</link> clustering that improved on their performance <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>.</p>

</ss1>
<ss1>
<st>
Trademark Retrieval</st>
<p>

Shape contexts were used to retrieve the closest matching trademarks from a database to a query trademark (useful in detecting trademark infringement). The figure to the left depicts nearest neighbor retrieval results from a database of 300 trademarks. No visually similar trademark was missed by the algorithm (verified manually by the authors).
</p>
</ss1>
</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sc_digits.html">
Matching with Shape Contexts</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://yann.lecun.com/exdb/mnist/">
MNIST database of handwritten digits</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php">
Columbia Object Image Library (COIL-20)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">
Caltech101 Database</weblink></entry>
</list>

</p>
</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">S. Belongie and J. Malik&#32;(2000). "<weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/iel5/6885/18538/00853834.pdf">
Matching with Shape Contexts</weblink>".&#32;<it>IEEE Workshop on Contentbased Access of Image and Video Libraries (CBAIVL-2000)</it>.</cite>&nbsp;</entry>
<entry id="2">
 <cite style="font-style:normal">S. Belongie, J. Malik, and J. Puzicha&#32;(April 2002).&#32;"<weblink xlink:type="simple" xlink:href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/belongie-pami02.pdf">
Shape Matching and Object Recognition Using Shape Contexts</weblink>". <it>IEEE Transactions on Pattern Analysis and Machine Intelligence</it>&#32;<b>24</b>&#32;(24): 509–521. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1109%2F34.993558">
10.1109/34.993558</weblink>.</cite>&nbsp;</entry>
<entry id="3">
 <cite style="font-style:normal">S. Belongie, J. Malik, and J. Puzicha&#32;(July 2001). "<weblink xlink:type="simple" xlink:href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/belongie-iccv01.pdf">
Matching Shapes</weblink>".&#32;<it>Eighth IEEE International Conference on Computer Vision (July 2001)</it>.</cite>&nbsp;</entry>
<entry id="4">
 <cite style="font-style:normal">S. Belongie, J. Malik, and J. Puzicha&#32;(2000). "Shape Context: A new descriptor for shape matching and object recognition".&#32;<it>NIPS 2000</it>.</cite>&nbsp;</entry>
<entry id="5">
 <cite style="font-style:normal">H. Chui and A. Rangarajan&#32;(June 2000). "A new algorithm for non-rigid point matching".&#32;<it>CVPR</it>&#32;<b>2</b>: 44-51.</cite>&nbsp;</entry>
<entry id="6">
 <cite style="font-style:normal">R. Jonker and A. Volgenant&#32;(1987).&#32;"A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems". <it>Computing</it>&#32;<b>38</b>: 325–340. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF02278710">
10.1007/BF02278710</weblink>.</cite>&nbsp;</entry>
<entry id="7">
 <cite style="font-style:normal">M.J.D. Powell&#32;(1995). "A Thin Plate Spline Method for Mapping Curves into Curves in Two Dimensions".&#32;<it>Computational Techniques and Applications (CTAC '95)</it>.</cite>&nbsp;</entry>
<entry id="8">
 <cite style="font-style:normal">J. Duchon.&#32;"Splines Minimizing Rotation-Invariant Semi-Norms in Sobolev Spaces". <it>Constructive Theory of Functions of Several Variables</it>: 85–100.</cite>&nbsp;</entry>
<entry id="9">
 <cite style="font-style:normal" class="book">G. Wahba&#32;(1990). Spline Models for Observational Data.&#32;Soc. Industrial and Applied Math.</cite>&nbsp;</entry>
<entry id="10">
 <cite style="font-style:normal">S. Jeannin and M. Bober&#32;(March 1999).&#32;"Description of core experiments for MPEG-7 motion/shape. Technical Report ISO/IEC JTC 1/SC 29/WG 11 MPEG99/N2690, MPEG-7, Seoul".</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
</bdy>
</article>
