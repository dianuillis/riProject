<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:21:51[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<difference  confidence="0.8" wordnetid="104748836">
<inequality  confidence="0.8" wordnetid="104752221">
<header>
<title>Chebyshev&apos;s inequality</title>
<id>156533</id>
<revision>
<id>241830505</id>
<timestamp>2008-09-29T18:51:52Z</timestamp>
<contributor>
<username>Alarob</username>
<id>358952</id>
</contributor>
</revision>
<categories>
<category>Articles containing proofs</category>
<category>Probability theory</category>
<category>All articles to be merged</category>
<category>Statistical inequalities</category>
<category>Articles to be merged&amp;#32;since June 2008</category>
</categories>
</header>
<bdy>

For the similarly named inequality involving series, see <series wordnetid="108457976" confidence="0.8">
<arrangement wordnetid="107938773" confidence="0.8">
<sequence wordnetid="108459252" confidence="0.8">
<ordering wordnetid="108456993" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../664/937664.xml">
Chebyshev's sum inequality</link></group>
</ordering>
</sequence>
</arrangement>
</series>
.&#32;&#32;<p>

In <link xlink:type="simple" xlink:href="../542/23542.xml">
probability theory</link>, <b>Chebyshev's inequality</b> (also known as <b>Tchebysheff's inequality</b>, <b>Chebyshev's theorem</b>, or the <b>Bienaymé-Chebyshev inequality</b>) states that in any data sample or <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link>, nearly all the values are close to the <link xlink:type="simple" xlink:href="../192/19192.xml">
mean value</link>, and provides a quantitative description of "nearly all" and "close to".</p>
<p>

In particular,</p>
<p>

<list>
<entry level="1" type="bullet">

 No more than 1/4 of the values are more than 2 <link xlink:type="simple" xlink:href="../590/27590.xml">
standard deviation</link>s away from the mean;</entry>
<entry level="1" type="bullet">

 No more than 1/9 are more than 3 standard deviations away;</entry>
<entry level="1" type="bullet">

 No more than 1/25 are more than 5 standard deviations away;</entry>
</list>
</p>
<p>

and so on.  In general:</p>
<p>

<list>
<entry level="1" type="bullet">

 No more than 1/<it>k</it>2 of the values are more than <it>k</it> standard deviations away from the mean.</entry>
</list>
</p>
<p>

Chebyshev's inequality is named for the Russian mathematician <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../568/149568.xml">
Pafnuty Chebyshev</link></scientist>
</person>
, although it was first formulated by his friend and colleague <link>
Irénée-Jules Bienaymé</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

<sec>
<st>
General statement</st>
<p>

The inequality can be stated quite generally using <link xlink:type="simple" xlink:href="../873/19873.xml">
measure theory</link>; the statement in the language of probability theory then follows as a particular case, for a space of measure 1.</p>

<ss1>
<st>
Measure-theoretic statement</st>
<p>

Let (<it>X</it>,Σ,μ) be a <link xlink:type="simple" xlink:href="../873/19873.xml">
measure space</link>, and let <it>f</it> be an <link xlink:type="simple" xlink:href="../698/51698.xml">
extended real</link>-valued <link xlink:type="simple" xlink:href="../055/44055.xml">
measurable function</link> defined on <it>X</it>. Then for any real number <it>t</it> &amp;gt; 0,</p>
<p>

<indent level="1">

<math>\mu(\{x\in X\,:\,\,|f(x)|\geq t\}) \leq {1\over t^2} \int_X f^2 \, d\mu.</math>
</indent>

More generally, if <it>g</it> is a nonnegative extended real-valued measurable function, nondecreasing on the range of <it>f</it>, then</p>
<p>

<indent level="1">

<math>\mu(\{x\in X\,:\,\,f(x)\geq t\}) \leq {1\over g(t)} \int_X g\circ f\, d\mu.</math>
</indent>

The previous statement then follows by defining <it>g</it>(<it>t</it>) as</p>
<p>

<indent level="1">

<math>g(t)=\begin{cases}t^2&amp;\mbox{if }t\geq0\\0&amp;\mbox{otherwise,}\end{cases}</math>
</indent>

and taking |<it>f</it>| instead of <it>f</it>.</p>

</ss1>
<ss1>
<st>
Probabilistic statement</st>
<p>

Let <it>X</it> be a <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> with <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> μ and finite <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link> σ2. Then for any <link xlink:type="simple" xlink:href="../491/19725491.xml">
real number</link> <it>k</it>&nbsp;&amp;gt;&nbsp;0,</p>
<p>

<indent level="1">

<math>\Pr(\left|X-\mu\right|\geq k\sigma)\leq\frac{1}{k^2}.</math>
</indent>

Only the cases <it>k</it> &amp;gt; 1 provide useful information. This can be equivalently stated as
<indent level="1">

<math>\Pr(\left|X-\mu\right|\geq \alpha)\leq\frac{\sigma^2}{\alpha^2}.</math>
</indent>

As an example, using <it>k</it> = √2 shows that at least half of the values lie in the interval (μ − √2 σ, μ + √2 σ).</p>
<p>

Typically, the theorem will provide rather loose bounds. However, the bounds provided by Chebyshev's inequality cannot, in general (remaining sound for variables of arbitrary distribution), be improved upon. For example, for any <it>k</it>&nbsp;&amp;gt;&nbsp;1, the following example (where σ&nbsp;=&nbsp;1/<it>k</it>) meets the bounds exactly.</p>
<p>

<indent level="1">

<math>\begin{align} &amp; \Pr(X=-1) = 1/(2k^2), \\ \\ &amp; \Pr(X=0) = 1 - 1/k^2, \\ \\ &amp; \Pr(X=1) = 1/(2k^2). \end{align} </math>
</indent>

For this distribution,</p>
<p>

<indent level="1">

<math>\mathrm{Pr}\left(\left|X-\mu\right| \ge k\sigma\right) = 1/k^2.\, </math>
</indent>

Equality holds exactly for any distribution that is a linear transformation of this one. Inequality holds for any distribution that is not a linear transformation of this one.</p>
<p>

The theorem can be useful despite loose bounds because it applies to random variables of any distribution, and because these bounds can be calculated knowing no more about the distribution than the mean and variance.</p>
<p>

Chebyshev's inequality is used for proving the <link xlink:type="simple" xlink:href="../055/157055.xml">
weak law of large numbers</link>.</p>

</ss1>
<ss1>
<st>
 Example </st>
<p>

For illustration, assume we have a large body of text, for example articles from a publication. Assume we know that the articles are on average 1000 characters long with a <link xlink:type="simple" xlink:href="../590/27590.xml">
standard deviation</link> of 200 characters. From Chebyshev's inequality we can then infer that the chance that a given article is between 600 and 1400 characters would be at least 75% (<it>k</it> = 2).</p>
<p>

The inequality is coarse: a more accurate guess would be possible if the distribution of the length of the articles is known. For example, a normal distribution would yield a 75% chance of an article being between 770 and 1230 characters long.</p>

</ss1>
</sec>
<sec>
<st>
Variant: One-sided Chebyshev inequality</st>

<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-move" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Mergefrom.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 It has been suggested that  be  into this article or section. ()</col>
</row>
</table>

</p>
<p>

A one-tailed variant with <it>k</it> &amp;gt; 0, is<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

<indent level="1">

<math>\Pr(X-\mu \geq k\sigma)\leq\frac{1}{1+k^2}.</math>
</indent>

The one-sided version of the Chebyshev inequality is called Cantelli's inequality, and is due to <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../906/3166906.xml">
Francesco Paolo Cantelli</link></mathematician>
</scientist>
</causal_agent>
</person>
</physical_entity>
.</p>

</sec>
<sec>
<st>
Proof (of the two-sided Chebyshev's inequality)</st>

<ss1>
<st>
Measure-theoretic proof</st>
<p>

Let <math>~A_t</math> be defined as <math>A_t := \{x \in X \mid f(x) \geq t\}</math>, and let <math>1_{A_t}</math> be the <link xlink:type="simple" xlink:href="../790/240790.xml">
indicator function</link> of the set <math>~A_t</math>. Then, it is easy to check that
<indent level="1">

<math>0\leq g(t) 1_{A_t}\leq g\circ f\,1_{A_t}\leq g\circ f,</math>
</indent>
and therefore,
<indent level="1">

<math>g(t)\mu(A_t)=\int_X g(t)1_{A_t}\,d\mu\leq\int_{A_t} g\circ f\,d\mu\leq\int_X g\circ f\,d\mu.</math>
</indent>
The desired inequality follows from dividing the above inequality by <it>g</it>(<it>t</it>).</p>

</ss1>
<ss1>
<st>
Probabilistic proof</st>
<p>

<difference wordnetid="104748836" confidence="0.8">
<inequality wordnetid="104752221" confidence="0.8">
<link xlink:type="simple" xlink:href="../232/238232.xml">
Markov's inequality</link></inequality>
</difference>
 states that for any real-valued random variable <it>Y</it> and any positive number <it>a</it>, we have Pr(|<it>Y</it>|&nbsp;&amp;gt;&nbsp;<it>a</it>) ≤ E(|<it>Y</it>|)/<it>a</it>. One way to prove Chebyshev's inequality is to apply Markov's inequality to the random variable <it>Y</it>&nbsp;=&nbsp;(<it>X</it>&nbsp;−&nbsp;μ)2 with <it>a</it> = (σ<it>k</it>)2.</p>
<p>

It can also be proved directly. For any event <it>A</it>, let <it>IA</it> be the indicator random variable of <it>A</it>, i.e. <it>IA</it> equals 1 if <it>A</it> occurs and 0 otherwise. Then</p>
<p>

<indent level="2">

<math>\Pr(|X-\mu| \geq k\sigma) = \operatorname{E}(I_{|X-\mu| \geq k\sigma})
= \operatorname{E}(I_{[(X-\mu)/(k\sigma)]^2 \geq 1})</math> 
</indent>

<indent level="1">

<math>\leq \operatorname{E}\left( \left( {X-\mu \over k\sigma} \right)^2 \right)
= {1 \over k^2} {\operatorname{E}((X-\mu)^2) \over \sigma^2} = {1 \over k^2}.</math>
</indent>

The direct proof shows why the bounds are quite loose in typical cases: the number 1 to the left of "≥" is replaced by [(''X''&nbsp;−&nbsp;μ)/(''k''σ)]2 to the right of "≥" whenever the latter exceeds 1. In some cases it exceeds 1 by a very wide margin.</p>

</ss1>
</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

<difference wordnetid="104748836" confidence="0.8">
<inequality wordnetid="104752221" confidence="0.8">
<link xlink:type="simple" xlink:href="../232/238232.xml">
Markov's inequality</link></inequality>
</difference>
</entry>
<entry level="1" type="bullet">

A stronger result applicable to <link xlink:type="simple" xlink:href="../087/1432087.xml">
unimodal probability distributions</link> is the <link>
Vysochanskiï-Petunin inequality</link>.</entry>
<entry level="1" type="bullet">

<statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<information wordnetid="105816287" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<evidence wordnetid="105823932" confidence="0.8">
<proof wordnetid="105824739" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../396/12440396.xml">
Proof of the weak law of large numbers</link></proposition>
</proof>
</evidence>
</theorem>
</information>
</message>
</statement>
 where Chebyshev's inequality is used.</entry>
<entry level="1" type="bullet">

<arrangement wordnetid="107938773" confidence="0.8">
<table wordnetid="108266235" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<array wordnetid="107939382" confidence="0.8">
<link xlink:type="simple" xlink:href="../634/73634.xml">
Table of mathematical symbols</link></array>
</group>
</table>
</arrangement>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../342/18729342.xml">
Multidimensional Chebyshev's inequality</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<reflist>
<entry id="1">
<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Donald Knuth</link></scientist>
</person>
, "<work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../358/31358.xml">
The Art of Computer Programming</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
", 3rd edition, volume 1, <link xlink:type="simple" xlink:href="../601/34601.xml">
1997</link>, p.76</entry>
<entry id="2">
Grimmett and Stirzaker, problem 7.11.9. Several proofs of this result can be found <weblink xlink:type="simple" xlink:href="http://www.mcdowella.demon.co.uk/Chebyshev.html">
here</weblink>.</entry>
</reflist>
</p>

</sec>
<sec>
<st>
 Further reading </st>
<p>

<list>
<entry level="1" type="bullet">

 A. Papoulis (1991), <it>Probability, Random Variables, and Stochastic Processes</it>, 3rd ed. McGraw-Hill. ISBN 0-07-100870-5. pp. 113-114.</entry>
<entry level="1" type="bullet">

 G. Grimmett and D. Stirzaker (2001), <it>Probability and Random Processes</it>, 3rd ed. Oxford. ISBN 0 19 857222 0. Section 7.3.</entry>
</list>
</p>


</sec>
</bdy>
</inequality>
</difference>
</article>
