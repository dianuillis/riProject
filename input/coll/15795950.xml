<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 03:26:34[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<software  confidence="0.8" wordnetid="106566077">
<application  confidence="0.8" wordnetid="106570110">
<program  confidence="0.8" wordnetid="106568978">
<written_communication  confidence="0.8" wordnetid="106349220">
<writing  confidence="0.8" wordnetid="106359877">
<code  confidence="0.8" wordnetid="106355894">
<coding_system  confidence="0.8" wordnetid="106353757">
<header>
<title>Activity recognition</title>
<id>15795950</id>
<revision>
<id>243692036</id>
<timestamp>2008-10-07T17:18:24Z</timestamp>
<contributor>
<username>CmdrObot</username>
<id>1079367</id>
</contributor>
</revision>
<categories>
<category>Artificial intelligence applications</category>
<category>Machine learning</category>
<category>Cognition</category>
</categories>
</header>
<bdy>

<b>Activity recognition</b> aims to recognize the actions and goals of one or more agents from a series of observations on the agents actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link> communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine.<p>

To understand activity recognition better, consider the following scenario.  An elderly man wakes up at dawn in his small studio apartment, where he stays alone. He lights the stove to make a pot of tea, switches on the toaster oven, and takes some bread and jelly from the cupboard.</p>
<p>

After taking his morning medication, a computer-generated voice gently reminds him to turn off the toaster. Later that day, his daughter accesses a secure website where she scans a check-list, which was created by a sensor network in her father's apartment. She finds that his father is eating normally, taking his medicine on schedule, and continuing to manage his daily life on his own. That information puts her mind at ease.</p>
<p>

Many different applications have been studied by researchers in activity recognition; examples include assisting the sick and disabled.  For example, <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> shows that by automatically monitoring human activities, home-based rehabilitation can be provided for people suffering from traumatic brain injuries. One can find applications ranging from security-related applications and logistics support to location-based services. Due to its many-faceted nature, different fields may refer to activity recognition as plan recognition, goal recognition, intent recognition, behavior recognition, location estimation and location based services.</p>

<sec>
<st>
 Types of activity recognition </st>

<ss1>
<st>
 Sensor-based activity recognition </st>
<p>

<link xlink:type="simple" xlink:href="../757/235757.xml">
Sensor</link>-based activity recognition integrates the emerging area of sensor networks with novel <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link> and <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> techniques to model a wide range of human activities. Sensor-based activity recognition researchers believe that by empowering <link xlink:type="simple" xlink:href="../871/31871.xml">
ubiquitous computers</link> and sensors to monitor the behavior of agents (under consent), these computers will be better suited to act on our behalf.</p>

<ss2>
<st>
 Levels of sensor-based activity recognition </st>
<p>

Sensor-based activity recognition is a challenging task due to the inherent noisy nature of the input.  Thus, <link xlink:type="simple" xlink:href="../576/27576.xml">
statistical modeling</link> has been the main thrust in this direction in layers, where the recognition at several intermediate levels is conducted and connected.  At the lowest level where the sensor data are collected, statistical learning concerns how to find the detailed locations of agents from the received signal data. At an intermediate level, <link xlink:type="simple" xlink:href="../577/27577.xml">
statistical inference</link> may be concerned about how to recognize individuals' activities from the inferred location sequences and environmental conditions at the lower levels. Furthermore, at the highest level a major concern is to find out the overall goal or subgoals of an agent from the activity sequences through a mixture of logical and statistical reasoning.</p>

</ss2>
</ss1>
<ss1>
<st>
 Vision-based activity recognition </st>
<p>

It is a very important and challenging problem to track and understand the behavior of agents through videos taken by various cameras.  The primary technique employed is computer vision.  Vision-based activity recognition has found many applications such as human-computer interaction, user interface design, robot learning, and surveillance, among others.
Conferences where vision based activity recognition work often appears is <link xlink:type="simple" xlink:href="../465/5338465.xml">
ICCV</link> and <link xlink:type="simple" xlink:href="../465/5338465.xml">
CVPR</link>.</p>
<p>

In vision-based activity recognition, a great deal of work has been done.
Researchers have attempted a number of methods such as <link xlink:type="simple" xlink:href="../825/869825.xml">
optical flow</link>, <link xlink:type="simple" xlink:href="../855/180855.xml">
Kalman filtering</link>, hidden <link xlink:type="simple" xlink:href="../876/60876.xml">
Markov model</link>s, etc, under different modalities such as  single camera, stereo, and infra-red. In addition, researchers have considered multiple aspects on this topic, including single pedestrian tracking, group tracking, and detecting dropped objects.</p>

<ss2>
<st>
 Levels of vision-based activity recognition </st>
<p>

In vision-based activity recognition, the computational process is often divided into four steps, namely human detection, human tracking, human activity recognition and then a high-level activity evaluation.</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
 Approaches of activity recognition </st>

<ss1>
<st>
 Activity recognition through logic and reasoning </st>
<p>

Logic-based approaches keep track of all <link xlink:type="simple" xlink:href="../802/75802.xml">
logically consistent</link> explanations of the observed actions. Thus, all possible and consistent plans or goals must be considered. Kautz <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> provided a formal theory of plan recognition. He described plan recognition as a logical inference process of circumscription. All actions, plans are uniformly referred to as goals, and a recognizer's knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events.</p>
<p>

Kautz's general framework for plan recognition has an exponential time complexity in worst case, measured in the size of input hierarchy. Lesh and Etzioni <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> went one step further and presented methods in scaling up goal recognition to scale up his work computationally. In contrast to Kautz's approach where the plan library is explicitly represented, Lesh and Etzioni’s approach enables automatic plan-library construction from domain primitives. Furthermore, they introduced compact representations and efficient algorithms for goal recognition on large plan libraries.</p>
<p>

Inconsistent plans and goals are repeatedly pruned when new actions arrived. Besides, they also presented methods for adapting a goal recognizer to handle individual idiosyncratic behavior given a sample of an individual’s recent behavior.  Pollack et al. described a direct argumentation model that can know about the relative strength of several kinds of arguments for belief and intention description.</p>
<p>

A serious problem of logic-based approaches is their inability or inherent infeasibility to represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed. There is also a lack of learning ability associated with logic based methods.</p>

</ss1>
<ss1>
<st>
 Activity recognition through probabilistic reasoning </st>
<p>

Probability theory and statistical learning models are more recently applied in activity recognition to reason about actions, plans and goals.</p>
<p>

Plan recognition can be done as a process of reasoning under uncertainty, which is convincingly argued by Charniak and Goldman <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>. They argued that any model that does not incorporate some theory of uncertainty reasoning cannot be adequate. In the literature, there have been several approaches which explicitly represent uncertainty in reasoning about an agent's plans and goals.</p>
<p>

Using sensor data as input, Hodges and Pollack designed machine learning based systems for identifying individuals as they perform routine daily activities such as making coffee <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>. Intel Research (Seattle) Lab and University of Washington at Seattle have done some important works on using sensors to detect human plans<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>. Some of these works infer user transportation modes from readings of radio-frequency identifiers (RFID) and global positioning systems (GPS).</p>

</ss1>
</sec>
<sec>
<st>
 Wifi based activity recognition </st>

<p>

When activity recognition is performed indoors and in cities using the widely available <link xlink:type="simple" xlink:href="../973/63973.xml">
Wifi</link> signals and <link xlink:type="simple" xlink:href="../739/14739.xml">
802.11</link> access points, there is much noise and uncertainty.  These uncertainty are modeled using a dynamic <link xlink:type="simple" xlink:href="../526/38526.xml">
Bayesian</link> network model in <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>.  A multiple goal model that can reason about user's interleaving goals is presented in <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref>, where a <link xlink:type="simple" xlink:href="../922/47922.xml">
deterministic</link> state transition model is applied. A better model that models the concurrent and interleaving activities in a probabilistic approach is proposed in <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref>. A user action discovery model is presented in <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref>, where the Wifi signals are segmented to produce possible actions.</p>
<p>

A fundamental problem in WiFi based activity recognition is to estimate the user locations.  Two important issues are how to reduce the human labelling effort and how to cope with the changing signal profiles when the environment changes.  <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref> dealt with the second issue by transferring the labelled knowledge between time periods.  <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref> proposed a hidden Markov model based method to extend labelled knowledge by leveraging the unlabelled user traces.  <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref> proposes to perform location estimation through online co-localization, and <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref> proposed to apply multi-view learning for migrating the labelled data to a new time period.</p>

</sec>
<sec>
<st>
 Labs in the world </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://ihome.ust.hk/~derekhh/ActivityRecognition/index.html">
Derek Hao Hu's Activity Recognition Page</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.intel.com/research/exploratory/activity_recognition.htm">
Intel Research Lab at Seattle</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.eecs.umich.edu/~pollackm/">
Martha Pollack's research group</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cse.ust.hk/~qyang/">
Prof Qiang Yang's research group</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.washington.edu/ai/Mobile_Robotics/">
RSE Lab @ University of Washington, leading by Dieter Fox</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.pancube.com/MLMC/MLWSN.html">
Jeffrey Junfeng Pan's Sensor-based Localization and Tracking Project</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cuslab.com/eng/template/vba.php">
Ajou University CUSLAB Vision-based Activity Awareness</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
 Related conferences </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.aaai.org/">
AAAI</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://vision.eecs.ucf.edu/">
CVPR</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.iccv2009.org/">
ICCV</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ijcai.org/">
IJCAI</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://nips.cc/">
NIPS</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.pervasive2008.org/">
PERVASIVE</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ubicomp.org/">
Ubicomp</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.percom.org/">
PerCom</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.iswc.net/">
ISWC</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../010/378010.xml">
Planning</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../339/87339.xml">
Naive Bayes classifier</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 </entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../309/65309.xml">
Support vector machines</link>  </entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../770/98770.xml">
Hidden Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../276/4118276.xml">
Conditional random field</link> </entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
Pollack, M. E., and et al., L. E. B. 2003. "Autominder: an intelligent cognitive orthotic system for people with memory impairment". <it>Robotics and Autonomous Systems</it> 44(3-4):273–282.</entry>
<entry id="2">
H. Kautz. "A formal theory of plan recognition". In PhD thesis, University of Rochester, 1987.</entry>
<entry id="3">
N. Lesh and O. Etzioni. "A sound and fast goal recognizer". In <it>Proceedings of the International Joint Conference on Artificial Intelligence</it>, 1995.</entry>
<entry id="4">
E. Charniak and R. P. Goldman. "A Bayesian model of plan recognition". <it>Artificial Intelligence</it>, 64:53–79, 1993.</entry>
<entry id="5">
M. R. Hodges and M. E. Pollack. "An 'object-use fingerprint': The use of electronic sensors for human identification". In <it>Proceedings of the 9th International Conference on Ubiquitous Computing</it>, 2007.</entry>
<entry id="6">
Mike Perkowitz, Matthai Philipose, Donald J. Patterson, and Kenneth P. Fishkin. "Mining models of human activities from the web". In <it>Proceedings of the Thirteenth International World Wide Web Conference (WWW 2004), pages 573–582, May 2004.</it></entry>
<entry id="7">
Matthai Philipose, Kenneth P. Fishkin, Mike Perkowitz, Donald J. Patterson, Dieter Fox, Henry Kautz, , and Dirk Hähnel. "Inferring activities from interactions with objects". In <it>IEEE Pervasive Computing</it>, pages 50–57, October 2004.</entry>
<entry id="8">
Dieter Fox Lin Liao, Donald J. Patterson and Henry A. Kautz. "Learning and inferring transportation routines". <it>Artif. Intell.</it>, 171(5-6):311–331, 2007.</entry>
<entry id="9">
Jie Yin, Xiaoyong Chai and Qiang Yang, "High-level Goal Recognition in a Wireless LAN".  In <it>Proceedings of the Nineteenth National Conference on Artificial Intelligence</it> (AAAI-04), San Jose, CA USA, July 2004. Pages
578-584 </entry>
<entry id="10">
Xiaoyong Chai and Qiang Yang, "Multiple-Goal Recognition From Low-level Signals".  <it>Proceedings of the Twentieth National Conference on Artificial Intelligence</it> (AAAI 2005), Pittsburg, PA USA, July 2005.  Pages 3-8.</entry>
<entry id="11">
Derek Hao Hu, Qiang Yang. "CIGAR: Concurrent and Interleaving Goal and Activity Recognition", to appear in AAAI 2008</entry>
<entry id="12">
Jie Yin, Dou Shen, Qiang Yang and Ze-nian Li  "Activity Recognition through Goal-Based Segmentation".  <it>Proceedings of the Twentieth National Conference on Artificial Intelligence</it> (AAAI 2005), Pittsburg, PA USA, July 2005. Pages 28-33. </entry>
<entry id="13">
Jie Yin, Qiang Yang and Lionel Ni. "Adaptive Temporal Radio Maps for Indoor Location Estimation". In <it>Proceedings of the 3rd Annual IEEE International Conference on Pervasive Computing and Communications</it> (IEEE PerCom 2005), Kauai Island, Hawaii, March, 2005. Pages 85-94.</entry>
<entry id="14">
Xiaoyong Chai and Qiang Yang. "Reducing the Calibration Effort for Location Estimation Using Unlabeled Samples". In <it>Proceedings of the 3rd Annual IEEE International Conference on Pervasive Computing and Communications</it>, (IEEE PerCom 2005) Kauai Island, Hawaii, March 2005. Pages 95--104.</entry>
<entry id="15">
Jeffrey Junfeng Pan, Qiang Yang and Sinno Jialin Pan.  "Online Co-Localization in Indoor Wireless Networks".  In <it>Proceedings of the 22nd AAAI Conference on Artificial Intelligence</it> (AAAI'07) Vancouver, British Columbia, Canada. July 2007.  1102-1107</entry>
<entry id="16">
Sinno Jialin Pan, James T. Kwok, Qiang Yang, Jeffrey Junfeng Pan. "Adaptive localization in a dynamic WiFi environment through multi-view learning". In <it>Proceedings of the 22nd AAAI Conference on Artificial Intelligence</it> (AAAI'07) Vancouver, British Columbia, Canada. July 2007. 1108-1113 </entry>
</reflist>
</p>

</sec>
</bdy>
</coding_system>
</code>
</writing>
</written_communication>
</program>
</application>
</software>
</article>
