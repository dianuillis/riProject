<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:47:51[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>AdaBoost</title>
<id>1645603</id>
<revision>
<id>237944627</id>
<timestamp>2008-09-12T15:00:14Z</timestamp>
<contributor>
<username>Feco</username>
<id>218800</id>
</contributor>
</revision>
<categories>
<category>Ensemble learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

<b>AdaBoost</b>, short for Adaptive <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../500/90500.xml">
Boosting</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, is a <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> algorithm, formulated by <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../826/8000826.xml">
Yoav Freund</link></scientist>
 and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../974/14590974.xml">
Robert Schapire</link></scientist>
. It is a <link xlink:type="simple" xlink:href="../458/774458.xml">
meta-algorithm</link>, and can be used in conjunction with many other learning algorithms to improve their performance. AdaBoost is adaptive in the sense that subsequent classifiers built are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and <link xlink:type="simple" xlink:href="../951/160951.xml">
outlier</link>s. Otherwise, it is less susceptible to the <link xlink:type="simple" xlink:href="../332/173332.xml">
overfitting</link> problem than most learning algorithms. <p>

AdaBoost calls a weak classifier repeatedly in a series of rounds <math> t = 1,\ldots,T</math>. For each call a distribution of weights <math>D_{t}</math> is updated that indicates the importance of examples in the data set for the classification. On each round, the weights of each incorrectly classified example are increased (or alternatively, the weights of each correctly classified example are decreased), so that the new classifier focuses more on those examples. </p>

<sec>
<st>
 The algorithm for the binary classification task </st>
<p>

Given: <math>(x_{1},y_{1}),\ldots,(x_{m},y_{m})</math> where <math>x_{i} \in X,\, y_{i} \in Y = \{-1, +1\}</math></p>
<p>

Initialise <math>D_{1}(i) = \frac{1}{m}, i=1,\ldots,m.</math></p>
<p>

For <math>t = 1,\ldots,T</math>:</p>
<p>

<list>
<entry level="1" type="bullet">

 Find the classifier <math>h_{t} : X \to \{-1,+1\}</math> that minimizes the error with respect to the distribution <math>D_{t}</math>: <math>h_{t} = \arg \min_{h_{j} \in \mathcal{H}} \epsilon_{j}</math>, where <math> \epsilon_{j} = \sum_{i=1}^{m} D_{t}(i)[y_i \ne h_{j}(x_{i})]</math></entry>
<entry level="1" type="bullet">

 Prerequisite: <math>\epsilon_{t} &amp;lt; 0.5</math>, otherwise stop.</entry>
<entry level="1" type="bullet">

 Choose <math>\alpha_{t} \in \mathbf{R}</math>, typically <math>\alpha_{t}=\frac{1}{2}\textrm{ln}\frac{1-\epsilon_{t}}{\epsilon_{t}}</math> where <math>\epsilon_{t}</math> is the weighted error rate of classifier <math>h_{t}</math>.</entry>
<entry level="1" type="bullet">

 Update:</entry>
</list>

<math>D_{t+1}(i) = \frac{ D_{t}(i) \, e^{- \alpha_{t} y_{i} h_{t}(x_{i})} }{ Z_{t} }</math>
where <math>Z_{t}</math> is a normalization factor (chosen so that <math>D_{t+1}</math> will be a <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link>, i.e. sum one over all x).</p>
<p>

Output the final classifier:</p>
<p>

<math>H(x) = \textrm{sign}\left( \sum_{t=1}^{T} \alpha_{t}h_{t}(x)\right)</math></p>
<p>

The equation to update the distribution <math>D_{t}</math> is constructed so that:</p>
<p>

<math>e^{- \alpha_{t} y_{i} h_{t}(x_{i})} \begin{cases} &amp;lt;1, &amp; y(i)=h_{t}(x_{i}) \\ &amp;gt;1, &amp; y(i) \ne h_{t}(x_{i}) \end{cases}</math></p>
<p>

Thus, after selecting an optimal classifier <math>h_{t} \,</math> for the distribution <math>D_{t} \,</math>, the examples <math>x_{i} \,</math> that the classifier <math>h_{t} \,</math> identified correctly are weighted less and those that it identified incorrectly are weighted more.  Therefore, when the algorithm is testing the classifiers on the distribution <math>D_{t+1} \,</math>, it will select a classifier that better identifies those examples that the previous classifer missed.</p>

</sec>
<sec>
<st>
Statistical Understanding of Boosting</st>

<p>

Boosting can be seen as minimization of a <link>
convex loss function</link> over a <link xlink:type="simple" xlink:href="../292/6292.xml">
convex set</link> of functions. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> Specifically, the loss being minimized is the exponential loss</p>
<p>

<indent level="1">

<math>\sum_i e^{-y_i f(x_i)}</math>
</indent>

and we are seeking a function</p>
<p>

<indent level="1">

<math>f = \sum_t \alpha_t h_t</math>
</indent>

</p>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../911/1307911.xml">
Bootstrap aggregating</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../434/9616434.xml">
LPBoost</link></entry>
<entry level="1" type="bullet">

 <link>
GentleBoost</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
T. Zhang, "Convex Risk Minimization", Annals of Statistics, 2004.</entry>
</reflist>
</p>


</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://cmp.felk.cvut.cz/~sochmj1/adaboost_talk.pdf">
AdaBoost</weblink> Presentation summarizing Adaboost (see page 4 for an illustrated example of performance)</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf">
A Short Introduction to Boosting</weblink> Introduction to Adaboost by Freund and Schapire from 1999</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/cache/papers/cs/2215/http:zSzzSzwww.first.gmd.dezSzpersonszSzMueller.Klaus-RobertzSzseminarzSzFreundSc95.pdf/freund95decisiontheoretic.pdf">
A decision-theoretic generalization of on-line learning and an application to boosting</weblink> <it>Journal of Computer and System Sciences</it>, no. 55. 1997  (Original paper of Yoav Freund and Robert E.Schapire where Adaboost is first introduced.)</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.ucsd.edu/~yfreund/adaboost/index.html">
An applet demonstrating AdaBoost</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://engineering.rowan.edu/~polikar/RESEARCH/PUBLICATIONS/csm06.pdf">
Ensemble Based Systems in Decision Making</weblink>, R. Polikar, IEEE Circuits and Systems Magazine, vol.6, no.3, pp. 21-45, 2006. A tutorial article on ensemble systems including pseudocode, block diagrams and implementation issues for AdaBoost and other ensemble learning algorithms.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.dgp.toronto.edu/~hertzman/courses/csc411/fall_2007/lectureNotesWeb/AdaBoost.pdf">
http://www.dgp.toronto.edu/~hertzman/courses/csc411/fall_2007/lectureNotesWeb/AdaBoost.pdf</weblink>adaboost</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=21317&amp;objectType=file">
A Matlab Implementation of AdaBoost</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.9525">
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.9525</weblink> by Jerome Friedman, Trevor Hastie, Robert Tibshirani. Paper introducing probabilistic theory for AdaBoost, and introducing GentleBoost</entry>
</list>
</p>



</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
