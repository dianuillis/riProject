<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 03:46:06[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>QCDOC</title>
<id>16727034</id>
<revision>
<id>237143432</id>
<timestamp>2008-09-08T21:16:40Z</timestamp>
<contributor>
<username>Bendono</username>
<id>95594</id>
</contributor>
</revision>
<categories>
<category>Parallel computing</category>
<category>Supercomputers</category>
<category>Power Architecture</category>
</categories>
</header>
<bdy>

The <b>QCDOC</b>, <it><link xlink:type="simple" xlink:href="../264/25264.xml">
Quantum ChromoDynamics</link> On a Chip</it>, is a <link xlink:type="simple" xlink:href="../153/37153.xml">
supercomputer</link> technology focusing on using relatively cheap low power processing elements to produce a massively parallel machine. As the name suggests, the machine is custom made to solve small but extremely demanding problems in the fields of <link xlink:type="simple" xlink:href="../202/25202.xml">
quantum physics</link>.
<sec>
<st>
 Overview </st>
<p>

The computers were designed and built jointly by <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../395/64395.xml">
University of Edinburgh</link></university>
 (UKQCD) , <university wordnetid="108286163" confidence="0.9508927676800064">
<ranking wordnetid="114429484" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../310/6310.xml">
Columbia University</link></ranking>
</university>
, the <institute wordnetid="108407330" confidence="0.8">
<association wordnetid="108049401" confidence="0.8">
<link xlink:type="simple" xlink:href="../694/1648694.xml">
RIKEN</link></association>
</institute>
 Brookhaven Research Center and <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link></company>
. The purpose of the collaboration was to exploit computing facilities for <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../072/977072.xml">
lattice field theory</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 calculations whose primary aim is to increase the predictive power of the <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../641/47641.xml">
Standard Model</link></concept>
</idea>
 of elementary particle interactions through numerical simulation of quantum chromodynamics (QCD). The target was to build a massively parallel supercomputer able to peak at 10 <link xlink:type="simple" xlink:href="../930/82930.xml">
Tflops</link> with sustained power at 50% capacity. </p>
<p>

There are three QCDOCs in service each reaching 10 Tflops peak operation.
<list>
<entry level="1" type="bullet">

 <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../395/64395.xml">
University of Edinburgh</link></university>
's Parallel Computing Centre (EPCC). In operation by the UKQCD since 2005</entry>
<entry level="1" type="bullet">

 RIKEN Brookhaven Research Center at <body wordnetid="107965085" confidence="0.8">
<social_group wordnetid="107950920" confidence="0.8">
<college wordnetid="108278169" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../299/53299.xml">
Brookhaven National Laboratory</link></group>
</college>
</social_group>
</body>
</entry>
<entry level="1" type="bullet">

 <agency wordnetid="108337324" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../866/62866.xml">
U.S. Department of Energy</link></agency>
 Program in High Energy and Nuclear Physics at Brookhaven National Laboratory</entry>
</list>
</p>
<p>

Around 23 <link xlink:type="simple" xlink:href="../927/1048927.xml">
UK</link> academic staff, their postdocs and students, from seven universities, belong to UKQCD. Costs were funded through a Joint Infrastructure Fund Award of £6.6 million. Staff costs (system support, physicist programmers and postdocs) are around £1 million per year, other computing and operating costs are around £0.2 million per year.<weblink xlink:type="simple" xlink:href="http://www.scitech.ac.uk/roadmap/rmProject.aspx?q=82">
http://www.scitech.ac.uk/roadmap/rmProject.aspx?q=82</weblink>  </p>
<p>

QCDOC was to replace an earlier design, <b>QCDSP</b>, where the power came from connecting large amounts of <link xlink:type="simple" xlink:href="../505/154505.xml">
DSPs</link> together in a similar fashion. The QCDSP strapped 12.288 nodes to a 4D network and reached 1 Tflops in 1998.</p>
<p>

QCDOC can be seen as a predecessor to the highly successful <link xlink:type="simple" xlink:href="../764/136764.xml">
BlueGene/L</link> supercomputer. They share a lot of design traits, and similarities goes beyond superficial characteristics. BlueGene is also a massively parallel supercomputer built with a large amount of cheap, relatively weak <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../286/1848286.xml#xpointer(//*[./st=%22PowerPC+440%22])">
PowerPC 440</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
 based <link xlink:type="simple" xlink:href="../563/100563.xml">
SoC</link> nodes conencted with a high bandwidth multidimensional mesh. They differ however in that the computing nodes in BG/L is more powerful and it have a much faster and more sophisticated network so it scales up to several 100.000 nodes per system.</p>

</sec>
<sec>
<st>
 Architecture </st>

<ss2>
<st>
 Computing node </st>
<p>

The computing nodes are custom built <link xlink:type="simple" xlink:href="../845/147845.xml">
ASIC</link>s with 50 million transistors using mainly existing building blocks from <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link></company>
. They are built around a 500 MHz <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../286/1848286.xml#xpointer(//*[./st=%22PowerPC+440%22])">
PowerPC 440</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
 core with 4 MB <link xlink:type="simple" xlink:href="../567/74567.xml">
DRAM</link>, memory management for external <standard wordnetid="107260623" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../922/8922.xml">
DDR SDRAM</link></system_of_measurement>
</standard>
, system I/O for inter node communications and dual Ethernet built in. The computing node is capable of 1 double precision <link xlink:type="simple" xlink:href="../930/82930.xml">
Gflops</link>. Each node has one <link xlink:type="simple" xlink:href="../030/142030.xml">
DIMM</link> socket capable of holding 128-2048 MB 333 MHz <link>
ECC</link> <standard wordnetid="107260623" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../922/8922.xml">
DDR SDRAM</link></system_of_measurement>
</standard>
.</p>

</ss2>
<ss2>
<st>
 Inter node communication </st>
<p>

Each node has the capability to send and receive data from each of its twelve nearest neighbors in a six-dimensional mesh at a rate of 500 Mbit/s each. This provides a total off-node bandwidth of 12 Gbit/s. Each of these 24 channels has <link>
DMA</link> to the other nodes' on-chip DRAM or the external SDRAM. In practice only four dimensions will be used to form a communications sub-torus where the remaining two dimensions will be used to partition the system.</p>
<p>

The operating system communicates with the computing nodes using the Ethernet network. This is also used for diagnostics, configuration and communications with disk storage.</p>

</ss2>
<ss2>
<st>
 Mechanical design </st>
<p>

Two nodes are placed together on a daughter card with one DIMM socket and a 4:1 Ethernet hub for off-card communications. The daughter cards have two connectors, one carrying the internode communications network and one carrying power, Ethernet, clock and other house keeping facilities.</p>
<p>

Thirty-two daughter cards are placed in two rows on a motherboard that supports 800&nbsp;Mbit/s off-board Ethernet communications. Eight motherboards are placed in crates with two backplanes supporting four motherboards each. Each crate consists of 512 processor nodes a and a 26 hypercube communications network. One node consumes about 5&nbsp;W of power and each crate is air and water cooled. A complete system can consist of any number of crates up to several 10,000 computing nodes.</p>

</ss2>
<ss1>
<st>
 Operating system </st>
<p>

The operating system, <b>QOS</b>, is a custom built operating system made to facilitate boot, runtime, monitoring, diagnostics, performance and ease of use of 10.000+ nodes. It uses a custom embedded <link xlink:type="simple" xlink:href="../394/50394.xml">
kernel</link> and provides single process <standard wordnetid="107260623" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../305/23305.xml">
POSIX</link></system_of_measurement>
</standard>
 ("unix-like") compatibility using the Cygnus <link xlink:type="simple" xlink:href="../717/1260717.xml">
newlib</link> library. The kernel includes a specially written <message wordnetid="106598915" confidence="0.8">
<protocol wordnetid="106665108" confidence="0.8">
<standard wordnetid="107260623" confidence="0.8">
<direction wordnetid="106786629" confidence="0.8">
<rule wordnetid="106652242" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../929/31929.xml">
UDP</link></system_of_measurement>
</rule>
</direction>
</standard>
</protocol>
</message>
/<message wordnetid="106598915" confidence="0.8">
<protocol wordnetid="106665108" confidence="0.8">
<direction wordnetid="106786629" confidence="0.8">
<rule wordnetid="106652242" confidence="0.8">
<link xlink:type="simple" xlink:href="../323/15323.xml">
IP</link></rule>
</direction>
</protocol>
</message>
 stack and <message wordnetid="106598915" confidence="0.8">
<protocol wordnetid="106665108" confidence="0.8">
<direction wordnetid="106786629" confidence="0.8">
<rule wordnetid="106652242" confidence="0.8">
<link xlink:type="simple" xlink:href="../252/51252.xml">
NFS</link></rule>
</direction>
</protocol>
</message>
 client for disk access.</p>
<p>

The operating system also maintains system partitions so several users can have access to separate parts of the system for different applications. Each partition will only run one client application at any given time. Any multitasking is scheduled by the host controller system which is a regular computer using a large amounts of Ethernet ports connecting to the QCDOC.</p>
<p>

A consequence of the UNIX-like operating system and the PowerPC based hardware, there's very little effort designing applications for the QCDOC.</p>

</ss1>
</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://phys.columbia.edu/~cqft/">
Computational Quantum Field Theory at Columbia – Columbia University</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://phys.columbia.edu/~cqft/qcdoc/qcdoc.htm">
QCDOC Architecture – Columbia University</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.theregister.co.uk/2008/03/07/edinburgh_qcdoc_calculations/">
UK supercomputer probes secrets of universe – The Register</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.bnl.gov/lqcd/linkable_files/pdf/pap231.pdf">
QCDOC: A 10 Teraﬂops Computer for Tightly-coupled Calculations - Brookhaven National Laboratory</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.research.ibm.com/journal/rd/492/boyle.html">
Overview of the QCDSP and QCDOC computers – IBM</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.scitech.ac.uk/roadmap/rmProject.aspx?q=82">
UKQCD – Science and Technology Facilities Council</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../909/4914909.xml">
Norman Christ</link></person>
</entry>
<entry level="1" type="bullet">

 <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../286/1848286.xml#xpointer(//*[./st=%22PowerPC+440%22])">
PowerPC 440</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../764/136764.xml">
BlueGene/L</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../683/6222683.xml">
Power Architecture</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../153/37153.xml">
Supercomputer</link></entry>
</list>
</p>


</sec>
</bdy>
</article>
