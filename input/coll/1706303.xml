<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:49:34[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Recurrent neural network</title>
<id>1706303</id>
<revision>
<id>242343443</id>
<timestamp>2008-10-01T21:29:13Z</timestamp>
<contributor>
<username>Thijs!bot</username>
<id>1392310</id>
</contributor>
</revision>
<categories>
<category>Miscellaneous articles needing expert attention</category>
<category>Artificial intelligence</category>
<category>Computer science articles needing expert attention</category>
<category>Neural networks</category>
<category>Articles needing expert attention</category>
<category>Pages needing expert attention</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-content" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_content.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>This article or section is in need of attention from an expert on the subject.</b><p>

  may be able to help recruit one.
If a more appropriate  or 
portalexists, please adjust this template accordingly.</p>
</col>
</row>
</table>


A <b>recurrent neural network</b> (RNN) is a class of <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link> where connections between units form a <link xlink:type="simple" xlink:href="../962/878962.xml">
directed cycle</link>. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior.<p>

Recurrent neural networks must be approached differently from <link xlink:type="simple" xlink:href="../332/1706332.xml">
feedforward neural networks</link>, both when analyzing their behavior and training them. Recurrent neural networks can also behave <link xlink:type="simple" xlink:href="../295/6295.xml">
chaotically</link>. Usually, <link xlink:type="simple" xlink:href="../632/990632.xml">
dynamical systems theory</link> is used to model and analyze them. While a feedforward network propagates data linearly from input to output, recurrent networks (RN) also propagate data from later processing stages to earlier stages. </p>

<sec>
<st>
Architectures</st>
<p>

Some of the most common recurrent neural network architectures are described here. The Elman and Jordan networks are also known as "simple recurrent networks" (SRN). </p>

<ss1>
<st>
Elman network</st>
<p>

This variation on the <link xlink:type="simple" xlink:href="../644/2266644.xml">
multilayer perceptron</link> was invented by <link xlink:type="simple" xlink:href="../690/6223690.xml">
Jeff Elman</link>. A three-layer network is used, with the addition of a set of "context units" in the input layer.  There are connections from the middle (hidden) layer to these context units fixed with a weight of one<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. At each time step, the input is propagated in a standard feed-forward fashion, and then a learning rule is applied. The fixed back connections result in the context units always maintaining a copy of the previous values of the hidden units (since they propagate over the connections before the learning rule is applied).  Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.</p>

</ss1>
<ss1>
<st>
Jordan network</st>
<p>

This network architecture is similar to the Elman network. The context units are however fed from the output layer instead of the hidden layer. </p>

</ss1>
<ss1>
<st>
MMC network</st>
<p>

See <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

</ss1>
<ss1>
<st>
Hopfield network</st>
<p>

The <link xlink:type="simple" xlink:href="../097/1170097.xml">
Hopfield network</link> is a recurrent neural network in which all connections are symmetric. Invented by <link xlink:type="simple" xlink:href="../572/649572.xml">
John Hopfield</link> in 1982, this network guarantees that its dynamics will converge. If the connections are trained using <link xlink:type="simple" xlink:href="../084/404084.xml">
Hebbian learning</link> then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.</p>

</ss1>
<ss1>
<st>
Echo state network</st>
<p>

The <link xlink:type="simple" xlink:href="../731/8887731.xml">
echo state network</link> (ESN) is a <link xlink:type="simple" xlink:href="../303/1706303.xml">
recurrent neural network</link> with a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change and be trained. ESN are good to (re)produce temporal patterns.</p>

</ss1>
<ss1>
<st>
Long short term memory network</st>
<p>

The <link xlink:type="simple" xlink:href="../453/10711453.xml">
Long short term memory</link> (LSTM) is an artificial neural net structure that unlike traditional RNNs doesn't have the problem of vanishing gradients. It can therefore use long delays and can handle signals that have a mix of low and high frequency components.</p>

</ss1>
<ss1>
<st>
RNN with parametric bias</st>
<p>

In this setup the recurrent neural network with parametric bias (RNNPB) is trained to reproduce a sequence with a constant bias input. The network is capable of learning different sequences with different parametric biases.  With a trained network is also possible to find the associated parameter for an observed sequence. The sequence is backpropagated through the network to recover the bias which would produce the given sequence.</p>

</ss1>
<ss1>
<st>
Continuous-time RNN</st>
<p>

(CTRNN)</p>

</ss1>
<ss1>
<st>
Hierarchical RNN</st>
<p>

Here RNN are sparsely connected together through bottlenecks with the idea to isolate different hierarchical functions to different parts of the composite network. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>

</ss1>
<ss1>
<st>
Recurrent Multilayer Perceptron</st>
<p>

(RMLP)</p>

</ss1>
<ss1>
<st>
Pollack’s Sequential Cascaded Networks</st>


</ss1>
</sec>
<sec>
<st>
Training</st>
<p>

Training in recurrent neural networks is generally very slow.</p>

<ss1>
<st>
Backpropagation through time (BPTT)</st>
<p>

In this approach the simple recurrent network is unfolded in time for some iterations and then trained through backpropagation as the feed forward network.</p>

</ss1>
<ss1>
<st>
Real-time recurrent learning (RTRL)</st>
<p>

Unlike BPTT this algorithm is <it>local in time but not local in space</it> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></p>

</ss1>
<ss1>
<st>
Genetic algorithms</st>
<p>

Since RNN learning is very slow, <link xlink:type="simple" xlink:href="../254/40254.xml">
genetic algorithm</link>s is a feasible alternative for weight optimization, especially in unstructured networks. </p>

</ss1>
</sec>
<sec>
<st>
 References </st>
<p>

<reflist>
<entry id="1">
Neural Networks as Cybernetic Systems 2nd and revised edition, Holk Cruse
<weblink xlink:type="simple" xlink:href="http://www.brains-minds-media.org/archive/615/bmm615.pdf">
http://www.brains-minds-media.org/archive/615/bmm615.pdf</weblink> </entry>
<entry id="2">
http://www.tech.plym.ac.uk/socce/ncpw9/Kuehn.pdf</entry>
<entry id="3">
<weblink xlink:type="simple" xlink:href="http://www.bdc.brain.riken.go.jp/~rpaine/PaineTaniSAB2004_h.pdf">
Dynamic Representation of Movement Primitives in an Evolved Recurrent Neural Network</weblink></entry>
<entry id="4">
<weblink xlink:type="simple" xlink:href="http://www-clmc.usc.edu/publications/P/paine-NN2004.pdf">
doi:10.1016/j.neunet.2004.08.005</weblink></entry>
<entry id="5">
http://adb.sagepub.com/cgi/reprint/13/3/211.pdf</entry>
<entry id="6">
 Neural and Adaptive Systems: Fundamentals through Simulation. J.C. Principe, N.R. Euliano, W.C. Lefebvre</entry>
</reflist>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Mandic, D. &amp; Chambers, J.&#32;(2001). Recurrent Neural Networks for Prediction: Learning Algorithms, Architectures and Stability.&#32;Wiley.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Elman, J.L.&#32;(1990).&#32;"Finding Structure in Time". <it>Cognitive Science</it>&#32;<b>14</b>: 179–211. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2F0364-0213%2890%2990002-E">
10.1016/0364-0213(90)90002-E</weblink>.</cite>&nbsp;</entry>
</list>
</p>




</sec>
</bdy>
</article>
