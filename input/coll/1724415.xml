<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:50:40[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Human reliability</title>
<id>1724415</id>
<revision>
<id>241480026</id>
<timestamp>2008-09-28T06:19:16Z</timestamp>
<contributor>
<username>Warofdreams</username>
<id>20855</id>
</contributor>
</revision>
<categories>
<category>Reliability engineering</category>
<category>Engineering</category>
</categories>
</header>
<bdy>

"Human error" redirects here.&#32;&#32;For other meanings, see <link xlink:type="simple" xlink:href="../299/7843299.xml">
Human Error</link>.  For other uses, see <link>
Human error (disambiguation)</link>.<p>

<b>Human reliability</b> is related to the field of <link xlink:type="simple" xlink:href="../247/257247.xml">
human factors</link> engineering, and refers to the <link xlink:type="simple" xlink:href="../651/41651.xml">
reliability</link> of <link xlink:type="simple" xlink:href="../482/682482.xml">
human</link>s in fields such as <link xlink:type="simple" xlink:href="../388/39388.xml">
manufacturing</link>, <link xlink:type="simple" xlink:href="../879/18580879.xml">
transport</link>ation, the <link xlink:type="simple" xlink:href="../357/92357.xml">
military</link>, or <link xlink:type="simple" xlink:href="../957/18957.xml">
medicine</link>.  Human <link xlink:type="simple" xlink:href="../515/224515.xml">
performance</link> can be affected by many factors such as <link xlink:type="simple" xlink:href="../539/146539.xml">
age</link>, <link xlink:type="simple" xlink:href="../565/56565.xml">
circadian rhythm</link>s, state of mind, physical <link xlink:type="simple" xlink:href="../381/80381.xml">
health</link>, <link xlink:type="simple" xlink:href="../996/363996.xml">
attitude</link>, <link xlink:type="simple" xlink:href="../406/10406.xml">
emotion</link>s, propensity for certain common mistakes, <link xlink:type="simple" xlink:href="../118/41118.xml">
error</link>s and <link xlink:type="simple" xlink:href="../278/47278.xml">
cognitive bias</link>es, etc.  </p>
<p>

Human reliability is very important due to the contributions of humans to the <link xlink:type="simple" xlink:href="../509/16290509.xml">
resilience</link> of systems and to possible adverse consequences of human errors or oversights, especially when the human is a crucial part of the large <link xlink:type="simple" xlink:href="../202/231202.xml">
socio-technical systems</link> as is common today.  <link xlink:type="simple" xlink:href="../357/504357.xml">
User-centered design</link> and <link xlink:type="simple" xlink:href="../007/1683007.xml">
error-tolerant design</link> are just two of many terms used to describe efforts to make <link xlink:type="simple" xlink:href="../816/29816.xml">
technology</link> better suited to operation by humans.</p>

<sec>
<st>
 Human Reliability Analysis Techniques </st>

<p>

A variety of methods exist for Human Reliability Analysis (HRA) (see Kirwan and Ainsworth, 1992; Kirwan, 1994).  Two general classes of methods are those based on <link xlink:type="simple" xlink:href="../354/2080354.xml">
probabilistic risk assessment</link> (PRA) and those based on a <link xlink:type="simple" xlink:href="../238/106238.xml">
cognitive</link> theory of <link xlink:type="simple" xlink:href="../039/7039.xml">
control</link>.</p>

<ss1>
<st>
PRA-Based Techniques</st>
<p>

One method for analyzing human reliability is a straightforward extension of <link xlink:type="simple" xlink:href="../354/2080354.xml">
probabilistic risk assessment</link> (PRA):  in the same way that equipment can fail in a plant, so can a human operator commit errors. In both cases, an analysis (<link xlink:type="simple" xlink:href="../691/11691.xml">
functional decomposition</link> for equipment and <link xlink:type="simple" xlink:href="../101/1225101.xml">
task analysis</link> for humans) would articulate a level of detail for which failure or error probabilities can be assigned. This basic idea is behind the <link xlink:type="simple" xlink:href="../746/19058746.xml">
Technique for Human Error Rate Prediction</link> (THERP) (Swain &amp; Guttman, 1983).  THERP is intended to generate human error probabilities that would be incorporated into a PRA. The Accident Sequence Evaluation Program (ASEP) Human Reliability Procedure is a simplified form of THERP; an associated computational tool is  <weblink xlink:type="simple" xlink:href="http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=10162198">
Simplified Human Error Analysis Code (SHEAN) (Wilson, 1993)</weblink>.  More recently, the US Nuclear Regulatory Commission has published the Standardized Plant Analysis Risk (SPAR) human reliability analysis method also because of human error (<weblink xlink:type="simple" xlink:href="http://www.nrc.gov/reading-rm/doc-collections/nuregs/contract/cr6883/">
SPAR-H</weblink>) (Gertman et al, 2005).</p>

</ss1>
<ss1>
<st>
Cognitive Control Based Techniques</st>

<p>

Erik Hollnagel has developed this line of thought in his work on the Contextual Control Model (COCOM) (Hollnagel, 1993) and the Cognitive Reliability and Error Analysis Method (CREAM) (Hollnagel, 1998).  COCOM models human performance as a set of control modes -- strategic (based on long-term planning), tactical (based on procedures), opportunistic (based on present context), and scrambled (random) -- and proposes a model of how  transitions between these control modes occur.  This model of control mode transition consists of a number of factors, including the human operator's estimate of the outcome of the action (success or failure),  the time remaining to accomplish the action (adequate or inadequate), and the number of simultaneous goals of the human operator at that time.  <link xlink:type="simple" xlink:href="../066/14583066.xml">
CREAM</link> is a human reliability analysis method that is based on COCOM.</p>

</ss1>
<ss1>
<st>
Related Techniques</st>
<p>

Related techniques in <link xlink:type="simple" xlink:href="../278/29278.xml">
safety engineering</link> and <link xlink:type="simple" xlink:href="../836/1724836.xml">
reliability engineering</link> include <link xlink:type="simple" xlink:href="../631/981631.xml">
Failure mode and effects analysis</link>, <link xlink:type="simple" xlink:href="../723/7051723.xml">
Hazop</link>, <link xlink:type="simple" xlink:href="../526/70526.xml">
Fault tree</link>, and <link xlink:type="simple" xlink:href="../362/149362.xml">
SAPHIRE</link>: Systems Analysis Programs for Hands-on Integrated Reliability Evaluations.</p>

</ss1>
</sec>
<sec>
<st>
 Human Error </st>
<p>

<b>Human <link xlink:type="simple" xlink:href="../118/41118.xml">
error</link></b> has been cited as a cause or contributing factor in disasters and accidents in industries as diverse as nuclear power (e.g., <trouble wordnetid="107289014" confidence="0.8">
<mishap wordnetid="107314427" confidence="0.8">
<misfortune wordnetid="107304852" confidence="0.8">
<accident wordnetid="107301336" confidence="0.8">
<happening wordnetid="107283608" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../856/105856.xml">
Three Mile Island accident</link></psychological_feature>
</event>
</happening>
</accident>
</misfortune>
</mishap>
</trouble>
), aviation (see <link xlink:type="simple" xlink:href="../078/2328078.xml">
pilot error</link>), space exploration (e.g., <link xlink:type="simple" xlink:href="../717/403717.xml">
Space Shuttle Challenger Disaster</link>), and medicine (see <link xlink:type="simple" xlink:href="../324/718324.xml">
medical error</link>). It is also important to stress that "human error" mechanisms are the same as "human performance" mechanisms; performance later categorized as 'error' is done so in hindsight (Reason, 1991; Woods, 1990): thereofore actions later termed  "human error" are actually part of the ordinary spectrum of human behaviour. The study of <link xlink:type="simple" xlink:href="../148/8339148.xml">
absent-mindedness</link> in everyday life provides ample documentation and categorization of such aspects of behavior. Recently, human error has been reconceptualized as <link xlink:type="simple" xlink:href="../509/16290509.xml">
 resiliency</link> to emphasize the positive aspects that humans bring to the operation of technical systems (see Hollnagel, Woods and Leveson, 2006).</p>

<ss1>
<st>
Categories of Human Error</st>
<p>

There are many ways to categorize human error (see Jones, 1999).
<list>
<entry level="1" type="bullet">

 exogenous versus endogenous (i.e., originating outside versus inside the individual) (Senders and Moray, 1991)</entry>
<entry level="1" type="bullet">

 situation assessment versus response planning (e.g., Roth et al, 1994) and related distinctions in </entry>
<entry level="2" type="bullet">

 errors in problem detection (also see <link xlink:type="simple" xlink:href="../527/1156527.xml">
signal detection theory</link>)</entry>
<entry level="2" type="bullet">

 errors in problem diagnosis (also see <link xlink:type="simple" xlink:href="../948/1467948.xml">
problem solving</link>)</entry>
<entry level="2" type="bullet">

 errors in action planning and execution (Sage, 1992) (for example: <b>slips</b> or errors of execution versus <b>mistakes</b> or errors of intention; see Norman, 1988; Reason, 1991)</entry>
<entry level="1" type="bullet">

 By level of analysis; for example, perceptual (e.g., <link xlink:type="simple" xlink:href="../497/53497.xml">
optical illusions</link>) versus cognitive versus <link xlink:type="simple" xlink:href="../052/228052.xml">
communication</link> versus <link xlink:type="simple" xlink:href="../262/1088262.xml">
organizational</link>.</entry>
</list>
</p>
<p>

The <link xlink:type="simple" xlink:href="../238/106238.xml">
cognitive</link> study of human error is a very active research field, including work related to limits of <link xlink:type="simple" xlink:href="../844/18844.xml">
memory</link> and <link xlink:type="simple" xlink:href="../753/68753.xml">
attention</link> and also to <link xlink:type="simple" xlink:href="../752/265752.xml">
decision making</link> strategies such as the <link xlink:type="simple" xlink:href="../396/403396.xml">
availability heuristic</link> and other <link xlink:type="simple" xlink:href="../791/510791.xml">
cognitive biases</link>.  Such heuristics and biases are strategies that are useful and often correct, but can lead to systematic patterns of error.</p>
<p>

Misunderstandings as a topic in human communication have been studied in <link xlink:type="simple" xlink:href="../874/379874.xml">
Conversation Analysis</link>, such as the examination of violations of the <work wordnetid="100575741" confidence="0.8">
<examination wordnetid="100635850" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<survey wordnetid="100644503" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../389/1745389.xml">
Cooperative principle</link></activity>
</psychological_feature>
</act>
</investigation>
</survey>
</event>
</examination>
</work>
 and <link xlink:type="simple" xlink:href="../839/1085839.xml">
Gricean maxims</link>.</p>
<p>

<link xlink:type="simple" xlink:href="../262/1088262.xml">
Organizational</link> studies of error or dysfunction have included studies of <link xlink:type="simple" xlink:href="../173/2994173.xml">
safety culture</link>. One technique for organizational analysis is the Management Oversight Risk Tree (MORT) (Kirwan and Ainsworth, 1992; also search for MORT on the  <weblink xlink:type="simple" xlink:href="http://www2.hf.faa.gov/workbenchtools/">
FAA Human Factors Workbench</weblink>.</p>

</ss1>
<ss1>
<st>
Human Factors Analysis and Classification System (HFACS)</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../038/12215038.xml">
Human Factors Analysis and Classification System</link></it>
</indent>
:<it>See Human Factors Analysis and Classification System in Main article: <link xlink:type="simple" xlink:href="../713/12038713.xml">
National Fire Fighter Near-Miss Reporting System</link></it>
The Human Factors Analysis and Classification System (HFACS) was developed initially as a framework to understand "human error" as a cause of aviation accidents (Shappell and Wiegmann, 2000; Wiegmann and Shappell, 2003).  It is based on James Reason's <link xlink:type="simple" xlink:href="../589/9162589.xml">
Swiss cheese model</link> of human error in complex systems. HFACS distinguishes between the "active failures" of unsafe acts, and "latent failures" of preconditions for unsafe acts, unsafe supervision, and organizational influences.  These categories were developed empirically on the basis of many aviation accident reports.</p>
<p>

<b>Unsafe acts</b> are performed by the human operator "on the front line" (e.g., the pilot, the air traffic controller, the driver).  Unsafe acts can be either errors (in perception, decision making or skill-based performance) or violations (routine or exceptional).  The "errors" here are similar to the above discussion.  Violations are the deliberate disregard for rules and procedures. As the name implies, routine violations are those that occur habitually and are usually tolerated by the organization or authority. Exceptional violations are unusual and often extreme. For example, driving 60 mph in a 55-mph zone speed limit is a routine violation, but driving 130 mph in the same zone is exceptional.</p>
<p>

There are two types of <b>preconditions for unsafe acts</b>: those that relate to the human operator's internal state and those that relate to the human operator's practices or ways of working.   Adverse internal states include those related to physiology (e.g., illness) and mental state (e.g., mentally fatigued, distracted). A third aspect of 'internal state' is really a mismatch between the operator's ability and the task demands; for example, the operator may be unable to make visual judgments or react quickly enough to support the task at hand.    Poor operator practices are another type of precondition for unsafe acts. These include poor crew resource management (issues such as leadership and communication) and poor personal readiness practices (e.g., violating the crew rest requirements in aviation).</p>
<p>

Four types of <b>unsafe supervision</b> are: Inadequate supervision; Planned inappropriate operations; Failure to correct a known problem; and Supervisory violations.  </p>
<p>

<b>Organizational influences</b> include those related to resources management (e.g., inadequate human or financial resources), organizational climate (structures, policies, and culture), and organizational processes (such as procedures, schedules, oversight).</p>

</ss1>
<ss1>
<st>
Controversies</st>
<p>

Some researchers have argued that the dichotomy of human actions as "correct" or "incorrect" is a harmful oversimplification of a complex phenomena (see Hollnagel and Amalberti, 2001). A focus on the variability of human performance and how human operators (and organizations) can manage that variability may be a more fruitful approach. Furthermore, as noted above, the concept of "resiliency" highlights the positive roles that humans can play in complex systems.</p>

</ss1>
<ss1>
<st>
See Also</st>
<p>

CCPS, Guidelines for Preventing Human Error. This book explains about qualitative and quantitative methodology for predicting human error. Qualitative methodology called SPEAR: Systems for Predicting Human Error and Recovery, and quantitative methodology also includes THERP, etc.</p>

</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link>
Performance shaping factor</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../112/14266112.xml">
Latent human error</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Gertman, D. L. and Blackman, H. S.&#32;(2001). Human reliability and safety analysis data handbook.&#32;Wiley.</cite>&nbsp; </entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Gertman, D., Blackman, H., Marble, J., Byers, J. and Smith, C.&#32;(2005). The SPAR-H human reliability analysis method. NUREG/CR-6883.  Idaho National Laboratory, prepared for U. S. Nuclear Regulatory Commission.</cite>&nbsp;<weblink xlink:type="simple" xlink:href="http://www.nrc.gov/reading-rm/doc-collections/nuregs/contract/cr6883/">
http://www.nrc.gov/reading-rm/doc-collections/nuregs/contract/cr6883/</weblink></entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Hollnagel, E.&#32;(1993). Human reliability analysis: Context and control.&#32;Academic Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Hollnagel, E.&#32;(1998). Cognitive reliability and error analysis method: CREAM.&#32;Elsevier.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Hollnagel, E. and Amalberti, R.&#32;(2001). The Emperor’s New Clothes, or whatever happened to “human error”? Invited keynote presentation at 4th International Workshop on Human Error, Safety and System Development..&#32;Linköping, June 11-12, 2001.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Hollnagel, E., Woods, D. D., and Leveson, N. (Eds.)&#32;(2006). Resilience engineering: Concepts and precepts.&#32;Ashgate.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Jones, P. M.&#32;(1999). Human error and its amelioration. In Handbook of Systems Engineering and Management (A. P. Sage and W. B. Rouse, eds.), 687-702.&#32;Wiley.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Kirwan, B.&#32;(1994). A practical guide to human reliability assessment.&#32;Taylor &amp; Francis.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Kirwan, B. and Ainsworth, L. (Eds.)&#32;(1992). A guide to task analysis.&#32;Taylor &amp; Francis.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Norman, D.&#32;(1988). The psychology of everyday things.&#32;Basic Books.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Reason, J.&#32;(1990). Human error.&#32;Cambridge University Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Roth, E. et al&#32;(1994). An empirical investigation of operator performance in cognitive demanding simulated emergencies. NUREG/CR-6208, Westinghouse Science and Technology Center.&#32;Report prepared for Nuclear Regulatory Commission.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Sage, A. P.&#32;(1992). Systems engineering.&#32;Wiley.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Senders, J. and Moray, N.&#32;(1991). Human error: Cause, prediction, and reduction.&#32;Lawrence Erlbaum Associates.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Shappell, S. &amp; Wiegmann, D.&#32;(2000). The human factors analysis and classification system - HFACS.  DOT/FAA/AM-00/7, Office of Aviation Medicine, Federal Aviation Administration, Department of Transportation..</cite>&nbsp;<weblink xlink:type="simple" xlink:href="http://www.nifc.gov/safety_study/accident_invest/humanfactors_class&amp;anly.pdf">
http://www.nifc.gov/safety_study/accident_invest/humanfactors_class&amp;anly.pdf</weblink></entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Swain, A. D., &amp; Guttman, H. E.&#32;(1983). Handbook of human reliability analysis with emphasis on nuclear power plant applications..&#32;NUREG/CR-1278 (Washington D.C.).</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Wiegmann, D. &amp; Shappell, S.&#32;(2003). A human error approach to aviation accident analysis: The human factors analysis and classification system..&#32;Ashgate.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Wilson, J.R.&#32;(1993). SHEAN (Simplified Human Error Analysis code) and automated THERP.&#32;United States Department of Energy Technical Report Number WINCO--11908.</cite>&nbsp; <weblink xlink:type="simple" xlink:href="http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=10162198">
http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=10162198</weblink></entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Woods, D. D.&#32;(1990). Modeling and predicting human error. In J. Elkind, S. Card, J. Hochberg, and B. Huey (Eds.), Human performance models for computer-aided engineering (248-274).&#32;Academic Press.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
Further reading</st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Autrey, T.D.&#32;(2007). [Mistake-Proofing Six Sigma: How to Minimize Project Scope and Reduce Human Error]http://www.practicingperfectioninstitute.com/reports/sixsigma.aspx.&#32;Practicing Perfection Institute.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Davies, J.B., Ross, A., Wallace, B. and Wright, L.&#32;(2003). Safety Management: a Qualitative Systems Approach.&#32;Taylor and Francis.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Dismukes, R. K., Berman, B. A., and Loukopoulos, L. D.&#32;(2007). The limits of expertise:  Rethinking pilot error and the causes of airline accidents.&#32;Ashgate.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Forester, J., Kolaczkowski, A., Lois, E., and Kelly, D.&#32;(2006). Evaluation of human reliability analysis methods against good practices.  NUREG-1842 Final Report.&#32;U. S. Nuclear Regulatory Commission.</cite>&nbsp; <weblink xlink:type="simple" xlink:href="http://www.nrc.gov/reading-rm/doc-collections/nuregs/staff/sr1842/">
http://www.nrc.gov/reading-rm/doc-collections/nuregs/staff/sr1842/</weblink></entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Goodstein, L. P., Andersen, H. B., and Olsen, S. E. (Eds.)&#32;(1988). Tasks, errors, and mental models.&#32;Taylor and Francis.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Grabowski, M. and Roberts, K. H.&#32;(1996). <weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1109/3468.477856">
Human and organizational error in large scale systems</weblink> , IEEE Transactions on Systems, Man, and Cybernetics, Volume 26, No. 1, January 1996, 2-16.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Greenbaum, J. and Kyng, M. (Eds.)&#32;(1991). Design at work: Cooperative design of computer systems.&#32;Lawrence Erlbaum Associates.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Harrison, M.&#32;(2004). Human error analysis and reliability assessment.&#32;Workshop on Human Computer Interaction and Dependability, 46th IFIP Working Group 10.4 Meeting, Siena, Italy, July 3-7, 2004.</cite>&nbsp; <weblink xlink:type="simple" xlink:href="http://www.laas.fr/IFIPWG/Workshops&amp;Meetings/46/05-Harrison.pdf">
http://www.laas.fr/IFIPWG/Workshops&amp;Meetings/46/05-Harrison.pdf</weblink></entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Hollnagel, E.&#32;(1991). The phenotype of erroneous actions: Implications for HCI design. In G. W. R. Weir and J. L. Alty (Eds.), Human-computer interaction and complex systems.&#32;Academic Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Hutchins, E.&#32;(1995). Cognition in the wild.&#32;MIT Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Kahneman, D., Slovic, P. and Tversky, A. (Eds.)&#32;(1982). Judgment under uncertainty: Heuristics and biases.&#32;Cambridge University Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Leveson, N.&#32;(1995). Safeware: System safety and computers.&#32;Addison-Wesley.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Morgan, G.&#32;(1986). Images of organization.&#32;Sage.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Mura, S. S.&#32;(1983). Licensing violations: Legitimate violations of Grice's conversational principle. In R. Craig and K. Tracy (Eds.), Conversational coherence: Form, structure, and strategy (101-115).&#32;Sage.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Perrow, C.&#32;(1984). Normal accidents: Living with high-risk technologies.&#32;Basic Books.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Rasmussen, J.&#32;(1983). Skills, rules, and knowledge: Signals, signs, and symbols and other distinctions in human performance models. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13, 257-267.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Rasmussen, J.&#32;(1986). Information processing and human-machine interaction: An approach to cognitive engineering.&#32;Wiley.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Silverman, B.&#32;(1992). Critiquing human error: A knowledge-based human-computer collaboration approach.&#32;Academic Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Swets, J.&#32;(1996). Signal detection theory and ROC analysis in psychology and diagnostics: Collected papers.&#32;Lawrence Erlbaum Associates.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Tversky, A. and Kahneman, D.&#32;(1974). Judgment under uncertainty: Heuristics and biases. Science, 185, 1124-1131.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Vaughan, D.&#32;(1996). The Challenger launch decision: Risky technology, culture, and deviance at NASA.&#32;University of Chicago Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Wallace, B. and Ross, A.&#32;(2006). Beyond human error.&#32;CRC Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Woods, D. D., Johannesen, L., Cook, R., and Sarter, N.&#32;(1994). Behind human error: Cognitive systems, computers, and hindsight. CSERIAC SOAR Report 94-01.&#32;Crew Systems Ergonomics Information Analysis Center, Wright-Patterson Air Force Base, Ohio.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>


<ss1>
<st>
Standards and Guidance Documents</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://stdsbbs.ieee.org/descr/1082-1997/">
IEEE Standard 1082 (1997):  IEEE Guide for Incorporating Human Action Reliability Analysis for Nuclear Power Generating Stations</weblink></entry>
</list>
</p>

</ss1>
<ss1>
<st>
Tools</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.eurocontrol.int/hifa/public/standard_page/Hifa_HifaData_Tools_HumErr.html">
Eurocontrol Human Error Tools</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.epri.com/hra/discuss.html">
EPRI HRA Calculator</weblink></entry>
</list>
</p>

</ss1>
<ss1>
<st>
Research Labs</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.ida.liu.se/~eriho/">
Erik Hollnagel</weblink> at the <weblink xlink:type="simple" xlink:href="http://www.ida.liu.se/labs/cselab/">
Cognitive Systems Engineering Laboratory</weblink> at <link xlink:type="simple" xlink:href="../529/165529.xml">
Linkoping University</link></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://reliability.sandia.gov/Human_Factor_Engineering/Human_Reliability_Analysis/human_reliability_analysis.html">
Human Reliability Analysis</weblink> at the US <site wordnetid="108651247" confidence="0.8">
<region wordnetid="108630985" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<weapon wordnetid="104565375" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<instrument wordnetid="103574816" confidence="0.8">
<link xlink:type="simple" xlink:href="../558/155558.xml">
Sandia National Laboratories</link></instrument>
</geographical_area>
</device>
</weapon>
</tract>
</location>
</instrumentality>
</artifact>
</region>
</site>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.orau.gov/chrs/chrs.htm">
Center for Human Reliability Studies</weblink> at the US <site wordnetid="108651247" confidence="0.8">
<region wordnetid="108630985" confidence="0.8">
<center wordnetid="108523483" confidence="0.8">
<area wordnetid="108497294" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<link xlink:type="simple" xlink:href="../147/38147.xml">
Oak Ridge National Laboratory</link></geographical_area>
</tract>
</location>
</area>
</center>
</region>
</site>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://hsi.arc.nasa.gov/flightcognition/|">
Flight Cognition Laboratory</weblink> at <artifact wordnetid="100021939" confidence="0.8">
<science_museum wordnetid="104147364" confidence="0.8">
<facility wordnetid="103315023" confidence="0.8">
<depository wordnetid="103177349" confidence="0.8">
<museum wordnetid="103800563" confidence="0.8">
<link xlink:type="simple" xlink:href="../477/47477.xml">
NASA Ames Research Center</link></museum>
</depository>
</facility>
</science_museum>
</artifact>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://csel.eng.ohio-state.edu/woods/">
David Woods </weblink> at the <weblink xlink:type="simple" xlink:href="http://csel.eng.ohio-state.edu/">
Cognitive Systems Engineering Laboratory</weblink> at The <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../217/22217.xml">
Ohio State University</link></university>
</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Media coverage </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.iienet.org/magazine/magazinefiles/IENOV2004_outliers_p66.pdf">
“Human Reliability. We break down just like machines“</weblink> Industrial Engineer - November 2004, 36(11): 66</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Networking </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.linkedin.com/groups?gid=673677">
High Reliability Management group at LinkedIn.com</weblink></entry>
</list>
</p>


</ss1>
</sec>
</bdy>
</article>
