<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:25:47[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Perceptron</title>
<id>172777</id>
<revision>
<id>243852936</id>
<timestamp>2008-10-08T09:46:13Z</timestamp>
<contributor>
<username>Boleyn</username>
<id>6127189</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Machine learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

The <b>perceptron</b> is a type of <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link> invented in <link xlink:type="simple" xlink:href="../606/34606.xml">
1957</link> at the <link xlink:type="simple" xlink:href="../328/2220328.xml">
Cornell Aeronautical Laboratory</link> by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../462/1945462.xml">
Frank Rosenblatt</link></psychologist>
</scientist>
</causal_agent>
</person>
</physical_entity>
. It can be seen as the simplest kind of <link xlink:type="simple" xlink:href="../332/1706332.xml">
feedforward neural network</link>: a <link xlink:type="simple" xlink:href="../974/98974.xml">
linear classifier</link>.
<sec>
<st>
 Definition </st>
<p>

The Perceptron uses <link xlink:type="simple" xlink:href="../728/19008728.xml">
matrix</link> <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalue</link>s to represent feedforward neural networks and is a tertiary classifier that maps its input <math>x</math> (a <link xlink:type="simple" xlink:href="../823/3823.xml">
binary</link> <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../370/32370.xml">
vector</link></concept>
</idea>
) to an output value <math>f(x)</math> (a single binary value) across the matrix.</p>
<p>

<indent level="1">

<math>
f(x) = \begin{cases}1 &amp; \text{if }w \cdot x + b &amp;gt; 0\\0 &amp; \text{else}\end{cases}
</math>
</indent>

where <math>w</math> is a vector of real-valued weights and <math>w \cdot x</math> is the <link xlink:type="simple" xlink:href="../093/157093.xml">
dot product</link> (which computes a weighted sum). <math>b</math> is the 'bias', a constant term that does not depend on any input value. </p>
<p>

The value of <math>f(x)</math> (0 or 1) is used to classify <math>x</math> as either a positive or a negative instance, in the case of a binary classification problem.  The bias can be thought of as offsetting the activation function, or giving the output neuron a "base" level of activity. If <math>b</math> is negative, then the weighted combination of inputs must produce a positive value greater than <math>-b</math> in order to push the classifier neuron over the 0 threshold. Spatially, the bias alters the position (though not the orientation) of the <link xlink:type="simple" xlink:href="../979/2714979.xml">
decision boundary</link>.</p>
<p>

Since the inputs are fed directly to the output unit via the weighted connections, the perceptron can be considered the simplest kind of feed-forward neural network.</p>

</sec>
<sec>
<st>
Learning algorithm</st>
<p>

The learning algorithm is the same across all neurons, therefore everything that follows is applied to a single neuron in isolation.  We first define some variables:
<list>
<entry level="1" type="bullet">

<math>x(j)</math> denotes the j-th item in the input vector</entry>
<entry level="1" type="bullet">

<math>w(j)</math> denotes the j-th item in the weight vector</entry>
<entry level="1" type="bullet">

<math>y</math> denotes the output from the neuron</entry>
<entry level="1" type="bullet">

<math>\delta</math> denotes the expected output</entry>
<entry level="1" type="bullet">

<math>\alpha</math> is a constant and <math>0 &amp;lt; \alpha &amp;lt; 1</math> (learning rate)</entry>
</list>
</p>
<p>

<image location="right" width="300px" src="Perceptron.gif" type="thumb">
<caption>

the appropriate weights are applied to the inputs that passed to a function which produces the output y
</caption>
</image>
</p>
<p>

The weights are updated after each input according to the update rule below:
<indent level="1">

<math>w(j)' = w(j) + \alpha(\delta-y)x(j)\,</math> 
</indent>

Therefore, learning is modeled as the weight vector being updated after one iteration, which will only take place if the output <math>y</math> is different from the desired output <math>\delta</math>.  Still considering a single neuron but trying to incorporate multiple iterations, let us first define some more variables:</p>
<p>

<list>
<entry level="1" type="bullet">

<math>x_i</math> denotes the input vector for the i-th iteration</entry>
<entry level="1" type="bullet">

<math>w_i</math> denotes the weight vector for the i-th iteration</entry>
<entry level="1" type="bullet">

<math>y_i</math> denotes the output for the i-th iteration</entry>
<entry level="1" type="bullet">

<math>D_m = \{(x_1,y_1),\dots,(x_m,y_m)\}</math> denotes a training set of <math>m</math> iterations</entry>
</list>
</p>

<p>

Each iteration the weight vector is updated as follows
<list>
<entry level="1" type="bullet">

For each <math>(x,y)</math> pair in <math>D_m = \{(x_1,y_1),\dots,(x_m,y_m)\}</math></entry>
<entry level="1" type="bullet">

Pass <math>(x_i, y_i, w_i)</math> to the update rule <math>w(j)' = w(j) + \alpha(\delta-y)x(j)</math></entry>
</list>
</p>
<p>

The training set <math>D_m</math> is said to be <link xlink:type="simple" xlink:href="../173/523173.xml">
linearly separable</link> if there exists a positive constant <math>\gamma </math> and a weight vector <math>w</math> such that <math>y_i \cdot\left( \langle w, x_i \rangle +b \right) &amp;gt; \gamma </math> for all <math>i</math>.  Novikoff (1962) proved that the perceptron algorithm converges after a finite number of iterations if the
<link xlink:type="simple" xlink:href="../495/8495.xml">
data set</link> is linearly separable and the number of mistakes is bounded by <math>\left(\frac{2R}{\gamma}\right)^2</math>.</p>
<p>

However, if the training set is not <link xlink:type="simple" xlink:href="../173/523173.xml">
linearly separable</link>, the above online algorithm is not guaranteed to converge.</p>

</sec>
<sec>
<st>
 Variants </st>
<p>

The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far "in its pocket". The pocket algorithm then returns the solution in the pocket, rather than the last solution.</p>
<p>

The <math>\alpha</math>-perceptron further utilised a preprocessing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify  patterns, by projecting them into a <link xlink:type="simple" xlink:href="../613/73613.xml">
binary space</link>. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.</p>
<p>

As an example, consider the case of having to classify data into two classes. Here is a small such data set, consisting of two points coming from two <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian distribution</link>s.
<image width="150px" src="Two_class_Gaussian_data.png">
<caption>

Two class gaussian data
</caption>
</image>

<image width="150px" src="Linear_classifier_on_Gaussian_data.png">
<caption>

A linear classifier operating on the original space
</caption>
</image>

<image width="150px" src="Hidden_space_linear_classifier_on_Gaussian_data.png">
<caption>

A linear classifier operating on a high-dimensional projection
</caption>
</image>
</p>

<p>

A linear classifier can only separate things with a <link xlink:type="simple" xlink:href="../862/99862.xml">
hyperplane</link>, so it's not possible to perfectly classify all the examples. On the other hand, we may project the data into a large number of dimensions. In this case a <link xlink:type="simple" xlink:href="../765/1648765.xml">
random matrix</link> was used to project the data linearly to a 1000-dimensional space; then each resulting data point was transformed through the <link xlink:type="simple" xlink:href="../567/56567.xml">
 hyperbolic tangent function</link>. A linear classifier can then separate the data, as shown in the third figure. However the data may still not be completely separable in this space, in which the perceptron algorithm would not converge. In the example shown, <link xlink:type="simple" xlink:href="../641/1180641.xml">
 stochastic steepest gradient descent</link> was used to adapt the parameters.</p>
<p>

Furthermore, by adding nonlinear layers between the input and output, one can separate all data and indeed, with enough training data, model any well-defined function to arbitrary precision. This model is a generalization known as a <link xlink:type="simple" xlink:href="../644/2266644.xml">
multilayer perceptron</link>.</p>
<p>

It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal.</p>
<p>

Other training algorithms for linear classifiers are possible: see, e.g., <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link> and <link xlink:type="simple" xlink:href="../631/226631.xml">
logistic regression</link>.</p>

</sec>
<sec>
<st>
 Example </st>

<p>

A perceptron (X1, X2 input, X0*W0=b, TH=0.5) learns how to perform a <link xlink:type="simple" xlink:href="../001/510001.xml">
NAND</link> function:</p>
<p>

<table class="wikitable">
<row>
<col height="13" width="50" valign="bottom"></col>
<col width="50" valign="bottom"></col>
<col colspan="4" align="center" width="50" valign="bottom">
Input</col>
<col colspan="3" align="center" width="40" valign="bottom">
Initial</col>
<col colspan="3" align="center" width="40" valign="bottom">
Output</col>
<col width="35" valign="bottom"></col>
<col width="70" valign="bottom"></col>
<col width="35" valign="bottom"></col>
<col width="70" valign="bottom"></col>
<col colspan="3" align="center" width="40" valign="bottom">
Final</col>
</row>
<row>
<col height="40" valign="bottom">
Threshold</col>
<col valign="bottom">
Learning Rate</col>
<col colspan="3" align="center" valign="bottom">
Sensor values</col>
<col valign="bottom">
Desired output</col>
<col colspan="3" align="center" valign="bottom">
Weights</col>
<col colspan="3" align="center" valign="bottom">
Calculated</col>
<col align="center" valign="bottom">
Sum</col>
<col align="center" valign="bottom">
Network</col>
<col align="center" valign="bottom">
Error</col>
<col align="center" valign="bottom">
Correction</col>
<col colspan="3" align="center" valign="bottom">
Weights</col>
</row>
<row align="center" valign="bottom">
<col height="13">
TH</col>
<col>
LR</col>
<col>
X0</col>
<col>
X1</col>
<col>
X2</col>
<col>
Z</col>
<col>
w0</col>
<col>
w1</col>
<col>
w2</col>
<col>
C0</col>
<col>
C1</col>
<col>
C2</col>
<col>
S</col>
<col>
N</col>
<col>
E</col>
<col>
R</col>
<col>
W0</col>
<col>
W1</col>
<col>
W2</col>
</row>
<row align="center" valign="bottom">
<col height="10"></col>








<col>
X0 x w0</col>
<col>
X1 x w1</col>
<col>
X2 x w2</col>
<col>
C0+C1+C2</col>
<col>
IF(S&amp;gt;TH,1,0)</col>
<col>
Z-N</col>
<col>
LR x E</col>



</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.1</col>
<col>
0</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.1</col>
<col>
0</col>
<col>
0</col>
<col>
0.1</col>
<col>
0</col>
<col>
0</col>
<col>
0.1</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.2</col>
<col>
0</col>
<col>
0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.2</col>
<col>
0</col>
<col>
0.1</col>
<col>
0.2</col>
<col>
0</col>
<col>
0</col>
<col>
0.2</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.3</col>
<col>
0.1</col>
<col>
0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
0.3</col>
<col>
0.1</col>
<col>
0.1</col>
<col>
0.3</col>
<col>
0.1</col>
<col>
0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0.3</col>
<col>
0.1</col>
<col>
0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0.3</col>
<col>
0.1</col>
<col>
0.1</col>
<col>
0.3</col>
<col>
0</col>
<col>
0</col>
<col>
0.3</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.4</col>
<col>
0.1</col>
<col>
0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.4</col>
<col>
0.1</col>
<col>
0.1</col>
<col>
0.4</col>
<col>
0</col>
<col>
0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.5</col>
<col>
0.1</col>
<col>
0.2</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.5</col>
<col>
0.1</col>
<col>
0.2</col>
<col>
0.5</col>
<col>
0.1</col>
<col>
0</col>
<col>
0.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0.1</col>
<col>
0.2</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
0.5</col>
<col>
0.1</col>
<col>
0.2</col>
<col>
0.5</col>
<col>
0.1</col>
<col>
0.2</col>
<col>
0.8</col>
<col>
1</col>
<col>
-1</col>
<col>
-0.1</col>
<col>
0.4</col>
<col>
0</col>
<col>
0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0.4</col>
<col>
0</col>
<col>
0.1</col>
<col>
0.4</col>
<col>
0</col>
<col>
0</col>
<col>
0.4</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0.1</col>
<col>
0.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0</col>
<col>
0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.6</col>
<col>
0.1</col>
<col>
0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
0.6</col>
<col>
0.1</col>
<col>
0.1</col>
<col>
0.6</col>
<col>
0.1</col>
<col>
0.1</col>
<col>
0.8</col>
<col>
1</col>
<col>
-1</col>
<col>
-0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
1</col>
<col>
-1</col>
<col>
-0.1</col>
<col>
0.5</col>
<col>
-0.1</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0.5</col>
<col>
-0.1</col>
<col>
-0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.6</col>
<col>
-0.1</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.6</col>
<col>
-0.1</col>
<col>
-0.1</col>
<col>
0.6</col>
<col>
0</col>
<col>
-0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.7</col>
<col>
-0.1</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.7</col>
<col>
-0.1</col>
<col>
0</col>
<col>
0.7</col>
<col>
-0.1</col>
<col>
0</col>
<col>
0.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
-0.1</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
0.7</col>
<col>
-0.1</col>
<col>
0</col>
<col>
0.7</col>
<col>
-0.1</col>
<col>
0</col>
<col>
0.6</col>
<col>
1</col>
<col>
-1</col>
<col>
-0.1</col>
<col>
0.6</col>
<col>
-0.2</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0.6</col>
<col>
-0.2</col>
<col>
-0.1</col>
<col>
0.6</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.6</col>
<col>
-0.2</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.6</col>
<col>
-0.2</col>
<col>
-0.1</col>
<col>
0.6</col>
<col>
0</col>
<col>
-0.1</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
0</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
0</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.8</col>
<col>
-0.1</col>
<col>
0</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
0.8</col>
<col>
-0.1</col>
<col>
0</col>
<col>
0.8</col>
<col>
-0.1</col>
<col>
0</col>
<col>
0.7</col>
<col>
1</col>
<col>
-1</col>
<col>
-0.1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.1</col>
<col>
0.7</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.1</col>
<col>
0.7</col>
<col>
0</col>
<col>
-0.1</col>
<col>
0.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
0</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.8</col>
<col>
-0.1</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
0.8</col>
<col>
-0.1</col>
<col>
-0.1</col>
<col>
0.8</col>
<col>
-0.1</col>
<col>
-0.1</col>
<col>
0.6</col>
<col>
1</col>
<col>
-1</col>
<col>
-0.1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.2</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.2</col>
<col>
0.7</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.2</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.7</col>
<col>
-0.2</col>
<col>
-0.2</col>
<col>
0.7</col>
<col>
0</col>
<col>
-0.2</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
+0.1</col>
<col>
0.8</col>
<col>
-0.2</col>
<col>
-0.1</col>
</row>
<row align="center" valign="bottom">
<col height="13">
<b> 0.5</b></col>
<col>
<b>0.1</b></col>
<col>
<b>1</b></col>
<col>
<b>1</b></col>
<col>
<b>0</b></col>
<col>
<b>1</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>0</b></col>
<col>
<b>0.6</b></col>
<col>
<b>1</b></col>
<col>
<b>0</b></col>
<col>
<b>0</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
</row>
<row align="center" valign="bottom">
<col height="13">
<b> 0.5</b></col>
<col>
<b>0.1</b></col>
<col>
<b>1</b></col>
<col>
<b>1</b></col>
<col>
<b>1</b></col>
<col>
<b>0</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
<col>
<b>0.5</b></col>
<col>
<b>0</b></col>
<col>
<b>0</b></col>
<col>
<b>0</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
</row>
<row align="center" valign="bottom">
<col height="13">
<b> 0.5</b></col>
<col>
<b>0.1</b></col>
<col>
<b>1</b></col>
<col>
<b>0</b></col>
<col>
<b>0</b></col>
<col>
<b>1</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
<col>
<b>0.8</b></col>
<col>
<b>0</b></col>
<col>
<b>0</b></col>
<col>
<b>0.8</b></col>
<col>
<b>1</b></col>
<col>
<b>0</b></col>
<col>
<b>0</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
</row>
<row align="center" valign="bottom">
<col height="13">
<b> 0.5</b></col>
<col>
<b>0.1</b></col>
<col>
<b>1</b></col>
<col>
<b>0</b></col>
<col>
<b>1</b></col>
<col>
<b>1</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
<col>
<b>0.8</b></col>
<col>
<b>0</b></col>
<col>
<b>-0.1</b></col>
<col>
<b>0.7</b></col>
<col>
<b>1</b></col>
<col>
<b>0</b></col>
<col>
<b>0</b></col>
<col>
<b>0.8</b></col>
<col>
<b>-0.2</b></col>
<col>
<b>-0.1</b></col>
</row>
</table>
</p>
<p>

Note: Initial weight equals final weight of previous iteration. A too high learning rate makes the perceptron periodically oscillate around the solution. A possible enhancement is to use <math>LR^n</math>  starting with n=1 and incrementing it by 1 when a loop in learning is found.</p>

</sec>
<sec>
<st>
 History </st>
<p>

<indent level="1">

<it>See also: <link xlink:type="simple" xlink:href="../560/2894560.xml#xpointer(//*[./st=%22Perceptrons+and+the+dark+age+of+connectionism%22])">
History of artificial intelligence</link>, <link xlink:type="simple" xlink:href="../574/3548574.xml#xpointer(//*[./st=%22The+abandonment+of+perceptrons+in+1969%22])">
AI Winter</link> and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../462/1945462.xml">
Frank Rosenblatt</link></psychologist>
</scientist>
</causal_agent>
</person>
</physical_entity>
</it>
</indent>
Although the perceptron initially seemed promising, it was eventually proved that perceptrons could not be trained to recognise many classes of patterns. This led to the field of neural network research stagnating for many years, before it was recognised that a feedforward neural network with three or more layers (also called a <link>
multilayer perceptron</link>) had far greater processing power than perceptrons with one layer (also called a <link>
single layer perceptron</link>) or two.
Single layer perceptrons are only capable of learning <link xlink:type="simple" xlink:href="../173/523173.xml">
linearly separable</link> patterns; in 1969 a famous book entitled <b><it>Perceptrons</it></b> by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../639/19639.xml">
Marvin Minsky</link></scientist>
</person>
 and <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../802/27802.xml">
Seymour Papert</link></scientist>
</person>
 showed that it was impossible for these classes of network to learn an <link xlink:type="simple" xlink:href="../979/105979.xml">
XOR</link> function. They conjectured (incorrectly) that a similar result would hold for a perceptron with three or more layers. Three years later <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../441/2593441.xml">
Stephen Grossberg</link></scholar>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</person>
</physical_entity>
 published a series of papers introducing networks capable of modelling differential, contrast-enhancing and XOR functions. (The papers were published in 1972 and 1973, see e.g.: Grossberg, Contour enhancement, short-term memory, and constancies in reverberating neural networks. Studies in Applied Mathematics, 52 (1973), 213-257, online <weblink xlink:type="simple" xlink:href="http://cns.bu.edu/Profiles/Grossberg/Gro1973StudiesAppliedMath.pdf">
http://cns.bu.edu/Profiles/Grossberg/Gro1973StudiesAppliedMath.pdf</weblink>). Nevertheless the often-cited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until the <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link> research experienced a resurgence in the 1980s.  This text was reprinted in 1987 as "Perceptrons - Expanded Edition" where some errors in the original text are shown and corrected.</p>
<p>

More recently, interest in the perceptron learning algorithm has increased again after Freund and Schapire (1998) presented a voted formulation of the original algorithm (attaining large margin) and suggested that one can apply the <link xlink:type="simple" xlink:href="../912/303912.xml">
kernel trick</link> to it. The kernel-perceptron not only can handle nonlinearly separable data but can also go beyond vectors and classify instances having a relational representation (e.g. trees, graphs or sequences).</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 Freund, Y. and Schapire, R. E. 1998. Large margin classification using the perceptron algorithm. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT' 98). ACM Press.</entry>
<entry level="1" type="bullet">

 Gallant, S. I. (1990). <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=80230">
Perceptron-based learning algorithms.</weblink> IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179-191.</entry>
<entry level="1" type="bullet">

 Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Cornell Aeronautical Laboratory, Psychological Review, v65, No. 6, pp. 386-408.</entry>
<entry level="1" type="bullet">

 Minsky M L and Papert S A 1969 <it>Perceptrons</it> (Cambridge, MA: MIT Press)</entry>
<entry level="1" type="bullet">

 Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615-622. Polytechnic Institute of Brooklyn.</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../636/4793636.xml">
Widrow, B.</link></research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
, Lehr, M.A., "30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation," <it>Proc. IEEE</it>, vol 78, no 9, pp. 1415-1442, (1990).</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 Chapter 3 <weblink xlink:type="simple" xlink:href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K3.pdf">
Weighted networks - the perceptron</weblink> and chapter 4 <weblink xlink:type="simple" xlink:href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf">
Perceptron learning</weblink> of <weblink xlink:type="simple" xlink:href="http://page.mi.fu-berlin.de/rojas/neural/index.html.html">
<it>Neural Networks - A Systematic Introduction''</it></weblink> by <link>
Ra√∫l Rojas</link> (ISBN 978-3540605058)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://dynamicnotions.blogspot.com/2008/09/single-layer-perceptron.html">
C# implementation of a perceptron</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.csulb.edu/~cwallis/artificialn/History.htm">
History of perceptrons</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cis.hut.fi/ahonkela/dippa/node41.html">
Mathematics of perceptrons</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://library.thinkquest.org/18242/perceptron.shtml">
Perceptron demo applet and an introduction by examples</weblink></entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
