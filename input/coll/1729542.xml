<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:52:18[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Neural network</title>
<id>1729542</id>
<revision>
<id>244042817</id>
<timestamp>2008-10-09T01:40:57Z</timestamp>
<contributor>
<username>Mdd</username>
<id>113850</id>
</contributor>
</revision>
<categories>
<category>Articles lacking in-text citations</category>
<category>Data mining</category>
<category>Information, knowledge, and uncertainty</category>
<category>Network architecture</category>
<category>Neural networks</category>
<category>Wikipedia external links cleanup</category>
<category>Networks</category>
<category>Computational neuroscience</category>
</categories>
</header>
<bdy>

<image location="right" width="150px" src="Neural_network_example.png" type="thumb">
<caption>

Simplified view of an artificial neural network
</caption>
</image>

Traditionally, the term <b>neural network</b> had been used to refer to a network or circuit of <link xlink:type="simple" xlink:href="../120/21120.xml">
biological neurons</link>. The modern usage of the term often refers to <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link>s, which are composed of <link xlink:type="simple" xlink:href="../771/349771.xml">
artificial neuron</link>s or nodes. Thus the term has two distinct usages:
<list>
<entry level="1" type="number">

 <link xlink:type="simple" xlink:href="../672/1726672.xml">
Biological neural network</link>s are made up of real biological neurons that are connected or functionally-related in the <link xlink:type="simple" xlink:href="../096/25096.xml">
peripheral nervous system</link> or the <link xlink:type="simple" xlink:href="../251/7251.xml">
central nervous system</link>. In the field of <link xlink:type="simple" xlink:href="../245/21245.xml">
neuroscience</link>, they are often identified as groups of neurons that perform a specific physiological function in laboratory analysis.</entry>
<entry level="1" type="number">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial neural network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
s are made up of interconnecting artificial neurons (programming constructs that mimic the properties of biological neurons). Artificial neural networks may either be used to gain an understanding of biological neural networks, or for solving artificial intelligence problems without necessarily creating a model of a real biological system. The real biological nervous system is highly complex including some features which may seem superflous to the understanding of the working of artificial networks.</entry>
</list>
<p>

This article focuses on the relationship between the two concepts; for detailed coverage of the two different concepts refer to the separate articles: <link xlink:type="simple" xlink:href="../672/1726672.xml">
Biological neural network</link> and <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial neural network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
.</p>

<sec>
<st>
 Overview </st>
<p>

In general a biological neural network is composed of a group or groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called <link xlink:type="simple" xlink:href="../809/27809.xml">
synapses</link>, are usually formed from <link xlink:type="simple" xlink:href="../958/958.xml">
axons</link> to <link xlink:type="simple" xlink:href="../131/8131.xml">
dendrites</link>, though dendrodendritic microcircuits<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> and other connections are possible. Apart from the electrical signaling, there are other forms of signaling that arise from <link xlink:type="simple" xlink:href="../865/21865.xml">
neurotransmitter</link> diffusion, which have an effect on electrical signaling. As such, neural networks are extremely complex.</p>
<p>

<link xlink:type="simple" xlink:href="../164/1164.xml">
Artificial intelligence</link> and <link xlink:type="simple" xlink:href="../240/355240.xml">
cognitive modeling</link> try to simulate some properties of neural networks. While similar in their techniques, the former has the aim of solving particular tasks, while the latter aims to build mathematical models of biological neural systems.</p>
<p>

In the <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> field, artificial neural networks have been applied successfully to <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, <link xlink:type="simple" xlink:href="../382/346382.xml">
image analysis</link> and adaptive <link xlink:type="simple" xlink:href="../900/491900.xml">
control</link>, in order to construct <link xlink:type="simple" xlink:href="../106/430106.xml">
software agents</link> (in <link xlink:type="simple" xlink:href="../363/5363.xml">
computer and video games</link>) or <link xlink:type="simple" xlink:href="../049/48049.xml">
autonomous robot</link>s. Most of the currently employed artificial neural networks for artificial intelligence are based on <link xlink:type="simple" xlink:href="../926/1565926.xml">
statistical estimation</link>, <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> and <link xlink:type="simple" xlink:href="../039/7039.xml">
control theory</link>.</p>
<p>

The <link xlink:type="simple" xlink:href="../240/355240.xml">
cognitive modelling</link> field involves the physical or mathematical modeling of the behaviour of neural systems; ranging from the individual neural level (e.g. modelling the spike response curves of neurons to a stimulus), through the neural cluster level (e.g. modelling the release and effects of dopamine in the basal ganglia) to the complete organism (e.g. behavioural modelling of the organism's response to stimuli).</p>

</sec>
<sec>
<st>
History of the neural network analogy</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../636/263636.xml">
Connectionism</link></it>
</indent>
The concept of neural networks started in the late-1800s as an effort to describe how the human mind performed. These ideas started being applied to computational models with <link xlink:type="simple" xlink:href="../513/2873513.xml">
Turing's B-type machines</link> and the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../777/172777.xml">
Perceptron</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
.</p>
<p>

In early 1950s <person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../646/11646.xml">
Friedrich Hayek</link></philosopher>
</person>
 was one of the first to posit the idea of <link xlink:type="simple" xlink:href="../764/866764.xml">
spontaneous order</link>  in the brain arising out of decentralized networks of simple units (neurons). In the late 1940s, <link xlink:type="simple" xlink:href="../121/323121.xml">
Donald Hebb</link> made one of the first hypotheses for a mechanism of neural plasticity (i.e. learning), <link xlink:type="simple" xlink:href="../084/404084.xml">
Hebbian learning</link>. Hebbian learning is considered to be a 'typical' unsupervised learning rule and it (and variants of it) was an early model for <link xlink:type="simple" xlink:href="../266/372266.xml">
long term potentiation</link>.</p>
<p>

The <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../777/172777.xml">
Perceptron</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 is essentially a linear classifier for classifying data <math> x \in R^n</math> specified by parameters <math>w \in R^n, b \in R</math> and an output function <math>f = w'x + b</math>. Its parameters are adapted with an ad-hoc rule similar to stochastic steepest gradient descent. Because the <link xlink:type="simple" xlink:href="../856/14856.xml">
inner product</link> is a <link xlink:type="simple" xlink:href="../102/18102.xml">
linear operator</link> in the input space, the Perceptron can only perfectly classify a set of data for which different classes are <link xlink:type="simple" xlink:href="../173/523173.xml">
linearly separable</link> in the input space, while it often fails completely for non-separable data. While the development of the algorithm initially generated some enthusiasm, partly because of its apparent relation to biological mechanisms, the later discovery of this inadequacy caused such models to be abandoned until the introduction of non-linear models into the field.</p>
<p>

The <link>
Cognitron</link> (1975) was an early multilayered neural network with a training algorithm. The actual structure of the network and the methods used to set the interconnection weights change from one neural strategy to another, each with its advantages and disadvantages. Networks can propagate information in one direction only, or they can bounce back and forth until self-activation at a node occurs and the network settles on a final state. The ability for bi-directional flow of inputs between neurons/nodes was produced with the <link xlink:type="simple" xlink:href="../097/1170097.xml">
Hopfield's network</link> (1982), and specialization of these node layers for specific purposes was introduced through the first <link xlink:type="simple" xlink:href="../217/3907217.xml">
hybrid network</link>.</p>
<p>

The <link xlink:type="simple" xlink:href="../636/263636.xml">
parallel distributed processing</link> of the mid-1980s became popular under the name <link xlink:type="simple" xlink:href="../636/263636.xml">
connectionism</link>.</p>
<p>

The rediscovery of the <link xlink:type="simple" xlink:href="../091/1360091.xml">
backpropagation</link> algorithm was probably the main reason behind the repopularisation of neural networks after the publication of "Learning Internal Representations by Error Propagation" in 1986 (Though backpropagation itself dates from 1974). The original network utilised multiple layers of weight-sum units of the type <math>f = g(w'x + b)</math>, where <math>g</math> was a <link xlink:type="simple" xlink:href="../210/87210.xml">
sigmoid function</link> or <link xlink:type="simple" xlink:href="../563/84563.xml">
logistic function</link> such as used in <link xlink:type="simple" xlink:href="../631/226631.xml">
logistic regression</link>. Training was done by a form of stochastic steepest gradient descent. The employment of the chain rule of differentiation in deriving the appropriate parameter updates results in an algorithm that seems to 'backpropagate errors', hence the nomenclature. However it is essentially a form of gradient descent. Determining the optimal parameters in a model of this type is not trivial, and steepest gradient descent methods cannot be relied upon to give the solution without a good starting point. In recent times, networks with the same architecture as the backpropagation network are referred to as <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../644/2266644.xml">
Multi-Layer Perceptrons</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
. This name does not impose any limitations on the type of algorithm used for learning.</p>
<p>

The backpropagation network generated much enthusiasm at the time and there was much controversy about whether such learning could be implemented in the brain or not, partly because a mechanism for reverse signalling was not obvious at the time, but most importantly because there was no plausible source for the 'teaching' or 'target' signal.</p>


</sec>
<sec>
<st>
 The brain, neural networks and computers </st>
<p>

Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is debated.</p>
<p>

A subject of current research in theoretical neuroscience is the question surrounding the degree of complexity and the properties that individual neural elements should have to reproduce something resembling animal intelligence.</p>
<p>

Historically, computers evolved from the <link xlink:type="simple" xlink:href="../091/478091.xml">
von Neumann architecture</link>, which is based on sequential processing and execution of explicit instructions. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems, which may rely largely on parallel processing as well as implicit instructions based on recognition of patterns of 'sensory' input from external sources. In other words, at its very heart a neural network is a complex statistical processor (as opposed to being tasked to sequentially process and execute).</p>

</sec>
<sec>
<st>
Neural networks and artificial intelligence</st>

<p>

<indent level="1">

<it>Main article: <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial neural network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</it>
</indent>
An <it>artificial neural network</it> (ANN), also called a <it>simulated neural network</it> (SNN) or commonly just <it>neural network</it> (NN) is an interconnected group of <link xlink:type="simple" xlink:href="../771/349771.xml">
artificial neuron</link>s that uses a <link xlink:type="simple" xlink:href="../590/20590.xml">
mathematical or computational model</link> for <link xlink:type="simple" xlink:href="../578/315578.xml">
information processing</link> based on a <link xlink:type="simple" xlink:href="../636/263636.xml">
connectionistic</link> approach to <link xlink:type="simple" xlink:href="../926/5926.xml">
computation</link>. In most cases an ANN is an <link xlink:type="simple" xlink:href="../588/739588.xml">
adaptive system</link> that changes its structure based on external or internal information that flows through the network.</p>
<p>

In more practical terms neural networks are <link xlink:type="simple" xlink:href="../103/146103.xml">
non-linear</link> <link xlink:type="simple" xlink:href="../685/26685.xml">
statistical</link> <link xlink:type="simple" xlink:href="../422/759422.xml">
data modeling</link> or <link xlink:type="simple" xlink:href="../752/265752.xml">
decision making</link> tools. They can be used to model complex relationships between inputs and outputs or to <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../706/126706.xml">
find patterns</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 in data.</p>

<ss1>
<st>
 Background </st>
<p>

An <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link> involves a network of simple processing elements (<link xlink:type="simple" xlink:href="../771/349771.xml">
artificial neurons</link>) which can exhibit complex global behaviour, determined by the connections between the processing elements and element parameters. One classical type of artificial neural network is the <link xlink:type="simple" xlink:href="../097/1170097.xml">
Hopfield net</link>.</p>
<p>

In a neural network model simple <link xlink:type="simple" xlink:href="../771/349771.xml">
nodes</link>, which can be called variously "neurons", "neurodes", "Processing Elements" (PE) or "units", are connected together to form a network of nodes &mdash; hence the term "neural network". While a neural network does not have to be adaptive <it>per se</it>, its practical use comes with algorithms designed to alter the strength (weights) of the connections in the network to produce a desired signal flow.</p>
<p>

In modern <link xlink:type="simple" xlink:href="../924/3712924.xml">
software implementations</link> of artificial neural networks the approach inspired by biology has more or less been abandoned for a more practical approach based on statistics and signal processing. In some of these systems neural networks, or parts of neural networks (such as <link xlink:type="simple" xlink:href="../771/349771.xml">
artificial neuron</link>s) are used as components in larger systems that combine both adaptive and non-adaptive elements.</p>
<p>

The concept of a neural network appears to have first been proposed by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Alan Turing</link></scientist>
</person>
 in his 1948 paper "Intelligent Machinery".</p>

</ss1>
<ss1>
<st>
 Applications </st>
<p>

The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. This is particularly useful in applications where the complexity of the data or task makes the design of such a function by hand impractical.</p>
<p>

<list>
<entry level="1" type="definition">

Real life applications</entry>
</list>

The tasks to which artificial neural networks are applied tend to fall within the following broad categories:
<list>
<entry level="1" type="bullet">

<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../897/336897.xml">
Function approximation</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
, or <link xlink:type="simple" xlink:href="../997/826997.xml">
regression analysis</link>, including <link xlink:type="simple" xlink:href="../624/406624.xml">
time series prediction</link> and modelling.</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../244/1579244.xml">
Classification</link>, including <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../706/126706.xml">
pattern</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 and sequence recognition, novelty detection and sequential decision making.</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../961/41961.xml">
Data processing</link>, including filtering, clustering, <link xlink:type="simple" xlink:href="../893/431893.xml">
blind signal separation</link> and compression.</entry>
</list>
</p>
<p>

Application areas include system identification and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition, etc.), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link> (or knowledge discovery in databases, "KDD"), visualization and <link xlink:type="simple" xlink:href="../847/459847.xml">
e-mail spam</link> filtering.</p>

</ss1>
<ss1>
<st>
Neural network software</st>
<p>

<it>Main article:</it> <link xlink:type="simple" xlink:href="../924/3712924.xml">
Neural network software</link>
<b>Neural network software</b> is used to <link xlink:type="simple" xlink:href="../444/43444.xml">
simulate</link>, <link xlink:type="simple" xlink:href="../524/25524.xml">
research</link>, <link xlink:type="simple" xlink:href="../932/248932.xml">
develop</link> and apply <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link>s, <link xlink:type="simple" xlink:href="../672/1726672.xml">
biological neural network</link>s and in some cases a wider array of <link xlink:type="simple" xlink:href="../588/739588.xml">
adaptive system</link>s.</p>

<ss2>
<st>
Learning paradigms</st>
<p>

There are three major learning paradigms, each corresponding to a particular abstract learning task. These are <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link>, <link xlink:type="simple" xlink:href="../497/233497.xml">
unsupervised learning</link> and <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link>. Usually any given type of network architecture can be employed in any of those tasks.</p>
<p>

<list>
<entry level="1" type="definition">

Supervised learning</entry>
</list>

In <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link>, we are given a set of example pairs <math> (x, y), x \in X, y \in Y</math> and the aim is to find a function <math>f</math> in the allowed class of functions that matches the examples. In other words, we wish to <it>infer</it> how the mapping implied by the data and the cost function is related to the mismatch between our mapping and the data.</p>
<p>

<list>
<entry level="1" type="definition">

Unsupervised learning</entry>
</list>

In <link xlink:type="simple" xlink:href="../497/233497.xml">
unsupervised learning</link> we are given some data <math>x</math>, and a cost function which is to be minimized which can be any function of <math>x</math> and the network's output, <math>f</math>. The cost function is determined by the task formulation. Most applications fall within the domain of <link>
estimation problems</link> such as <link xlink:type="simple" xlink:href="../576/27576.xml">
statistical modeling</link>, <link xlink:type="simple" xlink:href="../013/8013.xml">
compression</link>, <link xlink:type="simple" xlink:href="../001/1336001.xml">
filtering</link>, <link xlink:type="simple" xlink:href="../893/431893.xml">
blind source separation</link> and <link xlink:type="simple" xlink:href="../675/669675.xml">
clustering</link>.</p>
<p>

<list>
<entry level="1" type="definition">

Reinforcement learning</entry>
</list>

In <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link>, data <math>x</math> is usually not given, but generated by an agent's interactions with the environment. At each point in time <math>t</math>, the agent performs an action <math>y_t</math> and the environment generates an observation <math>x_t</math> and an instantaneous cost <math>c_t</math>, according to some (usually unknown) dynamics. The aim is to discover a <it>policy</it> for selecting actions that minimises some measure of a long-term cost, i.e. the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated. ANNs are frequently used in reinforcement learning as part of the overall algorithm. Tasks that fall within the paradigm of reinforcement learning are <link xlink:type="simple" xlink:href="../900/491900.xml">
control</link> problems, <link xlink:type="simple" xlink:href="../138/18723138.xml">
game</link>s and other <link>
sequential decision making</link> tasks.</p>

</ss2>
<ss2>
<st>
Learning algorithms</st>
<p>

There are many algorithms for training neural networks; most of them can be viewed as a straightforward application of <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> theory and <link xlink:type="simple" xlink:href="../926/1565926.xml">
statistical estimation</link>.</p>
<p>

<link xlink:type="simple" xlink:href="../020/268020.xml">
Evolutionary computation</link> methods, <link xlink:type="simple" xlink:href="../244/172244.xml">
simulated annealing</link>, <link>
expectation maximization</link> and <link xlink:type="simple" xlink:href="../369/223369.xml">
non-parametric methods</link> are among other commonly used methods for training neural networks. See also <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>.</p>
<p>

Recent developments in this field also saw the use of <link xlink:type="simple" xlink:href="../083/337083.xml">
particle swarm optimization</link> and other <link xlink:type="simple" xlink:href="../988/762988.xml">
swarm intelligence</link> techniques used in the training of neural networks.</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
Neural networks and neuroscience</st>
<p>

Theoretical and <link xlink:type="simple" xlink:href="../430/271430.xml">
computational neuroscience</link> is the field concerned with the theoretical analysis and computational modeling of biological neural systems.
Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.</p>
<p>

The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (<link xlink:type="simple" xlink:href="../672/1726672.xml">
biological neural network</link> models) and theory (statistical learning theory and <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link>).</p>

<ss1>
<st>
 Types of models </st>
<p>

Many models are used in the field, each defined at a different level of abstraction and trying to model different aspects of neural systems. They range from models of the short-term behaviour of <link xlink:type="simple" xlink:href="../479/14408479.xml">
individual neurons</link>, through models of how the dynamics of neural circuitry arise from interactions between individual neurons, to models of how behaviour can arise from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and its relation to learning and memory, from the individual neuron to the system level.</p>

</ss1>
<ss1>
<st>
Current research</st>
<p>

While initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of <link xlink:type="simple" xlink:href="../810/4458810.xml">
neuromodulators</link> such as <link xlink:type="simple" xlink:href="../548/48548.xml">
dopamine</link>, <link xlink:type="simple" xlink:href="../649/52649.xml">
acetylcholine</link>, and <link xlink:type="simple" xlink:href="../764/28764.xml">
serotonin</link> on behaviour and learning.</p>
<p>

<link xlink:type="simple" xlink:href="../000/54000.xml">
Biophysical</link> models, such as <link xlink:type="simple" xlink:href="../011/14200011.xml">
BCM theory</link>, have been important in understanding mechanisms for <link xlink:type="simple" xlink:href="../771/423771.xml">
synaptic plasticity</link>, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for <link xlink:type="simple" xlink:href="../443/9651443.xml">
radial basis networks</link> and <link xlink:type="simple" xlink:href="../608/14338608.xml">
neural backpropagation</link> as mechanisms for processing data.</p>

</ss1>
</sec>
<sec>
<st>
Criticism</st>
<p>

<link xlink:type="simple" xlink:href="../840/256840.xml">
A. K. Dewdney</link>, a former <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../507/29507.xml">
Scientific American</link></periodical>
</it> columnist, wrote in 1997, <it>“Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool.”</it> (Dewdney, p.82)</p>
<p>

Arguments against Dewdney's position are that neural nets have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft<weblink xlink:type="simple" xlink:href="http://www.nasa.gov/centers/dryden/news/NewsReleases/2003/03-49.html">
http://www.nasa.gov/centers/dryden/news/NewsReleases/2003/03-49.html</weblink> to detecting credit card fraud<weblink xlink:type="simple" xlink:href="http://www.visa.ca/en/about/visabenefits/innovation.cfm">
http://www.visa.ca/en/about/visabenefits/innovation.cfm</weblink>.</p>
<p>

Technology writer <link>
Roger Bridgman</link> commented on Dewdney's statements about neural nets: 
Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".</p>
<p>

In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>


</sec>
<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../913/8220913.xml">
ADALINE</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial neural network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../318/3929318.xml">
Biological cybernetics</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../157/361157.xml">
Biologically-inspired computing</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../176/1700176.xml">
Cognitive architecture</link></entry>
<entry level="1" type="bullet">

<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<component wordnetid="105868954" confidence="0.8">
<part wordnetid="105867413" confidence="0.8">
<link xlink:type="simple" xlink:href="../162/14246162.xml">
Memristor</link></part>
</component>
</concept>
</idea>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../924/3712924.xml">
Neural network software</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../076/2715076.xml">
Neuro-fuzzy</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../636/263636.xml">
Parallel distributed processing</link></entry>
<entry level="1" type="bullet">

<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../563/4141563.xml">
Predictive analytics</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../443/9651443.xml">
Radial basis function network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<work wordnetid="100575741" confidence="0.8">
<argument wordnetid="106648724" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<subject wordnetid="106599788" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../365/247365.xml">
Simulated reality</link></activity>
</psychological_feature>
</act>
</investigation>
</experiment>
</event>
</message>
</research>
</indication>
</evidence>
</subject>
</scientific_research>
</argument>
</work>
</entry>
<entry level="1" type="bullet">

<know-how wordnetid="105616786" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../309/65309.xml">
Support vector machine</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</method>
</rule>
</event>
</know-how>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../023/5099023.xml">
Tensor product network</link></entry>
<entry level="1" type="bullet">

<message wordnetid="106598915" confidence="0.8">
<information wordnetid="106634376" confidence="0.8">
<electronic_database wordnetid="106588511" confidence="0.8">
<lexical_database wordnetid="106638868" confidence="0.8">
<wordnet wordnetid="106639428" confidence="0.8">
<database wordnetid="106637824" confidence="0.8">
<link xlink:type="simple" xlink:href="../772/1548772.xml">
20Q</link></database>
</wordnet>
</lexical_database>
</electronic_database>
</information>
</message>
 is a neural network implementation of the 20 questions game</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../226/10839226.xml">
Cultured neuronal networks</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../245/21245.xml">
Neuroscience</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../626/5626.xml">
Cognitive science</link></entry>
</list>
</p>


</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
Arbib, p.666</entry>
<entry id="2">
<weblink xlink:type="simple" xlink:href="http://members.fortunecity.com/templarseries/popper.html">
Roger Bridgman's defence of neural networks</weblink></entry>
</reflist>
</p>

<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Text_document_with_red_question_mark.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 This article or section includes a  or , but its sources remain unclear because it lacks <b>.</b>
You can  this article by introducing more precise citations .</col>
</row>
</table>

</p>

</sec>
<sec>
<st>
 Further reading </st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Arbib, Michael A. (Ed.)&#32;(1995). The Handbook of Brain Theory and Neural Networks.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 Alspector, <weblink xlink:type="simple" xlink:href="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4874963">
U.S. Patent 4,874,963</weblink><weblink xlink:type="simple" xlink:href="http://www.pat2pdf.org/pat2pdf/foo.pl?number=4874963">
&nbsp;</weblink> "<it>Neuromorphic learning networks</it>". October 17, 1989.</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Agre, Philip E., et al.&#32;(1997). Comparative Cognitive Robotics: Computation and Human Experience.&#32;Cambridge University Press. ISBN 0-521-38603-9.</cite>&nbsp;, p. 80</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book"><physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../395/9251395.xml">
Yaneer Bar-Yam</link></educator>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
&#32;(2003). <weblink xlink:type="simple" xlink:href="http://necsi.org/publications/dcs/Bar-YamChap2.pdf">
Dynamics of Complex Systems, Chapter 2</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book"><physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../395/9251395.xml">
Yaneer Bar-Yam</link></educator>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
&#32;(2003). <weblink xlink:type="simple" xlink:href="http://necsi.org/publications/dcs/Bar-YamChap3.pdf">
Dynamics of Complex Systems, Chapter 3</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book"><physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../395/9251395.xml">
Yaneer Bar-Yam</link></educator>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
&#32;(2005). <weblink xlink:type="simple" xlink:href="http://necsi.org/publications/mtw/">
Making Things Work</weblink>.</cite>&nbsp; See chapter 3.</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Bertsekas, Dimitri P.&#32;(1999). Nonlinear Programming.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Bertsekas, Dimitri P. &amp; Tsitsiklis, John N.&#32;(1996). Neuro-dynamic Programming.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Bhadeshia H. K. D. H.&#32;(1992).&#32;"<weblink xlink:type="simple" xlink:href="http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.pdf">
Neural Networks in Materials Science</weblink>". <it>ISIJ International</it>&#32;<b>39</b>: 966&ndash;979. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.2355%2Fisijinternational.39.966">
10.2355/isijinternational.39.966</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Boyd, Stephen &amp; Vandenberghe, Lieven&#32;(2004). <weblink xlink:type="simple" xlink:href="http://www.stanford.edu/~boyd/cvxbook/">
Convex Optimization</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Dewdney, A. K.&#32;(1997). Yes, We Have No Neutrons: An Eye-Opening Tour through the Twists and Turns of Bad Science.&#32;Wiley, 192 pp.</cite>&nbsp;  See chapter 5.</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Egmont-Petersen, M., de Ridder, D., Handels, H.&#32;(2002).&#32;"Image processing with neural networks - a review". <it>Pattern Recognition</it>&#32;<b>35</b>: 2279&ndash;2301. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2FS0031-3203%2801%2900178-9">
10.1016/S0031-3203(01)00178-9</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Fukushima, K.&#32;(1975).&#32;"Cognitron: A Self-Organizing Multilayered Neural Network". <it>Biological Cybernetics</it>&#32;<b>20</b>: 121&ndash;136. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF00342633">
10.1007/BF00342633</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Frank, Michael J.&#32;(2005).&#32;"Dynamic Dopamine Modulation in the Basal Ganglia: A Neurocomputational Account of Cognitive Deficits in Medicated and Non-medicated Parkinsonism". <it>Journal of Cognitive Neuroscience</it>&#32;<b>17</b>: 51&ndash;72. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1162%2F0898929052880093">
10.1162/0898929052880093</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Gardner, E.J., &amp; Derrida, B.&#32;(1988).&#32;"Optimal storage properties of neural network models". <it>Journal of Physics a</it>&#32;<b>21</b>: 271&ndash;284. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1088%2F0305-4470%2F21%2F1%2F031">
10.1088/0305-4470/21/1/031</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Krauth, W., &amp; Mezard, M.&#32;(1989).&#32;"Storage capacity of memory with binary couplings". <it>Journal de Physique</it>&#32;<b>50</b>: 3057&ndash;3066. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1051%2Fjphys%3A0198900500200305700">
10.1051/jphys:0198900500200305700</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Maass, W., &amp; Markram, H.&#32;(2002).&#32;"<weblink xlink:type="simple" xlink:href="http://www.igi.tugraz.at/maass/publications.html">
On the computational power of recurrent circuits of spiking neurons</weblink>". <it>Journal of Computer and System Sciences</it>&#32;<b>69(4)</b>: 593&ndash;616.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">MacKay, David&#32;(2003). <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/book.html">
Information Theory, Inference, and Learning Algorithms</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Mandic, D. &amp; Chambers, J.&#32;(2001). Recurrent Neural Networks for Prediction: Architectures, Learning algorithms and Stability.&#32;Wiley.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Minsky, M. &amp; Papert, S.&#32;(1969). An Introduction to Computational Geometry.&#32;MIT Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Muller, P. &amp; Insua, D.R.&#32;(1995).&#32;"Issues in Bayesian Analysis of Neural Network Models". <it>Neural Computation</it>&#32;<b>10</b>: 571&ndash;592.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Reilly, D.L., Cooper, L.N. &amp; Elbaum, C.&#32;(1982).&#32;"A Neural Model for Category Learning". <it>Biological Cybernetics</it>&#32;<b>45</b>: 35&ndash;41. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF00387211">
10.1007/BF00387211</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Rosenblatt, F.&#32;(1962). Principles of Neurodynamics.&#32;Spartan Books.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Sutton, Richard S. &amp; Barto, Andrew G.&#32;(1998). <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~sutton/book/the-book.html">
Reinforcement Learning : An introduction</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

Van den Bergh, F. Engelbrecht, AP.&#32;"<it>Cooperative Learning in Neural Networks using Particle Swarm Optimizers</it>". &#32;CIRG 2000.</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Wilkes, A.L. &amp; Wade, N.J.&#32;(1997).&#32;"Bain on Neural Networks". <it>Brain and Cognition</it>&#32;<b>33</b>: 295&ndash;305. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1006%2Fbrcg.1997.0869">
10.1006/brcg.1997.0869</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Wasserman, P.D.&#32;(1989). Neural computing theory and practice.&#32;Van Nostrand Reinhold.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 Jeffrey T. Spooner, Manfredi Maggiore, Raul Ord onez, and Kevin M. Passino, Stable Adaptive Control and Estimation for Nonlinear Systems: Neural and Fuzzy Approximator Techniques, John Wiley and Sons, NY, 2002.</entry>
<entry level="1" type="bullet">

http://www.cs.stir.ac.uk/courses/31YF/Notes/Notes_PL.html</entry>
<entry level="1" type="bullet">

http://www.shef.ac.uk/psychology/gurney/notes/l1/section3_3.html</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Peter Dayan, L.F. Abbott. Theoretical Neuroscience.&#32;MIT Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Wulfram Gerstner, Werner Kistler. Spiking Neuron Models:Single Neurons, Populations, Plasticity.&#32;Cambridge University Press.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>

<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_style.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>The external links in this article may not follow Wikipedia's  or .</b>
Please <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Neural_network&amp;action=edit">
improve this article</weblink> by removing excessive or inappropriate external links. </col>
</row>
</table>
</p>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.learnartificialneuralnetworks.com/robotcontrol.html">
LearnArtificialNeuralNetworks</weblink> - Robot control and neural networks</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.html">
Review of Neural Networks in Materials Science</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.e-nns.org">
European Neural Network Society (ENNS)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inns.org">
International Neural Network Society (INNS)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ieee-cis.org">
IEEE Computational Intelligence Society (IEEE CIS)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.gc.ssr.upm.es/inves/neural/ann1/anntutorial.html">
Artificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.makhfi.com/tutorial/introduction.htm">
Introduction to Neural Networks and Knowledge Modeling</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.tandf.co.uk/journals/titles/0954898X.asp">
Network: Computation in Neural Systems</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.willamette.edu/~gorr/classes/cs449/intro.html">
Introduction to Artificial Neural Networks</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.hedengren.net/research/isat.htm">
In Situ Adaptive Tabulation:</weblink> - A neural network alternative.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html">
Another introduction to ANN</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.avaye.com/files/articles/nnintro/nn_intro.pdf">
An introduction to Neural Networks</weblink> (pdf)</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.obitko.com/tutorials/neural-network-prediction/">
Prediction with neural networks</weblink> - includes Java applet for online experimenting with prediction of a function</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://pl.youtube.com/watch?v=AyzOUbkUf3M">
Next Generation of Neural Networks</weblink> - Google Tech Talks</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://pages.sbcglobal.net/louis.savain/AI/perceptual_network.htm">
Perceptual Learning</weblink> - Artificial Perceptual Neural Network used for machine learning to play <link xlink:type="simple" xlink:href="../134/5134.xml">
Chess</link></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.softcomputing.es/en/home.php">
European Centre for Soft Computing</weblink></entry>
</list>






</p>

</sec>
</bdy>
</article>
