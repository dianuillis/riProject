<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:52:17[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Rényi entropy</title>
<id>1731689</id>
<revision>
<id>228206032</id>
<timestamp>2008-07-27T15:55:50Z</timestamp>
<contributor>
<username>Shakir</username>
<id>418403</id>
</contributor>
</revision>
<categories>
<category>Entropy and information</category>
<category>Information theory</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link>, the <b>Rényi entropy</b>, a generalisation of <link xlink:type="simple" xlink:href="../445/15445.xml">
Shannon entropy</link>, is one of a family of functionals for quantifying the diversity, uncertainty or randomness of a system. It is named after <link>
Alfréd Rényi</link>.<p>

The Rényi entropy of order &amp;alpha;, where &amp;alpha; <math>\geq</math> 0,  is defined as</p>
<p>

<indent level="1">

<math>H_\alpha(X) = \frac{1}{1-\alpha}\log\Bigg(\sum_{i=1}^n p_i^\alpha\Bigg)</math>
</indent>

where <it>p</it>i are the probabilities of {<it>x</it>1, <it>x</it>2 ... <it>x</it>n}.  If the probabilities are all the same then all the Rényi entropies of the distribution are equal, with <it>H</it>&amp;alpha;(<it>X</it>)=log <it>n</it>.  Otherwise the entropies are  weakly decreasing as a function of &amp;alpha;.</p>
<p>

Some particular cases:
<indent level="1">

<math>H_0 (X) = \log n = \log |X|,\,</math>
</indent>

which is the logarithm of the <link xlink:type="simple" xlink:href="../er)/6174_(number).xml">
cardinality</link> of <it>X</it>, sometimes called the <link xlink:type="simple" xlink:href="../180/3781180.xml">
Hartley entropy</link> of <it>X</it>.</p>
<p>

In the limit that <math>\alpha</math> approaches 1, it can be shown that <math>H_\alpha</math> converges to 
<indent level="1">

<math>H_1 (X) = - \sum_{i=1}^n p_i \log p_i </math>
</indent>
which is the <link xlink:type="simple" xlink:href="../445/15445.xml">
Shannon entropy</link>.  Sometimes Renyi entropy refers only to the case <math>\alpha = 2</math>, </p>
<p>

<indent level="1">

<math>H_2 (X) = - \log \sum_{i=1}^n p_i^2 = - \log P(X = Y)</math>
</indent>

where <it>Y</it> is a random variable independent of <it>X</it> but identically distributed to <it>X</it>.  As <math>\alpha \rightarrow \infty </math>, the limit exists as</p>
<p>

<indent level="1">

<math>H_\infty (X) = - \log \sup_{i=1..n} p_i </math>
</indent>

and this is called <link xlink:type="simple" xlink:href="../053/1052053.xml">
Min-entropy</link>, because it is smallest value of <math>H_\alpha</math>.  These two latter cases are related by <math> H_\infty &amp;lt; H_2 &amp;lt; 2 H_\infty </math>, while on the other hand Shannon entropy can be arbitrarily high for a random variable <it>X</it> with fixed min-entropy.</p>
<p>

The Rényi entropies are important in ecology and statistics as <link xlink:type="simple" xlink:href="../367/3099367.xml">
indices of diversity</link>.  They also lead to a spectrum of indices of <link xlink:type="simple" xlink:href="../907/285907.xml">
fractal dimension</link>.</p>

<sec>
<st>
 Rényi relative informations </st>

<p>

As well as the absolute Rényi entropies, Rényi also defined a spectrum of generalised relative  information gains (the negative of relative entropies), generalising the <link>
Kullback–Leibler divergence</link>.</p>
<p>

The <b>Rényi generalised divergence</b> of order &amp;alpha;, where &amp;alpha; &amp;gt; 0, of an approximate distribution or a prior distribution <it>Q</it>(<it>x</it>) from a "true" distribution or an updated distribution <it>P</it>(<it>x</it>) is defined to be:</p>
<p>

<indent level="1">

<math>D_\alpha (P \| Q) = \frac{1}{\alpha-1}\log\Bigg(\sum_{i=1}^n \frac{p_i^\alpha}{q_i^{\alpha-1}}\Bigg) = \frac{1}{\alpha-1}\log \sum_{i=1}^n p_i^\alpha q_i^{1-\alpha}\,</math>
</indent>

Like the Kullback-Leibler divergence, the Rényi generalised divergences are always non-negative. This divergence is also known as the alpha-divergence (<math>\alpha</math>-divergence).</p>
<p>

Some special cases:</p>
<p>

<indent level="1">

<math>D_0(P \| Q) = - \log \Pr(\{i : q_i &amp;gt; 0\})</math> : minus the log probability that <it>q</it>i&amp;gt;0;
</indent>

<indent level="1">

<math>D_{1/2}(P \| Q) = -2 \log \sum_{i=1}^n \sqrt{p_i q_i} </math> : minus twice the logarithm of the <link xlink:type="simple" xlink:href="../261/831261.xml">
Bhattacharyya coefficient</link>;
</indent>

<indent level="1">

<math>D_1(P \| Q) = \sum_{i=1}^n p_i \log \frac{p_i}{q_i}</math> : the Kullback-Leibler divergence;
</indent>

<indent level="1">

<math>D_2(P \| Q) = \log \Big\langle \frac{p_i}{q_i} \Big\rangle \, </math> : the log of the expected ratio of the probabilities;
</indent>

<indent level="1">

<math>D_\infty(P \| Q) = \log \sup_i \frac{p_i}{q_i} </math> : the log of the maximum ratio of the probabilities.
</indent>

</p>
</sec>
<sec>
<st>
Why &amp;alpha; = 1 is specia</st>
<p>
=</p>
<p>

The value &amp;alpha; = 1, which gives the <link xlink:type="simple" xlink:href="../445/15445.xml">
Shannon entropy</link> and the <link>
Kullback–Leibler divergence</link>, is special because it is only when &amp;alpha;=1 that one can separate out variables <it>A</it> and <it>X</it> from a joint probability distribution, and write:</p>
<p>

<indent level="1">

<math>H(A,X) =  H(A) + \mathbb{E}_{p(a)} \{ H(X|a) \}</math>
</indent>

for the absolute entropies, and</p>
<p>

<indent level="1">

<math>D_\mathrm{KL}(p(x|a)p(a)||m(x,a)) =  \mathbb{E}_{p(a)}\{D_\mathrm{KL}(p(x|a)||m(x|a))\} + D_\mathrm{KL}(p(a)||m(a)),</math>
</indent>

for the relative entropies.</p>
<p>

The latter in particular means that if we seek a distribution <it>p</it>(<it>x</it>,<it>a</it>) which minimises the divergence of some underlying prior measure <it>m</it>(<it>x</it>,<it>a</it>), and we acquire new information which only affects the distribution of <it>a</it>, then the distribution of <it>p</it>(<it>x</it>|<it>a</it>) remains <it>m</it>(<it>x</it>|<it>a</it>), unchanged.</p>
<p>

The other Rényi divergences satisfy the criteria of being positive and continuous; being invariant under 1-to-1 co-ordinate transformations; and of combining additively when <it>A</it> and <it>X</it> are independent, so that if <it>p</it>(<it>A</it>,<it>X</it>) = <it>p</it>(<it>A</it>)<it>p</it>(<it>X</it>), then</p>
<p>

<indent level="1">

<math>H_\alpha(A,X) = H_\alpha(A) + H_\alpha(X)\;</math>
</indent>

and</p>
<p>

<indent level="1">

<math>D_\alpha(P(A)P(X)\|Q(A)Q(X)) = D_\alpha(P(A)\|Q(A)) + D_\alpha(P(X)\|Q(X)).</math>
</indent>

The stronger properties of the &amp;alpha; = 1 quantities, which allow the definition of the <link xlink:type="simple" xlink:href="../548/908548.xml">
conditional information</link>s and <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>s which are so important in communication theory, may be very important in other applications, or entirely unimportant, depending on those applications' requirements.</p>

</sec>
<sec>
<st>
 References </st>

<p>

<cite style="font-style:normal">A. Rényi&#32;(1961). "<weblink xlink:type="simple" xlink:href="http://digitalassets.lib.berkeley.edu/math/ucb/text/math_s4_v1_article-27.pdf">
On measures of information and entropy</weblink>".&#32;<it>Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability  1960</it>: 547-561.</cite>&nbsp;</p>
<p>

A. O. Hero, O.Michael and J. Gorman.&#32;"<weblink xlink:type="simple" xlink:href="http://www.eecs.umich.edu/~hero/Preprints/cspl-328.pdf">
Alpha-divergences for Classification, Indexing and Retrieval</weblink>".&nbsp;</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../367/3099367.xml">
Diversity indices</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../165/3573165.xml">
Tsallis entropy</link></entry>
</list>
</p>




</sec>
</bdy>
</article>
