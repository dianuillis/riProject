<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:24:08[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Overfitting</title>
<id>173332</id>
<revision>
<id>243972373</id>
<timestamp>2008-10-08T19:55:15Z</timestamp>
<contributor>
<username>Doloco</username>
<id>5559270</id>
</contributor>
</revision>
<categories>
<category>Data mining</category>
<category>Regression analysis</category>
<category>Statistical inference</category>
</categories>
</header>
<bdy>

<indent level="1">

<it>For the machine learning concept see <link xlink:type="simple" xlink:href="../116/19674116.xml">
Overfitting (machine learning)</link>
</it></indent>
<image width="300px" src="Overfit.png" type="thumb">
<caption>

Noisy (roughly linear) data is fit to both linear and <link xlink:type="simple" xlink:href="../000/23000.xml">
polynomial</link> functions.  Although the polynomial function passes through each data point, and the line passes through few, the line is a better fit because it does not have the large excursions at the ends.  If the regression curves were used to extrapolate the data, the overfit would do much worse.
</caption>
</image>
<p>

In <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, <b>overfitting</b> is fitting a <link xlink:type="simple" xlink:href="../576/27576.xml">
statistical model</link> that has too many <link xlink:type="simple" xlink:href="../065/25065.xml">
parameter</link>s.  An absurd and false model may fit perfectly if the model has enough complexity by comparison to the amount of data available.  Overfitting is generally recognized to be a violation of <link xlink:type="simple" xlink:href="../797/36797.xml">
Occam's razor</link>.  When the <link xlink:type="simple" xlink:href="../606/2321606.xml">
degrees of freedom</link> in parameter selection exceed the information content of the data, this leads to arbitrariness in the final (fitted) model parameters which reduces or destroys the ability of the model to generalize beyond the fitting data.   The likelihood of overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data.</p>
<p>

In both statistics and machine learning, in order to avoid overfitting, it is necessary to use additional techniques (e.g. <link xlink:type="simple" xlink:href="../612/416612.xml">
cross-validation</link>, <link xlink:type="simple" xlink:href="../061/2009061.xml">
regularization</link>, <link xlink:type="simple" xlink:href="../214/213214.xml">
early stopping</link>, <link xlink:type="simple" xlink:href="../877/472877.xml">
Bayesian priors</link> on parameters or <link xlink:type="simple" xlink:href="../329/331329.xml">
model comparison</link>), that can indicate when further training is not resulting in better generalization. </p>

<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<inclination wordnetid="106196584" confidence="0.8">
<bias wordnetid="106201908" confidence="0.8">
<partiality wordnetid="106201136" confidence="0.8">
<link xlink:type="simple" xlink:href="../951/1311951.xml">
Data dredging</link></partiality>
</bias>
</inclination>
</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>

<p>

<list>
<entry level="1" type="bullet">

 http://www.cs.sunysb.edu/~skiena/jaialai/excerpts/node16.html</entry>
</list>
</p>


</sec>
</bdy>
</article>
