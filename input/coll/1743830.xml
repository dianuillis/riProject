<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:52:28[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<work  confidence="0.8" wordnetid="100575741">
<examination  confidence="0.8" wordnetid="100635850">
<event  confidence="0.8" wordnetid="100029378">
<investigation  confidence="0.8" wordnetid="100633864">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<activity  confidence="0.8" wordnetid="100407535">
<header>
<title>Computer-adaptive testing</title>
<id>1743830</id>
<revision>
<id>241865179</id>
<timestamp>2008-09-29T21:28:26Z</timestamp>
<contributor>
<username>Iulus Ascanius</username>
<id>2764249</id>
</contributor>
</revision>
<categories>
<category>School examinations</category>
<category>Educational technology</category>
<category>Psychometrics</category>
<category>Educational assessment and evaluation</category>
</categories>
</header>
<bdy>

A <b>computer-adaptive testing</b> (<b>CAT</b>) is a method for administering 
<work wordnetid="100575741" confidence="0.8">
<examination wordnetid="100635850" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../879/221879.xml">
tests</link></higher_cognitive_process>
</activity>
</trial>
</psychological_feature>
</act>
</investigation>
</experiment>
</event>
</thinking>
</inquiry>
</process>
</problem_solving>
</examination>
</work>
 that adapts to the examinee's ability level. For this reason, it has also been called <it>tailored testing</it>.  
<sec>
<st>
How CAT works</st>

<p>

CAT successively selects questions so as to maximize the precision of the exam based on what is known about the examinee from previous questions.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> From the examinee's perspective, the difficulty of the exam seems to tailor itself to their level of ability. For example, if an examinee performs well on an item of intermediate difficulty, he will then be presented with a more difficult question. Or, if he performed poorly, he would be presented with a simpler question. Compared to static <link xlink:type="simple" xlink:href="../573/617573.xml">
multiple choice</link> tests that nearly everyone has experienced, with a fixed set of items administered to all examinees, computer-adaptive tests require fewer test items to arrive at equally accurate scores.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>  (Of course, there is nothing about the CAT methodology that requires the items to be multiple-choice; but just as most exams are multiple-choice, most CAT exams also use this format.)</p>
<p>

The basic computer-adaptive testing method is an <link xlink:type="simple" xlink:href="../833/68833.xml">
iterative</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> with the following steps:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>
<list>
<entry level="1" type="number">

 The pool of available items is searched for the optimal item, based on the examinee's current <link xlink:type="simple" xlink:href="../868/1206868.xml">
ability</link> estimate</entry>
<entry level="1" type="number">

 The chosen item is presented to the examinee, who then answers it correctly or incorrectly</entry>
<entry level="1" type="number">

 The ability estimate is updated, based upon all prior answers</entry>
<entry level="1" type="number">

 Steps 1&ndash;3 are repeated until a termination criterion is met</entry>
</list>
</p>
<p>

Nothing is known about the examinee prior to the administration of the first item, so the algorithm is generally started by selecting an item of medium, or medium-easy, difficulty as the first item.</p>
<p>

As a result of adaptive administration, different examinees receive quite different tests.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>  The psychometric technology that allows equitable scores to be computed across different sets of items is <link xlink:type="simple" xlink:href="../159/420159.xml">
item response theory</link> (IRT).  IRT is also the preferred methodology for selecting optimal items which are typically selected on the basis of <it>information</it> rather than difficulty, per se.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

The <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../643/346643.xml">
GRE</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
 General Test and the <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../232/255232.xml">
Graduate Management Admission Test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
 are currently primarily administered as a computer-adaptive test.  A list of active CAT programs is found at <weblink xlink:type="simple" xlink:href="http://www.psych.umn.edu/psylabs/catcentral/">
CAT Central</weblink>, along with a list of current CAT research programs and a near-inclusive bibliography of all published CAT research.</p>
<p>

A related methodology called <link xlink:type="simple" xlink:href="../433/16006433.xml">
multistage testing</link> (MST) or <link xlink:type="simple" xlink:href="../121/8218121.xml">
CAST</link> is used in the <link xlink:type="simple" xlink:href="../242/5791242.xml">
Uniform Certified Public Accountant Examination</link>. MST avoids or reduces some of the disadvantages of CAT as described below.  See the <weblink xlink:type="simple" xlink:href="http://www.leaonline.com/toc/ame/19/3">
2006 special issue of Applied Measurement in Education</weblink> for more information on MST.</p>

</sec>
<sec>
<st>
Advantages</st>

<p>

Adaptive tests can provide uniformly precise scores for most test-takers.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>  In contrast, standard fixed tests almost always provide the best precision for test-takers of medium <link xlink:type="simple" xlink:href="../868/1206868.xml">
ability</link> and increasingly poorer precision for test-takers with more extreme test scores.</p>
<p>

An adaptive test can typically be shortened by 50% and still maintain a higher level of <link xlink:type="simple" xlink:href="../932/41932.xml">
precision</link> than a fixed version.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> This translates into a time savings for the test-taker.  Test-takers do not waste their time attempting items that are too hard or trivially easy.  Additionally, the testing organization benefits from the time savings; the cost of examinee seat time is substantially reduced.  However, because the development of a CAT involves much more expense than a standard fixed-form test, a large population is necessary for a CAT testing program to be financially fruitful.</p>
<p>

Like any computer-based test, adaptive tests may show results immediately after testing.</p>
<p>

Adaptive testing, depending on the item selection <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>, may reduce exposure of some items because examinees typically receive different sets of items rather than the whole population being administered a single set.  However, it may increase the exposure of others (namely the medium or medium/easy items presented to most examinees at the beginning of the test).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

</sec>
<sec>
<st>
Disadvantages</st>
<p>

The first issue encountered in CAT is the calibration of the item pool.  In order to model the characteristics of the items (e.g., to pick the optimal item), all the items of the test must be pre-administered to a sizable sample and then analyzed.  To achieve this, new items must be mixed into the operational items of an exam (the responses are recorded but do not contribute to the test-takers' scores), called "pilot testing," "pre-testing," or "seeding."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>   This presents logistical, ethical, and security issues.  For example, it is impossible to field an operational adaptive test with brand-new, unseen items;<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>  all items must be pretested with a large enough sample to obtain stable item statistics.  This sample may be required to be as large as 1,000 examinees.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> Each program must decide what percentage of the test can reasonably be composed of unscored pilot test items.</p>
<p>

Although adaptive tests have <it>exposure control</it> algorithms to prevent overuse of a few items,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>  the exposure conditioned upon ability is often not controlled and can easily become close to 1.  That is, it is common for some items to become very common on tests for people of the same ability.  This is a serious security concern because groups sharing items may well have a similar functional ability level.  In fact, a completely randomized exam is the most secure (but also least efficient).</p>
<p>

Review of past items is generally disallowed.  Adaptive tests tend to administer easier items after a person answers incorrectly.  Supposedly, an astute test-taker could use such clues to detect incorrect answers and correct them.  Or, test-takers could be coached to deliberately pick wrong answers,  leading to an increasingly easier test.  After tricking the adaptive test into building a maximally easy exam, they could then review the items and answer them correctly--possibly achieving a very high score.  Test-takers frequently complain about the inability to review.<weblink xlink:type="simple" xlink:href="http://edres.org/scripts/cat/catdemo.htm">
http://edres.org/scripts/cat/catdemo.htm</weblink></p>

</sec>
<sec>
<st>
CAT Components</st>

<p>

There are five technical components in building a CAT (the following is adapted from Weiss &amp; Kingsbury, 1984<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> ).  This list does not include practical issues, such as item pretesting or live field release.</p>
<p>

<list>
<entry level="1" type="number">

 Calibrated item pool</entry>
<entry level="1" type="number">

 Starting point or entry level</entry>
<entry level="1" type="number">

 Item selection <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link></entry>
<entry level="1" type="number">

 Scoring procedure</entry>
<entry level="1" type="number">

 Termination criterion</entry>
</list>
</p>

<ss1>
<st>
Calibrated Item Pool</st>
<p>

A pool of items must be available for the CAT to choose from.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>  The pool must be calibrated with a psychometric model, which is used as a basis for the remaining four components.  Typically, <link xlink:type="simple" xlink:href="../159/420159.xml">
item response theory</link> is employed as the psychometric model.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>  One reason item response theory is popular is because it places persons and items on the same metric (denoted by the Greek letter theta), which is helpful for issues in item selection (see below). </p>

</ss1>
<ss1>
<st>
Starting Point</st>
<p>

In CAT, items are selected based on the examinee's performance up to a given point in the test.  However, the CAT is obviously not able to make any specific estimate of examinee <link xlink:type="simple" xlink:href="../868/1206868.xml">
ability</link> when no items have been administered.  So some other initial estimate of examinee ability is necessary.  If some previous information regarding the examinee is known, it can be used,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>  but often the CAT just assumes that the examinee is of average ability - hence the first item often being of medium difficulty.</p>

</ss1>
<ss1>
<st>
Item Selection Algorithm</st>
<p>

As mentioned previously, <link xlink:type="simple" xlink:href="../159/420159.xml">
item response theory</link> places examinees and items on the same metric.  Therefore, if the CAT has an estimate of examinee ability, it is able to select an item that is most appropriate for that estimate.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>   Technically, this is done by selecting the item with the greatest <it>information</it> at that point.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>  <link xlink:type="simple" xlink:href="../062/18985062.xml">
Information</link> is a function of the discrimination parameter of the item, as well as the conditional variance and pseudoguessing parameter (if used).</p>

</ss1>
<ss1>
<st>
Scoring Procedure</st>
<p>

After an item is administered, the CAT updates its estimate of the examinee's <link xlink:type="simple" xlink:href="../868/1206868.xml">
ability</link> level.  If the examinee answered the item correctly, the CAT will likely estimate their ability to be somewhat higher, and vice versa.  This is done by using the item response function from <link xlink:type="simple" xlink:href="../159/420159.xml">
item response theory</link> to obtain a <link xlink:type="simple" xlink:href="../968/44968.xml">
likelihood function</link> of the examinee's ability.  Two methods for this are called <it>maximum likelihood estimation</it> and <it>Bayesian estimation</it>.  The latter assumes an <it>a priori</it> distribution of examinee ability, and has two commonly used estimators: <it>expectation a posteriori</it> and <it>maximum a posteriori</it>.  <link xlink:type="simple" xlink:href="../806/140806.xml">
Maximum likelihood</link> is equivalent to a Bayes maximum a posterior estimate if a uniform (f(x)=1) prior is assumed.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>  Maximum likelihood is asymptotically unbiased, but cannot provide a theta estimate for a nonmixed (all correct or incorrect) response vector, in which case a Bayesian method may have to be used temporarily.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

</ss1>
<ss1>
<st>
Termination Criterion</st>
<p>

The CAT <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> is designed to repeatedly administer items and update the estimate of examinee ability.  This will continue until the item pool is exhausted unless a termination criterion is incorporated into the CAT.  Often, the test is terminated when the examinee's standard error of measurement falls below a certain user-specified value, hence the statement above that an advantage is that examinee scores will be uniformly precise or "equiprecise."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>   Other termination criteria exist for different purposes of the test, such as if the test is designed only to determine if the examinee is should "Pass" or "Fail" the test, rather than obtaining a precise estimate of their ability.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>

</ss1>
</sec>
<sec>
<st>
Other Issues</st>

<ss1>
<st>
Pass-Fail CAT</st>
<p>

In many situations, the purpose of the test is to classify examinees into two or more <link xlink:type="simple" xlink:href="../648/312648.xml">
mutually exclusive</link> and <link xlink:type="simple" xlink:href="../716/303716.xml">
exhaustive</link> categories.  This includes the common "mastery test" where the two classifications are "pass" and "fail," but also includes situations where there are three or more classifications, such as "Insufficient," "Basic," and "Advanced" levels of knowledge or competency.  The kind of "item-level adaptive" CAT described in this article is most appropriate for tests that are not "pass/fail" or for pass/fail tests where providing good feedback is extremely important.)  Some modifications are necessary for a pass/fail CAT, also known as a <link xlink:type="simple" xlink:href="../447/7955447.xml">
computerized classification test (CCT)</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>  For examinees with true scores very close to the passing score, computerized classification tests will result in long tests while those with true scores far above or below the passing score will have shortest exams.</p>
<p>

For example, a new termination criterion and scoring algorithm must be applied that classifies the examinee into a category rather than providing a point estimate of ability.  There are two primary methodologies available for this.  The more prominent of the two is the <link xlink:type="simple" xlink:href="../283/7970283.xml">
sequential probability ratio test</link> (SPRT).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref> This formulates the examinee classification problem as a <link>
hypothesis test</link> that the examinee's ability is equal to either some specified point above the <link xlink:type="simple" xlink:href="../709/15530709.xml">
cutscore</link> or another specified point below the cutscore.  Note that this is a point hypothesis formulation rather than a composite hypothesis formulation<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> that is more conceptually appropriate.  A composite hypothesis formulation would be that the examinee's ability is in the region above the cutscore or the region below the cutscore.</p>
<p>

A <link xlink:type="simple" xlink:href="../911/280911.xml">
confidence interval</link> approach is also used, where after each item is administered, the algorithm determines the probability that the examinee's true-score is above or below the passing score.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref>  For example, the algorithm may continue until the 95% <link xlink:type="simple" xlink:href="../911/280911.xml">
confidence interval</link> for the true score no longer contains the passing score.  At that point, no further items are needed because the pass-fail decision is already 95% accurate, assuming that the psychometric models underlying the adaptive testing fit the examinee and test. This approach was originally called "adaptive mastery testing"<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref> but it can be applied to non-adaptive item selection and classification situations of two or more cutscores (the typical mastery test has a single cutscore).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></p>
<p>

As a practical matter, the algorithm is generally programmed to have a minimum and a maximum test length (or a minimum and maximum administration time). Otherwise, it would be possible for an examinee with ability very close to the cutscore to be administered every item in the bank without the algorithm making a decision.</p>
<p>

The item selection algorithm utilized depends on the termination criterion.  Maximizing information at the cutscore is more appropriate for the SPRT because it maximizes the difference in the probabilities used in the <link xlink:type="simple" xlink:href="../035/45035.xml">
likelihood ratio</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref>  Maximizing information at the ability estimate is more appropriate for the confidence interval approach because it minimizes the conditional standard error of measurement, which decreases the width of the confidence interval needed to make a classification.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></p>

</ss1>
<ss1>
<st>
Practical Constraints of Adaptivity</st>
<p>

<company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../550/347550.xml">
ETS</link></company>
 researcher Martha Stocking has quipped that most adaptive tests are actually <it>barely adaptive tests</it> (BATs) because, in practice, many constraints are imposed upon item choice. For example, CAT exams must usually meet content specifications<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>; a verbal exam may need to be composed of equal numbers of analogies, fill-in-the-blank and synonym item types. CATs typically have some form of item exposure constraints,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> to prevent the most informative items from being over-exposed.  Also, on some tests, an attempt is made to balance surface characteristics of the items such as <link xlink:type="simple" xlink:href="../076/38076.xml">
gender</link> of the people in the items or the ethnicities implied by their names.  Thus CAT exams are frequently constrained in which items it may choose and for some exams the constraints may be substantial and require complex search strategies (e.g., <link xlink:type="simple" xlink:href="../730/43730.xml">
linear programming</link>) to find suitable items.</p>
<p>

A simple method for controlling item exposure is the "randomesque" or strata method. Rather than selecting the most informative item at each point in the test, the algorithm randomly selects the next item from the next five or ten most informative items.  This can be used throughout the test, or only at the beginning.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>  Another method is the Sympson-Hetter method<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref> , in which a random number is drawn from U(0,1), and compared to a <it>ki</it> parameter determined for each item by the test user.  If the random number is greater than <it>ki</it>, the next most informative item is considered.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> </p>
<p>

Wim van der Linden and colleagues<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref> have advanced an alternative approach called <it>shadow testing</it> which involves creating entire <it>shadow tests</it> as part of selecting items. Selecting items from shadow tests helps adaptive tests meet selection criteria by focusing on globally optimal choices (as opposed to choices that are optimal <it>for a given item</it>).</p>

</ss1>
</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
Weiss, D. J., &amp; Kingsbury, G. G. (1984). Application of computerized adaptive testing to educational problems.  Journal of Educational Measurement, 21, 361-375.</entry>
<entry id="2">
Thissen, D., &amp; Mislevy, R.J. (2000).  Testing Algorithms. In Wainer, H. (Ed.) Computerized Adaptive Testing: A Primer. Mahwah, NJ: Lawrence Erlbaum Associates.</entry>
<entry id="3">
Green, B.F. (2000). System design and operation. In Wainer, H. (Ed.) Computerized Adaptive Testing: A Primer. Mahwah, NJ: Lawrence Erlbaum Associates.</entry>
<entry id="4">
Wainer, H., &amp; Mislevy, R.J. (2000).  Item response theory, C\calibration, and estimation. In Wainer, H. (Ed.) Computerized Adaptive Testing: A Primer. Mahwah, NJ: Lawrence Erlbaum Associates.</entry>
<entry id="5">
Lin, C.-J. &amp; Spray, J.A. (2000). Effects of item-selection criteria on classification testing with the sequential probability ratio test. (Research Report 2000-8). Iowa City, IA: ACT, Inc.</entry>
<entry id="6">
Wald, A. (1947). Sequential analysis. New York: Wiley.</entry>
<entry id="7">
Reckase, M. D. (1983). A procedure for decision making using tailored testing. In D. J. Weiss (Ed.), New horizons in testing: Latent trait theory and computerized adaptive testing (pp. 237-254). New York: Academic Press.</entry>
<entry id="8">
Weitzman, R. A. (1982). Sequential testing for selection. Applied Psychological Measurement, 6, 337-351.</entry>
<entry id="9">
Kingsbury, G.G., &amp; Weiss, D.J. (1983).  A comparison of IRT-based adaptive mastery testing and a sequential mastery testing procedure.  In D. J. Weiss (Ed.), New horizons in testing: Latent trait theory and computerized adaptive testing (pp. 237-254). New York: Academic Press.</entry>
<entry id="10">
Eggen, T. J. H. M, &amp; Straetmans, G. J. J. M. (2000). Computerized adaptive testing for classifying examinees into three categories.  Educational and Psychological Measurement, 60, 713-734.</entry>
<entry id="11">
Spray, J. A., &amp; Reckase, M. D. (1994). The selection of test items for decision making with a computerized adaptive test.  Paper presented at the Annual Meeting of the National Council for Measurement in Education (New Orleans, LA, April 5-7, 1994).</entry>
<entry id="12">
Sympson, B.J., &amp; Hetter, R.D. (1985). Controlling item-exposure rates in computerized adaptive testing.  Paper presented at the annual conference of the Military Testing Association, San Diego.</entry>
<entry id="13">
For example: van der Linden, W. J., &amp; Veldkamp, B. P. (2004). Constraining item exposure in computerized adaptive testing with shadow tests. Journal of Educational and Behavioral Statistics, 29, 273â€‘291.</entry>
</reflist>
</p>

<ss1>
<st>
Additional sources</st>
<p>

<list>
<entry level="1" type="bullet">

Drasgow, F., &amp; Olson-Buchanan, J. B. (Eds.). (1999). Innovations in computerized assessment. Hillsdale, NJ: Erlbaum. </entry>
<entry level="1" type="bullet">

Van der Linden, W. J., &amp; Glas, C.A.W. (Eds.). (2000). Computerized adaptive testing: Theory and practice. Boston, MA: Kluwer. </entry>
<entry level="1" type="bullet">

Wainer, H. (Ed.). (2000). Computerized adaptive testing: A Primer (2nd Edition). Mahwah, NJ: ELawrence Erlbaum Associates.</entry>
<entry level="1" type="bullet">

Weiss, D.J. (Ed.). (1983). New horizons in testing: Latent trait theory and computerized adaptive testing (pp. 237-254). New York: Academic Press.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../675/1944675.xml">
Educational technology</link></entry>
<entry level="1" type="bullet">

<work wordnetid="100575741" confidence="0.8">
<examination wordnetid="100635850" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../447/7955447.xml">
Computerized classification test</link></activity>
</psychological_feature>
</act>
</investigation>
</event>
</examination>
</work>
</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.psych.umn.edu/psylabs/catcentral/">
CAT Central</weblink> by David J. Weiss</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.gmathacks.com/cat-strategy/introduction-to-the-gmat-computer-adaptive-test.html">
GMAT Hacks: Introduction to the GMAT Computer-Adaptive Test</weblink> by Jeff Sackmann</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.carla.umn.edu/assessment/CATfaq.html">
Frequently Asked Questions about Computer-Adaptive Testing (CAT)</weblink>. Retrieved April 15, 2005.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://edres.org/scripts/cat/catdemo.htm">
An On-line, Interactive, Computer Adaptive Testing Tutorial</weblink> by Lawrence L. Rudner. November 1998. Retrieved April 15, 2005.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.leaonline.com/toc/ame/19/3">
<it>Special issue:  An introduction to multistage testing.''</it></weblink> Applied Measurement in Education, 19(3).</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ericdigests.org/pre-9213/tests.htm">
Computerized Adaptive Tests</weblink> - from the <link xlink:type="simple" xlink:href="../099/1958099.xml">
Education Resources Information Center</link> Clearinghouse on Tests Measurement and Evaluation, <link xlink:type="simple" xlink:href="../956/108956.xml">
Washington, DC</link></entry>
</list>
</p>




</sec>
</bdy>
</activity>
</psychological_feature>
</act>
</investigation>
</event>
</examination>
</work>
</article>
