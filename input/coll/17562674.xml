<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 04:15:33[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Margin classifier</title>
<id>17562674</id>
<revision>
<id>231227562</id>
<timestamp>2008-08-11T13:43:59Z</timestamp>
<contributor>
<username>Oliver202</username>
<id>1460323</id>
</contributor>
</revision>
<categories>
<category>Wikipedia articles needing context</category>
<category>Machine Learning</category>
<category>Wikipedia introduction cleanup</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_style.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 The introduction to this article provides <b>insufficient context</b> for those unfamiliar with the subject.
Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Margin_classifier&amp;action=edit">
improve the article</weblink> with a .</col>
</row>
</table>

<p>

In <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, a <b>margin classifer</b> is a <link xlink:type="simple" xlink:href="../543/1508543.xml">
classifier</link> which is able to give an associated distance from decision boundary for each example.  For instance, if a <link xlink:type="simple" xlink:href="../974/98974.xml">
linear classifier</link> (e.g. <link xlink:type="simple" xlink:href="../777/172777.xml">
perceptron</link> or <link xlink:type="simple" xlink:href="../657/1470657.xml">
linear discriminant analysis</link>) is used, the distance (typically <link xlink:type="simple" xlink:href="../932/53932.xml">
euclidean distance</link>, though others may be used) of an example from the separating hyperplane is the margin of that example.</p>
<p>

The notion of margin is important in several machine learning classification algorithms, as it can be used to bound the <link xlink:type="simple" xlink:href="../249/2456249.xml">
generalization error</link> of the classifier.  These bounds are frequently shown using the <link xlink:type="simple" xlink:href="../846/305846.xml">
VC dimension</link>.  Of particular prominence is the generalization error bound on <link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link> algorithms and <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link>s.</p>


<sec>
<st>
 Support Vector Machine Definition of Margin </st>

<p>

See <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link>s and <link xlink:type="simple" xlink:href="../781/1423781.xml">
maximum-margin hyperplane</link> for details.</p>

</sec>
<sec>
<st>
 Boosting Definition of Margin </st>

<p>

The margin for an iterative <link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link> algorithm given a set of examples with two classes can be defined as follows.  The classifier is given an example pair <math>(x,y)</math> where <math>x \in X</math> is a domain space and <math>y \in Y = \{-1, +1\}</math> is the label of the example.  The iterative boosting algorithm then selects a classifier <math>h_j \in C</math> at each iteration <math>j</math> where <math>C</math> is a space of possible classifiers that predict real values.  This hypothesis is then weighted by <math>\alpha_j \in R</math> as selected by the boosting algorithm.  At iteration <math>t</math>, The margin of an example <math>x</math> can thus be defined as</p>
<p>

<math>\frac{y \sum_j^t \alpha_j h_j (x)}{\sum |\alpha_j|}</math></p>
<p>

By this definition, the margin is positive if the example is labeled correctly and negative is the example is labeled incorrectly.</p>
<p>

This definition may be modified and is not the only way to define margin for boosting algorithms.  However, there are reasons why this definition may be appealing (see the Schapire et al paper for details <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>).</p>

</sec>
<sec>
<st>
 Bounds </st>


</sec>
<sec>
<st>
 Examples of Margin-Based Algorithms </st>

<p>

Many classifiers can give an associated margin for each example.  However, only some classifiers utilize information of the margin while learning from a data set.  </p>
<p>

Many <link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link> algorithms rely on the notion of a margin to give weights to examples.  If a convex loss is utilized (as in <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../465/17627465.xml">
LogitBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, and all members of the <link>
AnyBoost</link> family of algorithms) then an example with higher margin will receive less (or equal) weight than an example with lower margin.  This leads the boosting algorithm to focus weight on low margin examples.  In nonconvex algorithms (e.g. <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../016/11448016.xml">
BrownBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
), the margin still dictates the weighting of an example, though the weighting is non-monotone with respect to margin.  There exists boosting algorithms that provably maximize the minimum margin (e.g. see <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>).</p>
<p>

<know-how wordnetid="105616786" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../309/65309.xml">
Support vector machine</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</method>
</rule>
</event>
</know-how>
s provably maximize the margin of the separating hyperplane.  Support vector machines that are trained using noisy data (there exists no perfect separation of the data in the given space) maximize the soft margin.  More discussion of this can be found in the <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link> article.</p>
<p>

The <link>
voted-perceptron</link> algorithm is a margin maximizing algorithm based on an iterative application of the classic <link xlink:type="simple" xlink:href="../777/172777.xml">
perceptron</link> algorithm.</p>

</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
Robert E. Schapire, Yoav Freund, Peter Bartlett and Wee Sun Lee.
Boosting the margin: A new explanation for the effectiveness of voting methods.
The Annals of Statistics, 26(5):1651-1686, 1998.</entry>
<entry id="2">
Manfred Warmuth and Karen Glocer and Gunnar R&amp;auml;tsch. Boosting Algorithms for Maximizing the Soft Margin. In the Proceedings of Advances in Neural Information Processing Systems 20, 2007, pp 1585--1592.</entry>
</reflist>
</p>


</sec>
</bdy>
</article>
