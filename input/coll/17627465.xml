<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 04:19:36[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>LogitBoost</title>
<id>17627465</id>
<revision>
<id>219715525</id>
<timestamp>2008-06-16T15:30:21Z</timestamp>
<contributor>
<username>Michael Hardy</username>
<id>4626</id>
</contributor>
</revision>
<categories>
<category>Ensemble learning</category>
<category>Machine learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

<b>LogitBoost</b> is a <link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link> algorithm formulated by <link xlink:type="simple" xlink:href="../900/728900.xml">
Jerome Friedman</link>, <link>
Trevor Hastie</link>, and <link>
Robert Tibshirani</link>.  The original paper <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> casts the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 algorithm into a statistics framework.  Specifically, if one considers <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 as a <link xlink:type="simple" xlink:href="../284/3608284.xml">
generalized additive model</link> and then applies the cost functional of <link xlink:type="simple" xlink:href="../631/226631.xml">
logistic regression</link>, one can derive the LogitBoost algorithm.
<sec>
<st>
Minimizing the LogitBoost cost functional</st>

<p>

LogitBoost can be seen as a <link xlink:type="simple" xlink:href="../411/1674411.xml">
convex optimization</link>. Specifically, given that we seek an additive model of the form</p>
<p>

<indent level="1">

<math>f = \sum_t \alpha_t h_t</math>
</indent>

the LogitBoost algorithm minimizes the logistic loss:</p>
<p>

<indent level="1">

<math>\sum_i \log\left( 1 + e^{-y_i f(x_i)}\right)</math>
</indent>

</p>
</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
Jerome Friedman, Trevor Hastie and Robert Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics 28(2), 2000.  337-407</entry>
</reflist>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
