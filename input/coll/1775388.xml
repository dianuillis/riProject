<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:55:57[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>K-nearest neighbor algorithm</title>
<id>1775388</id>
<revision>
<id>239431035</id>
<timestamp>2008-09-19T01:35:12Z</timestamp>
<contributor>
<username>Robbot</username>
<id>25261</id>
</contributor>
</revision>
<categories>
<category>Search algorithms</category>
<category>Articles lacking in-text citations</category>
<category>Articles with invalid date parameter in template</category>
<category>All articles to be expanded</category>
<category>Machine learning</category>
<category>Classification algorithms</category>
<category>Articles to be expanded since April 2007</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Text_document_with_red_question_mark.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 This article or section includes a  or , but its sources remain unclear because it lacks <b>.</b>
You can  this article by introducing more precise citations . <it>(August 2008)''</it></col>
</row>
</table>


"KNN" redirects here. For other uses, see <link xlink:type="simple" xlink:href="../252/13465252.xml">
KNN (disambiguation)</link>.
<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-notice" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="44px" src="Wiki_letter_w.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=K-nearest_neighbor_algorithm&amp;action=edit">
improve this article or section</weblink> by expanding it.</b> Further information might be found on the  or at . 
<it>(April 2007)''</it></col>
</row>
</table>


In <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link>, the <b><it>k</it></b><b>-nearest neighbor algorithm</b> (<it>k</it>-NN) is a method for <link xlink:type="simple" xlink:href="../244/1579244.xml">
classifying</link> objects based on closest training examples in the <link xlink:type="simple" xlink:href="../063/5245063.xml">
feature space</link>.  <it>k</it>-NN is a type of <link>
instance-based learning</link>, or <link xlink:type="simple" xlink:href="../879/10747879.xml">
lazy learning</link> where the function is only approximated locally and all computation is deferred until classification. It can also be used for <link xlink:type="simple" xlink:href="../568/26568.xml">
regression</link>.</p>

<sec>
<st>
Overview</st>

<p>

The <b> <it>k</it></b><b>-nearest neighbor algorithm</b> is amongst the simplest of all <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> algorithms. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common amongst its <it>k</it> nearest neighbors. <it>k</it> is a positive <link xlink:type="simple" xlink:href="../563/14563.xml">
integer</link>, typically small. If <it>k</it> = 1, then the object is simply assigned to the class of its nearest neighbor. In binary (two class) classification problems, it is helpful to choose <it>k</it> to be an odd number as this avoids tied votes.</p>
<p>

The same method can be used for <link xlink:type="simple" xlink:href="../568/26568.xml">
regression</link>, by simply assigning the property value for the object to be the average of the values of its <it>k</it> nearest neighbors. It can be useful to weight the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones.</p>
<p>

The neighbors are taken from a set of objects for which the correct classification (or, in the case of regression, the value of the property) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required. In order to identify neighbors, the objects are represented by position vectors in a multidimensional feature space. It is usual to use the <link xlink:type="simple" xlink:href="../932/53932.xml">
Euclidean distance</link>, though other distance measures, such as the <link xlink:type="simple" xlink:href="../354/408354.xml">
Manhattan distance</link> could in principle be used instead. The <it>k</it>-nearest neighbor algorithm is sensitive to the local structure of the data.</p>

</sec>
<sec>
<st>
Algorithm</st>

<p>

<image location="right" width="150px" src="KnnClassification.svg" type="thumb">
<caption>

Example of <it>k</it>-NN classification. The test sample (green circle) should be classified either to the first class of blue squares or to the second class of red triangles. If <it>k = 3</it> it is classified to the second class because there are 2 triangles and only 1 square inside the inner circle. If <it>k = 5</it> it is classified to first class (3 squares vs. 2 triangles inside the outer circle).
</caption>
</image>
</p>
<p>

The training examples are vectors in a multidimensional feature space. The space is partitioned into regions by locations and labels of the training samples. A point in the space is assigned to the class <it>c</it> if it is the most frequent class label among the <it>k</it> nearest training samples. Usually <link xlink:type="simple" xlink:href="../932/53932.xml">
Euclidean distance</link> is used.</p>
<p>

The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples. In the actual classification phase, the test sample (whose class is not known) is represented as a vector in the feature space. Distances from the new vector to all stored vectors are computed and <it>k</it> closest samples are selected. There are a number of ways to classify the new vector to a particular class, one of the most used techniques is to predict the new vector to the most common class amongst the K nearest neighbors. A major drawback to using this technique to classify a new vector to a class is that the classes with the more frequent examples tend to dominate the prediction of the new vector, as they tend to come up in the K nearest neighbors when the neighbors are computed due to their large number. One of the ways to overcome this problem is to take into account the distance of each K nearest neighbors with the new vector that is to be classified and predict the class of the new vector based on these distances.</p>

</sec>
<sec>
<st>
Parameter selection</st>

<p>

The best choice of <it>k</it> depends upon the data; generally, larger values of <it>k</it> reduce the effect of noise on the classification, but make boundaries between classes less distinct. A good <it>k</it> can be selected by various <link xlink:type="simple" xlink:href="../509/846509.xml">
heuristic</link> techniques, for example, <link xlink:type="simple" xlink:href="../612/416612.xml">
cross-validation</link>. The special case where the class is predicted to be the class of the closest training sample (i.e. when <it>k</it> = 1) is called the nearest neighbor algorithm.</p>
<p>

The accuracy of the <it>k</it>-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into <link xlink:type="simple" xlink:href="../950/1179950.xml">
selecting or scaling</link> features to improve classification.  A particularly popular approach is the use of <link xlink:type="simple" xlink:href="../837/190837.xml">
evolutionary algorithm</link>s to optimize feature scaling(needs citation).  Another popular approach is to scale features by the <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link> of the training data with the training classes(needs citation).</p>

</sec>
<sec>
<st>
Properties</st>

<p>

The naive version of the algorithm is easy to implement by computing the distances from the test sample to all stored vectors, but it is computationally intensive, especially when the size of the training set grows. Many <link xlink:type="simple" xlink:href="../022/7309022.xml">
nearest neighbor search</link> algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed. Some optimizations involve partitioning the feature space, and only computing distances within specific nearby volumes. Several different types of nearest neighbor finding algorithms include:
<list>
<entry level="1" type="bullet">

 <link>
Linear scan</link></entry>
<entry level="1" type="bullet">

 <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../725/1676725.xml">
Kd-tree</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</artifact>
</structure>
s</entry>
<entry level="1" type="bullet">

 <link>
Balltrees</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../216/3417216.xml">
Metric tree</link>s</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../012/11634012.xml">
Locality sensitive hashing</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (LSH)</entry>
<entry level="1" type="bullet">

 <link>
Agglomerative-Nearest-Neighbor</link></entry>
<entry level="1" type="bullet">

 <link>
Redundant Bit Vectors (RBV)</link></entry>
</list>
</p>
<p>

The nearest neighbor algorithm has some strong <link xlink:type="simple" xlink:href="../569/1497569.xml">
consistency</link> results. As the amount of data approaches infinity, the algorithm is guaranteed to yield an error rate no worse than twice the <link>
Bayes error rate</link> (the minimum achievable error rate given the distribution of the data). <it>k</it>-nearest neighbor is guaranteed to approach the Bayes error rate, for some value of <it>k</it> (where <it>k</it> increases as a function of the number of data points).</p>
<p>

The <it>k</it>-NN algorithm can also be adapted for use in estimating continuous variables. One such implementation uses an inverse distance weighted average of the <it>k</it>-nearest multivariate neighbors. This algorithm functions as follows:
<list>
<entry level="1" type="number">

 Compute Euclidean or <link xlink:type="simple" xlink:href="../760/799760.xml">
Mahalanobis distance</link> from target plot to those that were sampled.</entry>
<entry level="1" type="number">

 Order samples taking for account calculated distances. </entry>
<entry level="1" type="number">

 Choose heuristically optimal <it>k</it> nearest neighbor based on <link xlink:type="simple" xlink:href="../608/8648608.xml">
RMSE</link> done by cross validation technique. </entry>
<entry level="1" type="number">

 Calculate an inverse distance weighted average with the <it>k</it>-nearest multivariate neighbors.</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../022/7309022.xml">
Nearest neighbor search</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../164/1164.xml">
Artificial intelligence</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../253/42253.xml">
Data mining</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../057/2090057.xml">
Parzen window</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../706/126706.xml">
Pattern recognition</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../563/4141563.xml">
Predictive analytics</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../685/26685.xml">
Statistics</link></entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../309/65309.xml">
Support vector machine</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</method>
</rule>
</event>
</know-how>
</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../791/12259791.xml">
Belur V. Dasarathy</link></associate>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
, editor (1991) <it>Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques</it>, ISBN 0-8186-8930-7</entry>
<entry level="1" type="bullet">

 <it>Nearest-Neighbor Methods in Learning and Vision</it>, edited by Shakhnarovish, Darrell, and Indyk, The MIT Press, 2005, ISBN 0-262-19547-X</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Estimation of forest stand volumes by Landsat TM imagery and stand-level field-inventory data. <link xlink:type="simple" xlink:href="../066/14981066.xml">
Forest Ecology and Management</link>, Volume 196, Issues 2-3, 26 July 2004, Pages 245-255. Helena Mäkelä and Anssi Pekkarinen</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Estimation and mapping of forest stand density, volume, and cover type using the <it>k</it>-nearest neighbors method. Remote Sensing of Environment, Volume 77, Issue 3, September 2001, Pages 251-274. Hector Franco-Lopez, Alan R. Ek and Marvin E. Bauer</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://paul.luminos.nl/documents/show_document.php?d=197">
<it>k</it>-nearest neighbor algorithm in Visual Basic and Java</weblink> (includes executable and source code)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://people.revoledu.com/kardi/tutorial/KNN/index.html">
<it>k</it>-nearest neighbor tutorial using MS Excel</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.radwin.org/michael/projects/learning/">
Agglomerative-Nearest-Neighbor Algorithm, by Michael J. Radwin</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dsic.upv.es/~rparedes/english/research/CPW/index.html">
Weighted distance learning for nearest neighbor error minimization by R. Paredes and E. Vidal</weblink></entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
