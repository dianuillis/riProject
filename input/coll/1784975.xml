<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 18:56:56[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Quadratic classifier</title>
<id>1784975</id>
<revision>
<id>228418088</id>
<timestamp>2008-07-28T16:30:51Z</timestamp>
<contributor>
<username>Eubot</username>
<id>231599</id>
</contributor>
</revision>
<categories>
<category>Statistical classification</category>
<category>Machine learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

This article is about machine learning.&#32;&#32;For other uses of the word "quadratic" in mathematics, see <link xlink:type="simple" xlink:href="../761/239761.xml">
quadratic</link>.&#32;&#32;
A <b>quadratic classifier</b> is used in <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> to separate measurements of two or more classes of objects or events by a <link xlink:type="simple" xlink:href="../570/145570.xml">
quadric</link> surface. It is a more general version of the <link xlink:type="simple" xlink:href="../974/98974.xml">
linear classifier</link>.
<sec>
<st>
The classification problem</st>

<p>

<link xlink:type="simple" xlink:href="../244/1579244.xml">
Statistical classification</link> considers a set of <link xlink:type="simple" xlink:href="../133/217133.xml">
vectors</link> of observations <b>x</b> of an object or event, each of which has a known type <it>y</it>. This set is referred to as the <link xlink:type="simple" xlink:href="../228/1817228.xml">
training set</link>. The problem is then to determine for a given new observation vector, what the best class should be. For a quadratic classifier, the correct solution is assumed to be quadratic in the measurements, so <it>y</it> will be decided based on</p>
<p>

<indent level="1">

<math> \mathbf{x^T A x} + \mathbf{b^T x} + c </math>
</indent>

In the special case where each observation consists of two measurements, this means that the surfaces separating the classes will be <link xlink:type="simple" xlink:href="../673/19008673.xml">
conic sections</link> (<it>i.e.</it> either a <link xlink:type="simple" xlink:href="../323/76323.xml">
line</link>, a <link xlink:type="simple" xlink:href="../220/6220.xml">
circle</link> or <link xlink:type="simple" xlink:href="../277/9277.xml">
ellipse</link>, a <link xlink:type="simple" xlink:href="../231/23231.xml">
parabola</link> or a <link xlink:type="simple" xlink:href="../052/14052.xml">
hyperbola</link>).</p>

</sec>
<sec>
<st>
Quadratic discriminant analysis</st>

<p>

Quadratic discriminant analysis (QDA) is closely related to <link xlink:type="simple" xlink:href="../657/1470657.xml">
linear discriminant analysis</link> (LDA), where it is assumed that there are only two classes of points (so <math> y \in \{0,1 \} </math>), and that the measurements are <link>
 normally distributed</link>. Unlike LDA however, in QDA there is no assumption that the <link xlink:type="simple" xlink:href="../059/157059.xml">
covariance</link> of each of the classes is identical. When the assumption is true, the best possible test for the hypothesis that a given measurement is from a given class is the <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../035/45035.xml">
 likelihood ratio test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
. Suppose the means of each class are known to be <math> \mu_{y=0},\mu_{y=1} </math> and the covariances <math> \Sigma_{y=0}, \Sigma_{y=1} </math>. Then the likelihood ratio will be given by</p>
<p>

<indent level="1">

Likelihood ratio = <math> \frac{ \sqrt{2 \pi |\Sigma_{y=1}|}^{-1} \exp \left( -\frac{1}{2}(x-\mu_{y=1})^T \Sigma_{y=1}^{-1} (x-\mu_{y=1}) \right) }{ \sqrt{2 \pi |\Sigma_{y=0}|}^{-1} \exp \left( -\frac{1}{2}(x-\mu_{y=0})^T \Sigma_{y=0}^{-1} (x-\mu_{y=0}) \right)} &amp;lt; t </math>
</indent>

for some threshold t. After some rearrangement, it can be shown that the resulting separating surface between the classes is a quadratic.</p>

</sec>
<sec>
<st>
Other quadratic classifiers</st>

<p>

While QDA is the most commonly used method for obtaining a classifier, other methods are also possible. One such method is to create a longer measurement vector from the old one by adding all pairwise products of
individual measurements. For instance, the vector </p>
<p>

<indent level="1">

<math> [x_1, \; x_2, \; x_3] </math> 
</indent>

would become</p>
<p>

<indent level="1">

<math> [x_1, \; x_2, \; x_3, \; x_1^2, \; x_1x_2, \; x_1 x_3, \; x_2^2, \; x_2x_3, \; x_3^2] </math>.
</indent>

Finding a quadratic classifier for the original measurements would then become the same as finding a linear classifier based on the expanded measurement vector. For linear classifiers based only on <link xlink:type="simple" xlink:href="../093/157093.xml">
dot products</link>, these expanded measurements do not have to be actually computed, since the dot product in the higher dimensional space is simply related to that in the original space. This is an example of the so-called <link xlink:type="simple" xlink:href="../912/303912.xml">
kernel trick</link>, which can be applied to linear discriminant analysis, as well as the <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link>.</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
