<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 04:49:57[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Co-training</title>
<id>18586131</id>
<revision>
<id>227913643</id>
<timestamp>2008-07-25T22:44:44Z</timestamp>
<contributor>
<username>WikiEditCompSci</username>
<id>7536638</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

<b>Co-training</b> is a <link xlink:type="simple" xlink:href="../632/2829632.xml">
semi-supervised learning</link> technique that requires two <it>views</it> of the data. It was introduced by Avrim Blum and Tom Mitchell. It assumes that each example is described using two different feature sets that provide different, complementary information about the instance. Ideally, the two views are <link xlink:type="simple" xlink:href="../135/801135.xml">
conditionally independent</link> (i.e., the two feature sets of each instance are conditionally independent given the class) and each view is sufficient (i.e., the class of an instance can be accurately predicted from each view alone). Co-training first learns a separate classifier for each view using any labeled examples. The most confident predictions of each classifier on the unlabeled data are then used to iteratively construct additional labeled <link xlink:type="simple" xlink:href="../228/1817228.xml">
training data</link>.<p>

The original co-training paper received 10 year's Best Paper Award at 25th <link xlink:type="simple" xlink:href="../449/18586449.xml">
International Conference on Machine Learning</link> (<link xlink:type="simple" xlink:href="../449/18586449.xml">
ICML</link> 2008). ICML is one of the renowned conferences in <link xlink:type="simple" xlink:href="../323/5323.xml">
Computer Science</link>.</p>
<p>

Co-training was initially used to classify web-pages using the text on the page as one view and the anchor text of hyperlinks on other pages that point to the page as the other view.</p>

<sec>
<st>
 References </st>

<p>

Blum, A., Mitchell, T. <weblink xlink:type="simple" xlink:href="http://www.cs.wustl.edu/~zy/paper/cotrain.ps">
Combining labeled and unlabeled data with co-training</weblink>. <it>COLT: Proceedings of the Workshop on Computational Learning Theory</it>, Morgan Kaufmann, 1998, p. 92-100.</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
