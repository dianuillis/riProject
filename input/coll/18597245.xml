<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 04:50:23[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Context of computational complexity</title>
<id>18597245</id>
<revision>
<id>234549472</id>
<timestamp>2008-08-27T12:01:21Z</timestamp>
<contributor>
<username>Mcld</username>
<id>504497</id>
</contributor>
</revision>
<categories>
<category>Computational complexity theory</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../543/7543.xml">
computational complexity theory</link> and <link xlink:type="simple" xlink:href="../ury/23rd_century.xml">
analysis of algorithms</link>, a number of metrics are defined describing the resources, such as time or space, that a machine needs to solve a particular problem. Interpreting these metrics meaningfully requires context, and this context is frequently implicit and depends on the field and the problem under consideration. This article describes a number of important pieces of context and how they affect metrics.
<sec>
<st>
 Definitions of variables </st>

<p>

Metrics are usually described in terms of variables that are a function of the input. For example, the statement that <link xlink:type="simple" xlink:href="../205/15205.xml">
insertion sort</link> requires <link xlink:type="simple" xlink:href="../578/44578.xml">
O</link>(<it>n</it>2) comparisons is meaningless without defining <it>n</it>, which in this case is the number of elements in the input list.</p>
<p>

Because many different contexts use the same letters for their variables, confusion can arise. For example, the complexity of <link xlink:type="simple" xlink:href="../751/183751.xml">
primality test</link>s and <link xlink:type="simple" xlink:href="../411/57411.xml">
multiplication algorithm</link>s can be measured in two different ways: one in terms of the integers being tested or multiplied, and one in terms of the number of <link xlink:type="simple" xlink:href="../686/238686.xml">
binary</link> digits (bits) in those integers. For example, if <it>n</it> is the integer being tested for primality, <link xlink:type="simple" xlink:href="../660/557660.xml">
trial division</link> can test it in &amp;Theta;(n1/2) arithmetic operations; but if <it>n</it> is the number of bits in the integer being tested for primality, it requires &amp;Theta;(2n/2) time. In the fields of <link xlink:type="simple" xlink:href="../432/18934432.xml">
cryptography</link> and <link xlink:type="simple" xlink:href="../466/511466.xml">
computational number theory</link>, it is more typical to define the variable as the number of bits in the input integers.</p>
<p>

In the field of <link xlink:type="simple" xlink:href="../543/7543.xml">
computational complexity theory</link>, the input is usually specified as a binary string (or a string in some fixed alphabet), and the variable is usually the number of bits in this string. This measure depends on the specific encoding of the input, which must be specified. For example, if the input is an integer specified using <link xlink:type="simple" xlink:href="../041/236041.xml">
unary coding</link>, trial division will require only &amp;Theta;(n1/2) arithmetic operations; but if the same input is specified in binary (or any larger base) the complexity rises to &amp;Theta;(2n/2) operations, not because the algorithm is taking any additional time, but because the number of bits in the input <it>n</it> has become exponentially smaller. In the other direction, <it>succinct circuits</it> are compact representations of a limited class of <link xlink:type="simple" xlink:href="../806/325806.xml">
graph</link>s that occupy exponentially less space than ordinary representations like adjacency lists. Many graph algorithms on succinct circuits are <link xlink:type="simple" xlink:href="../694/54694.xml">
EXPTIME-complete</link>, whereas the same problems expressed with conventional representations are only <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../683/54683.xml">
P-complete</link></group>
</collection>
</class>
, because the succinct circuit inputs have smaller encodings.</p>
<p>

<link xlink:type="simple" xlink:href="../990/12127990.xml">
Output-sensitive algorithm</link>s define their complexity not only in terms of their input but also their output. For example, <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../430/8320430.xml">
Chan's algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 can compute the <link xlink:type="simple" xlink:href="../634/40634.xml">
convex hull</link> of a set of points in O(<it>n</it> log <it>h</it>) time, where <it>n</it> is the number of points in the input and <it>h</it> is the number of points in the resulting convex hull, a subset of the input points. Because every input point <it>might</it> be in the convex hull, an analysis in terms of the input alone would yield the less precise O(<it>n</it> log <it>n</it>) time.</p>
<p>

The complexity of some algorithms depends not only on parameters of the input but also parameters of the machine the algorithm is being run on; as mentioned in <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Metric+being+measured%22])">
<list>
<entry level="1" type="number">

Metric being measured</entry>
</list>
</link> below, this is typical in analyzing algorithms that run on systems with fixed cache hierarchies, where the complexity may depend on parameters such as cache size and block size.</p>

</sec>
<sec>
<st>
 Abstract machine </st>

<p>

To analyze an algorithm precisely, one must assume it is being executed by a particular <link xlink:type="simple" xlink:href="../492/60492.xml">
abstract machine</link>. For example, on a <link xlink:type="simple" xlink:href="../227/544227.xml">
random access machine</link>, <link xlink:type="simple" xlink:href="../266/4266.xml">
binary search</link> can be used to rapidly locate a particular value in a sorted list in only O(log <it>n</it>) comparisons, where <it>n</it> is the number of elements in the list; on a <invention wordnetid="105633385" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../403/30403.xml">
Turing machine</link></method>
</know-how>
</invention>
, this is not possible, since it can only move one memory cell at a time and so requires &amp;Omega;(<it>n</it>) steps to even reach an arbitrary value in the list.</p>
<p>

Moreover, different abstract machines define different <it>primitive</it> operations, which are operations that can be performed in constant time. Some machines, like Turing machines, only permit one bit at a time to be read or written; these are called bit operations, and the number of bit operations required to solve a problem is called its <b>bit complexity</b>. Bit complexity generalizes to any machine where the memory cells are of a fixed size that does not depend on the input; for this reason, algorithms that manipulate numbers much larger than the registers on ordinary PCs are typically analyzed in terms of their bit complexity. Put another way, the bit complexity is the complexity when the word size is a single bit, where word size is the size of each memory cell and register.</p>
<p>

Another commonly-used model has words with log <it>n</it> bits, where <it>n</it> is a variable depending on the input. For example, in <link>
graph algorithm</link>s, it is typical to assume that the vertices are numbered 1 through <it>n</it> and that each memory cell can hold any of these values, so that they can refer to any vertex. This is justified in problems where the input uses &amp;Omega;(<it>n</it>) words of storage, since on real computers, the memory cell and register size is typically selected in order to be able to index any word in memory. Operations on these words, such as copies and arithmetic operations, are assumed to operate in constant time, rather than O(log <it>n</it>) time. The number of word operations required to solve a problem in this model is sometimes called its <b>word complexity</b>.</p>
<p>

In <link xlink:type="simple" xlink:href="../543/7543.xml">
computational complexity theory</link>, researchers intentionally define <link xlink:type="simple" xlink:href="../426/502426.xml">
complexity class</link>es in a way intended to make them machine-independent - that is, if a problem lies in a particular class relative to a particular "reasonable" machine, it will lie in that class relative to any "reasonable" machine. For example, as mentioned above, the time complexity of <link xlink:type="simple" xlink:href="../266/4266.xml">
binary search</link> depends on whether a Turing machine or a random access machine is used; but regardless of the machine choice, it lies in <b><class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../550/658550.xml">
P</link></group>
</collection>
</class>
</b>, the class of polynomial-time algorithms. In general, <b>P</b> is considered a machine-independent class because any operation that can be simulated in polynomial time can be assumed to require constant time, since it can be treated as a subroutine without exceeding the polynomial-time bound.</p>
<p>

<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../431/22431.xml">
Oracle machine</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s are machines that have a specific operation that they can perform in constant time; this operation can be arbitrarily complex and can dramatically affect the complexity of algorithms performed on the machine. For example, if one has an oracle to solve any <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../466/39466.xml">
NP-complete</link></group>
</collection>
</class>
 problem, then any problem in <b><class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../562/21562.xml">
NP</link></group>
</collection>
</class>
</b> can be solved in polynomial time (whereas without the oracle, no polynomial-time algorithm is known for many of these problems). Oracle machines are impractical to construct but useful in theory for determining which proof techniques will be effective.</p>

</sec>
<sec>
<st>
 Metric being measured </st>

<p>

It's typical to say without qualification that <link xlink:type="simple" xlink:href="../205/15205.xml">
insertion sort</link> requires O(<it>n</it>2) time; however, it doesn't make sense to say that the bit complexity of insertion sort is O(<it>n</it>2), unless the elements being sorted are of constant size. If the elements are assumed to be integers between 1 and <it>n</it>, then the word complexity where words have log <it>n</it> bits would be O(<it>n</it>2), but it's preferable to have a model that allows sorting of lists other than lists of small integers, such as lists of strings. In this case, instead of measuring the number of time steps the abstract machine takes, it's preferable to define a particular metric appropriate to the problem at hand. For <link xlink:type="simple" xlink:href="../304/3189304.xml">
comparison sort</link> algorithms, which examine the input using only comparisons and modify it using only exchanges (swaps), the typical metric is either the number of element comparisons performed, the number of element exchanges performed, or the sum of these. Different comparison sort algorithms can be compared using these metrics, but for useful comparison with non-comparison sorting algorithms, such as <link xlink:type="simple" xlink:href="../980/25980.xml">
radix sort</link>, a different metric must be used, and the set of elements must be restricted.</p>
<p>

Because disk operations are orders of magnitude slower than accesses to main memory, the typical metric used in disk-based algorithms is the number of disk seeks or the number of blocks read from the disk, which depend on both the input and the parameters of the disk. RAM accesses and CPU operations are "free." Similarly, in many models used to study data structures, such as the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../377/1773377.xml">
cache-oblivious model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
, operations on cached data are considered "free" because they are typically orders of magnitude faster in practice than access to main memory. Consequently, the typical metric used is the number of cache misses, which depends on both the input and parameters of the cache.</p>


</sec>
</bdy>
</article>
