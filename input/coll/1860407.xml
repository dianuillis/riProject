<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:03:31[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<information  confidence="0.8" wordnetid="105816287">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<datum  confidence="0.8" wordnetid="105816622">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>K-means algorithm</title>
<id>1860407</id>
<revision>
<id>244369885</id>
<timestamp>2008-10-10T13:48:37Z</timestamp>
<contributor>
<username>BOTijo</username>
<id>3729068</id>
</contributor>
</revision>
<categories>
<category>Statistical algorithms</category>
<category>Neural networks</category>
<category>Data clustering algorithms</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

The <b>k-means algorithm</b> is an algorithm to <link xlink:type="simple" xlink:href="../675/669675.xml">
cluster</link> <math>n</math> objects based on attributes into <math>k</math> <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<family wordnetid="108078020" confidence="0.8">
<link xlink:type="simple" xlink:href="../240/340240.xml">
partitions</link></family>
</concept>
</idea>
, <math>k &amp;lt; n</math>. It is similar to the <link xlink:type="simple" xlink:href="../752/470752.xml">
expectation-maximization algorithm</link> for mixtures of <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussians</link> in that they both attempt to find the centers of natural clusters in the data. It assumes that the object attributes form a <link xlink:type="simple" xlink:href="../370/32370.xml">
vector space</link>. The objective it tries to achieve is to minimize total intra-cluster variance, or, the squared error function<p>

<indent level="2">

<math>V = \sum_{i=1}^{k} \sum_{x_j \in S_i} (x_j - \mu_i)^2 </math>
</indent>

where there are <it>k</it> clusters <it>Si</it>, i = 1, 2, ..., <it>k</it>, and <it>µi</it> is the <link xlink:type="simple" xlink:href="../926/187926.xml">
centroid</link> or mean point of all the points <it>xj</it> ∈ <it>Si</it>. </p>
<p>

The k-means clustering was invented in 1956.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> The most common form of the algorithm uses an iterative refinement heuristic known as <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../912/2607912.xml">
Lloyd's algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.  Lloyd's algorithm starts by partitioning the input points into k initial sets, either at random or using some heuristic data. It then calculates the mean point, or centroid, of each set. It constructs a new partition by associating each point with the closest centroid. Then the centroids are recalculated for the new clusters, and algorithm repeated by alternate application of these two steps until convergence, which is obtained when the points no longer switch clusters (or alternatively centroids are no longer changed).</p>
<p>

Lloyd's algorithm and k-means are often used synonymously, but in reality Lloyd's algorithm is a heuristic for solving the k-means problem<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>, as with certain combinations of starting points and centroids, Lloyd's algorithm can in fact converge to the wrong answer (ie a different and optimal answer to the minimization function above exists.)  </p>
<p>

Other variations exist<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>, but Lloyd's algorithm has remained popular because it converges extremely quickly in practice. In fact, many have observed that the number of iterations is typically much less than the number of points. Recently, however, David Arthur and Sergei Vassilvitskii showed that there exist certain point sets on which k-means takes <link xlink:type="simple" xlink:href="../576/44576.xml">
superpolynomial time</link>: 2Ω(√<it>n</it>) to converge.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>
<p>

Approximate k-means algorithms have been designed that make use of <link xlink:type="simple" xlink:href="../657/10364657.xml">
coreset</link>s: small subsets of the original data.</p>
<p>

In terms of performance the algorithm is not guaranteed to return a global optimum. The quality of the final solution depends largely on the initial set of clusters, and may, in practice, be much poorer than the global optimum.  Since the algorithm is extremely fast, a common method is to run the algorithm several times and return the best clustering found. </p>
<p>

A drawback of the k-means algorithm is that the number of clusters <it>k</it> is an input parameter. An inappropriate choice of <it>k</it> may yield poor results. The algorithm also assumes that the <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link> is an appropriate measure of cluster scatter.</p>

<sec>
<st>
Demonstration of the algorithm</st>
<p>

The following images demonstrate the k-means clustering algorithm in action, for the two-dimensional case. The initial centres are generated randomly to demonstrate the stages in more detail. The background space partitions are only for illustration and are not generated by the k-means algorithm.</p>
<p>

<image width="150px" src="K_Means_Example_Step_1.svg">
<caption>

Shows the initial randomized centroids and a number of points.
</caption>
</image>

<image width="150px" src="K_Means_Example_Step_2.svg">
<caption>

Points are associated with the nearest centroid.
</caption>
</image>

<image width="150px" src="K_Means_Example_Step_3.svg">
<caption>

Now the centroids are moved to the center of their respective clusters.
</caption>
</image>

<image width="150px" src="K_Means_Example_Step_4.svg">
<caption>

Steps 2 &amp; 3 are repeated until a suitable level of convergence has been reached.
</caption>
</image>
</p>


</sec>
<sec>
<st>
 Applications of the algorithm </st>

<ss1>
<st>
 Image Segmentation </st>

<p>

The k-means clustering algorithm is commonly used in <link xlink:type="simple" xlink:href="../596/6596.xml">
computer vision</link> as a form of <link xlink:type="simple" xlink:href="../717/505717.xml">
image segmentation</link>. The results of the segmentation are used to aid <link>
border detection</link> and <link xlink:type="simple" xlink:href="../466/14661466.xml">
object recognition</link>. In this context, the standard <link xlink:type="simple" xlink:href="../932/53932.xml">
euclidean distance</link> is usually insufficient in forming the clusters. Instead, a weighted distance measure utilizing <link xlink:type="simple" xlink:href="../665/23665.xml">
pixel</link> coordinates, <link xlink:type="simple" xlink:href="../989/25989.xml">
RGB</link> pixel color and/or intensity, and image texture is commonly used.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></p>

</ss1>
</sec>
<sec>
<st>
 Relation to PCA </st>

<p>

It has been shown recently (2001)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>
that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by the PCA (<link xlink:type="simple" xlink:href="../340/76340.xml">
principal component analysis</link>) principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace specified by the between-class scatter matrix.</p>

</sec>
<sec>
<st>
Enhancements</st>
<p>

In 2006 a new way of choosing the initial centers was proposed
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>, dubbed "k-means++".
The idea is to select centers in a way that they are already initially close to large quantities of points. The authors use <math>L^2</math> norm in selecting the centers, but general <math>L^n</math> may be used to tune the aggressiveness of the seeding.</p>
<p>

This seeding method gives out considerable improvements in the final error of k-means. Although the initial selection in the algorithm takes considerable time, the k-means itself converges very fast after this seeding and thus the seeding actually lowers the computation time too. The authors tested their method with real and synthetic datasets and obtained typically 2-fold to 10-fold improvements in speed, and for certain datasets close to 1000-fold improvements in error. Their tests almost always showed the new method to be at least as good as vanilla k-means in both speed and error.</p>
<p>

Additionally, the authors calculate an approximation ratio for their algorithm. This is something that has not been done with vanilla k-means (although with several variations of it). The k-means++ guarantees to have approximation ratio <math>O(\log(k))</math> where <math>k</math> is the number of clusters used.</p>

</sec>
<sec>
<st>
Variations</st>
<p>

The set of squared error minimizing cluster functions also includes the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../095/6406095.xml">
<it>K</it>-medoids</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 algorithm, an approach which forces the center point of each cluster to be one of the actual points.</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 J. B. MacQueen (1967): "Some Methods for classification and Analysis of Multivariate Observations", Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, University of California Press, 1:281-297</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 J. A. Hartigan (1975) "Clustering Algorithms". Wiley.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal">J. A. Hartigan and M. A. Wong&#32;(1979).&#32;"A K-Means Clustering Algorithm". <it><link xlink:type="simple" xlink:href="../685/26685.xml">
Applied Statistics</link></it>&#32;<b>28</b>&#32;(1): 100&ndash;108. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.2307%2F2346830">
10.2307/2346830</weblink>.</cite>&nbsp;</entry>
</list>
</p>

<p>

<reflist>
<entry id="1">
H. Steinhaus. Sur la division des corp materiels en parties. Bull. Acad. Polon. Sci., C1. III vol IV:801– 804, 1956.</entry>
<entry id="2">
S. Lloyd,  Last square quantization in PCM’s. Bell Telephone Laboratories Paper (1957). Published in journal much later: S. P. Lloyd. Least squares quantization in PCM. Special issue on quantization, IEEE Trans. Inform.
Theory, 28:129–137, 1982.
</entry>
<entry id="3">
D. Arthur, S. Vassilvitskii: <weblink xlink:type="simple" xlink:href="http://www.stanford.edu/~darthur/kMeansPlusPlus.pdf">
"k-means++ The Advantages of Careful Seeding"</weblink> 2007 Symposium on
Discrete Algorithms (SODA).
</entry>
<entry id="4">
 	 An efficient k-means clustering algorithm: Analysis and implementation, T. Kanungo, D. M. Mount, N. Netanyahu, C. Piatko, R. Silverman, and A. Y. Wu, IEEE Trans. Pattern Analysis and Machine Intelligence, 24 (2002), 881-892.
</entry>
<entry id="5">
 <cite style="font-style:normal"><link>
David Arthur</link> &amp; <link>
Sergei Vassilvitskii</link>&#32;(2006). "<weblink xlink:type="simple" xlink:href="http://www.cs.duke.edu/courses/spring07/cps296.2/papers/kMeans-socg.pdf">
How Slow is the k-means Method?</weblink>".&#32;<it>Proceedings of the 2006 Symposium on Computational Geometry (SoCG)</it>.</cite>&nbsp;</entry>
<entry id="6">
Shapiro, Linda G. &amp; Stockman, George C. (2001). <it>Computer Vision.</it> Upper Saddle River, NJ: Prentice Hall.</entry>
<entry id="7">
H. Zha, C. Ding, M. Gu, X. He and H.D. Simon.
"Spectral Relaxation for K-means Clustering",
Neural Information Processing Systems vol.14 (NIPS 2001). pp. 1057-1064, Vancouver, Canada. Dec. 2001.
</entry>
<entry id="8">
Chris Ding and Xiaofeng He. "K-means Clustering via Principal Component Analysis".
Proc. of Int'l Conf. Machine Learning (ICML 2004), pp 225-232. July 2004.
</entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://people.revoledu.com/kardi/tutorial/kMean/NumericalExample.htm">
Numerical Example of K-means clustering</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.leet.it/home/lale/clustering/">
Application example which uses K-means clustering to reduce the number of colors in images</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/AppletKM.html">
Interactive demo of the K-means-algorithm(Applet)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.javaworld.com/javaworld/jw-11-2006/jw-1121-thread.html">
An example of multithreaded application which uses K-means in Java</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www25.brinkster.com/denshade/kmeans.php.htm">
K-means application in php </weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://informationandvisualization.de/blog/kmeans-and-voronoi-tesselation-built-processing">
Another animation of the K-means-algorithm</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://mloss.org/software/view/48/">
A fast implementation of the K-means algorithm which uses the triangle inequality to speed up computation </weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.user.cifnet.com/~lwebzem/k_means/test3.cgi">
K-means clustering using Perl. Online clustering.</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../675/669675.xml">
Data clustering</link></entry>
<entry level="1" type="bullet">

 <information wordnetid="105816287" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../414/1796414.xml">
Linde-Buzo-Gray algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</datum>
</rule>
</event>
</information>
</entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</datum>
</rule>
</event>
</information>
</article>
