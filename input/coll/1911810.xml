<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:06:34[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Language model</title>
<id>1911810</id>
<revision>
<id>240736346</id>
<timestamp>2008-09-24T19:58:47Z</timestamp>
<contributor>
<username>Kathy mom</username>
<id>7932100</id>
</contributor>
</revision>
<categories>
<category>Statistical natural language processing</category>
</categories>
</header>
<bdy>

A statistical <b>language model</b> assigns a <link xlink:type="simple" xlink:href="../934/22934.xml">
probability</link> to a sequence of <it>m</it> words <math>P(w_1,\ldots,w_m)</math> by means of a <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link>.<p>

Language modeling is used in many <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link> applications such as <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, <link xlink:type="simple" xlink:href="../980/19980.xml">
machine translation</link>, <link xlink:type="simple" xlink:href="../912/746912.xml">
part-of-speech tagging</link>, <link xlink:type="simple" xlink:href="../015/310015.xml">
parsing</link> and <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link>. </p>
<p>

In <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link> and in <link xlink:type="simple" xlink:href="../013/8013.xml">
data compression</link>, such a model tries to capture the properties of a language, and to predict the next word in a speech sequence. </p>
<p>

When used in information retrieval, a language model is associated with a <link xlink:type="simple" xlink:href="../228/161228.xml">
document</link> in a collection. With query <it>Q</it> as input, retrieved documents are ranked based on the probability that the document's language model would generate the terms of the query, <it>P(Q|Md)</it>. </p>
<p>

Estimating the probability of sequences can become difficult in <link xlink:type="simple" xlink:href="../244/2890244.xml">
corpora</link>, in which <link xlink:type="simple" xlink:href="../975/44975.xml">
phrase</link>s or <unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link xlink:type="simple" xlink:href="../352/870352.xml">
sentence</link></definite_quantity>
</unit_of_measurement>
s can be arbitrarily long and hence some sequences are not observed during <link xlink:type="simple" xlink:href="../423/156423.xml">
training</link> of the language model (<link>
data sparseness problem</link> of <link xlink:type="simple" xlink:href="../332/173332.xml">
overfitting</link>). For that reason these models are often approximated using smoothed <link xlink:type="simple" xlink:href="../182/986182.xml">
N-gram</link> models.</p>

<sec>
<st>
 N-gram models </st>

<p>

In an n-gram model, the probability <math>P(w_1,\ldots,w_m)</math> of observing the sentence w1,...,wm is approximated as</p>
<p>

<math>
P(w_1,\ldots,w_m) = \prod^m_{i=1} P(w_i|w_1,\ldots,w_{i-1})
 \approx \prod^m_{i=1} P(w_i|w_{i-(n-1)},\ldots,w_{i-1})
</math></p>
<p>

Here, it is assumed that the probability of observing the <it>ith</it> word <it>wi</it> in the context history of the preceding <it>i-1</it> words can be approximated by the probability of observing it in the shortened context history of the preceding <it>n-1</it> words (<it>nth order <link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link>).</it></p>
<p>

The conditional probability can be calculated from n-gram frequency counts:
<math>
P(w_i|w_{i-(n-1)},\ldots,w_{i-1}) = \frac{count(w_{i-(n-1)},w_{i-1},\ldots,w_i)}{count(w_{i-(n-1)},\ldots,w_{i-1})}
</math></p>

<p>

The words <b>bigram</b> and <b>trigram</b> language model denote n-gram language models with <it>n=2</it> and <it>n=3</it>, respectively.</p>

<ss1>
<st>
 Example </st>
<p>

In a bigram (n=2) language model, the probability of the sentence <it>I saw the red house</it> is approximated as 
<math>
P(I,saw,the,red,house) \approx P(I) P(saw|I) P(the|saw) P(red|the) P(house|red)
</math></p>
<p>

whereas in a trigram (n=3) language model, the approximation is
<math>
P(I,saw,the,red,house) \approx P(I) P(saw|I) P(the|I,saw) P(red|saw,the) P(house|the,red)
</math></p>

</ss1>
</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../040/2229040.xml">
Factored language model</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal">J M Ponte and W B Croft&#32;(1998). "<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/ponte98language.html">
A Language Modeling Approach to Information Retrieval</weblink>".&#32;<it>Research and Development in Information Retrieval</it>: 275-281.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">F Song and W B Croft&#32;(1999). "<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/song99general.html">
A General Language Model for Information Retrieval</weblink>".&#32;<it>Research and Development in Information Retrieval</it>: 279-280.</cite>&nbsp;</entry>
</list>
</p>




</sec>
</bdy>
</article>
