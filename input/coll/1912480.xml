<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:04:27[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<statement  confidence="0.8" wordnetid="106722453">
<message  confidence="0.8" wordnetid="106598915">
<contradiction  confidence="0.8" wordnetid="107206887">
<social_science  confidence="0.8" wordnetid="106143154">
<paradox  confidence="0.8" wordnetid="106724559">
<falsehood  confidence="0.8" wordnetid="106756407">
<knowledge_domain  confidence="0.8" wordnetid="105999266">
<economics  confidence="0.8" wordnetid="106149484">
<discipline  confidence="0.8" wordnetid="105996646">
<science  confidence="0.8" wordnetid="105999797">
<header>
<title>Ellsberg paradox</title>
<id>1912480</id>
<revision>
<id>242146154</id>
<timestamp>2008-10-01T02:28:03Z</timestamp>
<contributor>
<username>SmackBot</username>
<id>433328</id>
</contributor>
</revision>
<categories>
<category>Economics of uncertainty</category>
<category>Paradoxes</category>
<category>Statistical paradoxes</category>
<category>Decision theory</category>
<category>Economics paradoxes</category>
<category>Utility</category>
</categories>
</header>
<bdy>

The <b>Ellsberg paradox</b> is a <link xlink:type="simple" xlink:href="../390/24390.xml">
paradox</link> in <link xlink:type="simple" xlink:href="../216/446216.xml">
decision theory</link> and <link xlink:type="simple" xlink:href="../102/548102.xml">
experimental economics</link> in which people's choices violate the <link xlink:type="simple" xlink:href="../803/736803.xml">
expected utility hypothesis</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>  It is generally taken to be evidence for <link xlink:type="simple" xlink:href="../128/4751128.xml">
ambiguity aversion</link>.  The paradox was popularized by <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../128/80128.xml">
Daniel Ellsberg</link></person>
, although a version of it was noted considerably earlier by <baron wordnetid="109840520" confidence="0.8">
<link xlink:type="simple" xlink:href="../973/37973.xml">
John Maynard Keynes</link></baron>
 (Keynes 1921, pp.75&ndash;76, p.315, ft.2).
<sec>
<st>
 The paradox </st>

<p>

Suppose you have an <link xlink:type="simple" xlink:href="../442/1241442.xml">
urn</link> containing 30 red balls and 60 other balls that are either black or yellow.  You don't know how many black or yellow balls there are, but that the total number of black balls plus the total number of yellow balls equals 60.  The balls are well mixed so that each individual ball is as likely to be drawn as any other.  You are now given a choice between two gambles:</p>
<p>

<table align="center" border="1">
<header>
Gamble A</header>
<header>
Gamble B</header>
<row>
<col>
You receive $100 if you draw a red ball</col>
<col>
You receive $100 if you draw a black ball</col>
</row>
</table>
</p>
<p>

Also you are given the choice between these two gambles (about a different draw from the same urn):</p>
<p>

<table align="center" border="1">
<header>
Gamble C</header>
<header>
Gamble D</header>
<row>
<col>
You receive $100 if you draw a red or yellow ball</col>
<col>
You receive $100 if you draw a black or yellow ball</col>
</row>
</table>
</p>
<p>

Since the prizes are exactly the same, it follows that you will <it>prefer</it> Gamble A to Gamble B <it>if, and only if</it>, you believe that drawing a red ball is more likely than drawing a black ball (according to expected utility theory). Also, there would be no clear preference between the choices if you thought that a red ball was as likely as a black ball. Similarly it follows that you will <it>prefer</it> Gamble C to Gamble D <it>if, and only if</it>, you believe that drawing a red or yellow ball is more likely than drawing a black or yellow ball.  If drawing a red ball is more likely than drawing a black ball, then drawing a red or yellow ball is also more likely than drawing a black or yellow ball. So, supposing you <it>prefer</it> Gamble A to Gamble B, it follows that you will also <it>prefer</it> Gamble C to Gamble D. And, supposing instead that you <it>prefer</it> Gamble D to Gamble C, it follows that you will also <it>prefer</it> Gamble B to Gamble A.</p>
<p>

When surveyed, however, most people <it>strictly prefer</it> Gamble A to Gamble B and Gamble D to Gamble C. Therefore, some assumptions of the expected utility theory are violated.</p>

<ss1>
<st>
Mathematical demonstration</st>

<p>

Mathematically, your estimated probabilities of each color ball can be represented as: <it>R</it>, <it>Y</it>, and <it>B</it>.  If you <it>strictly prefer</it> Gamble A to Gamble B, by utility theory, it is presumed this preference is reflected by the expected utilities of the two gambles: specifically, it must be the case that</p>
<p>

<indent level="1">

 <math>R \cdot U(\$100) + (1-R)  \cdot U(\$0) &amp;gt; B\cdot U(\$100) + (1-B) \cdot U(\$0) </math>
</indent>

where <math>U(\cdot)</math> is your utility function. If <math>U(\$100) &amp;gt; U(\$0)</math> (you strictly prefer $100 to nothing), this simplifies to:</p>
<p>

<indent level="1">

 <math>R &amp;gt; B \; </math>
</indent>

If you also strictly prefer Gamble D to Gamble C, the following inequality is similarly obtained:</p>
<p>

<indent level="1">

 <math>B\cdot U(\$100) + Y\cdot U(\$100)  + R \cdot U(\$0) &amp;gt; R \cdot U(\$100) + Y\cdot U(\$100) + B \cdot U(\$0) </math>
</indent>

This simplifies to:</p>
<p>

<indent level="1">

 <math>B &amp;gt; R \;</math>
</indent>

This contradiction indicates that your preferences are inconsistent with expected-utility theory.</p>

</ss1>
<ss1>
<st>
Generality of the paradox</st>

<p>

Note that the result holds regardless of your <link xlink:type="simple" xlink:href="../479/45479.xml">
utility function</link>. Indeed, the amount of the payoff is likewise irrelevant. Whichever gamble you choose, the prize for winning it is the same, and the cost of losing it is the same (no cost), so ultimately, there are only two outcomes: you receive a specific amount of money, or you receive nothing. Therefore it is sufficient to assume that you prefer receiving some money to receiving nothing (and in fact, this assumption is not necessary &mdash; in the mathematical treatment above, it was assumed <it>U</it>($100) &amp;gt; <it>U</it>($0), but a contradiction can still be obtained for <it>U</it>($100)  <it>U</it>($0) and for <it>U</it>($100) = <it>U</it>($0)).</p>
<p>

In addition, the result holds regardless of your <link xlink:type="simple" xlink:href="../700/177700.xml">
risk aversion</link>. All the gambles involve risk. By choosing Gamble D, you have a 1 in 3 chance of receiving nothing, and by choosing Gamble A, you have a 2 in 3 chance of receiving nothing. If Gamble A was less risky than Gamble B, it would follow that Gamble C was less risky than Gamble D (and vice versa), so, risk is not averted in this way.</p>
<p>

However, because the exact chances of winning are known for Gambles A and D, and not known for Gambles B and C, this can be taken as evidence for some sort of <link xlink:type="simple" xlink:href="../128/4751128.xml">
ambiguity aversion</link> which cannot be accounted for in expected utility theory. It has been demonstrated that this phenomenon occurs only when the choice set permits comparison of the ambiguous proposition with a less vague proposition (but not when ambiguous propositions are evaluated in isolation)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.</p>

</ss1>
<ss1>
<st>
Possible explanations</st>

<p>

There have been various attempts to provide decision-theoretic explanations of Ellsberg's observation. Since the probabilistic information available to the decision-maker is incomplete, these attempts sometimes focus on quantifying the non-probabilistic ambiguity which the decision-maker faces. That is, these alternative approaches sometimes suppose that the agent formulates a subjective (though not necessarily <representation wordnetid="105926676" confidence="0.8">
<interpretation wordnetid="105928513" confidence="0.8">
<link xlink:type="simple" xlink:href="../890/4890.xml">
Bayesian</link></interpretation>
</representation>
) probability for possible outcomes.</p>
<p>

One such attempt is based on <link xlink:type="simple" xlink:href="../173/4839173.xml">
info-gap decision theory</link>. The agent is told precise probabilities of some outcomes, though the practical meaning of the probability numbers is not entirely clear. For instance, in the gambles discussed above, the probability of a red ball is 30/90, which is a precise number. Nonetheless, the agent may not distinguish, intuitively, between this and, say, 30/91. No probability information whatsoever is provided regarding other outcomes, so the agent has very unclear subjective impressions of these probabilities.</p>
<p>

In light of the ambiguity in the probabilities of the outcomes, the agent is unable to evaluate a precise expected utility. Consequently, a choice based on <it>maximizing</it> the expected utility is also impossible. The info-gap approach supposes that the agent implicitly formulates <link xlink:type="simple" xlink:href="../173/4839173.xml">
info-gap models</link> for the subjectively uncertain probabilities. The agent then tries to <link xlink:type="simple" xlink:href="../401/70401.xml">
satisfice</link> the expected utility and to maximize the robustness against uncertainty in the imprecise probabilities. This  robust-satisficing approach can be developed explicitly to show that the  choices of decision-makers should display precisely the preference reversal which Ellsberg observed.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

An obvious explanation is that people prefer the bet where cheating can be detected. If the urn is turned over and the marbles counted, a cheat on the red marbles would be detected, because there must be 30. However, if, for example, no black marbles and 60 yellow ones are found, one cannot find out whether this was an honest coincidence or cheating. Thus bet A requires less trust than bet B and should be preferred.</p>

</ss1>
</sec>
<sec>
<st>
See also</st>

<p>

<list>
<entry level="1" type="bullet">

<statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<contradiction wordnetid="107206887" confidence="0.8">
<social_science wordnetid="106143154" confidence="0.8">
<paradox wordnetid="106724559" confidence="0.8">
<falsehood wordnetid="106756407" confidence="0.8">
<knowledge_domain wordnetid="105999266" confidence="0.8">
<economics wordnetid="106149484" confidence="0.8">
<discipline wordnetid="105996646" confidence="0.8">
<science wordnetid="105999797" confidence="0.8">
<link xlink:type="simple" xlink:href="../960/3223960.xml">
Allais paradox</link></science>
</discipline>
</economics>
</knowledge_domain>
</falsehood>
</paradox>
</social_science>
</contradiction>
</message>
</statement>
</entry>
<entry level="1" type="bullet">

<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../102/548102.xml">
Experimental economics</link></kind>
</type>
</category>
</concept>
</idea>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../479/45479.xml">
Utility theory</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
 <cite id="CITEREFEllsberg1961" style="font-style:normal"><link>
Ellsberg, Daniel</link>&#32;(1961),&#32;"Risk, Ambiguity, and the Savage Axioms",&#32;<it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../999/2309999.xml">
Quarterly Journal of Economics</link></periodical>
</it>&#32;<b>75</b>(4):  643&ndash;669</cite>&nbsp;</entry>
<entry id="2">
 <cite id="CITEREFFoxTversky1995" style="font-style:normal"><link>
Fox, Craig R.</link>&#32;&amp;&#32;<link>
Tversky, Amos</link>&#32;(1995),&#32;"Ambiguity Aversion and Comparative Ignorance",&#32;<it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../999/2309999.xml">
Quarterly Journal of Economics</link></periodical>
</it>&#32;<b>110</b>(3):  585&ndash;603</cite>&nbsp;</entry>
<entry id="3">
 <cite id="Reference-Ben-Haim-2006" style="font-style:normal" class="book"><link>
Ben-Haim, Yakov</link>&#32;(2006). Info-gap Decision Theory: Decisions Under Severe Uncertainty, 2nd edition,&#32;Academic Press,&#32;section 11.1. ISBN 0123735521.</cite>&nbsp;</entry>
</reflist>

<list>
<entry level="1" type="bullet">

  <cite id="Reference-Anand-1993" style="font-style:normal" class="book"><link>
Anand, Paul</link>&#32;(1993). Foundations of Rational Choice Under Risk.&#32;Oxford University Press, USA. ISBN 0198233035.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Keynes, John Maynard. (1921). <it>A Treatise on Probability</it>. Macmillan, London.</entry>
</list>
</p>


</sec>
</bdy>
</science>
</discipline>
</economics>
</knowledge_domain>
</falsehood>
</paradox>
</social_science>
</contradiction>
</message>
</statement>
</article>
