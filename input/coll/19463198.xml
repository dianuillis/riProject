<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 05:28:12[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Apprenticeship learning</title>
<id>19463198</id>
<revision>
<id>240815979</id>
<timestamp>2008-09-25T02:58:43Z</timestamp>
<contributor>
<username>Speaking fish</username>
<id>7622632</id>
</contributor>
</revision>
<categories>
<category>Articles to be expanded since September 2008</category>
<category>Articles with invalid date parameter in template</category>
<category>All articles to be expanded</category>
</categories>
</header>
<bdy>

<b>Apprenticeship learning</b>, or <b>apprenticeship via inverse reinforcement learning</b> (AIRP), is a concept in the field of <link>
Artificial Intelligence</link> and <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link>, developed by Pieter Abbeel, Assistant Professor in <link xlink:type="simple" xlink:href="../186/18629186.xml">
Berkeley</link>'s EECS department, and Andrew Ng, Assistant Professor in the <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../977/26977.xml">
Stanford University's</link></university>
 Computer Science Department.  AIRP deals with "Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform"<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><p>

AIRP concept is closely related to <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link> that is a sub-area of <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link>] concerned with how an <it>agent</it> ought to take <it>actions</it> in an <it>environment</it> so as to maximize some notion of long-term <it>reward</it>.  AIRP algorithms are used when reward function is unknown.  The algorithms use observing the behavior of an expert for teaching <it>agent</it> the best <it>actions</it> in certain states of <it>environment</it>. </p>

<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
<weblink xlink:type="simple" xlink:href="http://www.cs.utexas.edu/~ianfasel/OtherPapers/AbbeelNgApprenticeship.pdf">
Pieter Abbeel, Andrew Ng, “Apprenticeship learning via inverse reinforcement learning.” In 21st International Conference on Machine Learning (ICML). 2005.</weblink></entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<weblink xlink:type="simple" xlink:href="http://www.cs.stanford.edu/people/ang//rl-videos/">
Videos of robots performing <it>actions</it> learned through apprenticeship learning algorithms</weblink></p>

<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-notice" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="44px" src="Wiki_letter_w.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Apprenticeship_learning&amp;action=edit">
improve this article or section</weblink> by expanding it.</b> Further information might be found on the  or at . 
<it>(September 2008)''</it></col>
</row>
</table>

</p>
</sec>
</bdy>
</article>
