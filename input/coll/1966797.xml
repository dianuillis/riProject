<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:07:33[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<plant  confidence="0.8" wordnetid="100017222">
<tree  confidence="0.8" wordnetid="113104059">
<vascular_plant  confidence="0.8" wordnetid="113083586">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<woody_plant  confidence="0.8" wordnetid="113103136">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>ID3 algorithm</title>
<id>1966797</id>
<revision>
<id>237947364</id>
<timestamp>2008-09-12T15:15:54Z</timestamp>
<contributor>
<username>Giftlite</username>
<id>37986</id>
</contributor>
</revision>
<categories>
<category>Knowledge discovery in databases</category>
<category>Decision trees</category>
<category>Machine learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

<b>ID3</b> (Iterative Dichotomiser 3) is an <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> used to generate a <link xlink:type="simple" xlink:href="../003/577003.xml">
decision tree</link> invented by <link xlink:type="simple" xlink:href="../337/4373337.xml">
Ross Quinlan</link>.<p>

The algorithm is based on <link xlink:type="simple" xlink:href="../797/36797.xml">
Occam's razor</link>: it prefers smaller decision trees (simpler theories) over larger ones. However, it does not always produce the smallest tree, and is therefore a <link xlink:type="simple" xlink:href="../452/63452.xml">
heuristic</link>. Occam's razor is formalized using the concept of <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link>:</p>
<p>

<indent level="1">

<math> I_{E}(i) = - \sum^{m}_{j=1}  f (i,j) \log_{2} f (i, j). </math>
</indent>

The ID3 algorithm can be summarized as follows:</p>
<p>

<list>
<entry level="1" type="number">

 Take all unused attributes and count their entropy concerning test samples</entry>
<entry level="1" type="number">

 Choose attribute for which entropy is minimum</entry>
<entry level="1" type="number">

 Make node containing that attribute</entry>
</list>
</p>
<p>

An explanation of the implementation of ID3 can be found at <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../814/1966814.xml">
C4.5 algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
, which is an extended version of ID3.</p>

<sec>
<st>
Algorithm</st>
<p>

The actual algorithm is as follows:</p>
<p>

ID3 (Examples, Target_Attribute, Attributes)
<list>
<entry level="1" type="bullet">

Create a root node for the tree</entry>
<entry level="1" type="bullet">

If all examples are positive, Return the single-node tree Root, with label = +.</entry>
<entry level="1" type="bullet">

If all examples are negative, Return the single-node tree Root, with label = -.</entry>
<entry level="1" type="bullet">

If number of predicting attributes is empty, then Return the single node tree Root, with label = most common        value of the target attribute in the examples.</entry>
<entry level="1" type="bullet">

Otherwise Begin</entry>
<entry level="2" type="bullet">

   A = The Attribute that best classifies examples.</entry>
<entry level="2" type="bullet">

   Decision Tree attribute for Root = A.</entry>
<entry level="2" type="bullet">

   For each possible value, <math>v_i</math>, of A,</entry>
<entry level="3" type="bullet">

      Add a new tree branch below Root, corresponding to the test A = <math>v_i</math>.</entry>
<entry level="3" type="bullet">

      Let Examples(<math>v_i</math>), be the subset of examples that have the value <math>v_i</math> for A</entry>
<entry level="3" type="bullet">

      If Examples(<math>v_i</math>) is empty</entry>
<entry level="4" type="bullet">

         Then below this new branch add a leaf node with label = most common target value in the examples</entry>
<entry level="3" type="bullet">

      Else below this new branch add the subtree ID3 (Examples(<math>v_i</math>), Target_Attribute, Attributes â€“ {A})</entry>
<entry level="1" type="bullet">

End</entry>
<entry level="1" type="bullet">

Return Root</entry>
</list>
</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../814/1966814.xml">
C4.5 algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 Mitchell, Tom M. <it>Machine Learning</it>. McGraw-Hill, 1997.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 Seminars - <weblink xlink:type="simple" xlink:href="http://www2.cs.uregina.ca/~hamilton/courses/831/notes/ml/dtrees/4_dtrees1.html">
http://www2.cs.uregina.ca/</weblink></entry>
<entry level="1" type="bullet">

 Description and examples - <weblink xlink:type="simple" xlink:href="http://www.cise.ufl.edu/~ddd/cap6635/Fall-97/Short-papers/2.htm">
http://www.cise.ufl.edu/</weblink></entry>
<entry level="1" type="bullet">

 Description and examples - <weblink xlink:type="simple" xlink:href="http://www.cis.temple.edu/~ingargio/cis587/readings/id3-c45.html">
http://www.cis.temple.edu/</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html">
An implementation of ID3 in Python</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://ai4r.rubyforge.org/machineLearning.html">
An implementation of ID3 in Ruby</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.pvv.ntnu.no/~oyvinht/static/OSS/cl-id3/">
An implementation of ID3 in Common Lisp</weblink></entry>
<entry level="1" type="bullet">

 Implementation of ID3 algorithm in C# - <weblink xlink:type="simple" xlink:href="http://www.codeproject.com/cs/algorithms/id3.asp">
http://www.codeproject.com/cs/algorithms/id3.asp</weblink></entry>
<entry level="1" type="bullet">

 Decision tree Perl Module - <weblink xlink:type="simple" xlink:href="http://search.cpan.org/~kwilliams/AI-DecisionTree">
An implementation of ID3 in Perl</weblink>atomic resolution.</entry>
</list>
</p>




</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</article>
