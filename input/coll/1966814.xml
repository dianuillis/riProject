<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:16:24[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<plant  confidence="0.8" wordnetid="100017222">
<tree  confidence="0.8" wordnetid="113104059">
<vascular_plant  confidence="0.8" wordnetid="113083586">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<woody_plant  confidence="0.8" wordnetid="113103136">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>C4.5 algorithm</title>
<id>1966814</id>
<revision>
<id>239381856</id>
<timestamp>2008-09-18T21:29:42Z</timestamp>
<contributor>
<username>Pgan002</username>
<id>54984</id>
</contributor>
</revision>
<categories>
<category>Decision trees</category>
<category>Articles lacking in-text citations</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Text_document_with_red_question_mark.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 This article or section includes a  or , but its sources remain unclear because it lacks <b>.</b>
You can  this article by introducing more precise citations . <it>(July 2008)''</it></col>
</row>
</table>


<b>C4.5</b> is an algorithm used to generate a <link xlink:type="simple" xlink:href="../003/577003.xml">
decision tree</link> developed by <link xlink:type="simple" xlink:href="../337/4373337.xml">
Ross Quinlan</link>. C4.5 is an extension of Quinlan's earlier <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../797/1966797.xml">
ID3 algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a <link xlink:type="simple" xlink:href="../244/1579244.xml">
statistical classifier</link>.
<sec>
<st>
Algorithm</st>
<p>

C4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link>.  The training data is a set <math>S = {s_1, s_2, ...}</math> of already classified samples.  Each sample <math> s_i = {x_1, x_2, ...} </math> is a vector where <math> x_1, x_2, ... </math> represent attributes or features of the sample.  The training data is augmented with a vector <math>C = {c_1, c_2, ...} </math> where <math> c_1, c_2, ... </math> represent the class that each sample belongs to.</p>
<p>

C4.5 uses the fact that each attribute of the data can be used to make a decision that splits the data into smaller subsets.  C4.5 examines the normalized <link xlink:type="simple" xlink:href="../527/467527.xml">
information gain</link> (difference in entropy) that results from choosing an attribute for splitting the data.  The attribute with the highest normalized information gain is the one used to make the decision.  The algorithm then recurs on the smaller sublists.</p>
<p>

This algorithm has a few base cases, the most common base case is when all the samples in your list belong to the same class.  Once this happens, you simply create a leaf node for your decision tree telling you to choose that class.  It might also happen that none of the features give you any information gain, in this case C4.5 creates a decision node higher up the tree using the expected value of the class.  It also might happen that you've never seen any instances of a class; again, C4.5 creates a decision node higher up the tree using expected value. </p>
<p>

In <link xlink:type="simple" xlink:href="../185/24185.xml">
pseudocode</link> the algorithm is:</p>
<p>

1. Check for base cases
2. For each attribute <it>a</it>
3.     Find the normalized information gain from splitting on <it>a</it>
4. Let <it>a_best</it> be the attribute with the highest normalized information gain
5. Create a decision <it>node</it> that splits on <it>a_best</it>
6. Recur on the sublists obtained by splitting on <it>a_best</it> and add those nodes as children of <it>node</it></p>

</sec>
<sec>
<st>
 Information gain and information entropy </st>

<p>

Although explained further in their respective sections, <math>Entropy(S)</math> can be thought of as a measure of how random the class distribution is in <it>S</it>.  Information gain is a measure given to an attribute <it>a</it>. Attribute <it>a</it> can separate <it>S</it> into subsets <math>S_a1, S_a2, S_a3, ..., S_an</math>  the information gain of <it>a</it> is then <math> Entropy(S) - Entropy(S_a1) - Entropy(S_a2) - ... - Entropy(S_an)</math>.  Information gain is then normalized by multiplying the entropy of each attribute choice by the proportion of attribute values that have that choice.</p>

</sec>
<sec>
<st>
 Improvements from ID3 algorithm </st>

<p>

C4.5 made a number of improvements to ID3.  Some of these are:</p>
<p>

<list>
<entry level="1" type="bullet">

 Handling both continuous and discrete attributes - In order to handle continuous attributes, C4.5 creates a threshold and then splits the list into those whose attribute value is above the threshold and those that are less than or equal to it. [Quinlan, 96]</entry>
<entry level="1" type="bullet">

 Handling training data with missing attribute values - C4.5 allows attribute values to be marked as ? for missing.  Missing attribute values are simply not used in gain and entropy calculations.</entry>
<entry level="1" type="bullet">

 Handling attributes with differing costs.</entry>
<entry level="1" type="bullet">

 Pruning trees after creation - C4.5 goes back through the tree once it's been created and attempts to remove branches that do not help by replacing them with leaf nodes.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Improvements in C5.0/See5 algorithm </st>

<p>

Quinlan went on to create C5.0 and See5 (C5.0 for Unix/Linux, See5 for Windows) which he markets commercially.  C5.0 offers a number of improvements on C4.5. Some of these are:</p>
<p>

<list>
<entry level="1" type="bullet">

 Speed - C5.0 is significantly faster than C4.5 (several orders of magnitude)</entry>
<entry level="1" type="bullet">

 Memory usage - C5.0 is more memory efficient than C4.5</entry>
<entry level="1" type="bullet">

 Smaller decision trees - C5.0 gets similar results to C4.5 with considerably smaller decision trees.</entry>
<entry level="1" type="bullet">

 Support for <link xlink:type="simple" xlink:href="../500/90500.xml">
boosting</link> - Boosting improves the trees and gives them more accuracy. </entry>
<entry level="1" type="bullet">

 Weighting - C5.0 allows you to weight different attributes and misclassification types.</entry>
<entry level="1" type="bullet">

 Winnowing - C5.0 automatically winnows the data to help reduce noise.</entry>
</list>
</p>
<p>

C5.0/See5 is a commercial and closed-source product, although free source code is available for interpreting and using the decision trees and rule sets it outputs.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../797/1966797.xml">
ID3 algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 Quinlan, J. R.  C4.5:  Programs for Machine Learning.  Morgan Kaufmann Publishers, 1993.</entry>
<entry level="1" type="bullet">

 J. R. Quinlan. Improved use of continuous attributes in c4.5. Journal of Artificial Intelligence Research, 4:77-90, 1996.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>

<p>

<list>
<entry level="1" type="bullet">

 Original implementation on Ross Quinlan's homepage:  <weblink xlink:type="simple" xlink:href="http://www.rulequest.com/Personal/">
http://www.rulequest.com/Personal/</weblink></entry>
</list>
</p>



</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</article>
