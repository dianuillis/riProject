<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 05:33:37[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Overfitting (machine learning)</title>
<id>19674116</id>
<revision>
<id>243972598</id>
<timestamp>2008-10-08T19:56:02Z</timestamp>
<contributor>
<username>Doloco</username>
<id>5559270</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<indent level="1">

<it>For the statistical concept see <link xlink:type="simple" xlink:href="../332/173332.xml">
Overfitting</link>
</it></indent>
<image width="300px" src="Overfitting_svg.svg" type="thumb">
<caption>

Overfitting/Overtraining in supervised learning (e.g. <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link>). Training error is shown in blue, validation error in red. If the validation error increases while the training error steadily decreases then a situation of overfitting may have occurred.
</caption>
</image>
<p>

The concept of <b>overfitting</b> is important in <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>.  Usually a learning <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> is trained using some set of training examples, i.e. exemplary situations for which the desired output is known. The learner is assumed to reach a state where it will also be able to predict the correct output for other examples, thus generalizing to situations not presented during training (based on its <link xlink:type="simple" xlink:href="../926/173926.xml">
inductive bias</link>).  However, especially in cases where learning was performed too long or where training examples are rare, the learner may adjust to very specific random features of the training data, that have no <link>
causal relation</link> to the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../897/336897.xml">
target function</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
.  In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse. </p>
<p>

In order to avoid overfitting, it is necessary to use additional techniques (e.g. <link xlink:type="simple" xlink:href="../612/416612.xml">
cross-validation</link>, <link xlink:type="simple" xlink:href="../061/2009061.xml">
regularization</link>, <link xlink:type="simple" xlink:href="../214/213214.xml">
early stopping</link>, that can indicate when further training is not resulting in better generalization. The process of overfitting of <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link> during the training is also known as overtraining. In <link xlink:type="simple" xlink:href="../063/577063.xml">
treatment learning</link>, overfitting is avoided by using a minimum best support value.</p>

<sec>
<st>
Literature</st>

<p>

<list>
<entry level="1" type="bullet">

 Tetko, I.V.; Livingstone, D.J.; Luik, A.I. Neural network studies. 1. Comparison of Overfitting and Overtraining, <weblink xlink:type="simple" xlink:href="http://www.vcclab.org/articles/tetko.html#overtraining">
 J. Chem. Inf. Comput. Sci., 1995, 35, 826-833</weblink></entry>
</list>
</p>



</sec>
<sec>
<st>
 External links </st>

<p>

<list>
<entry level="1" type="bullet">

 http://www.cs.sunysb.edu/~skiena/jaialai/excerpts/node16.html</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.vcclab.org/articles/tetko.html#overtraining">
Overtraining</weblink></entry>
</list>
</p>

</sec>
</bdy>
</article>
