<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:10:00[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<condition  confidence="0.8" wordnetid="113920835">
<state  confidence="0.8" wordnetid="100024720">
<problem  confidence="0.8" wordnetid="114410605">
<difficulty  confidence="0.8" wordnetid="114408086">
<header>
<title>Regularization (mathematics)</title>
<id>2009061</id>
<revision>
<id>243868279</id>
<timestamp>2008-10-08T11:45:02Z</timestamp>
<contributor>
<username>Gareth McCaughan</username>
<id>216203</id>
</contributor>
</revision>
<categories>
<category>Inverse problems</category>
<category>Machine learning</category>
<category>Mathematical analysis</category>
</categories>
</header>
<bdy>

For other uses in related fields, see <link xlink:type="simple" xlink:href="../394/323394.xml">
Regularization</link>.<p>

In several fields of <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link>, in particular <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> and <link xlink:type="simple" xlink:href="../956/203956.xml">
inverse problem</link>s, <b>regularization</b> involves introducing additional information in order to solve an <link xlink:type="simple" xlink:href="../673/176673.xml">
ill-posed problem</link> or prevent <link xlink:type="simple" xlink:href="../332/173332.xml">
overfitting</link>. This information is usually of the form of a penalty for complexity, such as restrictions for <link xlink:type="simple" xlink:href="../140/292140.xml">
smoothness</link> or bounds on the <link xlink:type="simple" xlink:href="../538/21538.xml">
vector space norm</link>.</p>
<p>

A theoretical justification for regularization is that it attempts to impose <link xlink:type="simple" xlink:href="../797/36797.xml">
Occam's razor</link> on the solution. From a <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian</link> point of view, many regularization techniques correspond to imposing certain <link xlink:type="simple" xlink:href="../877/472877.xml">
prior</link> distributions on model parameters.</p>
<p>

The same idea arose in many fields of <link xlink:type="simple" xlink:href="../700/26700.xml">
science</link>. For example, the <link xlink:type="simple" xlink:href="../359/82359.xml">
least-squares method</link> can be viewed as a very simple form of regularization. A simple form of regularization applied to <link xlink:type="simple" xlink:href="../234/474234.xml">
integral equation</link>s, generally termed <link xlink:type="simple" xlink:href="../323/954323.xml">
Tikhonov regularization</link> after <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<hero wordnetid="110325013" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../048/616048.xml">
Andrey Nikolayevich Tychonoff</link></scholar>
</mathematician>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</leader>
</hero>
</person>
</physical_entity>
,  is essentially a trade-off between fitting the data and reducing a norm of the solution. More recently, <link>
non-linear regularization</link> methods, including <link>
total variation regularization</link> have become popular.</p>

<sec>
<st>
 Regularization in statistics</st>
<p>

In statistics and machine learning, regularization is used to prevent <link xlink:type="simple" xlink:href="../332/173332.xml">
overfitting</link>. Typical examples of regularization in statistical machine learning include <link xlink:type="simple" xlink:href="../328/954328.xml">
ridge regression</link>, lasso, and <link>
L2-norm</link> in <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machines</link>.</p>
<p>

Regularization methods are also used for model selection, where they work by implicitly or explicitly penalizing models based on the number of their parameters. For example, <link xlink:type="simple" xlink:href="../329/331329.xml">
Bayesian learning</link> methods make use of a <link xlink:type="simple" xlink:href="../877/472877.xml">
prior probability</link> that (usually) gives lower probability to more complex models. Well-known model selection techniques include the <link xlink:type="simple" xlink:href="../512/690512.xml">
Akaike information criterion</link> (AIC), <link xlink:type="simple" xlink:href="../325/331325.xml">
minimum description length</link> (MDL), and the <link xlink:type="simple" xlink:href="../272/2473272.xml">
Bayesian information criterion</link> (BIC). Alternative methods of controlling overfitting include <link xlink:type="simple" xlink:href="../612/416612.xml">
cross validation</link>.</p>
<p>

Examples of applications of different methods of regularization to the <link xlink:type="simple" xlink:href="../904/17904.xml">
linear model</link> are:
<table class="wikitable sortable">
<header>
Model</header>
<header>
Fit measure</header>
<header>
Entropy measure</header>
<row>
<col>
<link xlink:type="simple" xlink:href="../512/690512.xml">
AIC</link>/<link xlink:type="simple" xlink:href="../272/2473272.xml">
BIC</link></col>
<col>
<math>\|Y-X\beta\|_2</math></col>
<col>
<math>\|\beta\|_0</math></col>
</row>
<row>
<col>
<link xlink:type="simple" xlink:href="../328/954328.xml">
Ridge regression</link></col>
<col>
<math>\|Y-X\beta\|_2</math></col>
<col>
<math>\|\beta\|_2</math></col>
</row>
<row>
<col>
Lasso<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></col>
<col>
<math>\|Y-X\beta\|_2</math></col>
<col>
<math>\|\beta\|_1</math></col>
</row>
<row>
<col>
RLAD<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></col>
<col>
<math>\|Y-X\beta\|_1</math></col>
<col>
<math>\|\beta\|_1</math></col>
</row>
<row>
<col>
Dantzig Selector<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></col>
<col>
<math>\|X^\top (Y-X\beta)\|_\infty</math></col>
<col>
<math>\|\beta\|_1</math></col>
</row>
</table>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 A. Neumaier, Solving ill-conditioned and singular linear systems: A tutorial on regularization, SIAM Review 40 (1998), 636-666. Available in <weblink xlink:type="simple" xlink:href="http://www.mat.univie.ac.at/~neum/ms/regtutorial.pdf">
pdf</weblink> from <weblink xlink:type="simple" xlink:href="http://www.mat.univie.ac.at/~neum/">
author's website</weblink>.</entry>
</list>
</p>

<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">Tibshirani, Robert&#32;(1996).&#32;"<weblink xlink:type="simple" xlink:href="http://www-stat.stanford.edu/~tibs/ftp/lasso.ps">
Regression Shrinkage and Selection via the Lasso</weblink>"&#32;(<link xlink:type="simple" xlink:href="../080/24080.xml">
PostScript</link>). <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../143/13327143.xml">
Journal of the Royal Statistical Society</link></periodical>
, Series B (Methodology)</it>&#32;<b>58</b>&#32;(1): 267&ndash;288.</cite>&nbsp;</entry>
<entry id="2">
 <cite style="font-style:normal">Li Wang, Michael D. Gordon &amp; Ji Zhu&#32;(December 2006). "Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning".&#32;<it>Sixth International Conference on Data Mining</it>: 690&ndash;700. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1109/ICDM.2006.134">
10.1109/ICDM.2006.134</weblink>.</cite>&nbsp;</entry>
<entry id="3">
 <cite style="font-style:normal">Candes, Emmanuel; <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../498/1118498.xml">
Tao, Terence</link></scientist>
</person>
&#32;(2007).&#32;"<weblink xlink:type="simple" xlink:href="http://arxiv.org/abs/math.ST/0506081">
The Dantzig selector: Statistical estimation when <it>p</it> is much larger than <it>n''</it></weblink>"&#32;(<link xlink:type="simple" xlink:href="../751/38751.xml">
arXiv</link> Reprint). <it>Annals of Statistics</it>&#32;<b>35</b>&#32;(6): 2313&ndash;2351. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1214%2F009053606000001523">
10.1214/009053606000001523</weblink>.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
</bdy>
</difficulty>
</problem>
</state>
</condition>
</article>
