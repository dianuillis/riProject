<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:30:51[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Gradient descent</title>
<id>201489</id>
<revision>
<id>244337098</id>
<timestamp>2008-10-10T09:18:11Z</timestamp>
<contributor>
<username>DoubleBlue</username>
<id>220505</id>
</contributor>
</revision>
<categories>
<category>Optimization algorithms</category>
</categories>
</header>
<bdy>

<indent level="1">

 <it>For the <link xlink:type="simple" xlink:href="../396/48396.xml">
analytical</link> method called "steepest descent" see <link xlink:type="simple" xlink:href="../006/642006.xml">
Method of steepest descent</link>.</it>
</indent>
<b>Gradient descent</b> is an <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>. To find a <link>
local minimum</link> of a function using gradient descent, one takes steps proportional to the <it>negative</it> of the <link xlink:type="simple" xlink:href="../461/12461.xml">
gradient</link> (or the approximate gradient) of the function at the current point. If instead one takes steps proportional to the gradient, one approaches a <link xlink:type="simple" xlink:href="../420/298420.xml">
local maximum</link> of that function; the procedure is then known as <b>gradient ascent</b>. <p>

Gradient descent is also known as <b>steepest descent</b>, or the <b>method of steepest descent</b>. When known as the latter, gradient descent should not be confused with the <link xlink:type="simple" xlink:href="../006/642006.xml">
method of steepest descent</link> for approximating integrals.</p>

<sec>
<st>
Description</st>

<p>

Gradient descent is based on the observation that if the real-valued function <math>F(\mathbf{x})</math> is defined and differentiable in a neighborhood of a point <math>\mathbf{a}</math>, then <math>F(\mathbf{x})</math> decreases <it>fastest</it> if one goes from <math>\mathbf{a}</math> in the direction of the negative gradient of <math>F</math> at <math>\mathbf{a}</math>,  <math>-\nabla F(\mathbf{a})</math>. It follows that, if </p>
<p>

<indent level="1">

<math>\mathbf{b}=\mathbf{a}-\gamma\nabla F(\mathbf{a})</math>
</indent>

for <math>\gamma&amp;gt;0</math> a small enough number, then  <math>F(\mathbf{a})\geq F(\mathbf{b})</math>.  With this observation in mind, one starts with a guess <math>\mathbf{x}_0</math> for a local minimum of <math>F</math>, and considers the sequence
<math>\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \dots</math> such that</p>
<p>

<indent level="1">

<math>\mathbf{x}_{n+1}=\mathbf{x}_n-\gamma_n \nabla F(\mathbf{x}_n),\ n \ge 0.</math>
</indent>

We have </p>
<p>

<indent level="1">

<math>F(\mathbf{x}_0)\ge F(\mathbf{x}_1)\ge F(\mathbf{x}_2)\ge \dots,</math> 
</indent>

so hopefully the sequence <math>(\mathbf{x}_n)</math> converges to the desired local minimum. Note that the value of the <it>step size</it> <math>\gamma</math> is allowed to change at every iteration.</p>
<p>

<image location="right" width="400px" src="gradient_descent.png">
</image>

This process is illustrated in the picture to the right.
Here <math>F</math> is assumed to be defined on the plane, and that its graph has a <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<container wordnetid="103094503" confidence="0.8">
<link xlink:type="simple" xlink:href="../580/1231580.xml">
bowl</link></container>
</instrumentality>
</artifact>
 shape.  The blue curves are the <link xlink:type="simple" xlink:href="../086/650086.xml">
contour line</link>s, that is, the regions on which the value of <math>F</math> is constant. A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point is <link xlink:type="simple" xlink:href="../221/102221.xml">
orthogonal</link> to the contour line going through that point. We see that gradient <it>descent</it> leads us to the bottom of the bowl, that is, to the point where the value of the function <math>F</math> is minimal.</p>

</sec>
<sec>
<st>
 Examples </st>
<p>

Gradient descent has problems with pathological functions such as the <link xlink:type="simple" xlink:href="../234/3555234.xml">
Rosenbrock function</link> shown here. The <link xlink:type="simple" xlink:href="../234/3555234.xml">
Rosenbrock function</link> has a narrow curved valley which contains the minimum. The bottom of the valley is very flat. Because of the curved flat valley the optimisation is zig-zagging slowly with small stepsizes towards the minimum.
<indent level="1">

<image width="400px" src="Banana-SteepDesc.gif">
</image>

</indent>

The gradient ascent method applied to <math>F(x,y)=\sin\left(\frac{1}{2} x^2 - \frac{1}{4} y^2 + 3 \right) \cos(2 x+1-e^y)</math>:
<table >
<row>
<col>

<image width="350px" src="gradient_ascent_(contour).png">
<caption>

The gradient descent algorithm in action. (1: contour)
</caption>
</image>

<image width="450px" src="gradient_ascent_(surface).png">
<caption>

The gradient descent algorithm in action. (2: surface)
</caption>
</image>
</col>
</row>
</table>
</p>

</sec>
<sec>
<st>
Comments</st>

<p>

Gradient descent works in spaces of any number of dimensions, even in infinite-dimensional ones. In the latter case the search space is typically a <link xlink:type="simple" xlink:href="../630/340630.xml">
function space</link>, and one calculates the <link>
GÃ¢teaux derivative</link> of the functional to be minimized to determine the descent direction. </p>
<p>

Two weaknesses of gradient descent are:
<list>
<entry level="1" type="number">

 The algorithm can take many iterations to converge towards a local minimum, if the curvature in different directions is very different.</entry>
<entry level="1" type="number">

 Finding the optimal <math>\gamma</math> per step can be time-consuming. Conversely, using a fixed <math>\gamma</math> can yield poor results. Methods based on <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/1244523.xml">
Newton's method</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 and inversion of the <link xlink:type="simple" xlink:href="../108/412108.xml">
Hessian</link> using <link xlink:type="simple" xlink:href="../821/1448821.xml">
conjugate gradient</link> techniques are often a better alternative.</entry>
</list>
</p>
<p>

A more powerful algorithm is given by the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../409/1926409.xml">
BFGS method</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 which consists in calculating on every step a matrix by which the gradient vector is multiplied to go into a "better" direction, combined with a more sophisticated <link xlink:type="simple" xlink:href="../058/1537058.xml">
line search</link> algorithm, to find the "best" value of <math>\gamma.</math></p>
<p>

Gradient descent is in fact <link xlink:type="simple" xlink:href="../098/3011098.xml">
Euler's method</link> for solving <link xlink:type="simple" xlink:href="../297/8297.xml">
ordinary differential equations</link> applied to a gradient flow. As the goal is to find the minimum, not the flow line, the error in finite methods is less significant.</p>

</sec>
<sec>
<st>
A computational example</st>
<p>

The gradient descent algorithm is applied to find a local minimum of the function <it>f</it>(<it>x</it>)=<it>x</it>4-3<it>x</it>3+2 , with derivative <it>f</it>'(<it>x</it>)=4<it>x</it>3-9<it>x</it>2. Here is an implementation in the <link xlink:type="simple" xlink:href="../021/6021.xml">
C programming language</link>.</p>

<p>

<list>
<entry level="1" type="number">

include </entry>
<entry level="1" type="number">

include </entry>
<entry level="1" type="number">

include </entry>
</list>
</p>
<p>

int main ()
{
// From calculation, we expect that the local minimum occurs at x=9/4
// The algorithm starts at x=6</p>
<p>

double xOld = 0;
double xNew = 6;
double eps = 0.01; // step size
double precision = 0.00001;
while (fabs(xNew - xOld) &amp;gt; precision)
{
xOld = xNew;
xNew = xNew - eps*(4*xNew*xNew*xNew-9*xNew*xNew);
}</p>
<p>

printf ("Local minimum occurs at %lg\n", xNew);</p>
<p>

}</p>

<p>

With this precision, the algorithm converges to a local minimum of 2.24996 in 70 iterations.</p>
<p>

A more robust implementation of the algorithm would also check whether the function value indeed decreases at every iteration and would make the step size smaller otherwise. One can also use an adaptive step size which may make the algorithm converge faster.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<table>
<row>
<col>
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../821/1448821.xml">
Conjugate gradient</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../641/1180641.xml">
Stochastic gradient descent</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/1244523.xml">
Newton's method</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
</list>
</col>
<col>
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../033/52033.xml">
Optimization</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../058/1537058.xml">
Line search</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../612/1237612.xml">
Delta rule</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../343/1671343.xml">
Wolfe conditions</link></entry>
</list>
</col>
</row>
</table>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 Mordecai Avriel (2003). <it>Nonlinear Programming: Analysis and Methods.</it> Dover Publishing. ISBN 0-486-43227-0.</entry>
<entry level="1" type="bullet">

 Jan A. Snyman (2005). <it>Practical Mathematical Optimization: An Introduction to Basic Optimization Theory and Classical and  New Gradient-Based Algorithms.</it> Springer Publishing. ISBN 0-387-24348-8 </entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
