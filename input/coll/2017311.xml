<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:10:28[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Least mean squares filter</title>
<id>2017311</id>
<revision>
<id>220968595</id>
<timestamp>2008-06-22T13:12:57Z</timestamp>
<contributor>
<username>Zvika</username>
<id>413489</id>
</contributor>
</revision>
<categories>
<category>Filter theory</category>
<category>Digital signal processing</category>
</categories>
</header>
<bdy>

<b>Least mean squares (LMS)</b> algorithms are used in <link xlink:type="simple" xlink:href="../071/172071.xml">
adaptive filter</link>s to find the filter coefficients that relate to producing the least mean squares of the error signal (difference between the desired and the actual signal). It is a <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link>
stochastic gradient descent</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 method in that the filter is only adapted based on the error at the current time.  It was invented in <link xlink:type="simple" xlink:href="../664/34664.xml">
1960</link> by <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../977/26977.xml">
Stanford University</link></university>
 professor <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../636/4793636.xml">
Bernard Widrow</link></research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
 and his first Ph.D. student, <link>
Ted Hoff</link>.
<sec>
<st>
 Problem Formulation </st>

<p>

<image width="150px" src="Lms_filter.png">
<caption>

LMS filter
</caption>
</image>
</p>
<p>

Most linear adaptive filtering problems can be formulated using the block diagram above. That is, an unknown system <math>\mathbf{h}(n)</math> is to be identified and the adaptive filter attempts to adapt the filter <math>\hat{\mathbf{h}}(n)</math> to make it as close as possible to <math>\mathbf{h}(n)</math>, while using only observable signals <math>x(n)</math>, <math>d(n)</math> and <math>e(n)</math>; but <math>y(n)</math>, <math>v(n)</math> and <math>h(n)</math> are not directly observable. Its solution is closely related to the <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<filter wordnetid="103339643" confidence="0.8">
<link xlink:type="simple" xlink:href="../721/1216721.xml">
Wiener filter</link></filter>
</device>
</instrumentality>
</artifact>
.</p>

</sec>
<sec>
<st>
 Idea </st>

<p>

The idea behind LMS filters is to use the method of <link xlink:type="simple" xlink:href="../489/201489.xml">
steepest descent</link> to find a coefficient vector <math> \mathbf{h}(n)</math> which minimizes a <link xlink:type="simple" xlink:href="../033/52033.xml">
cost function</link>. 
We start the discussion by defining the cost function as 
<indent level="1">

<math> C(n) = E\left\{|e(n)|^{2}\right\}</math>
</indent>
where <math>e(n)</math> is defined in the block diagram section of the general <link xlink:type="simple" xlink:href="../071/172071.xml">
adaptive filter</link> and <math>E\{.\}</math> denotes the <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link>. Applying the <link xlink:type="simple" xlink:href="../489/201489.xml">
steepest descent</link> method means to take the partial derivatives with respect to the individual entries of the filter coefficient vector
<indent level="1">

<math> 
\nabla C(n) = \nabla E\left\{e(n) \, e^{*}(n)\right\}=2E\left\{\nabla ( e(n) \, e^{*}(n) )\right\}
</math>
</indent>
where <math>\nabla</math> is the <link xlink:type="simple" xlink:href="../461/12461.xml">
gradient</link> operator. With <math>\mathbf{x}(n) = \left[x(n), x(n-1), \dots, x(n-p+1)\right]^T</math> and <math>\nabla e(n)= -\mathbf{x}(n)</math> it follows
<indent level="1">

<math> 
\nabla C(n) = -2E\left\{\mathbf{x}(n) \, e^{*}(n)\right\}
</math>
</indent>
Now, <math>\nabla C(n)</math> is a vector which points towards the steepest ascent of the cost function. To find the minimum of the cost function we need to take a step in the opposite direction of <math>\nabla C(n)</math>. To express that in mathematical terms 
<indent level="1">

<math>\hat{\mathbf{h}}(n+1)=\hat{\mathbf{h}}(n)-\frac{\mu}{2} \nabla C(n)=\hat{\mathbf{h}}(n)+\mu \, E\left\{\mathbf{x}(n) \, e^{*}(n)\right\}</math>
</indent>
where <math>\frac{\mu}{2}</math> is the step size. That means we have found a sequential update algorithm which minimizes the cost function. Unfortunately, this algorithm is not realizable until we know <math>E\left\{\mathbf{x}(n) \, e^{*}(n)\right\} </math>.</p>

</sec>
<sec>
<st>
 Simplifications </st>
<p>

For most systems the expectation function <math>{E}\left\{\mathbf{x}(n) \, e^{*}(n)\right\} </math> must be approximated. This can be done with the following unbiased <link xlink:type="simple" xlink:href="../043/10043.xml">
estimator</link>
<indent level="1">

<math> 
\hat{E}\left\{\mathbf{x}(n) \, e^{*}(n)\right\}=\frac{1}{N}\sum_{i=0}^{N-1}\mathbf{x}(n-i) \, e^{*}(n-i)
</math> 
</indent>
where <math>N</math> indicates the number of samples we use for that estimate. The simplest case is <math>N=1</math>
<indent level="1">

<math> 
\hat{E}\left\{\mathbf{x}(n) \, e^{*}(n)\right\}=\mathbf{x}(n) \, e^{*}(n)
</math> 
</indent>
For that simple case the update algorithm follows as
<indent level="1">

<math>\hat{\mathbf{h}}(n+1)=\hat{\mathbf{h}}(n)+\mu \mathbf{x}(n) \, e^{*}(n)</math>
</indent>
Indeed this constitutes the update algorithm for the LMS filter.</p>

</sec>
<sec>
<st>
 LMS algorithm summary </st>
<p>

The LMS algorithm for a <math>p</math>th order algorithm can be summarized as
<table>
<row>
<col>
Parameters:</col>
<col>
<math>p=</math> filter order</col>
</row>
<row>

<col>
<math>\mu=</math> step size</col>
</row>
<row>
<col>
Initialisation:</col>
<col>
<math>\hat{\mathbf{h}}(0)=0</math></col>
</row>
<row>
<col>
Computation:</col>
<col>
For <math>n=0,1,2,... </math></col>
</row>
<row>
<col>
|<math>\mathbf{x}(n) = \left[x(n), x(n-1), \dots, x(n-p+1)\right]^T</math></col>
</row>
<row>
<col>
|<math> e(n) = d(n)-\hat{\mathbf{h}}^{H}(n)\mathbf{x}(n)</math></col>
</row>
<row>
<col>
|<math> \hat{\mathbf{h}}(n+1) = \hat{\mathbf{h}}(n)+\mu\,e^{*}(n)\mathbf{x}(n)</math></col>
</row>
</table>
</p>
<p>

where <math>\hat{\mathbf{h}}^{H}(n)</math> denotes the <link xlink:type="simple" xlink:href="../548/217548.xml">
Hermitian transpose</link> of <math>\hat{\mathbf{h}}(n)</math>.</p>

</sec>
<sec>
<st>
 Normalised least mean squares filter (NLMS) </st>

<p>

The main drawback of the "pure" LMS algorithm is that it is sensitive to the scaling of its input <math>x(n)</math>. This makes it very hard (if not impossible) to choose a learning rate <math>\mu</math> that guarantees stability of the algorithm. The <it>Normalised least mean squares filter</it> (NLMS) is a variant of the LMS algorithm that solves this problem by normalising with the power of the input. The NLMS algorithm can be summarised as:</p>
<p>

<table>
<row>
<col>
Parameters:</col>
<col>
<math>p=</math> filter order</col>
</row>
<row>

<col>
<math>\mu=</math> step size</col>
</row>
<row>
<col>
Initialization:</col>
<col>
<math>\hat{\mathbf{h}}(0)=0</math></col>
</row>
<row>
<col>
Computation:</col>
<col>
For <math>n=0,1,2,... </math></col>
</row>
<row>
<col>
|<math>\mathbf{x}(n) = \left[x(n), x(n-1), \dots, x(n-p)\right]^T</math></col>
</row>
<row>
<col>
|<math> e(n) = d(n)-\hat{\mathbf{h}}^{H}(n)\mathbf{x}(n)</math></col>
</row>
<row>
<col>
|<math> \hat{\mathbf{h}}(n+1) = \hat{\mathbf{h}}(n)+\frac{\mu\,e^{*}(n)\mathbf{x}(n)}{\mathbf{x}^H(n)\mathbf{x}(n)}</math></col>
</row>
</table>
</p>

<ss1>
<st>
 Optimal learning rate </st>

<p>

It can be shown that if there is no interference (<math>v(n)=0</math>), then the optimal learning rate for the NLMS algorithm is 
<indent level="1">

<math>\mu_{opt}=1</math> 
</indent>
and is independent of the input <math>x(n)</math> and the real (unknown) impulse response <math>\mathbf{h}(n)</math>. In the general case with interference (<math>v(n) \ne 0</math>), the optimal learning rate is
<indent level="1">

<math>
\mu_{opt}=\frac{E\left[\left|y(n)-\hat{y}(n)\right|^2\right]}{E\left[|e(n)|^2\right]}
</math>
</indent>

The results above assume that the signals <math>v(n)</math> and <math>x(n)</math> are uncorrelated to each other, which is generally the case in practice.</p>

</ss1>
<ss1>
<st>
 Proof </st>

<p>

Let the filter misalignment be defined as <math>\Lambda(n) = \left| \mathbf{h}(n) - \hat{\mathbf{h}}(n) \right|^2</math>, we can derive the expected misalignment for the next sample as:
<indent level="1">

 <math> E\left[ \Lambda(n+1) \right] = E\left[ \left| \hat{\mathbf{h}}(n) + \frac{\mu\,e^{*}(n)\mathbf{x}(n)}{\mathbf{x}^H(n)\mathbf{x}(n)} - \mathbf{h}(n) \right|^2 \right]</math>
</indent>
: <math> E\left[ \Lambda(n+1) \right] = E\left[ \left| \hat{\mathbf{h}}(n) + \frac{\mu\, \left(  v^*(n)+y^*(n)-\hat{y}^*(n)  \right) \mathbf{x}(n)}{\mathbf{x}^H(n)\mathbf{x}(n)} - \mathbf{h}(n) \right|^2 \right]</math></p>
<p>

Let <math>\mathbf{\delta}=\hat{\mathbf{h}}(n)-\mathbf{h}(n)</math> and <math>r(n) = \hat{y}(n)-y(n)</math></p>
<p>

<indent level="1">

 <math> E\left[ \Lambda(n+1) \right] = E\left[ \left| \mathbf{\delta}(n) - \frac{\mu\, \left(  v(n)+r(n) \right) \mathbf{x}(n)}{\mathbf{x}^H(n)\mathbf{x}(n)} \right|^2 \right]</math>
</indent>
: <math> E\left[ \Lambda(n+1) \right] = E\left[ \left( \mathbf{\delta}(n) - \frac{\mu\, \left(  v(n)+r(n) \right) \mathbf{x}(n)}{\mathbf{x}^H(n)\mathbf{x}(n)} \right)^H \left( \mathbf{\delta}(n) - \frac{\mu\, \left(  v(n)+r(n) \right) \mathbf{x}(n)}{\mathbf{x}^H(n)\mathbf{x}(n)} \right)  \right]</math></p>
<p>

Assuming independence, we have:
<indent level="1">

 <math> E\left[ \Lambda(n+1) \right] = \Lambda(n) + E\left[ \left( \frac{\mu\, \left(  v(n)-r(n) \right) \mathbf{x}(n)}{\mathbf{x}^H(n)\mathbf{x}(n)} \right)^H \left( \frac{\mu\, \left(  v(n)-r(n) \right) \mathbf{x}(n)}{\mathbf{x}^H(n)\mathbf{x}(n)} \right)  \right] - 2 E\left[\frac{\mu|r(n)|^2}{\mathbf{x}^H(n)\mathbf{x}(n)}\right]</math>
</indent>
: <math> E\left[ \Lambda(n+1) \right] = \Lambda(n) + \frac{\mu^2 E\left[|e(n)|^2\right]}{\mathbf{x}^H(n)\mathbf{x}(n)} - \frac{2 \mu E\left[|r(n)|^2\right]}{\mathbf{x}^H(n)\mathbf{x}(n)}</math></p>
<p>

The optimal learning rate is found at <math>\frac{dE\left[ \Lambda(n+1) \right]}{d\mu} = 0 </math>, which leads to:
<indent level="1">

 <math>2 \mu E\left[|e(n)|^2\right] - 2 E\left[|r(n)|^2\right] = 0</math>
</indent>
: <math>\mu = \frac{E\left[|r(n)|^2\right]}{E\left[|e(n)|^2\right]}</math></p>

</ss1>
</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 Monson H. Hayes <it>Statistical Digital Signal Processing and Modeling,</it> Wiley, 1996, ISBN 0-471-59431-8</entry>
<entry level="1" type="bullet">

 Simon Haykin <it>Adaptive Filter Theory,</it> Prentice Hall, 2002, ISBN 0-13-048434-2</entry>
<entry level="1" type="bullet">

 Simon S. Haykin, Bernard Widrow (Editor) <it>Least-Mean-Square Adaptive Filters,</it> Wiley, 2003, ISBN 0-471-21570-8</entry>
<entry level="1" type="bullet">

 Bernard Widrow, Samuel D. Stearns <it>Adaptive Signal Processing,</it> Prentice Hall, 1985, ISBN 0-13-004029-0</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<filter wordnetid="103339643" confidence="0.8">
<link xlink:type="simple" xlink:href="../071/172071.xml">
Adaptive filter</link></filter>
</device>
</instrumentality>
</artifact>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../338/2017338.xml">
Recursive least squares</link></entry>
<entry level="1" type="bullet">

For statistical techniques relevant to LMS filter see <link xlink:type="simple" xlink:href="../359/82359.xml">
Least squares</link>.</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../035/14621035.xml">
Similarities between Wiener and LMS</link></entry>
</list>
</p>


</sec>
</bdy>
</article>
