<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:12:22[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<application  confidence="0.9511911446218017" wordnetid="106570110">
<header>
<title>Tf–idf</title>
<id>2057290</id>
<revision>
<id>243918509</id>
<timestamp>2008-10-08T16:17:17Z</timestamp>
<contributor>
<username>Richard Barlow</username>
<id>160716</id>
</contributor>
</revision>
<categories>
<category>Information retrieval</category>
<category>Artificial intelligence applications</category>
<category>Natural language processing</category>
</categories>
</header>
<bdy>

The <b>tf–idf</b> weight (term frequency–inverse document frequency) is a weight often used in <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link> and <link xlink:type="simple" xlink:href="../439/318439.xml">
text mining</link>. This weight is a statistical measure used to evaluate how important a word is to a <link xlink:type="simple" xlink:href="../228/161228.xml">
document</link> in a collection or <link xlink:type="simple" xlink:href="../887/53887.xml">
corpus</link>.  The importance increases <link xlink:type="simple" xlink:href="../863/81863.xml">
proportionally</link> to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.  Variations of the tf–idf weighting scheme are often used by <link xlink:type="simple" xlink:href="../023/4059023.xml">
search engine</link>s as a central tool in scoring and ranking a document's <link xlink:type="simple" xlink:href="../684/442684.xml">
relevance</link> given a user <link xlink:type="simple" xlink:href="../390/679390.xml">
query</link>.
<sec>
<st>
 Mathematical details </st>
<p>

The <it>term frequency</it> in the given document is simply the number of times a given <link xlink:type="simple" xlink:href="../476/430476.xml">
term</link> appears in that document.  This count is usually normalized to prevent a bias towards longer documents (which may have a higher term frequency regardless of the actual importance of that term in the document) to give a measure of the importance of the term <math> t_{i} </math>  within the particular document <math>d_{j}</math>.</p>
<p>

<indent level="1">

<math> \mathrm{tf_{i,j}} = \frac{n_{i,j}}{\sum_k n_{k,j}}</math>
</indent>

where <math>n_{i,j}</math> is the number of occurrences of the considered term in document <math>d_{j}</math>, and the denominator is the number of occurrences of all terms in document <math>d_{j}</math>.</p>
<p>

The <it>inverse document frequency</it> is a measure of the general importance of the term (obtained by dividing the number of all <link xlink:type="simple" xlink:href="../228/161228.xml">
documents</link> by the number of documents containing the term, and then taking the <link xlink:type="simple" xlink:href="../860/17860.xml">
logarithm</link> of that <link xlink:type="simple" xlink:href="../640/332640.xml">
quotient</link>).</p>
<p>

<indent level="1">

<math> \mathrm{idf_{i}} =  \log \frac{|D|}{|\{d_{j}: t_{i} \in d_{j}\}|}</math>
</indent>

with </p>
<p>

<list>
<entry level="1" type="bullet">

 <math> |D| </math>: total number of documents in the corpus</entry>
<entry level="1" type="bullet">

 <math> |\{d_{j} : t_{i} \in d_{j}\}| </math> : number of documents where the term <math> t_{i} </math> appears (that is  <math> n_{i,j} \neq 0</math>).</entry>
</list>
</p>
<p>

Then</p>
<p>

<indent level="1">

<math> \mathrm{tf{}idf_{i,j}} = \mathrm{tf_{i,j}} \cdot  \mathrm{idf_{i}} </math>
</indent>

A high weight in tf–idf is reached by a high term <link xlink:type="simple" xlink:href="../019/4839019.xml">
frequency</link> (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms.</p>

</sec>
<sec>
<st>
 Example </st>
<p>

Consider a document containing 100 words wherein the word <it>cow</it> appears 3 times.  Following the previously defined formulas, the term frequency (TF) for <it>cow</it> is then 0.03 (3 / 100).  Now, assume we have 10 million documents and <it>cow</it> appears in one thousand of these.  Then, the inverse document frequency is calculated as ln(10 000 000 / 1 000) = 9.21.  The TF-IDF score is the product of these quantities: 0.03 * 9.21 = 0.28.</p>

</sec>
<sec>
<st>
Applications in Vector Space Model</st>
<p>

The tf-idf weighting scheme is often used in the <link xlink:type="simple" xlink:href="../134/1256134.xml">
vector space model</link> together with <link xlink:type="simple" xlink:href="../592/8966592.xml">
cosine similarity</link> to determine the <link xlink:type="simple" xlink:href="../523/532523.xml">
similarity</link> between two documents.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../958/64958.xml">
Noun phrase</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../233/1189233.xml">
Word count</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler divergence</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../282/427282.xml">
Transinformation</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../427/689427.xml">
Latent semantic analysis</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><link>
Spärck Jones, Karen</link>&#32;(1972).&#32;"<weblink xlink:type="simple" xlink:href="http://www.soi.city.ac.uk/~ser/idfpapers/ksj_orig.pdf">
A statistical interpretation of term specificity and its application in retrieval</weblink>". <it><link>
Journal of Documentation</link></it>&#32;<b>28</b>&#32;(1): 11–21. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1108%2Feb026526">
10.1108/eb026526</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book"><physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<pioneer wordnetid="110434725" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<originator wordnetid="110383816" confidence="0.8">
<creator wordnetid="109614315" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../624/509624.xml">
Salton, G.</link></associate>
</creator>
</originator>
</scientist>
</causal_agent>
</colleague>
</pioneer>
</person>
</peer>
</physical_entity>
 and M. J. McGill&#32;(1983). Introduction to modern information retrieval.&#32;<company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../269/651269.xml">
McGraw-Hill</link></company>
. ISBN 0070544840.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Salton, Gerard, Edward A. Fox &amp; Harry Wu&#32;(November 1983).&#32;"<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=358466">
Extended Boolean information retrieval</weblink>". <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<periodical wordnetid="106593296" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../161/291161.xml">
Communications of the ACM</link></publication>
</periodical>
</artifact>
</creation>
</product>
</work>
</it>&#32;<b>26</b>&#32;(11): 1022–1036. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1145%2F182.358466">
10.1145/182.358466</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Salton, Gerard and Buckley, C.&#32;(1988).&#32;"Term-weighting approaches in automatic text retrieval". <it><link>
Information Processing &amp; Management</link></it>&#32;<b>24</b>&#32;(5): 513–523. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2F0306-4573%2888%2990021-0">
10.1016/0306-4573(88)90021-0</weblink>.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=866292">
Term Weighting Approaches in Automatic Text Retrieval</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://bscit.berkeley.edu/cgi-bin/pl_dochome?query_src=&amp;format=html&amp;collection=Wilensky_papers&amp;id=3&amp;show_doc=yes">
Robust Hyperlinking</weblink>: An application of tf–idf for stable document addressability.</entry>
</list>
</p>


</sec>
</bdy>
</application>
</article>
