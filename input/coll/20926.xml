<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:31:17[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Supervised learning</title>
<id>20926</id>
<revision>
<id>244424878</id>
<timestamp>2008-10-10T19:06:45Z</timestamp>
<contributor>
<username>Doloco</username>
<id>5559270</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Supervised learning</b> is a <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> technique for learning a function from training data. The <link xlink:type="simple" xlink:href="../228/1817228.xml">
training data</link> consist of pairs of input objects (typically vectors), and desired outputs. The output of the function
can be a continuous value (called <link xlink:type="simple" xlink:href="../997/826997.xml">
regression</link>), or can predict a class label of the input object (called <link xlink:type="simple" xlink:href="../244/1579244.xml">
classification</link>). The task of the supervised learner is to predict the value of the function for any valid input object after having seen a number of training examples (i.e. pairs of input and target output). To achieve this, the learner has to generalize from the presented data to unseen situations in a "reasonable" way (see <link xlink:type="simple" xlink:href="../926/173926.xml">
inductive bias</link>).
(Compare with <link xlink:type="simple" xlink:href="../497/233497.xml">
unsupervised learning</link>.)  The parallel task in human and animal psychology is often referred to as <link xlink:type="simple" xlink:href="../451/6968451.xml">
concept learning</link>.
<sec>
<st>
Overview</st>
<p>

Supervised learning can generate models of two types. Most commonly, supervised learning generates a global model that maps input objects to desired outputs. In some cases, however, the map is implemented as a set of local models (such as in <link xlink:type="simple" xlink:href="../333/170333.xml">
case-based reasoning</link> or the <link xlink:type="simple" xlink:href="../388/1775388.xml">
nearest neighbor algorithm</link>).</p>
<p>

In order to solve a given problem of supervised learning (e.g. learning to <link xlink:type="simple" xlink:href="../619/203619.xml">
recognize handwriting</link>) one has to consider various steps:
<list>
<entry level="1" type="number">

 Determine the type of training examples. Before doing anything else, the engineer should decide what kind of data is to be used as an example. For instance, this might be a single handwritten character, an entire handwritten word, or an entire line of handwriting.</entry>
<entry level="1" type="number">

 Gathering a training set. The training set needs to be characteristic of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements. </entry>
<entry level="1" type="number">

 Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the <link xlink:type="simple" xlink:href="../776/787776.xml">
curse of dimensionality</link>; but should be large enough to accurately predict the output.</entry>
<entry level="1" type="number">

 Determine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link>s or <link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link>s.</entry>
<entry level="1" type="number">

 Complete the design. The engineer then runs the learning algorithm on the gathered training set. Parameters of the learning algorithm may be adjusted by optimizing performance on a subset (called a <it>validation</it> set) of the training set, or via <link xlink:type="simple" xlink:href="../612/416612.xml">
cross-validation</link>. After parameter adjustment and learning, the performance of the algorithm may be measured on a test set that is separate from the training set.</entry>
</list>
</p>
<p>

Another term for supervised learning is classification. A wide range of classifiers are available, each with its strengths and weaknesses. Classifier performance depend greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems, this is also referred to as the 'No free lunch theorem'. Various empirical tests have been performed to compare classifier performance and to find the characteristics of data that determine classifier performance. Determining a suitable classifier for a given problem is however still more an art than a science.</p>
<p>

The most widely used classifiers are the Neural Network (Multi-layer Perceptron), Support Vector Machines, k-Nearest Neighbors, Gaussian Mixture Model, Gaussian, Naive Bayes, Decision Tree and RBF classifiers.</p>

</sec>
<sec>
<st>
 Empirical risk minimization </st>

<p>

The goal of supervised learning of a global model is to find a function <it>g</it>, given a set of points of the form (<it>x</it>, <it>g</it>(<it>x</it>)). </p>
<p>

It is assumed that the set of points for which the behavior of <it>g</it> is known is an <link xlink:type="simple" xlink:href="../067/453067.xml">
independent and identically-distributed random variables</link> sample drawn according to an unknown <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> <it>p</it> of a larger, possibly infinite, population.  Furthermore, one assumes the existence of a task-specific <link xlink:type="simple" xlink:href="../137/442137.xml">
loss function</link> <it>L</it> of type</p>
<p>

<indent level="1">

<math>L: Y\times Y \to \Bbb{R}^+</math>
</indent>

where <it>Y</it> is the codomain of <it>g</it> and <it>L</it> maps into the nonnegative <link xlink:type="simple" xlink:href="../491/19725491.xml">
real number</link>s (further restrictions may be placed on <it>L</it>).  The quantity <it>L</it>(<it>z</it>, <it>y</it>) is the loss incurred by predicting <it>z</it> as the value of <it>g</it> at a given point when the true value is <it>y</it>.</p>
<p>

The <it>risk</it> associated with a function <it>f</it> is then defined as the <link xlink:type="simple" xlink:href="../653/9653.xml">
expectation</link> of the loss function, as follows:</p>
<p>

<indent level="1">

<math>R(f) = \sum_i L(f(x_i), g(x_i)) \; p(x_i)</math>
</indent>

if the probability distribution <it>p</it> is discrete (the analogous continuous case employs a <link xlink:type="simple" xlink:href="../532/15532.xml">
definite integral</link> and a <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link>).</p>
<p>

The goal is now to find a function <it>f</it>* among a fixed subclass of functions for which the risk <it>R</it>(<it>f</it>*) is <link xlink:type="simple" xlink:href="../033/52033.xml">
minimal</link>.</p>
<p>

However, since the behavior of <it>g</it> is generally only known for a finite set of points (<it>x</it>1,&nbsp;<it>y</it>1), ..., (<it>x</it>n,&nbsp;<it>y</it>n), one can only approximate the true risk, for example with the <it>empirical risk</it>:</p>
<p>

<indent level="1">

<math>\tilde{R}_n(f) = \frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i)</math>
</indent>

Selecting the function <it>f</it>* that minimizes the empirical risk is known as the principle of <it>empirical risk minimization</it>.  Statistical learning theory investigates under what conditions empirical risk minimization is admissible and how good the approximations can be expected to be.</p>

</sec>
<sec>
<st>
Active Learning</st>
<p>
 
There are situations in which unlabeled data is abundant but labeling data is expensive. In such a scenario the learning algorithm can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach there is a risk that the algorithm might focus on unimportant or even invalid examples.</p>
<p>

Active learning can be especially useful in biological research problems such as <link xlink:type="simple" xlink:href="../104/216104.xml">
Protein engineering</link> where a few proteins have been discovered with a certain interesting function and one wishes to determine which of many possible mutants to make next that will have a similar function<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.  </p>

<ss1>
<st>
Syntax</st>
<p>

Let <math>T</math> be the total set of all data under consideration.  For example, in a protein engineering problem, <math>T</math> would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.</p>
<p>

During each iteration, <math>i</math>, <math>T</math> is broken up in to three subsets:
<list>
<entry level="1" type="number">

<math>\mathbf{T}_{K,i}</math>: Data points where the label is <b>known</b>.</entry>
<entry level="1" type="number">

<math>\mathbf{T}_{U,i}</math>: Data points where the label is <b>unknown</b>.</entry>
<entry level="1" type="number">

<math>\mathbf{T}_{C,i}</math>: A subset of <math>T_{U,i}</math> that is <b>chosen</b> to be labeled.</entry>
</list>
</p>
<p>

Most of the current research in active learning involves the best method to chose the data points for <math>T_{C,i}</math>.</p>

</ss1>
<ss1>
<st>
Minimum Marginal Hyperplane</st>
<p>

Most active learning algorithms are built upon <know-how wordnetid="105616786" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../309/65309.xml">
Support vector machines (SVMs)</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</method>
</rule>
</event>
</know-how>
 and exploit the structure of the SVM to determine which data points to label.  Such methods usually calculate the margin, <math>W</math>, of each unlabeled datum in <math>T_{U,i}</math> and treat <math>W</math> as a n-dimensional distance from that datum to separating hyperplane.</p>
<p>

Minimum Marginal Hyperplane methods assume that the datum with the smallest <math>W</math> are those that the SVM is most uncertain about and therefore should be placed in <math>T_{C,i}</math> to be labeled.  Other similar methods such as Maximum Marginal Hyperplane chose datum with the largest <math>W</math> or Tradeoff methods chose a mix of the smallest and largest <math>W</math>s</p>

</ss1>
<ss1>
<st>
Maximum Curiosity</st>
<p>

Another active learning method, that typically learns a data set with fewer examples than Minimum Marginal Hyperplane but is more computationally intensive and only works for discrete classifiers is Maximum Curiosity<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.</p>
<p>

Maximum curiosity takes each unlabeled datum in <math>T_{U,i}</math> and assumes all possible labels that that datum might have.  This datum with each assumed class is added to <math>T_{K,i}</math> and then the new <math>T_{K,i}</math> is <link xlink:type="simple" xlink:href="../612/416612.xml">
cross-validated</link>.  It is assumed that the when the datum is paired up with its correct label, the cross-validated accuracy (or <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation coefficient</link>) of <math>T_{K,i}</math> will most improve.  The datum with the most improved accuracy are placed in <math>T_{C,i}</math> to be labeled</p>

</ss1>
</sec>
<sec>
<st>
 Approaches and algorithms </st>

<p>

<list>
<entry level="1" type="bullet">

 <link>
Analytical learning</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial neural network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../091/1360091.xml">
Backpropagation</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../500/90500.xml">
Boosting</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian statistics</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../333/170333.xml">
Case-based reasoning</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../602/232602.xml">
Decision tree</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 learning</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../069/54069.xml">
Inductive logic programming</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../026/477026.xml">
Gaussian process regression</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../742/3274742.xml">
Learning Automata</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../210/302210.xml">
Minimum message length</link> (<link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link>s, decision graphs, etc.)</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../339/87339.xml">
Naive bayes classifier</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../388/1775388.xml">
Nearest Neighbor Algorithm</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../008/380008.xml">
Probably approximately correct learning</link> (PAC) learning</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../366/1432366.xml">
Ripple down rules</link>, a knowledge acquisition methodology</entry>
<entry level="1" type="bullet">

 <link>
Symbolic machine learning</link> algorithms</entry>
<entry level="1" type="bullet">

 <link>
Subsymbolic machine learning</link> algorithms</entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../309/65309.xml">
Support vector machine</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</method>
</rule>
</event>
</know-how>
s</entry>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../880/1363880.xml">
Random Forests</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../987/12304987.xml">
Ensembles of Classifiers</link></entry>
<entry level="1" type="bullet">

 <link>
Ordinal Classification</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../904/12386904.xml">
Data Pre-processing</link></entry>
<entry level="1" type="bullet">

 <link>
Handling imbalanced datasets</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Applications </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../214/4214.xml">
Bioinformatics</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../697/575697.xml">
Cheminformatics</link></entry>
<entry level="2" type="bullet">

<link xlink:type="simple" xlink:href="../669/826669.xml">
Quantitative structure-activity relationship</link></entry>
<entry level="1" type="bullet">

 <software wordnetid="106566077" confidence="0.8">
<application wordnetid="106570110" confidence="0.8">
<program wordnetid="106568978" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106359877" confidence="0.8">
<code wordnetid="106355894" confidence="0.8">
<coding_system wordnetid="106353757" confidence="0.8">
<link xlink:type="simple" xlink:href="../619/203619.xml">
Handwriting recognition</link></coding_system>
</code>
</writing>
</written_communication>
</program>
</application>
</software>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../271/15271.xml">
Information retrieval</link></entry>
<entry level="1" type="bullet">

 Object recognition in <link xlink:type="simple" xlink:href="../596/6596.xml">
computer vision</link></entry>
<entry level="1" type="bullet">

 <software wordnetid="106566077" confidence="0.8">
<application wordnetid="106570110" confidence="0.8">
<program wordnetid="106568978" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106359877" confidence="0.8">
<code wordnetid="106355894" confidence="0.8">
<coding_system wordnetid="106353757" confidence="0.8">
<link xlink:type="simple" xlink:href="../091/49091.xml">
Optical character recognition</link></coding_system>
</code>
</writing>
</written_communication>
</program>
</application>
</software>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../368/28368.xml">
Spam detection</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../706/126706.xml">
Pattern recognition</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../468/29468.xml">
Speech recognition</link></entry>
<entry level="1" type="bullet">

 <link>
Forecasting Fraudulent Financial Statements</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 General issues </st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../537/387537.xml">
Computational learning theory</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../926/173926.xml">
Inductive bias</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../116/19674116.xml">
Overfitting (machine learning)</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../809/7578809.xml">
Version space</link>s</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
Danziger, S.A., Swamidass, S.J., Zeng, J., Dearth, L.R., Lu, Q., Chen, J.H., Cheng, J., Hoang, V.P., Saigo, H., Luo, R., Baldi, P., Brachmann, R.K. and Lathrop, R.H. <b>Functional census of mutation sequence spaces: the example of p53 cancer rescue mutants</b>, (2006) <it>IEEE/ACM transactions on computational biology and bioinformatics</it>, <b>3</b>, 114-125. </entry>
<entry id="2">
Danziger, S.A., Zeng, J., Wang, Y., Brachmann, R.K. and Lathrop, R.H. <b>Choosing where to look next in a mutation sequence space: Active Learning of informative p53 cancer rescue mutants</b>,(2007) <it>Bioinformatics</it>, <b>23(13)</b>, 104-114.</entry>
</reflist>
</p>
<p>

<list>
<entry level="1" type="bullet">

S. Kotsiantis, Supervised Machine Learning: A Review of Classification Techniques, Informatica Journal 31 (2007) 249-268 (http://www.informatica.si/PDF/31-3/11_Kotsiantis%20-%20Supervised%20Machine%20Learning%20-%20A%20Review%20of...pdf).</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://sumo.intec.ugent.be/?q=SUMO_toolbox">
Matlab <b>SU</b>rrogate <b>MO</b>deling Toolbox - SUMO Toolbox</weblink> - Matlab code for Active Learning + Model Selection + Supervised Learning (Surrogate Modeling)</entry>
</list>
</p>


</sec>
</bdy>
</article>
