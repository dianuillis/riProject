<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:32:41[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Early stopping</title>
<id>213214</id>
<revision>
<id>242745180</id>
<timestamp>2008-10-03T13:52:46Z</timestamp>
<contributor>
<username>Raven1977</username>
<id>7984680</id>
</contributor>
</revision>
<categories>
<category>Data mining</category>
<category>Neural networks</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, <b>early stopping</b> is a form of <link xlink:type="simple" xlink:href="../061/2009061.xml">
regularization</link> used when a <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> model (such as a <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link>) is trained by on-line <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link>. In early stopping, the <link xlink:type="simple" xlink:href="../228/1817228.xml">
training set</link> is split into a new training set and a validation set. Gradient descent is applied to the new training set. After each sweep through the new training set, the network is evaluated on the validation set. The network with the best performance on the validation set is then used for actual testing.<p>

This technique is a simple but efficient hack to deal with the problem of <link xlink:type="simple" xlink:href="../332/173332.xml">
overfitting</link>. Overfitting is a phenomenon in which a learning system, such as a <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link> gets very good at dealing with one data set at the expense of becoming very bad at dealing with other data sets. Early stopping is effectively limiting the used weights in the network and thus imposes a regularization, effectively lowering the <link xlink:type="simple" xlink:href="../846/305846.xml">
VC dimension</link>.</p>
<p>

Early stopping is a very common practice in neural network training and often produces networks that generalize well. However, while often improving the generalization it does not do so in a mathematically well-defined way. </p>
<p>

See related topic: <link xlink:type="simple" xlink:href="../612/416612.xml">
Cross-validation</link> in particular using a "Validation Set"</p>






</bdy>
</article>
