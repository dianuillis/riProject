<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:30:13[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Factored language model</title>
<id>2229040</id>
<revision>
<id>229412152</id>
<timestamp>2008-08-02T14:45:35Z</timestamp>
<contributor>
<username>Dmcarter</username>
<id>7581098</id>
</contributor>
</revision>
<categories>
<category>Statistical natural language processing</category>
</categories>
</header>
<bdy>

The <b>factored language model</b> (<b>FLM</b>) is an extension of a conventional <link xlink:type="simple" xlink:href="../810/1911810.xml">
language model</link>.  In an FLM, each word is viewed as a vector of <it>k</it> factors: <math>w_i = \{f_i^1, ..., f_i^k\}</math>.  An FLM provides the probabilistic model <math>P(f|f_i, ..., f_N)</math> where the prediction of a factor <math>f</math> is based on <math>N</math> parents <math>\{f_1, ..., f_N\}</math>.  For example, if <math>w</math> represents a word token and <math>t</math> represents a <link xlink:type="simple" xlink:href="../059/45059.xml">
Part of speech</link> tag for English, the expression <math>P(w_i|w_{i-2}, w_{i-1}, t_{i-1})</math> gives a model for predicting current word token based on a traditional <link xlink:type="simple" xlink:href="../182/986182.xml">
Ngram</link> model as well as the <link xlink:type="simple" xlink:href="../059/45059.xml">
Part of speech</link> tag of the previous word.<p>

A major advantage of factored language models is that they allow users to specify linguistic knowledge such as the relationship between word tokens and <link xlink:type="simple" xlink:href="../059/45059.xml">
Part of speech</link> in English, or morphological information (stems, root, etc.) in Arabic.</p>
<p>

Like <link xlink:type="simple" xlink:href="../182/986182.xml">
N-gram</link> models, smoothing techniques are necessary in parameter estimation.  In particular, generalized back-off is used in training an FLM.</p>

<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal">J Bilmes and K Kirchhoff&#32;(2003). "<weblink xlink:type="simple" xlink:href="http://ssli.ee.washington.edu/people/bilmes/mypapers/hlt03.pdf">
Factored Language Models and Generalized Parallel Backoff</weblink>".&#32;<it>Human Language Technology Conference</it>.</cite>&nbsp;</entry>
</list>
</p>

</sec>
</bdy>
</article>
