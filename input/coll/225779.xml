<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:33:03[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Optimization (computer science)</title>
<id>225779</id>
<revision>
<id>244262732</id>
<timestamp>2008-10-10T00:31:48Z</timestamp>
<contributor>
<username>SpBot</username>
<id>7016748</id>
</contributor>
</revision>
<categories>
<category>Software performance optimization</category>
</categories>
</header>
<bdy>

For algorithms to solve optimization problems, see <link xlink:type="simple" xlink:href="../033/52033.xml">
Optimization (mathematics)</link>.<p>

In <link xlink:type="simple" xlink:href="../213/5213.xml">
computing</link>, <b>optimization</b> is the process of modifying a system to make some aspect of it work more efficiently or use fewer resources.  For instance, a <link xlink:type="simple" xlink:href="../783/5783.xml">
computer program</link> may be optimized so that it executes more rapidly, or is capable of operating with less <link xlink:type="simple" xlink:href="../300/5300.xml">
memory storage</link> or other resources, or draw less power.  The system may be a single <link xlink:type="simple" xlink:href="../783/5783.xml">
computer program</link>, a collection of <link xlink:type="simple" xlink:href="../457/7878457.xml">
computer</link>s or even an entire network such as the <link xlink:type="simple" xlink:href="../539/14539.xml">
Internet</link>. See also <link xlink:type="simple" xlink:href="../128/145128.xml">
algorithmic efficiency</link> for further discussion on factors relating to improving the efficiency of an algorithm.</p>

<sec>
<st>
General</st>
<p>

Although the word "optimization" shares the same root as "optimal," it is rare for the process of optimization to produce a truly optimal system. The optimized system will typically only be optimal in one application or for one audience. One might reduce the amount of time that a program takes to perform some task at the price of making it consume more memory.  In an application where memory space is at a premium, one might deliberately choose a slower <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> in order to use less memory. Often there is no “one size fits all” design which works well in all cases, so <link xlink:type="simple" xlink:href="../223/38223.xml">
engineer</link>s make <link xlink:type="simple" xlink:href="../213/406213.xml">
trade-off</link>s to optimize the attributes of greatest interest.  Additionally, the effort required to make a piece of software completely optimal—incapable of any further improvement— is almost always more than is reasonable for the benefits that would be accrued; so the process of optimization may be halted before a completely optimal solution has been reached.  Fortunately, it is often the case that the greatest improvements come early in the process.
</p>
<ss1>
<st>
Categories</st>
<p>

Code optimization can be broadly categorized as <link xlink:type="simple" xlink:href="../196/81196.xml">
platform</link> dependent and platform independent techniques.
Platform independent techniques are generic techniques (such as loop unrolling, reduction in function calls, memory efficient routines, reduction in conditions, etc.) and are effective for most digital signal processors (DSP) platforms. Generally, these serve to reduce the total <link xlink:type="simple" xlink:href="../800/18839800.xml">
Instruction path length</link> required to complete the program and/or reduce total memory usage during the process.  Platform dependent techniques involve instruction level parallelism, data level parallelism, cache optimization techniques, i.e. parameters that differ among various platforms.</p>

</ss1>
<ss1>
<st>
'Levels' of optimization</st>
<p>

Optimization can occur at a number of 'levels':</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Design level</b></entry>
</list>

At the highest level, the design may be optimized to make best use of the available resources.  The implementation of this design will benefit from a good choice of <link xlink:type="simple" xlink:href="../128/145128.xml">
efficient algorithms</link> and the implementation of these algorithms will benefit from writing good quality code. The architectural design of a system overwhelmingly affects its performance. The choice of <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> affects efficiency more than any other item of the design.  
<list>
<entry level="1" type="bullet">

 <b>Compile level</b></entry>
</list>

Use of an <link xlink:type="simple" xlink:href="../355/40355.xml">
optimizing compiler</link> tends to ensure that the <link xlink:type="simple" xlink:href="../392/217392.xml">
executable program</link> is optimized at least as much as the compiler can predict. 
<list>
<entry level="1" type="bullet">

 <b>Assembly level</b></entry>
</list>

At the lowest level, writing code using an <link xlink:type="simple" xlink:href="../368/1368.xml">
Assembly language</link> designed for a particular <link xlink:type="simple" xlink:href="../615/13615.xml">
hardware</link> platform will normally produce the most efficient code since the programmer can take advantage of the full repertoire of <link xlink:type="simple" xlink:href="../683/20683.xml">
machine instruction</link>s. The <link xlink:type="simple" xlink:href="../194/22194.xml">
operating system</link>s of most machines has been traditionally written in Assembler code for this reason. </p>
<p>

With more modern <link xlink:type="simple" xlink:href="../355/40355.xml">
optimizing compiler</link>s and the greater complexity of recent <link xlink:type="simple" xlink:href="../218/5218.xml">
CPU</link>s, it is more difficult to write code that is optimized better than the compiler itself generates, and few projects need resort to this 'ultimate' optimization step.</p>
<p>

However, a large amount of code written today is still compiled with the intent to run on the greatest percentage of machines possible.  As a consequence, programmers and compilers don't always take advantage of the more efficient instructions provided by newer CPUs or quirks of older models.  Since optimization often relies on making use of special cases and performing complex trade-offs, a fully optimized program can sometimes, if insufficiently commented, be more difficult for less inexperienced programmers to comprehend  and hence may contain more <link xlink:type="simple" xlink:href="../085/37085.xml">
faults</link> than unoptimized versions.</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Run time</b> </entry>
</list>

<link xlink:type="simple" xlink:href="../399/7914399.xml">
Just in time</link> compilers and Assembler programmers may be able to perform <link xlink:type="simple" xlink:href="../434/460434.xml">
run time</link> optimization exceeding the capability of static compilers by dynamically adjusting parameters according to the actual input or other factors.</p>

</ss1>
</sec>
<sec>
<st>
Different algorithms</st>
<p>

Computational tasks can be performed in several different ways with varying efficiency. For example, consider the following <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
 code snippet whose intention is to obtain the sum of all integers from 1 to N:</p>

<p>

int i, sum = 0;
for (i = 1; i = N; i++)
sum += i;
printf ("sum: %d\n", sum);</p>

<p>

This code can (assuming no <link xlink:type="simple" xlink:href="../724/40724.xml">
arithmetic overflow</link>) be rewritten using a mathematical formula like:</p>

<p>

int sum = (N * (N+1)) / 2;
printf ("sum: %d\n", sum);</p>

<p>

The optimization, sometimes performed automatically by an optimizing compiler, is to select a method (<link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>) that is more computationally efficient while retaining the same functionality. See <link xlink:type="simple" xlink:href="../128/145128.xml">
Algorithmic efficiency</link> for a discussion of some of these techniques. However, a significant improvement in performance can often be achieved by solving only the actual problem and removing extraneous functionality. </p>
<p>

Optimization is not always an obvious or intuitive process.  In the example above, the ‘optimized’ version might actually be slower than the original version if N were sufficiently small and the particular hardware happens to be much faster at performing addition and <link>
loop</link>ing operations than multiplication and division.</p>

</sec>
<sec>
<st>
 Trade-offs </st>
<p>

Optimization will generally focus on improving just one or two aspects of performance: execution time, memory usage, disk space, bandwidth, power consumption or some other resource. This will usually require a trade-off:  where one factor is optimized at the expense of others. For example, increasing the size of <link xlink:type="simple" xlink:href="../829/6829.xml">
cache</link> improves runtime performance, but also increases the memory consumption. Other common trade-offs include code clarity and conciseness.</p>
<p>

There are instances where the programmer performing the optimization must decide to make the software more optimal for some operations but at the cost of making other operations less efficient.  These trade-offs may sometimes be of a non-technical nature - such as when a competitor has published a <link xlink:type="simple" xlink:href="../498/4498.xml">
benchmark</link> result that must be beaten in order to improve commercial success but comes perhaps with the burden of making normal usage of the software less efficient.  Such changes are sometimes jokingly referred to as <it>pessimizations</it>.</p>

</sec>
<sec>
<st>
Bottlenecks</st>
<p>

Optimization may include finding a <link xlink:type="simple" xlink:href="../435/2207435.xml">
bottleneck</link>, a critical part of the code that is the primary consumer of the needed resource - sometimes known as a "hot spot". As a rule of thumb, improving 20% of the code is responsible for 80% of the results.</p>
<p>

In computer science, the <law wordnetid="108441203" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../562/24562.xml">
Pareto principle</link></group>
</collection>
</rule>
</concept>
</idea>
</law>
 can be applied to resource optimization by observing that 80% of the resources are typically used by 20% of the operations. In software engineering, it is often a better approximation that 90% of the execution time of a computer program is spent executing 10% of the code (known as the 90/10 law in this context).</p>
<p>

More complex algorithms and data structures perform well with many items, while simple algorithms are more suitable for small amounts of data—the setup and initialization time of the more complex algorithm can outweigh the benefit.</p>
<p>

In some cases, adding more memory can help to make a program run faster. For example, a filtering program will commonly read each line and filter and output that line immediately. This only uses enough memory for one line, but performance is typically poor. Performance can be greatly improved by reading the entire file then writing the filtered result, though this uses much more memory. Caching the result is similarly effective, though also requiring larger memory use.</p>

</sec>
<sec>
<st>
When to optimize</st>

<p>

Optimization can reduce <link xlink:type="simple" xlink:href="../291/355291.xml">
readability</link> and add code that is used only to improve the <link xlink:type="simple" xlink:href="../515/224515.xml">
performance</link>. This may complicate programs or systems, making them harder to maintain and debug. As a result, optimization or performance tuning is often performed at the end of the <link xlink:type="simple" xlink:href="../901/220901.xml">
development stage</link>.</p>
<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Donald Knuth</link></scientist>
</person>
 said
<list>
<entry level="1" type="bullet">

"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil." (Knuth, Donald.  <it>Structured Programming with go to Statements</it>, ACM Journal <b>Computing Surveys</b>, Vol 6, No.  4, Dec. 1974.  p.268.)</entry>
</list>
</p>
<p>

"Premature optimization" is a phrase used to describe a situation where a programmer lets performance considerations affect the design of a piece of code.  This can result in a design that is not as clean as it could have been or code that is incorrect, because the code is complicated by the optimization and the programmer is distracted by optimizing.</p>
<p>

An alternative approach is to design first, code from the design and then <link xlink:type="simple" xlink:href="../080/2310080.xml">
profile</link>/<link xlink:type="simple" xlink:href="../498/4498.xml">
benchmark</link> the resulting code to see which parts should be optimized.  A simple and elegant design is often easier to optimize at this stage, and profiling may reveal unexpected performance problems that would not have been addressed by premature optimization.</p>
<p>

In practice, it is often necessary to keep performance goals in mind when first designing software, but the programmer balances the goals of design and optimization.</p>

<ss1>
<st>
 Macros </st>
<p>

Optimization during code development using <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../560/20560.xml">
macros</link></concept>
</idea>
 takes on different forms in different languages. In some procedural languages, such as <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
 and <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../038/72038.xml">
C++</link></programming_language>
, macros are implemented using textual substitution, and so their benefit is mostly limited to avoiding function-call overhead.</p>
<p>

In many <link xlink:type="simple" xlink:href="../933/10933.xml">
functional programming</link> languages, however, macros are implemented using compile-time evaluation and substitution of non-textual, compiled code. Because of this difference, it is possible to perform complex compile-time computations, moving some work out of the resulting program. <link xlink:type="simple" xlink:href="../016/18016.xml">
Lisp</link> originated this style of macro [Fact September 2008]], and such macros are often called “Lisp-like macros.”</p>
<p>

As with any optimization, however, it is often difficult to predict where such tools will have the most impact before a project is complete.</p>

</ss1>
</sec>
<sec>
<st>
Automated and manual optimization</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../355/40355.xml">
Compiler optimization</link></it>
</indent>
<it>See also 
Compiler optimizations</it></p>
<p>

Optimization can be automated by compilers or performed by programmers. Gains are usually limited for local optimization, and larger for global optimizations. Usually, the most powerful optimization is to find a superior <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>.</p>
<p>

Optimizing a whole system is usually undertaken by programmers because it is too complex for automated optimizers. In this situation, programmers or system administrators explicitly change code so that the overall system performs better.  Although it can produce better efficiency, it is far more expensive than automated optimizations.</p>
<p>

Use a <link xlink:type="simple" xlink:href="../080/2310080.xml">
profiler</link> (or <link xlink:type="simple" xlink:href="../080/2310080.xml">
performance analyzer</link>) to find the sections of the program that are taking the most resources — the <it>bottleneck</it>. Programmers sometimes believe they have a clear idea of where the bottleneck is, but intuition is frequently wrong. Optimizing an unimportant piece of code will typically do little to help the overall performance.</p>
<p>

When the bottleneck is localized, optimization usually starts with a rethinking of the algorithm used in the program: more often than not, a particular algorithm can be specifically tailored to a particular problem, yielding better performance than a generic algorithm. For example, the task of sorting a huge list of items is usually done with a <link xlink:type="simple" xlink:href="../249/3268249.xml">
quicksort</link> routine, which is one of the most efficient generic algorithms. But if some characteristic of the items is exploitable (for example, they are already arranged in some particular order), a different method can be used, or even a custom-made sort routine.</p>
<p>

After one is reasonably sure that the best algorithm is selected, code optimization can start: loops can be unrolled (for lower loop overhead, although this can often lead to <it>lower</it> speed if it overloads the <link xlink:type="simple" xlink:href="../181/849181.xml">
CPU cache</link>), data types as small as possible can be used, integer arithmetic can be used instead of floating-point, and so on.</p>
<p>

Performance bottlenecks can be due to language limitations rather than algorithms or data structures used in the program. Sometimes, a critical part of the program can be re-written in a different <link xlink:type="simple" xlink:href="../015/23015.xml">
programming language</link> that gives more direct access to the underlying machine. For example, it is common for very <link xlink:type="simple" xlink:href="../842/189842.xml">
high-level</link> languages like <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../862/23862.xml">
Python</link></programming_language>
 to have modules written in <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
 for greater speed. Programs already written in C can have modules written in <link xlink:type="simple" xlink:href="../368/1368.xml">
assembly</link>. Programs written in <link xlink:type="simple" xlink:href="../881/243881.xml">
D</link> can use the <link xlink:type="simple" xlink:href="../956/1031956.xml">
inline assembler</link>.</p>
<p>

Rewriting pays off because of a general rule known as the <link>
90/10 law</link>, which states that 90% of the time is spent in 10% of the code, and only 10% of the time in the remaining 90% of the code. So putting intellectual effort into optimizing just a small part of the program can have a huge effect on the overall speed if the correct part(s) can be located.</p>
<p>

Manual optimization often has the side-effect of undermining readability.   Thus code optimizations should be carefully documented and their effect on future development evaluated.</p>
<p>

The program that does the automated optimization is called an <b>optimizer</b>. Most optimizers are embedded in compilers and operate during compilation. Optimizers can often tailor the generated code to specific processors.</p>
<p>

Today, automated optimizations are almost exclusively limited to <link xlink:type="simple" xlink:href="../355/40355.xml">
compiler optimization</link>.</p>
<p>

Some high-level languages (<programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../838/9838.xml">
Eiffel</link></programming_language>
, <language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../078/1285078.xml">
Esterel</link></language>
) optimize their programs by using an <link xlink:type="simple" xlink:href="../537/1209537.xml">
intermediate language</link>.</p>
<p>

<link xlink:type="simple" xlink:href="../373/49373.xml">
Grid computing</link> or <link xlink:type="simple" xlink:href="../501/8501.xml">
distributed computing</link> aims to optimize the whole system, by moving tasks from computers with high usage to computers with idle time.</p>

</sec>
<sec>
<st>
Time taken for optimization</st>
<p>

Sometimes, the time taken to undertake optimization in itself may be an issue.</p>
<p>

Optimizing existing code usually does not add new features, and worse, it might add new <invertebrate wordnetid="101905661" confidence="0.8">
<arthropod wordnetid="101767661" confidence="0.8">
<bug wordnetid="102236355" confidence="0.8">
<insect wordnetid="102159955" confidence="0.8">
<animal wordnetid="100015388" confidence="0.8">
<link xlink:type="simple" xlink:href="../085/37085.xml">
bugs</link></animal>
</insect>
</bug>
</arthropod>
</invertebrate>
 in previously working code (as any change might). Because manually optimized code might sometimes have less 'readability' than unoptimized code, optimization might impact maintainability of it also. Optimization comes at a price and it is important to be sure that the investment is worthwhile.</p>
<p>

An automatic optimizer (or <link xlink:type="simple" xlink:href="../355/40355.xml">
optimizing compiler</link>) a program that performs code optimization) may itself have to be optimized - either to to further improve the efficiency of its target programs or else speed up its own operation. A compilation performed with  optimization 'turned on' usually takes longer, although this is usually only a problem when programs are quite large (but probably more than compensated for over many run time savings of the code).</p>
<p>

In particular, for <link xlink:type="simple" xlink:href="../632/220632.xml">
just-in-time compiler</link>s the performance of the <link xlink:type="simple" xlink:href="../434/460434.xml">
run time</link> compile component, executing together with its target code, is the key to improving overall execution speed.</p>



</sec>
<sec>
<st>
Quotes</st>
<p>

<list>
<entry level="1" type="bullet">

 <it>“The order in which the operations shall be performed in every particular case is a very interesting and curious question, on which our space does not permit us fully to enter. In almost every computation a great variety of arrangements for the succession of the processes is possible, and various considerations must influence the selection amongst them for the purposes of a Calculating Engine. One essential object is to choose that arrangement which shall tend to reduce to a minimum the time necessary for completing the calculation.”</it> - <link xlink:type="simple" xlink:href="../311/1311.xml">
Ada Byron's notes on the analytical engine</link> 1842.</entry>
<entry level="1" type="bullet">

 <it>“More computing sins are committed in the name of efficiency (without necessarily achieving it) than for any other single reason - including blind stupidity.”</it> - <link xlink:type="simple" xlink:href="../160/276160.xml">
W.A. Wulf</link></entry>
<entry level="1" type="bullet">

 <it>“We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> - <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Donald Knuth</link></scientist>
</person>
</entry>
<entry level="1" type="bullet">

 <it>“Bottlenecks occur in surprising places, so don't try to second guess and put in a speed hack until you have proven that's where the bottleneck is.”</it> - <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../033/26033.xml">
Rob Pike</link></person>
</entry>
<entry level="1" type="bullet">

 <it>“The First Rule of Program Optimization: Don't do it. The Second Rule of Program Optimization (for experts only!): Don't do it yet.”</it> - <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<computer_user wordnetid="109951274" confidence="0.8">
<programmer wordnetid="110481268" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../392/215392.xml">
Michael A. Jackson</link></educator>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</engineer>
</programmer>
</computer_user>
</person>
</physical_entity>
</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>

<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../128/145128.xml">
Algorithmic efficiency</link></entry>
<entry level="1" type="bullet">

<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../490/60490.xml">
Abstract interpretation</link></method>
</know-how>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../829/6829.xml">
Caching</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../653/43653.xml">
Control flow graph</link></entry>
<entry level="1" type="bullet">

<change_of_state wordnetid="100199130" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<change wordnetid="100191142" confidence="0.8">
<improvement wordnetid="100248977" confidence="0.8">
<action wordnetid="100037396" confidence="0.8">
<optimization wordnetid="100260051" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../155/18155.xml">
Lazy evaluation</link></psychological_feature>
</act>
</optimization>
</action>
</improvement>
</change>
</event>
</change_of_state>
</entry>
<entry level="1" type="bullet">

<change_of_state wordnetid="100199130" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<change wordnetid="100191142" confidence="0.8">
<improvement wordnetid="100248977" confidence="0.8">
<action wordnetid="100037396" confidence="0.8">
<optimization wordnetid="100260051" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../754/1837754.xml">
Loop optimization</link></psychological_feature>
</act>
</optimization>
</action>
</improvement>
</change>
</event>
</change_of_state>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../611/654611.xml">
Low level virtual machine</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../483/723483.xml">
Memoization</link></entry>
<entry level="1" type="bullet">

<link>
Memory locality</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../080/2310080.xml">
Performance analysis</link> (profiling)</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../578/50578.xml">
Queueing theory</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../416/375416.xml">
Simulation</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../504/319504.xml">
Speculative execution</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../371/373371.xml">
SSA form</link></entry>
<entry level="1" type="bullet">

<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../051/1029051.xml">
Worst-case execution time</link></instrumentality>
</artifact>
</system>
</entry>
</list>
</p>


</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Knuth, Donald</link></scientist>
</person>
: <weblink xlink:type="simple" xlink:href="http://pplab.snu.ac.kr/courses/adv_pl05/papers/p261-knuth.pdf">
Structured Programming with Goto Statements</weblink>. <it>Computing Surveys</it> <b>6</b>:4 (1974), 261–301. </entry>
</reflist>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../316/3635316.xml">
Jon Bentley</link></scientist>
</causal_agent>
</person>
</physical_entity>
: <it>Writing Efficient Programs</it>, ISBN 0-13-970251-2.</entry>
<entry level="1" type="bullet">

 <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Donald Knuth</link></scientist>
</person>
: <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../358/31358.xml">
The Art of Computer Programming</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
''</it></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.azillionmonkeys.com/qed/optimize.html">
Programming Optimization</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.eventhelix.com/RealtimeMantra/Basics/OptimizingCAndCPPCode.htm">
C,C++ optimization</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.abarnett.demon.co.uk/tutorial.html">
C optimization tutorial</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.arizona.edu/solar/">
Software Optimization at Link-time And Run-time</weblink></entry>
<entry level="1" type="bullet">

 Article "<weblink xlink:type="simple" xlink:href="http://doi.ieeecomputersociety.org/10.1109/2.348001">
A Plea for Lean Software</weblink>" by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../642/21642.xml">
Niklaus Wirth</link></scientist>
</person>
</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://c2.com/cgi/wiki?CategoryOptimization">
Description from the Portland Pattern Repository</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.daemon.be/maarten/ipperf.html">
Performance tuning of Computer Networks</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.thinkingparallel.com/2006/08/07/my-views-on-high-level-optimization/">
An article describing high-level optimization</weblink></entry>
</list>
</p>



</sec>
</bdy>
</article>
