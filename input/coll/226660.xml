<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:35:02[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Dempster-Shafer theory</title>
<id>226660</id>
<revision>
<id>235892272</id>
<timestamp>2008-09-02T21:02:16Z</timestamp>
<contributor>
<username>Phoebe</username>
<id>19217</id>
</contributor>
</revision>
<categories>
<category>Probability theory</category>
</categories>
</header>
<bdy>

The <b>Dempster-Shafer theory</b> is a mathematical theory of <link xlink:type="simple" xlink:href="../854/5236854.xml">
evidence</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> based on <it>belief functions</it> and <it>plausible reasoning</it>, which is used to combine separate pieces of information (evidence) to calculate the probability of an event. The theory was developed by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../761/7040761.xml">
Arthur P. Dempster</link></associate>
</scholar>
</mathematician>
</scientist>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</statistician>
</person>
</peer>
</physical_entity>
 and <link xlink:type="simple" xlink:href="../660/226660.xml">
Glenn Shafer</link>.
<sec>
<st>
Consider two possible gambles</st>
<p>

The first gamble is that we bet on a head turning up when we toss a coin that is known to be fair. Now consider the second gamble, in which we bet on the outcome of a fight between the world's greatest boxer and the world's greatest wrestler. Assume we are fairly ignorant about martial arts and would have great difficulty making a choice of who to bet on.</p>
<p>

Many people would feel more unsure about taking the second gamble, in which the probabilities are unknown, rather than the first gamble, in which the probabilities are easily seen to be one half for each outcome. Dempster-Shafer theory allows one to consider the confidence one has in the probabilities assigned to the various outcomes.</p>

</sec>
<sec>
<st>
Formalism</st>
<p>

Let <it>X</it> be the <it>universal set</it>: the set of all states under consideration. The <link xlink:type="simple" xlink:href="../799/23799.xml">
power set</link>, <math>\mathbb P(X)\,\!</math>, is the set of all possible sub-sets of <it>X</it>, including the <link xlink:type="simple" xlink:href="../566/9566.xml">
empty set</link>. For example, if:</p>
<p>

<indent level="1">

<math>X = \left \{ a, b \right \} \,\!</math>
</indent>

then</p>
<p>

<indent level="1">

<math>\mathbb P(X) = \left \{ \varnothing, \left \{ a \right \}, \left \{ b \right \}, X \right \}. \,\!</math>
</indent>

The elements of the power set can be taken to represent propositions that one might be interested in, by containing all and only the states in which this proposition is true.</p>
<p>

The theory of evidence assigns a belief mass to each subset of the power set. Formally, a function <math>m: \mathbb P(X) \rightarrow [0,1]</math>, is called a <it>basic belief assignment</it> (BBA), when it verifies two axioms. First, the mass of the empty set is zero:</p>
<p>

<indent level="1">

<math>m(\varnothing) = 0. \,\!</math>
</indent>

Second, the masses of the remaining members of the power set add up to a total of 1:</p>
<p>

<indent level="1">

<math>1 = \sum_{A \in \mathbb P(X)} m(A). \,\!</math>
</indent>

The mass <it>m</it>(<it>A</it>) of a given member of the power set, <it>A</it>, expresses the proportion of all relevant and available evidence that supports the claim that the actual state belongs to <it>A</it> but to no particular subset of <it>A</it>. The value of <it>m</it>(<it>A</it>) pertains <it>only</it> to the set <it>A</it> and makes no additional claims about any subsets of <it>A</it>, each of which has, by definition, its own mass.</p>
<p>

From the mass assignments, the upper and lower bounds of a probability interval can be defined. This interval contains the precise probability of a set of interest (in the classical sense), and is bounded by two non-additive continuous measures called <b>belief</b> (or <b>support</b>) and <b>plausibility</b>:</p>
<p>

<indent level="1">

<math>\operatorname{bel}(A) \le P(A) \le \operatorname{pl}(A).\,\!</math>
</indent>

The belief bel(<it>A</it>) for a set <it>A</it> is defined as the sum of all the masses of (not necessarily proper) subsets of the set of interest:</p>
<p>

<indent level="1">

<math>\operatorname{bel}(A) = \sum_{B \mid B \subseteq A} m(B).</math>
</indent>

The plausibility pl(<it>A</it>) is the sum of all the masses of the sets <it>B</it> that intersect the set of interest <it>A</it>:</p>
<p>

<indent level="1">

<math>\operatorname{pl}(A) = \sum_{B \mid B \cap A \ne \varnothing} m(B).</math>
</indent>

The two measures are related to each other as follows:</p>
<p>

<indent level="1">

<math>\operatorname{pl}(A) = 1 - \operatorname{bel}(\overline{A}).\,\!</math>
</indent>

It follows from the above that you need know but one of the three (mass, belief, or plausibility) to deduce the other two, though you may need to know the values for many sets in order to calculate one of the other values for a particular set.</p>

</sec>
<sec>
<st>
Dempster's rule of combination</st>
<p>

The problem we now face is how to combine two independent sets of mass assignments. The original combination rule, known as Dempster's rule of combination, is a generalization of <link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' rule</link>. This rule strongly emphasises the agreement between multiple sources and ignores <it>all</it> the conflicting evidence through a normalization factor. Use of that rule has come under serious criticism when significant conflict in the information is encountered.</p>
<p>

Specifically, the combination (called the <b>joint mass</b>) is calculated from the two sets of masses <math>m_1\,\!</math> and <math>m_2\,\!</math> in the following manner:</p>
<p>

<indent level="1">

<math>m_{1,2}(\varnothing) = 0 \,\!</math>
</indent>

<indent level="1">

<math>m_{1,2}(A) = (m_1 \oplus m_2) (A) = \frac {1}{1 - K} \sum_{B \cap C = A \ne \varnothing} m_1(B) m_2(C) \,\!</math>
</indent>

where</p>
<p>

<indent level="1">

<math>K = \sum_{B \cap C = \varnothing} m_1(B) m_2(C). \,\!</math>
</indent>

<math>K\,\!</math> is a measure of the amount of conflict between the two mass sets. The normalization factor, <math>1-K\,\!</math>, has the effect of completely ignoring conflict and attributing <it>any</it> mass associated with conflict to the null set. Consequently, this operation yields counterintuitive results in the face of significant conflict in certain contexts.</p>

</sec>
<sec>
<st>
Discussion</st>

<p>

Dempster-Shafer theory is a generalization of the Bayesian theory of subjective probability; whereas the latter requires probabilities for each question of interest, belief functions base degrees of belief (or confidence, or trust) for one question on the probabilities for a related question. These degrees of belief may or may not have the mathematical properties of probabilities; how much they differ depends on how closely the two questions are related<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>. Put another way, it is a way of representing <link xlink:type="simple" xlink:href="../247/9247.xml">
epistemic</link> plausibilities but it can yield answers which contradict those arrived at using <link xlink:type="simple" xlink:href="../542/23542.xml">
probability theory</link>.</p>
<p>

Often used as a method of <link xlink:type="simple" xlink:href="../077/1461077.xml">
sensor fusion</link>, Dempster-Shafer theory is based on two ideas: obtaining degrees of belief for one question from subjective probabilities for a related question, and Dempster's rule<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> for combining such degrees of belief when they are based on independent items of evidence.  In essence, the degree of belief in a proposition depends primarily upon the number of answers (to the related questions) containing the proposition, and the subjective probability of each answer. Also contributing are the rules of combination that reflect general assumptions about the data.</p>
<p>

In this formalism a <b>degree of belief</b> (also referred to as a <b>mass</b>) is represented as a <b>belief function</b> rather than a <link xlink:type="simple" xlink:href="../890/4890.xml">
Bayesian</link> <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link>. Probability values are assigned to <it>sets</it> of possibilities rather than single events: their appeal rests on the fact they naturally encode evidence in favor of propositions.</p>
<p>

Dempster-Shafer theory assigns its masses to all of the subsets of the entities that comprise a system. Suppose for example that a system has five members, that is to say five independent states, exactly one of which is actual. If the original set is called S, <math>|S|=5</math>, then the set of all subsets &mdash;the <it>power set</it>&mdash; is called 2S. Since you can express each possible subset as a binary vector (describing whether any particular member is present or not by writing a “1” or a “0” for that member's slot), it can be seen that there are 25 subsets possible (<math>2^{|S|}</math> in general), ranging from the empty subset (0, 0, 0, 0, 0) to the "everything" subset (1, 1, 1, 1, 1). The empty subset represents a contradiction, which is not true in any state, and is thus assigned a mass of zero; the remaining masses are normalised so that their total is 1. The "everything" subset is often labelled "unknown" as it represents the state where all elements are present, in the sense that you cannot tell which is actual.</p>

<ss1>
<st>
Belief and plausibility</st>
<p>

Shafer's framework allows for belief about propositions to be represented as intervals, bounded by two values, <it>belief</it> (or <it>support</it>) and <it>plausibility</it>:</p>
<p>

<indent level="1">

<it>belief</it> &amp;le; <it>plausibility</it>.
</indent>

<it>Belief</it> in a hypothesis is constituted by the sum of the masses of all sets enclosed by it (i.e. the sum of the masses of all subsets of the hypothesis). It is the amount of belief that directly supports a given hypothesis at least in part, forming a lower bound. <it>Plausibility</it> is 1 minus the sum of the masses of all sets whose intersection with the hypothesis is empty. It is an upper bound on the possibility that the hypothesis could possibly happen, i.e. it "could possibly happen" up to that value, because there is only so much evidence that contradicts that hypothesis.</p>
<p>

For example, suppose we have a belief of 0.5 and a plausibility of 0.8 for a proposition, say "the cat in the box is dead." This means that we have evidence that allows us to state strongly that the proposition is true with a confidence of 0.5. However, the evidence contrary to that hypothesis (i.e. "the cat is alive") only has a confidence of 0.2. The remaining mass of 0.3 (the gap between the 0.5 supporting evidence on the one hand, and the 0.2 contrary evidence on the other) is "indeterminate," meaning that the cat could either be dead or alive.
This interval represents the level of uncertainty based on the evidence in your system.</p>
<p>

<table cellpadding="1" border="1">
<header>
Hypothesis</header>
<header>
Mass</header>
<header>
Belief</header>
<header>
Plausibility</header>
<row>
<col>
Null (neither alive nor dead)</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
</row>
<row>
<col>
Alive</col>
<col>
0.2</col>
<col>
0.2</col>
<col>
0.5</col>
</row>
<row>
<col>
Dead</col>
<col>
0.5</col>
<col>
0.5</col>
<col>
0.8</col>
</row>
<row>
<col>
Either (alive or dead)</col>
<col>
0.3</col>
<col>
1.0</col>
<col>
1.0</col>
</row>
</table>
</p>
<p>

The null hypothesis is set to zero by definition (it corresponds to "no solution"). The orthogonal hypotheses "Alive" and "Dead" have probabilities of 0.2 and 0.5, respectively. This could correspond to "Live/Dead Cat Detector" signals, which have respective reliabilities of 0.2 and 0.5. Finally, the all-encompassing "Either" hypothesis (which simply acknowledges there is a cat in the box) picks up the slack so that the sum of the masses is 1. The support for the "Alive" and "Dead" hypotheses matches their corresponding masses because they have no subsets; support for "Either" consists of the sum of all three masses (Either, Alive, and Dead) because "Alive" and "Dead" are each subsets of "Either". The "Alive" plausibility is 1-m(Death) and the "Dead" plausibility is 1-m(Alive). Finally, the "Either" plausibility sums m(Alive)+m(Dead)+m(Either). The universal hypothesis ("Either") will always have 100% support and plausibility &mdash;it acts as a checksum of sorts.</p>
<p>

Here is a somewhat more elaborate example where the behaviour of support and plausibility begins to emerge. We're looking at a faraway object, which can only be coloured in one of three colours (red, white, and blue) through a variety of detector modes:</p>
<p>

<table cellpadding="1" border="1">
<header>
Hypothesis</header>
<header>
Mass</header>
<header>
Belief</header>
<header>
Plausibility</header>
<row>
<col>
Null</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
</row>
<row>
<col>
Red</col>
<col>
0.35</col>
<col>
0.35</col>
<col>
0.56</col>
</row>
<row>
<col>
White</col>
<col>
0.25</col>
<col>
0.25</col>
<col>
0.45</col>
</row>
<row>
<col>
Blue</col>
<col>
0.15</col>
<col>
0.15</col>
<col>
0.34</col>
</row>
<row>
<col>
Red or white</col>
<col>
0.06</col>
<col>
0.66</col>
<col>
0.85</col>
</row>
<row>
<col>
Red or blue</col>
<col>
0.05</col>
<col>
0.55</col>
<col>
0.75</col>
</row>
<row>
<col>
White or blue</col>
<col>
0.04</col>
<col>
0.44</col>
<col>
0.65</col>
</row>
<row>
<col>
Any</col>
<col>
0.1</col>
<col>
1.0</col>
<col>
1.0</col>
</row>
</table>
</p>
<p>

Although these are rather bad examples, as events of that kind would not be modeled as disjoint sets in the probability space, rather would the event "red or blue" be considered as the union of the events "red" and "blue", thereby (see the axioms of probability theory) p(red or white)&amp;gt;= p(white) = 0.25 and p(any)=1.
Only the three disjoint events "Blue" "Red" and "White" would need to add up to 1. In fact one could model a probability measure on the space linear proportional to "plausibility" (normalized so that p(red)+p(white)+p(blue) = 1, and with the exception that still all probabilities are =1)</p>

</ss1>
<ss1>
<st>
Combining beliefs</st>
<p>

Beliefs corresponding to independent pieces of information are combined using <it><link>
Dempster's rule of combination</link></it> which is a generalisation of the special case of <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
 where events are independent. Note that the probability masses from propositions that contradict each other can also be used to obtain a measure of how much conflict there is in a system. This measure has been used as a criterion for clustering multiple pieces of seemingly conflicting evidence around competing hypotheses.</p>
<p>

In addition, one of the computational advantages of the Dempster-Shafer framework is that priors and conditionals need not be specified, unlike Bayesian methods which often use a symmetry (minimax error) argument to assign prior probabilities to random variables (e.g. assigning 0.5 to binary values for which no information is available about which is more likely).  However, any information contained in the missing priors and conditionals is not used in the Dempster-Shafer framework unless it can be obtained indirectly - and arguably is then available for calculation using Bayes equations.</p>
<p>

Dempster-Shafer theory allows one to specify a degree of ignorance in this situation instead of being forced to supply prior probabilities which add to unity. This sort of situation, and whether there is a real distinction between <it><link xlink:type="simple" xlink:href="../617/25617.xml">
risk</link></it> and <it><link xlink:type="simple" xlink:href="../326/306326.xml">
ignorance</link></it>, has been extensively discussed by statisticians and economists. See, for example, the contrasting views of <link xlink:type="simple" xlink:href="../480/1912480.xml">
Daniel Ellsberg</link>, <peer wordnetid="109626238" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../813/2529813.xml">
Howard Raiffa</link></mathematician>
</causal_agent>
</academician>
</statistician>
</associate>
</educator>
</professional>
</scientist>
</adult>
</colleague>
</person>
</physical_entity>
</peer>
, <link xlink:type="simple" xlink:href="../033/1473033.xml">
Kenneth Arrow</link> and <link xlink:type="simple" xlink:href="../033/1473033.xml">
Frank Knight</link>.</p>

</ss1>
<ss1>
<st>
Critics</st>
<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../964/699964.xml">
Judea Pearl</link></scientist>
</person>
 (1988a, chapter 9<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>; 1988b<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> and 1990)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>; has argued that it is 
misleading to interpret belief functions as representing either 
"probabilities of an event," or "the confidence one has in the 
probabilities assigned to various outcomes," or "degrees of belief (or
confidence, or trust) in a proposition," or "degree of
ignorance in a situation." Instead, belief
functions represent the probability that a given proposition
is <it>provable</it> from a set of other propositions, to which
probabilities are assigned. Confusing probabilities
of <it>truth</it> with probabilities of <it>provability</it> may lead to
counterintuitive results in reasoning tasks such as
(1) representing incomplete knowledge, (2) belief-updating
and (3) evidence pooling. He further demonstrated that, if
partial knowledge is encoded and updated by belief
function methods, the resulting beliefs
cannot serve as a basis for rational decisions.</p>
<p>

Kłopotek and Wierzchoń:
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>
proposed to interpret the Dempster-Shafer theory in terms of statistics of decision tables (of the <link xlink:type="simple" xlink:href="../778/1634778.xml">
rough set theory</link>), whereby the operator of combining evidence should be seen as relational join of decision tables.
In another interpretation 
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>
they propose to view this theory as describing destructive material processing (under loss of properties), e.g. like in some semiconductor production processes. Under both interpretations reasoning in DST gives correct results, contrary to the earlier probabilistic interpretations, criticized by Pearl in the cited papers and by other researches.</p>

</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../425/1504425.xml">
Possibility theory</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../542/23542.xml">
Probability theory</link></entry>
<entry level="1" type="bullet">

<statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
</entry>
<entry level="1" type="bullet">

<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian network</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../023/4612023.xml">
G.L.S. Shackle</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../139/6226139.xml">
Transferable belief model</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../173/4839173.xml">
Info-gap decision theory</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../580/12413580.xml">
Subjective logic</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
Shafer, Glenn; <it>A Mathematical Theory of Evidence</it>, Princeton University Press, 1976, ISBN 0-608-02508-9</entry>
<entry id="2">
Shafer, Glenn; <weblink xlink:type="simple" xlink:href="http://www.glennshafer.com/assets/downloads/articles/article48.pdf">
<it>Dempster-Shafer theory''</it></weblink>, 2002</entry>
<entry id="3">
Dempster, Arthur P.; <it>A generalization of Bayesian inference</it>, Journal of the Royal Statistical Society, Series B, Vol. 30, pp. 205-247, 1968</entry>
<entry id="4">
Pearl, J. (1988a), <it>Probabilistic Reasoning in Intelligent Systems,</it> (Revised Second Printing) San Mateo, CA: Morgan Kaufmann.</entry>
<entry id="5">
Pearl, J. (1988b) "On Probability Intervals," <it>International Journal of Approximate Reasoning,</it> 2(3):211-216.</entry>
<entry id="6">
Pearl, J. (1990) Reasoning with Belief Functions: An Analysis of Compatibility. <it>The International Journal of Approximate Reasoning,</it> 4(5/6):363-389.</entry>
<entry id="7">
 M.A. Kłopotek, S.T. Wierzchoń: A New Qualitative Rough-Set Approach to Modeling Belief Functions. [in:] L. Polkowski, A, Skowron eds: Rough Sets And Current Trends In Computing. Proc. 1st International Conference RSCTC'98, Warsaw, June 22 - 26 1998, Lecture Notes in Artificial Intelligence 1424, Springer-Verlag, pp. 346-353. 
</entry>
<entry id="8">
 
M.A.Kłopotek, S.T.Wierzchoń: Empirical Models for the Dempster-Shafer Theory. in: Srivastava, R.P., Mock, T.J., (Eds.). Belief Functions in Business Decisions. Series: Studies in Fuzziness and Soft Computing. VOL. 88 Springer-Verlag. March 2002. ISBN 3-7908-1451-2, pp. 62-112 
</entry>
</reflist>

<list>
<entry level="1" type="bullet">

 Joseph C. Giarratano and Gary D. Riley (2005); <it>Expert Systems: principles and programming</it>, ed. Thomson Course Tech., ISBN 0-534-38447-1</entry>
<entry level="1" type="bullet">

 Kari Sentz and Scott Ferson (2002); <weblink xlink:type="simple" xlink:href="http://www.sandia.gov/epistemic/Reports/SAND2002-0835.pdf">
<it>Combination of Evidence in Dempster-Shafer Theory''</it></weblink>, Sandia National Laboratories SAND 2002-0835</entry>
</list>
</p>

</sec>
<sec>
<st>
Further reading</st>
<p>
 
<list>
<entry level="1" type="bullet">

 Yager, R. R., &amp; Liu, L. (2008). <it>Classic works of the Dempster-Shafer theory of belief functions.</it> Studies in fuzziness and soft computing, v. 219. Berlin: <company wordnetid="108058098" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../949/1418949.xml">
Springer</link></institution>
</company>
. ISBN 9783540253815.</entry>
</list>
</p>


</sec>
</bdy>
</article>
