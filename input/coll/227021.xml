<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:33:22[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Computer Go</title>
<id>227021</id>
<revision>
<id>244098191</id>
<timestamp>2008-10-09T10:14:13Z</timestamp>
<contributor>
<username>Maproom</username>
<id>2524922</id>
</contributor>
</revision>
<categories>
<category>All articles with dead external links</category>
<category>Articles with invalid date parameter in template</category>
<category>Articles needing additional references from October 2007</category>
<category>Articles with dead external links since May 2008</category>
<category>Computer Go</category>
</categories>
</header>
<bdy>

| class="infobox" style="text-align:center;"
|-
| Part of a series of articles on<b><game wordnetid="100456199" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../971/64971.xml">
Go (board game)</link></game>
</b>
|-
|<image width="150px" src="Go_board_part.jpg">
<caption>

Go_board_part.jpg
</caption>
</image>

|-
<table style="border: 0px; background:transparent" align="left">
<row>
<col style="font-size:12px" align="left">
<b>Game specifics</b>
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../360/102360.xml">
Go rules</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../370/102370.xml">
Go handicaps</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../036/157036.xml">
Go proverb</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../777/756777.xml">
Go terms</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../977/48977.xml">
Go strategy and tactics</link></entry>
</list>
<p>

<b>History &amp; Culture</b>
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../583/638583.xml">
History of Go</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../846/2997846.xml">
Go equipment</link></entry>
<entry level="1" type="bullet">

 <diversion wordnetid="100426928" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<show wordnetid="100520257" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<entertainment wordnetid="100429048" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../554/234554.xml">
Hikaru no Go</link></activity>
</entertainment>
</psychological_feature>
</act>
</show>
</event>
</diversion>
</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<game wordnetid="100456199" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../316/6795316.xml">
Go variants</link></psychological_feature>
</game>
</contest>
</social_event>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../454/1248454.xml">
Four go houses</link></entry>
</list>
</p>
<p>

<b>Players &amp; Organisations</b>
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../976/186976.xml">
Go players</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../399/965399.xml">
Go ranks and ratings</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../243/102243.xml">
Go professional</link></psychological_feature>
</contest>
</social_event>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../467/4913467.xml">
Go organizations</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../100/3728100.xml">
Go competitions</link></entry>
</list>
</p>
<p>

<b>Computers &amp; Mathematics</b>
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/11422488.xml">
Go and mathematics</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../021/227021.xml">
Computer Go</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../714/14620714.xml">
Go software</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../761/14662761.xml">
Internet Go server</link></entry>
</list>
</p>
</col>
</row>
</table>

<b>Computer go</b> is the field of <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> (AI) dedicated to creating a <link xlink:type="simple" xlink:href="../783/5783.xml">
computer program</link> that plays <game wordnetid="100456199" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../971/64971.xml">
go</link></game>
, an ancient <link xlink:type="simple" xlink:href="../401/3401.xml">
board game</link>.
<sec>
<st>
Performance</st>
<p>

Go has long been considered a difficult challenge in the field of <link xlink:type="simple" xlink:href="../268/1268.xml">
AI</link> and has not yielded as easily as Chess. The first Go program was written by <link>
Albert Zobrist</link> in 1968 as part of his thesis on <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link>. It introduced an <link xlink:type="simple" xlink:href="../001/311001.xml">
influence function</link> to estimate territory and <link xlink:type="simple" xlink:href="../880/1963880.xml">
Zobrist hashing</link> to detect <link>
ko</link>.
Recent developments in <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo Tree Search</link></method>
</know-how>
</technique>
 and <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> have brought the best programs to good <link xlink:type="simple" xlink:href="../399/965399.xml">
dan</link> level on the small 9x9 board; however, while the techniques which have brought such progress in the 9x9 case have been applied on the 19x19 board with some success, dan level play has not yet been reached at least with publicly available software on ordinary personal computers.</p>
<p>

Currently, the best Go programs running on stock hardware are ranked as (1-3 <link xlink:type="simple" xlink:href="../370/102370.xml">
kyu</link>)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. Only a decade ago, very strong players were able to beat computer programs at handicaps of 25-30 stones, an enormous  handicap that few human players would ever take. There is a case where the winning program in the 1994 World Computer Go Championship, Go Intellect, lost all 3 games against the youth players on a 15 stone handicap.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> In general, players who understood and exploit a program's weaknesses could win with much larger handicaps than typical players.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

On August 7, 2008, the computer program MoGo running on 25 nodes (800 cores) of the Huygens cluster in Amsterdam beat professional Go player Myungwan Kim (8p) in a handicap game on the 19x19 board. The handicap given to the computer was nine stones. The game was broadcast live on the <link xlink:type="simple" xlink:href="../592/12509592.xml">
KGS Go Server</link>. In after-game commentary, Kim estimated the playing strength of this machine as being in the range of 2-3 amateur dan.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>. Later, on August 26, Mogo beat an Amateur 6d with five stones of handicap, this time running on 200 cores of the Huygens cluster.</p>
<p>

On September 4, 2008, the program CrazyStone running on an 8-core personal computer won against 30 year old female professional player, Aoba Kaori (4p), receiving a handicap of eight stones. The time control was 30 seconds per move. White resigned after 185 moves. The game was played during the FIT2008 conference in Japan.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>
<p>

These results can be viewed as evidence pointing towards the possibility of amateur dan-level play if contemporary software is combined with strong hardware, but more games will need to be played at this level until solid conclusions of any kind can be drawn.</p>

</sec>
<sec>
<st>
 Obstacles to high level performance </st>

<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-content" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Question_book-new.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>This section needs additional  for .</b>
Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Computer_Go&amp;action=edit">
improve this article</weblink> by adding . Unsourced material may be and removed. <it>(October 2007)''</it></col>
</row>
</table>

 </p>
<p>

For a long time it was a widely held opinion that computer Go posed a problem fundamentally different to computer chess insofar as it was believed that methods relying on fast global search compared to human experts combined to relatively little domain knowledge would not be effective for Go. Therefore, a large part of the computer Go development effort was during these times focused on ways of representing human-like expert knowledge and combining this with local search to answer questions of a tactical nature. The result of this were programs that handled many situations well but which had very pronounced weaknesses compared to their overall handling of the game. Also, these classical programs gained almost nothing from increases in available computing power per se and progress in the field was generally slow. Therefore, creating a strong Go-playing program was by many seen as something that could, if at all, be achieved only in the far future and possibly only with fundamental advances in general artificial intelligence technology. Even writing a program capable of automatically determining the winner of a finished game was seen as no trivial matter.</p>
<p>

The advent of programs based on Monte-Carlo search starting in 2006 changed this situation in many ways, although the gap between really strong human players and the strongest go programs remains considerable.</p>

<ss1>
<st>
Board is too large </st>
<p>

The large board (19x19, 361 intersections) is often noted as one of the primary reasons why a strong program is hard to create. The large board size is a problem to the extent that it prevents an <link xlink:type="simple" xlink:href="../501/159501.xml">
alpha-beta searcher</link> without significant search extensions or pruning heuristics from achieving deep look-ahead.</p>
<p>

So far, the largest game of Go being completely solved has been played on a 5×5 board. It was achieved in 2002, with black winning by 25 points (the entire board), by a computer program called MIGOS (MIni GO Solver).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></p>

</ss1>
<ss1>
<st>
Most moves are possible</st>
<p>

Continuing the comparison to chess, Go moves are not as limited by the rules of the game.  For the first move in chess, the player has twenty choices. Go players begin with a choice of 55 distinct legal moves, accounting for symmetry. This number rises quickly as symmetry is broken and soon almost all of the 361 points of the board must be evaluated. Some are much more popular than others, some are almost never played, but all are possible.</p>

</ss1>
<ss1>
<st>
 Additive nature of the game</st>
<p>

As a chess game progresses (as well as most other games such as checkers, draughts, and backgammon), pieces disappear from the board, simplifying the game. Each new Go move, on the contrary, adds new complexities and possibilities to the situation, at least until an area becomes developed to the point of being 'settled'.</p>

</ss1>
<ss1>
<st>
Techniques in chess that cannot be applied to Go</st>
<p>

The fact that computer Go programs are significantly weaker than <link xlink:type="simple" xlink:href="../367/68367.xml">
computer chess</link> programs has served to generate research into many new programming techniques. The techniques which proved to be the most effective in computer chess have generally shown to be mediocre at Go.</p>
<p>

While a simple material counting evaluation is not sufficient for decent play in chess, it is often the backbone of a chess evaluation function, when combined with more subtle considerations like isolated pawns, rooks on open verticals, pawns in the center of the board and so on. All positional factors combined are typically scaled so as not exceed the value of a single pawn. These rules can be formalised easily, providing a reasonably good evaluation function that can run quickly.</p>
<p>

These types of positional evaluation rules cannot efficiently be applied to Go. The value of a Go position depends on a complex analysis to determine whether or not the group is alive, which stones can be connected to one another, and heuristics around which a strong position has influence, or the extent to which a weak position can be attacked.</p>

</ss1>
<ss1>
<st>
Evaluation function </st>
<p>

Another problem comes from the difficulty of creating a good <link xlink:type="simple" xlink:href="../513/159513.xml">
evaluation function</link> for Go. More than one move can be regarded as the best depending on how you use that stone and what your strategy is. In order to choose a move, the computer must evaluate different possible outcomes and decide which is best. This is difficult due to the delicate trade-offs present in Go. For example, it may be possible to capture some enemy stones at the cost of strengthening the opponent's stones elsewhere. Whether this is a good trade or not can be a difficult decision, even for human players. The computational complexity also shows here as a move might not be immediately important, but after many moves could become highly important as other areas of the board take shape.</p>

</ss1>
<ss1>
<st>
Combinatorial problems</st>
<p>

Sometimes it is mentioned in this context that various difficult combinatorial problems (in fact, any <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../466/39466.xml">
NP-complete</link></group>
</collection>
</class>
 problem) can be converted to Go problems; however, the same is true for other abstract board games, including <link xlink:type="simple" xlink:href="../134/5134.xml">
chess</link> and <condition wordnetid="113920835" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<state wordnetid="100024720" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<problem wordnetid="114410605" confidence="0.8">
<difficulty wordnetid="114408086" confidence="0.8">
<game wordnetid="100456199" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../433/19654433.xml">
minesweeper</link></psychological_feature>
</game>
</difficulty>
</problem>
</contest>
</social_event>
</state>
</event>
</condition>
, when suitably generalised to a board of arbitrary size. <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../466/39466.xml">
NP-complete</link></group>
</collection>
</class>
 problems do not tend in their general case to be easier for unaided humans than for suitably programmed computers: it is doubtful that unaided humans would be able to compete successfully against computers in solving, for example, instances of the <link xlink:type="simple" xlink:href="../811/36811.xml">
subset sum problem</link>. Hence, the idea that we can convert some NP-complete problems into Go problems does not help in explaining the present human superiority in Go.</p>

</ss1>
<ss1>
<st>
Endgame</st>
<p>

Given that the endgame contains fewer possible moves than the opening or middle game, one could suppose that it was easier to play, and thus that computers should be easily able to tackle it. In chess, computer programs perform worse in endgames because the ideas are long-term; unless the number of pieces is reduced to an extent that allows taking advantage of solved endgame <link xlink:type="simple" xlink:href="../308/2658308.xml">
tablebase</link>s.</p>
<p>

The application of <link xlink:type="simple" xlink:href="../432/51432.xml">
surreal number</link>s to the endgame in Go, a general game analysis pioneered by <link xlink:type="simple" xlink:href="../807/15807.xml">
John H. Conway</link>, has been further developed by <link xlink:type="simple" xlink:href="../893/655893.xml">
Elwyn R. Berlekamp</link> and <link>
David Wolfe</link> and outlined in their book, <it>Mathematical Go</it> (ISBN 1-56881-032-6). While not of general utility in most playing circumstances, it greatly aids the analysis of certain classes of positions.</p>
<p>

Nonetheless, although elaborate study has been conducted, Go endgames have been proven to be <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../565/1283565.xml">
PSPACE-hard</link></group>
</collection>
</class>
. There are many reasons why they are so hard:
<list>
<entry level="1" type="bullet">

 Even if a computer can play each local endgame area flawlessly, we cannot conclude that its plays would be flawless in regards to the entire board. Additional areas of consideration in endgames include <link xlink:type="simple" xlink:href="../105/1079105.xml">
Sente</link> and <link xlink:type="simple" xlink:href="../275/9962275.xml">
Gote</link> relationships, prioritisation of different local endgames, territory counting &amp; estimation, and so on.</entry>
<entry level="1" type="bullet">

 The endgame may involve many other aspects of Go, including 'life and death' which are also known to be <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../681/54681.xml">
NP-hard</link></group>
</collection>
</class>
. </entry>
<entry level="1" type="bullet">

 Each of the local endgame areas may affect one another. In other words, they are dynamic in nature although visually isolated. This makes it much more difficult for computers to deal with. This nature leads to some very complex situations like <weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?TripleKo">
Triple Ko</weblink>, <weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?QuadrupleKo">
Quadruple Ko</weblink>, <weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?MolassesKo">
Molasses Ko</weblink> and <weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?MoonshineLife">
Moonshine Life</weblink>.</entry>
</list>
</p>
<p>

Thus, it is very unlikely that it will be possible to program a reasonably fast algorithm for playing the Go endgame flawlessly, let alone the whole Go game.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></p>

</ss1>
<ss1>
<st>
Why Humans are better at Go</st>


<p>

People feel that computers are bad at Go because we compare them with humans. Perhaps computers aren't particularly bad at Go; rather, humans are particularly good at it. Go, compared with other two-player games of complete information, has features that make it particularly easy for humans. The pieces never move about (as they do in <link xlink:type="simple" xlink:href="../134/5134.xml">
Chess</link>), nor change state (as they do in <link xlink:type="simple" xlink:href="../978/25978.xml">
Othello</link>). These features make it easy for humans to "read" long sequences of moves, while being irrelevant to a computer program.</p>
<p>

In those rare Go positions known as "<weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?IshiNoShita">
ishi-no-shita</weblink>", in which stones are repeatedly captured and re-played on the same points, humans have reading problems, while they are easy for computers.</p>

</ss1>
</sec>
<sec>
<st>
 Tactical search </st>
<p>

One of the main concerns for a Go player is which groups of stones can be kept alive and which can be captured. This general class of problems is known as <link xlink:type="simple" xlink:href="../751/756751.xml">
life and death</link>.  The most direct strategy for calculating life and death is to perform a <link xlink:type="simple" xlink:href="../584/597584.xml">
tree search</link> on the moves which potentially affect the stones in question, and then to record the status of the stones at the end of the main line of play.</p>
<p>

However, within time and memory constraints, it is not generally possible to determine with complete accuracy which moves could affect the 'life' of a group of stones.  This implies that some <link xlink:type="simple" xlink:href="../452/63452.xml">
heuristic</link> must be applied to select which moves to consider.  The net effect is that for any given program, there is a trade-off between playing speed and life and death reading abilities.</p>

</sec>
<sec>
<st>
 State representation </st>
<p>

A problem that all Go programs must solve is how to represent the current state of the game.  For programs that use extensive searching techniques, this representation needs to be copied and modified for each new hypothetical move considered.  This need places the additional constraint that the representation should either be small enough to be copied quickly or flexible enough that a move can be made and undone easily.</p>
<p>

The most direct way of representing a board is as a 1 or 2-dimensional array, where each space in the array represents a point on the board, and can take on a value corresponding to a white stone, a black stone, or an empty space.  Additional data is needed to store how many stones have been captured, whose turn it is, and which spaces are illegal due to <link>
Ko rule</link>.</p>
<p>

Most programs, however, use more than just the raw board information to evaluate positions.  Data such as which stones are connected in strings, which strings are associated with each other, which groups of stones are in risk of capture and which groups of stones are effectively dead is necessary to make an accurate evaluation of the position.  While this information can be extracted from just the stone positions, much of it can be computed more quickly if it is updated in an incremental, per-move basis.  This incremental updating requires more information to be stored as the state of the board, which in turn can make copying the board take longer. This kind of trade-off is very indicative of the problems involved in making fast computer Go programs. </p>
<p>

An alternative method is to have a single board and make and takeback moves so as to minimise the demands on computer memory and have the results of the evaluation of the board stored. This avoids having to copy the information over and over again.</p>

</sec>
<sec>
<st>
System design</st>

<ss1>
<st>
New approaches to problems</st>
<p>

Historically, <link xlink:type="simple" xlink:href="../417/339417.xml">
GOFAI</link> (Good Old Fashioned AI) techniques have been used to approach the problem of Go AI. More recently, <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
neural networks</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 are being looked at as an alternative approach.  One example of a program which uses neural networks is WinHonte<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>.</p>
<p>

These approaches attempt to mitigate the problems of the game of Go having a high <link xlink:type="simple" xlink:href="../471/438471.xml">
branching factor</link> and numerous other difficulties.</p>
<p>

Computer Go research results are being applied to other similar fields such as <link xlink:type="simple" xlink:href="../626/5626.xml">
cognitive science</link>, <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link> and <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref> <link xlink:type="simple" xlink:href="../231/292231.xml">
Combinatorial Game Theory</link>, a branch of <link xlink:type="simple" xlink:href="../988/18950988.xml">
applied mathematics</link>, is a topic relevant to computer Go.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref></p>

</ss1>
<ss1>
<st>
Design philosophies</st>
<p>

The only choice a program needs to make is where to place its next stone.  However, this decision is made difficult by the wide range of impacts a single stone can have across the entire board, and the complex interactions various stones groups can have with each other.  Various architectures have arisen for handing this problem.  The most popular are using some form of <link xlink:type="simple" xlink:href="../584/597584.xml">
tree search</link>, the application of <link xlink:type="simple" xlink:href="../098/56098.xml">
Monte-Carlo methods</link>, the creation of <link xlink:type="simple" xlink:href="../612/1377612.xml">
knowledge-based systems</link>, and the use of <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>.  Few programs use only one of these techniques exclusively; most combine portions of each into one synthetic system.</p>

<ss2>
<st>
 Minimax tree search </st>
<p>

One <link xlink:type="simple" xlink:href="../417/339417.xml">
traditional AI</link> technique for creating game playing software is to use a <link xlink:type="simple" xlink:href="../589/19589.xml">
minimax</link> <link xlink:type="simple" xlink:href="../584/597584.xml">
tree search</link>.  This involves playing out all hypothetical moves on the board up to a certain point, then using an <link xlink:type="simple" xlink:href="../513/159513.xml">
evaluation function</link> to estimate the value of that position for the current player.  The move which leads to the best hypothetical board is selected, and the process is repeated each turn.  While tree searches have been very effective in <link xlink:type="simple" xlink:href="../367/68367.xml">
computer chess</link>, they have seen less success in Computer Go programs.  This is partly because it has traditionally been difficult to create an effective evaluation function for a Go board, and partly because the large number of possible moves each side can make each leads to a high <link xlink:type="simple" xlink:href="../471/438471.xml">
branching factor</link>.  This makes this technique very computationally expensive.  Because of this, many programs which use search trees extensively can only play on the smaller 9×9 board, rather than full 19×19 ones.</p>
<p>

There are several techniques, which can greatly improve the performance of search trees in terms of both speed and memory.  Pruning techniques such as <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../501/159501.xml">
Alpha-beta pruning</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, <link xlink:type="simple" xlink:href="../305/2292305.xml">
Principal Variation Search</link>, and <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../022/2381022.xml">
MTD-f</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 can reduce the effective branching factor without loss of strength.  Likewise, caching techniques, such as <link xlink:type="simple" xlink:href="../748/242748.xml">
transposition table</link>s can reduce the amount of repeated effort, especially when combined with an <link xlink:type="simple" xlink:href="../326/433326.xml">
iterative deepening</link> approach.  In order to quickly store a full sized Go board in a transposition table, a <link xlink:type="simple" xlink:href="../679/684679.xml">
hashing</link> technique for mathematically summarizing is generally necessary.  <link xlink:type="simple" xlink:href="../880/1963880.xml">
Zobrist hashing</link> is very popular in Go programs because it has low collision rates, and can be iteratively updated at each move with just two <link xlink:type="simple" xlink:href="../979/105979.xml">
XOR</link>s, rather than being calculated from scratch.  Even using these performance-enhancing techniques, full tree searches on a full sized board are still prohibitively slow.  Searches can be sped up by using large amounts of domain specific pruning techniques, such as not considering moves where your opponent is already strong, and selective extensions like always considering moves next to groups of stones which are <link xlink:type="simple" xlink:href="../777/756777.xml#xpointer(//*[./st=%22Atari%22])">
about to be captured</link>.  However, both of these options introduce a significant risk of not considering a vital move which would have changed the course of the game.</p>
<p>

Results of computer competitions show that pattern matching techniques for choosing a handful of appropriate moves combined with fast localized tactical searches (explained above) are sufficient to produce a competitive program. For example, <event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<game wordnetid="100456199" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../595/341595.xml">
GNU Go</link></psychological_feature>
</game>
</contest>
</social_event>
</event>
 is competitive, yet does not have a full-board search.</p>

</ss2>
<ss2>
<st>
 Knowledge-based systems </st>
<p>

Novices often learn a lot from the game records of old games played by master players. There is a strong hypothesis that suggests that acquiring Go knowledge is a key to make a strong computer Go. For example, Tim Kinger and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<player wordnetid="110439851" confidence="0.8">
<contestant wordnetid="109613191" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../709/6645709.xml">
David Mechner</link></causal_agent>
</contestant>
</player>
</person>
</physical_entity>
 argue that "it is our belief that with better tools for representing and maintaining Go knowledge, it will be possible to develop stronger Go programs."  They propose two ways: recognizing common configurations of stones and their positions and concentrating on local battles. "... Go programs are still lacking in both quality and quantity of knowledge."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></p>
<p>

After implementation, the use of expert knowledge has been proved very effective in programming Go software. Hundreds of guidelines and rules of thumb for strong play have been formulated by both high level amateurs and professionals. The programmer's task is to take these <link xlink:type="simple" xlink:href="../452/63452.xml">
heuristics</link>, formalize them into computer code, and utilize <link xlink:type="simple" xlink:href="../688/279688.xml">
pattern matching</link> and <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link> algorithms to recognize when these rules apply.  It is also important to have a system for determining what to do in the event that two conflicting guidelines are applicable. </p>
<p>

Most of the relatively successful results come from programmers' individual skills at Go and their personal conjectures about Go, but not from formal mathematical assertions; they are trying to make the computer mimic the way they play Go. "Most competitive programs have required 5–15 person-years of effort, and contain 50–100 modules dealing with different aspects of the game."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref></p>
<p>

This method has until recently been the most successful technique in generating competitive Go programs on a full sized board. Some example of programs which have relied heavily on expert knowledge are Handtalk (later known as Goemate), The Many Faces of Go, Go Intellect, and Go++, each of which has at some point been considered the world's best go program.</p>
<p>

Nevertheless, adding knowledge of Go sometimes weakens the program because some superficial knowledge might bring mistakes: "the best programs usually play good, master level moves. However, as every games player knows, just one bad move can ruin a good game. Program performance over a full game can be much lower than master level."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref></p>

</ss2>
<ss2>
<st>
 Monte-Carlo methods </st>
<p>

One major alternative to using hand-coded knowledge and searches is the use of <link xlink:type="simple" xlink:href="../098/56098.xml">
Monte-Carlo method</link>s.  This is done by generating a list of potential moves, and for each move playing out thousands of games at random on the resulting board.  The move which leads to the best set of random games for the current player is chosen as the best move.  The advantage of this technique is that it requires very little domain knowledge or expert input, the tradeoff being increased memory and processor requirements.  However, because the moves used for evaluation are generated at random it is possible that a move which would be excellent except for one specific opponent response would be mistakenly evaluated as a good move. The result of this are programs which are strong in an overall strategic sense, but are weak tactically. This problem can be mitigated by adding some domain knowledge in the move generation and a greater level of search depth on top of the random evolution.  Some programs which use Monte-Carlo techniques are <weblink xlink:type="simple" xlink:href="http://www.lri.fr/~gelly/MoGo.htm">
MoGo</weblink>, <weblink xlink:type="simple" xlink:href="http://remi.coulom.free.fr/CrazyStone/">
CrazyStone</weblink>, Olga and Gobble.</p>
<p>

In 2006, a new search technique, <it>upper confidence bounds applied to trees</it> (<weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?UCT">
UCT</weblink>), was developed and applied to many 9x9 Monte-Carlo Go programs with excellent results. UCT uses the results of the <it>play outs</it> collected so far to guide the search along the more successful lines of play, while still allowing alternative lines to be explored. The UCT technique along with many other optimizations for playing on the larger 19x19 board has led MoGo to become one of the strongest research programs. Successful early applications of UCT methods to 19x19 go include MoGo, CrazyStone, and <weblink xlink:type="simple" xlink:href="http://www.cs.unimaas.nl/go4go/mango/">
Mango</weblink>. MoGo won the 2007 <event wordnetid="100029378" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../187/64187.xml">
Computer Olympiad</link></psychological_feature>
</event>
 and won one (out of three) blitz game against Guo Juan, 5th Dan Pro, in 9x9 Go.  <weblink xlink:type="simple" xlink:href="http://www.smart-games.com">
The Many Faces of Go</weblink> won the 2008 <event wordnetid="100029378" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../187/64187.xml">
Computer Olympiad</link></psychological_feature>
</event>
 after adding UCT search to its traditional knowledge-based engine.</p>
<p>

In 2008, thanks to an efficient message-passing parallelization, <weblink xlink:type="simple" xlink:href="http://www.lri.fr/~teytaud/mogo.html">
MoGo</weblink> won <weblink xlink:type="simple" xlink:href="http://www.lri.fr/~teytaud/crmogo.en.html">
one game</weblink> (out of three) against Catalin Taranu, 5th Dan Pro, in 9x9 with standard time settings (30 minutes per side). MoGo was running on a cluster provided by <weblink xlink:type="simple" xlink:href="http://www.bull.net">
Bull</weblink> (32 nodes with 8 cores per node, 3 GHz); the machine was down during one of the lost games. The results of this event were approved by the <weblink xlink:type="simple" xlink:href="http://ffg.jeudego.org/">
French Federation of Go</weblink>. MoGo also played a 19x19 Game against Catalin Taranu and lost in spite of 9 stones handicap. However, MoGo was in good position during most of the game, and lost due to a bad choice in a ko situation at the end. The machine used for this event (the IAGO challenge, organized by the company "Recitsproque") is a good one, but far from the top level in industry - as the speed-up curve is linear until at least 32 nodes, we can guess that much better results are possible with bigger machines.</p>
<p>

On August 7th, 2008, MoGo won a 19x19 game vs. Kim MyungWan 8p with MoGo receiving a 9 stone handicap. MoGo won by 1.5 points. Mr. Kim used around 13 minutes of time while MoGo took around 55; however, he felt that using more time would not have helped him win. MoGo was run from the Netherlands on an 800 node supercomputer, which contained 4 cores per node with each core running at 4.7 GHz to produce 15 Teraflops.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref>  MyungWan and MoGo played a total of 4 games of varying handicaps and time limits, each side winning two games.  The game records are accessible on <link xlink:type="simple" xlink:href="../592/12509592.xml">
 KGS</link> where MoGo played as MogoTitan.</p>

</ss2>
<ss2>
<st>
 Machine learning </st>
<p>

While knowledge-based systems have been very effective at Go, their skill level is closely linked to the knowledge of their programmers and associated domain experts.  One way to break this limitation is to use <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> techniques in order to allow the software to automatically generate rules, patterns, and/or rule conflict resolution strategies.</p>
<p>

This is generally done by allowing a <link xlink:type="simple" xlink:href="../523/21523.xml">
neural network</link> or <link xlink:type="simple" xlink:href="../254/40254.xml">
genetic algorithm</link> to either review a large database of professional games, or play many games against itself or other people or programs.  These algorithms are then able to utilize this data as a means of improving their performance. Notable programs using neural nets are NeuroGo and WinHonte.</p>
<p>

Machine learning techniques can also be used in a less ambitious context to tune specific parameters of programs which rely mainly on other techniques. For example, CrazyStone learns move generation patterns from several hundred sample games, using a generalization of the <record wordnetid="106647206" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<link xlink:type="simple" xlink:href="../421/70421.xml">
Elo rating system</link></evidence>
</indication>
</record>
. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref></p>

</ss2>
</ss1>
</sec>
<sec>
<st>
Competitions among computer Go programs</st>
<p>

Several annual competitions take place between Go computer programs, the most prominent being the Go events at the <event wordnetid="100029378" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../187/64187.xml">
Computer Olympiad</link></psychological_feature>
</event>
. Regular, less formal, competitions between programs occur on the <weblink xlink:type="simple" xlink:href="http://www.weddslist.com/kgs/index.html">
Kiseido Go Server</weblink> (monthly) and the <weblink xlink:type="simple" xlink:href="http://cgos.boardspace.net/">
Computer Go Server</weblink> (continuous).</p>
<p>

Prominent go-playing programs include North Korean Silver Star/<link>
KCC Igo</link>, ZhiXing Chen's <link>
Handtalk</link>, Michael Reiss's <link>
Go++</link> and David Fotland's <link>
Many Faces of Go</link>. <event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<game wordnetid="100456199" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../595/341595.xml">
GNU Go</link></psychological_feature>
</game>
</contest>
</social_event>
</event>
 is a free computer go implementation which has also won computer competitions.</p>

<ss1>
<st>
History</st>

<p>

The first computer Go competitions were sponsored by <link xlink:type="simple" xlink:href="../584/253584.xml">
USENIX</link>. They ran from 1984-1988. These competitions introduced Nemesis, the first competitive go program from <link>
Bruce Wilcox</link>, and G2.5 by <link>
David Fotland</link>, which would later evolve into Cosmos and The Many Faces of Go.</p>
<p>

One of the early drivers of computer go research was the Ing Prize, a relatively large money award sponsored by Taiwanese banker <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<player wordnetid="110439851" confidence="0.8">
<contestant wordnetid="109613191" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../937/8374937.xml">
Ing Chang-ki</link></causal_agent>
</contestant>
</player>
</person>
</physical_entity>
, offered annually between 1985 and 2000 at the World Computer Go Congress (or Ing Cup). The winner of this tournament was allowed to challenge young professionals at a handicap in a short match. If the computer won the match, the prize was awarded and a new prize announced: a larger prize for beating the professionals at a lesser handicap. The series of Ing prizes was set to expire either 1) in the year 2000 or 2) when a program could beat a 1-dan professional at no handicap for 40,000,000 <currency wordnetid="113385913" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../749/309749.xml">
NT dollar</link></currency>
s. The last winner was Handtalk in 1997, claiming 250,000 NT dollars for winning an 11-stone handicap match against three 8-9 year old professionals. At the time the prize expired in 2000, the unclaimed prize was 400,000 NT dollars for winning a 9-stone handicap match.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref></p>
<p>

Many other large regional Go tournaments ("congresses") had an attached computer Go event.  The European Go Congress has sponsored a computer tournament since 1987, and the USENIX event evolved into the US/North American Computer Go Championship, held annually from 1988-2000 at the US Go Congress.</p>
<p>

Surprisingly, Japan has only recently started sponsoring its own computer Go competitions. The FOST Cup was held annually from 1995-1999 in Tokyo. That tournament was supplanted by the Gifu Challenge, which was held annually from 2003-2006 in Ogaki, Gifu.</p>

</ss1>
<ss1>
<st>
Problems in computer-computer games</st>
<p>

When two computers play a game of Go against each other, the ideal is to treat the game in a manner identical to two humans playing.  However, this can be difficult especially during the end game.  The main problem is that Go playing software has no capacity to communicate in a dialog with its opponents.  So if there is a disagreement about the status of a group of stones, there is no general way for two different programs to “talk it out” and resolve the conflict.  One method for resolving this problem is to have an expert human or well-crafted piece of software judge the final board.  However, this introduces subjectivity into the results and the risk that the expert would miss something the program saw.  An alternative method is to send a special command to the two programs that indicates they should continue placing stones until there is no question about the status of any particular group.  The main problem with this system is that some <link xlink:type="simple" xlink:href="../360/102360.xml#xpointer(//*[./st=%22Rule+sets%22])">
rule sets</link> (such as the traditional Japanese rules) penalize the players for making these extra moves.  Additionally this introduces the risk that a program which was in a winning position at the traditional end of the game (when both players have passed), could be penalized for poor play that is made <it>after</it> the game was technically over.</p>

</ss1>
</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <game wordnetid="100456199" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../971/64971.xml">
Go (board game)</link></game>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../088/4273088.xml">
Go Text Protocol</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Notes and references </st>
<p>

<reflist>
<entry id="1">
AyaMC, Crazy Stone, Leela and MoGo all have accounts in this range on the <link xlink:type="simple" xlink:href="../592/12509592.xml">
KGS Go Server</link></entry>
<entry id="2">
<weblink xlink:type="simple" xlink:href="http://www.itee.uq.edu.au/~janetw/Computer%20Go/CS-TR-339.html#6.2">
Program versus Human Performance</weblink></entry>
<entry id="3">
See for instance http://www.intgofed.org/history/computer_go_dec2005.pdf&#91;&#93;</entry>
<entry id="4">
Computer Beats Pro at U.S. Go Congress http://www.usgo.org/index.php?%23_id=4602</entry>
<entry id="5">
<weblink xlink:type="simple" xlink:href="http://remi.coulom.free.fr/FIT2008/">
Crazy Stone defeated 4-dan professional player with a handicap of 8 stones.</weblink></entry>
<entry id="6">
<weblink xlink:type="simple" xlink:href="http://www.cs.unimaas.nl/~vanderwerf/5x5/5x5solved.html">
5×5 Go is solved by MIni GO Solver</weblink></entry>
<entry id="7">
See <weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?ComputerGoProgramming">
Computer Go Programming</weblink> pages at <social_group wordnetid="107950920" confidence="0.8">
<gathering wordnetid="107975026" confidence="0.8">
<community wordnetid="108223802" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../616/10323616.xml">
Sensei's Library</link></group>
</community>
</gathering>
</social_group>
</entry>
<entry id="8">
<weblink xlink:type="simple" xlink:href="http://www.jellyfish-go.com/ai.htm">
WinHonte 2.01</weblink></entry>
<entry id="9">
Müller, Martin. <weblink xlink:type="simple" xlink:href="http://web.cs.ualberta.ca/~mmueller/ps/Go2000.pdf.gz">
<it>Computer Go''</it></weblink>, Artificial Intelligence 134 (2002): p150</entry>
<entry id="10">
Müller, Martin. <weblink xlink:type="simple" xlink:href="http://web.cs.ualberta.ca/~mmueller/ps/Go2000.pdf.gz">
<it>Computer Go''</it></weblink>, Artificial Intelligence 134 (2002): p151</entry>
<entry id="11">
Müller, Martin. <weblink xlink:type="simple" xlink:href="http://web.cs.ualberta.ca/~mmueller/ps/Go2000.pdf.gz">
<it>Computer Go''</it></weblink>, Artificial Intelligence 134 (2002): p148</entry>
<entry id="12">
<weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?MoGo">
Sensei's Library: MoGo</weblink></entry>
<entry id="13">
<weblink xlink:type="simple" xlink:href="http://remi.coulom.free.fr/Amsterdam2007/">
Computing Elo Ratings of Move Patterns in the Game of Go</weblink></entry>
<entry id="14">
<weblink xlink:type="simple" xlink:href="http://www.smart-games.com/worldcompgo.html">
World Computer Go Championships</weblink></entry>
</reflist>
</p>

<ss1>
<st>
Academic articles</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/bouzy01computer.html">
AI-oriented survey of Go</weblink> </entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://nn.cs.utexas.edu/downloads/papers/lubberts.coevolution-gecco01.pdf">
Co-Evolving a Go-Playing Neural Network</weblink>, written by Alex Lubberts &amp; Risto Miikkulainen, 2001</entry>
<entry level="1" type="bullet">

 <it>Computer Game Playing: Theory and Practice</it>, edited by M.A. Brauner (The Ellis Horwood Series in Artificial Intelligence), Halstead Press, 1983. A collection of computer-go articles. The American Go Journal, vol. 18, No 4. page 6. [ISSN 0148-0243]</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.princeton.edu/~jbagdis/jp.pdf">
A Machine-Learning Approach to Computer Go</weblink>, Jeffrey Bagdis, 2007.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://affect.media.mit.edu/pdfs/04.wren-reynolds.pdf">
Minimalism in Ubiquitous Interface Design</weblink> Wren, C. and Reynolds, C. (2004) Personal and Ubiquitous Computing, 8(5), pages 370 - 374. <weblink xlink:type="simple" xlink:href="http://www.youtube.com/watch?v=ntYq8PdKLCA&amp;feature=RecentlyWatched&amp;page=1&amp;t=t&amp;f=b">
Video of computer go vision system in operation</weblink> shows interaction and users exploring <link xlink:type="simple" xlink:href="../678/761678.xml">
Joseki</link> and <link xlink:type="simple" xlink:href="../532/2268532.xml">
Fuseki</link>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~emarkus/monte-carlo/monte-carlo.pdf">
Monte-Carlo Go</weblink>, presented by Markus Enzenberger, Computer Go Seminar, University of Alberta, April 2004</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.math-info.univ-paris5.fr/~bouzy/publications/bouzy-helmstetter.pdf">
Monte-Carlo Go</weblink>, written by B. Bouzy and B. Helmstetter from Scientific Literature Digital Library</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~games/go/seminar/2002/020703/ld.pdf">
Static analysis of life and death in the game of Go</weblink>, written by Ken Chen &amp; Zhixing Chen, 20 February 1999</entry>
</list>
</p>

</ss1>
<ss1>
<st>
Related websites</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.reiss.demon.co.uk/webgo/compgo.htm">
Mick's Computer Go Page</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.computer-go.info/events/index.html">
Extensive list of computer Go events</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://mechner.com/david/compgo/sciences/">
 All systems Go</weblink> by David A. Mechner (1998), discusses the game where professional go player <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<player wordnetid="110439851" confidence="0.8">
<contestant wordnetid="109613191" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../353/1668353.xml">
Janice Kim</link></causal_agent>
</contestant>
</player>
</person>
</physical_entity>
 won a game against program <link>
Handtalk</link> after giving a 25-stone handicap.</entry>
<entry level="1" type="bullet">

 Kinger, Tim and Mechner, David. <it><weblink xlink:type="simple" xlink:href="http://mechner.com/david/compgo/acg/">
An Architecture for Computer Go</weblink></it> (1996)</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?ComputerGo">
Computer Go</weblink> and <weblink xlink:type="simple" xlink:href="http://senseis.xmp.net/?ComputerGoProgramming">
Computer Go Programming</weblink> pages at <weblink xlink:type="simple" xlink:href="http://senseis.xmp.net">
Sensei's Library</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~games/go/compgo_biblio/">
Computer Go bibliography</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.xs4all.nl/~janrem/Artikelen/Artikelen.html">
Another Computer Go Bibliography</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.computer-go.org/mailman/listinfo/computer-go/">
Computer Go mailing list</weblink></entry>
<entry level="1" type="bullet">

 Published articles about computer go on <weblink xlink:type="simple" xlink:href="http://www.ideosphere.com/fx-bin/Claim?claim=GoCh">
Ideosphere</weblink> gives current estimate of whether a Go program will be best player in the world</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.lysator.liu.se/~gunnar/gtp/">
Information on the Go Text Protocol</weblink> commonly used for interfacing Go playing engines with graphical clients and internet servers</entry>
<entry level="1" type="bullet">

 The Computer Go Room on the <weblink xlink:type="simple" xlink:href="http://www.gokgs.com">
K Go Server</weblink> (KGS) for online discussion and running "bots"</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~mmueller/cgo/survey/twogames.html">
Two Representative Computer Go Games</weblink>, an article about two computer go games played in 1999, one with two computers players, and the other a 29-stone handicap human-computer game</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://research.microsoft.com/displayArticle.aspx?id=1062">
What A Way to Go</weblink> describes work at Microsoft Research on building a computer go player.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.spectrum.ieee.org/oct07/5552">
Cracking Go, by Feng-hsiung Hsu, IEEE Spectrum magazine, October 2007</weblink> argues why it should be possible to build a Go machine stronger than any human player</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Computer programs </st>


<p>

<indent level="1">

<it>See also: <link xlink:type="simple" xlink:href="../714/14620714.xml">
Go software</link></it>
</indent>
* <weblink xlink:type="simple" xlink:href="http://www32.ocn.ne.jp/~yss/">
AYA</weblink> by Hiroshi Yamashita
<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://remi.coulom.free.fr/CrazyStone/">
CrazyStone</weblink> by Rémi Coulom</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<game wordnetid="100456199" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../595/341595.xml">
GNU Go</link></psychological_feature>
</game>
</contest>
</social_event>
</event>
, the strongest <link xlink:type="simple" xlink:href="../758/18938758.xml">
open source</link> Go program</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.goplusplus.com">
Go++</weblink> by Michael Reiss (sold as <it>Strongest Go</it> or Tuyoi Igo in Japan)</entry>
<entry level="1" type="bullet">

 Go Intellect by Ken Chen</entry>
<entry level="1" type="bullet">

 Handtalk/Goemate, developed in China by Zhixing Chen (sold as Shudan Taikyoku in Japan)</entry>
<entry level="1" type="bullet">

 Haruka by Ryuichi Kawa (sold as Saikouhou in Japan)</entry>
<entry level="1" type="bullet">

 Indigo by Bruno Bouzy</entry>
<entry level="1" type="bullet">

 Katsunari by Shin-ichi Sei</entry>
<entry level="1" type="bullet">

 KCC Igo, from North Korea (sold as Silver Star or Ginsei Igo in Japan)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.sjeng.org/leela">
Leela</weblink>, the first Monte Carlo program for sale to the public</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.smart-games.com/manyfaces.html">
The Many Faces of Go</weblink> by David Fotland (sold as AI Igo in Japan)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.lri.fr/~gelly/MoGo.htm">
MoGo</weblink> by Sylvain Gelly; parallel version <weblink xlink:type="simple" xlink:href="http://www.lri.fr/~teytaud/mogo.html">
http://www.lri.fr/~teytaud/mogo.html</weblink> by many people.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.smartgo.com/">
Smart Go</weblink> by Anders Kierulf, inventor of the <message wordnetid="106598915" confidence="0.8">
<information wordnetid="106634376" confidence="0.8">
<format wordnetid="106636806" confidence="0.8">
<link xlink:type="simple" xlink:href="../367/102367.xml">
Smart Game Format</link></format>
</information>
</message>
</entry>
</list>
</p>


</ss1>
</sec>
</bdy>
</article>
