<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:35:01[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Viterbi algorithm</title>
<id>228015</id>
<revision>
<id>238820401</id>
<timestamp>2008-09-16T15:18:44Z</timestamp>
<contributor>
<username>Patrick</username>
<id>4388</id>
</contributor>
</revision>
<categories>
<category>Error detection and correction</category>
<category>Machine learning</category>
<category>Dynamic programming</category>
</categories>
</header>
<bdy>

The <b>Viterbi algorithm</b> is a <link xlink:type="simple" xlink:href="../297/125297.xml">
dynamic programming</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> for finding the most <link xlink:type="simple" xlink:href="../968/44968.xml">
likely</link> sequence of hidden states &ndash; called the <b>Viterbi path</b> &ndash; that results in a sequence of observed events, especially in the context of <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../029/17878029.xml">
Markov information source</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s, and more generally, <link xlink:type="simple" xlink:href="../770/98770.xml">
hidden Markov model</link>s.  The <b>forward algorithm</b> is a closely related algorithm for computing the probability of a sequence of observed events. These algorithms belong to the realm of <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link>.<p>

The algorithm makes a number of assumptions.  First, both the observed events and hidden events must be in a sequence.  This sequence often corresponds to time.  Second, these two sequences need to be aligned, and an instance of an observed event needs to correspond to exactly one instance of a hidden event.  Third, computing the most likely hidden sequence up to a certain point <it>t</it> must depend only on the observed event at point <it>t</it>, and the most likely sequence at point <it>t</it> &amp;minus; 1.  These assumptions are all satisfied in a first-order hidden Markov model.</p>
<p>

The terms "Viterbi path" and "Viterbi algorithm" are also applied to related dynamic programming algorithms that discover the single most likely explanation for an observation.  For example, in <link xlink:type="simple" xlink:href="../544/7024544.xml">
statistical parsing</link> a dynamic programming algorithm can be used to discover the single most likely context-free derivation (parse) of a string, which is sometimes called the "Viterbi parse".</p>
<p>

The Viterbi algorithm was conceived by <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../326/326326.xml">
Andrew Viterbi</link></person>
 in 1967 as an <link xlink:type="simple" xlink:href="../375/10375.xml">
error-correction</link> scheme for noisy digital communication links, finding universal application in decoding the <link xlink:type="simple" xlink:href="../962/40962.xml">
convolutional code</link>s used in both <link xlink:type="simple" xlink:href="../143/7143.xml">
CDMA</link> and <standard wordnetid="107260623" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../808/12808.xml">
GSM</link></system_of_measurement>
</standard>
 digital cellular, <link xlink:type="simple" xlink:href="../546/59546.xml">
dial-up</link> modems, satellite, deep-space communications, and <link xlink:type="simple" xlink:href="../739/14739.xml">
802.11</link> wireless LANs. It is now also commonly used in <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, <link xlink:type="simple" xlink:href="../963/1232963.xml">
keyword spotting</link>, <link xlink:type="simple" xlink:href="../561/5561.xml">
computational linguistics</link>, and <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>. For example, in <link xlink:type="simple" xlink:href="../468/29468.xml">
speech-to-text</link> (speech recognition), the acoustic signal is treated as the observed sequence of events, and a string of text is considered to be the "hidden cause" of the acoustic signal. The Viterbi algorithm finds the most likely string of text given the acoustic signal.</p>

<sec>
<st>
 Overview </st>
<p>

The assumptions listed above can be elaborated as follows. The Viterbi algorithm operates on a state machine assumption. That is, at any time the system being modeled is in some state.  There are a finite number of states, however large, that can be listed. Each state is represented as a node. Multiple sequences of states (paths) can lead to a given state, but one is the most likely path to that state, called the "survivor path". This is a fundamental assumption of the algorithm because the algorithm will examine all possible paths leading to a state and only keep the one most likely. This way the algorithm does not have to keep track of all possible paths, only one per state. </p>
<p>

A second key assumption is that a transition from a previous state to a new state is marked by an incremental metric, usually a number. This transition is computed from the event. The third key assumption is that the events are cumulative over a path in some sense, usually additive. So the crux of the algorithm is to keep a number for each state. When an event occurs, the algorithm examines moving forward to a new set of states by combining the metric of a possible previous state with the incremental metric of the transition due to the event and chooses the best. The incremental metric associated with an event depends on the transition possibility from the old state to the new state. For example in data communications, it may be possible to only transmit half the symbols from an odd numbered state and the other half from an even numbered state. Additionally, in many cases the state transition graph is not fully connected. A simple example is a car that has 3 states — forward, stop and reverse — and a transition from forward to reverse is not allowed. It must first enter the stop state. After computing the combinations of incremental metric and state metric, only the best survives and all other paths are discarded. There are modifications to the basic algorithm which allow for a forward search in addition to the backwards one described here.</p>
<p>

Path history must be stored. In some cases, the search history is complete because the state machine at the encoder starts in a known state and there is sufficient memory to keep all the paths. In other cases, a programmatic solution must be found for limited resources: one example is convolutional encoding, where the decoder must truncate the history at a depth large enough to keep performance to an acceptable level. Although the Viterbi algorithm is very efficient and there are modifications that reduce the computational load, the memory requirements tend to remain constant.</p>

</sec>
<sec>
<st>
A concrete example</st>

<p>

Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.</p>
<p>

Alice believes that the weather operates as a discrete <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
. There are two states, "Rainy" and "Sunny", but she cannot observe them directly, that is, they are <it>hidden</it> from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: "walk", "shop", or "clean". Since Bob tells Alice about his activities, those are the <it>observations</it>. The entire system is that of a hidden Markov model (HMM).</p>
<p>

Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be written them down in the <link xlink:type="simple" xlink:href="../862/23862.xml">
Python programming language</link>:</p>
<p>

states = ('Rainy', 'Sunny')</p>
<p>

observations = ('walk', 'shop', 'clean')</p>
<p>

start_probability = {'Rainy': 0.6, 'Sunny': 0.4}</p>
<p>

transition_probability = {
'Rainy' : {'Rainy': 0.7, 'Sunny': 0.3},
'Sunny' : {'Rainy': 0.4, 'Sunny': 0.6},
}</p>
<p>

emission_probability = {
'Rainy' : {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},
'Sunny' : {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},
}</p>
<p>

In this piece of code, start_probability represents Alice's belief about which state the HMM is in when Bob first calls her (all she knows is that it tends to be rainy on average). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) actually approximately {'Rainy': 0.571, 'Sunny': 0.429}. The transition_probability represents the change of the weather in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow will be sunny if today is rainy. The emission_probability represents how likely Bob is to perform a certain activity on each day. If it is rainy, there is a 50% chance that he is cleaning his apartment; if it is sunny, there is a 60% chance that he is outside for a walk.</p>
<p>

Alice talks to Bob three days in a row and discovers that on the first day he went for a walk, on the second day he went shopping, and on the third day he cleaned his apartment.  Alice has two questions: What is the overall probability of this sequence of observations?  And what is the most likely sequence of rainy/sunny days that would explain these observations?  The first question is answered by the forward algorithm; the second question is answered by the Viterbi algorithm.  These two algorithms are structurally so similar (in fact, they are both instances of the same abstract algorithm) that they can be implemented in a single function:</p>
<p>

def forward_viterbi(obs, states, start_p, trans_p, emit_p):
T = {}
for state in states:
<list>
<entry level="2" type="number">

          prob.           V. path  V. prob.</entry>
</list>

T[state] = (start_p[state], [state], start_p[state])
for output in obs:
U = {}
for next_state in states:
total = 0
argmax = None
valmax = 0
for source_state in states:
(prob, v_path, v_prob) = T[source_state]
p = emit_p[source_state][output] * trans_p[source_state][next_state]
prob *= p
v_prob *= p
total += prob
if v_prob &amp;gt; valmax:
argmax = v_path + [next_state]
valmax = v_prob
U[next_state] = (total, argmax, valmax)
T = U
<list>
<entry level="2" type="number">

 apply sum/max to the final states:</entry>
</list>

total = 0
argmax = None
valmax = 0
for state in states:
(prob, v_path, v_prob) = T[state]
total += prob
if v_prob &amp;gt; valmax:
argmax = v_path
valmax = v_prob
return (total, argmax, valmax)</p>
<p>

The function forward_viterbi takes the following arguments: obs is the sequence of observations, e.g. ['walk', 'shop', 'clean']; states is the set of hidden states; start_p is the start probability; trans_p are the transition probabilities; and emit_p are the emission probabilities.</p>
<p>

The algorithm works on the mappings T and U.  Each is a mapping from a state to a triple (prob, v_path, v_prob), where prob is the total probability of all paths from the start to the current state (constrained by the observations), v_path is the Viterbi path up to the current state, and v_prob is the probability of the Viterbi path up to the current state.  The mapping T holds this information for a given point <it>t</it> in time, and the main loop constructs U, which holds similar information for time <it>t</it>+1.  Because of the <link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link>, information about any point in time prior to <it>t</it> is not needed.</p>
<p>

The algorithm begins by initializing <it>T</it> to the start probabilities: the total probability for a state is just the start probability of that state; and the Viterbi path to a start state is the singleton path consisting only of that state; the probability of the Viterbi path is the same as the start probability.</p>
<p>

The main loop considers the observations from obs in sequence.  Its <link xlink:type="simple" xlink:href="../460/578460.xml">
loop invariant</link> is that T contains the correct information up to but excluding the point in time of the current observation.  The algorithm then computes the triple (prob, v_path, v_prob) for each possible next state.  The total probability of a given next state, total is obtained by adding up the probabilities of all paths reaching that state.  More precisely, the algorithm iterates over all possible source states.  For each source state, T holds the total probability of all paths to that state.  This probability is then multiplied by the emission probability of the current observation and the transition probability from the source state to the next state.  The resulting probability prob is then added to total.  The probability of the Viterbi path is computed in a similar fashion, but instead of adding across all paths one performs a discrete maximization.  Initially the maximum value valmax is zero.  For each source state, the probability of the Viterbi path to that state is known.  This too is multiplied with the emission and transition probabilities and replaces valmax if it is greater than its current value.  The Viterbi path itself is computed as the corresponding <link xlink:type="simple" xlink:href="../004/348004.xml">
argmax</link> of that maximization, by extending the Viterbi path that leads to the current state with the next state.  The triple (prob, v_path, v_prob) computed in this fashion is stored in U and once U has been computed for all possible next states, it replaces T, thus ensuring that the loop invariant holds at the end of the iteration.</p>
<p>

In the end another summation/maximization is performed (this could also be done inside the main loop by adding a pseudo-observation after the last real observation).</p>
<p>

In the running example, the forward/Viterbi algorithm is used as follows:</p>

<p>

def example():
return forward_viterbi(observations,
states,
start_probability,
transition_probability,
emission_probability)
print example()</p>

<p>

This reveals that the total probability of ['walk', 'shop', 'clean'] is 0.033612 and that the Viterbi path is ['Sunny', 'Rainy', 'Rainy', 'Rainy'], with probability 0.009408.  The Viterbi path contains four states because the third observation was generated by the third state and a transition to the fourth state.  In other words, given the observed activities, it was most likely sunny when Bob went for a walk and then it started to rain the next day and kept on raining.</p>
<p>

When implementing this algorithm, it should be noted that many languages use <link xlink:type="simple" xlink:href="../376/11376.xml">
Floating Point</link> arithmetic - as p is small, this may lead to underflow in the results. A common technique to avoid this is to take the logarithm of the probabilities and use it throughout the computation, the same technique used in the <link xlink:type="simple" xlink:href="../455/9636455.xml">
Logarithmic Number System</link>. Once the algorithm has terminated, an accurate value can be obtained by performing the appropriate exponentiation.</p>

</sec>
<sec>
<st>
Extensions</st>
<p>

With the algorithm called <link xlink:type="simple" xlink:href="../914/1232914.xml">
iterative Viterbi decoding</link> one can find the subsequence of an observation that matches best (on average) to a given HMM. Iterative Viterbi decoding works by iteratively invoking a modified Viterbi algorithm, reestimating the score for a filler until convergence.</p>
<p>

An alternate algorithm, the <link>
Lazy Viterbi algorithm</link>, has been proposed recently. This works by not expanding any nodes until it really needs to, and usually manages to get away with doing a lot less work (in software) than the ordinary Viterbi algorithm for the same result - however, it is not so easy to parallelize in hardware.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../778/822778.xml">
Baum-Welch algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../749/9292749.xml">
Forward-backward algorithm</link></entry>
<entry level="1" type="bullet">

 <link>
Error-correcting code</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../500/3772500.xml">
Soft output Viterbi algorithm</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../015/1653015.xml">
Viterbi decoder</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 Andrew J. Viterbi. <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1054010">
Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</weblink>, <it>IEEE Transactions on Information Theory</it> 13(2):260&ndash;269, April 1967.  (The Viterbi decoding algorithm is described in section IV.)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 G. D. Forney. <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1450960">
The Viterbi algorithm</weblink>. <it>Proceedings of the IEEE</it> 61(3):268&ndash;278, March 1973.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 L. R. Rabiner. <weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1109/5.18626">
A tutorial on hidden Markov models and selected applications in speech recognition</weblink> . <it>Proceedings of the IEEE</it> 77(2):257&ndash;286, February 1989.  (Describes the forward algorithm and Viterbi algorithm for HMMs).</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 J Feldman, I Abou-Faycal and M Frigo. A Fast Maximum-Likelihood Decoder for Convolutional Codes.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://search.cpan.org/~koen/Algorithm-Viterbi-0.01/lib/Algorithm/Viterbi.pm">
An implementation of the Viterbi algorithm in Perl</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.biais.org/blog/index.php/2007/09/05/52-viterbi-algorithm-variant-in-python">
An implementation of a variant of the Viterbi algorithm in Python</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://pcarvalho.com/forward_viterbi/">
An implementation of the demonstrated Viterbi algorithm in C#</weblink></entry>
</list>
</p>



</sec>
</bdy>
</article>
