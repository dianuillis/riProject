<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:35:06[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<tree  confidence="0.9511911446218017" wordnetid="113104059">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Decision tree</title>
<id>232602</id>
<revision>
<id>234073954</id>
<timestamp>2008-08-25T04:35:29Z</timestamp>
<contributor>
<username>ImageRemovalBot</username>
<id>4851336</id>
</contributor>
</revision>
<categories>
<category>Decision trees</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../476/43476.xml">
operations research</link>, specifically in <link xlink:type="simple" xlink:href="../842/1190842.xml">
decision analysis</link>, a <b>decision tree</b> (or <link xlink:type="simple" xlink:href="../560/48560.xml">
tree</link> diagram) is a decision support tool that uses a <link xlink:type="simple" xlink:href="../669/598669.xml">
graph</link> or <link xlink:type="simple" xlink:href="../748/6672748.xml">
model</link> of decisions and their possible consequences, including <link xlink:type="simple" xlink:href="../912/47912.xml">
chance</link> event outcomes, resource costs, and <link xlink:type="simple" xlink:href="../479/45479.xml">
utility</link>. A decision tree is used to identify the strategy most likely to reach a <link xlink:type="simple" xlink:href="../094/14850094.xml">
goal</link>. Another use of trees is as a descriptive means for calculating <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probabilities</link>.<p>

In <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link> and <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, a <b>decision tree</b> is a predictive model; that is, a mapping from observations about an item to conclusions about its target value. More descriptive names for such tree models are <b>classification tree</b> (discrete outcome) or <b>regression tree</b> (continuous outcome). In these tree structures, leaves represent classifications and branches represent conjunctions of features that lead to those classifications. The machine learning technique for inducing a decision tree from data is called <link xlink:type="simple" xlink:href="../003/577003.xml">
decision tree learning</link>, or (colloquially) <b>decision trees</b>.</p>

<sec>
<st>
General</st>

<p>

In <link xlink:type="simple" xlink:href="../842/1190842.xml">
decision analysis</link>, a "decision tree" — and a closely related model form, an <link xlink:type="simple" xlink:href="../259/1194259.xml">
influence diagram</link> — is used as a visual and analytical decision support tool, where the <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link>s (or <link xlink:type="simple" xlink:href="../803/736803.xml">
expected utility</link>) of competing alternatives are calculated. </p>
<p>

Decision trees have traditionally been created manually, as the following example is showing:</p>
<p>

<image width="150px" src="Manual_decision_tree.jpg">
<caption>

Manual decision tree.jpg
</caption>
</image>
</p>
<p>

A decision Tree consists of 3 types of nodes:</p>
<p>

1. Decision nodes - commonly represented with squares
2. Chance nodes - represented with circles
3. End nodes - represented with triangles</p>
<p>

<image width="150px" src="Decision-Tree-Elements.png">
<caption>

Decision-Tree-Elements.png
</caption>
</image>
</p>

<p>

Being drawn from the left to the right hand side, a decision tree has only burst nodes (splitting paths) but no sink nodes (converging paths). Therefore, used manually, they can grow very big and are then often hard to draw.</p>
<p>

Analysis can take into account the decision maker's (e.g., the company's) <link xlink:type="simple" xlink:href="../162/170162.xml">
preference</link> or <link xlink:type="simple" xlink:href="../479/45479.xml">
utility function</link>, for example:</p>
<p>

<image width="150px" src="RiskPrefSensitivity2Threshold.png">
<caption>

RiskPrefSensitivity2Threshold.png
</caption>
</image>
</p>
<p>

The basic interpretation in this situation is that the company prefers B's risk and payoffs under realistic risk preference coefficients (greater than $400K -- in that range of risk aversion, the company would need to model a third strategy, "Neither A nor B").</p>

</sec>
<sec>
<st>
Influence diagram</st>

<p>

A decision tree can be represented more compactly as an influence diagram, focusing attention on the issues and relationships between events.</p>
<p>

<image width="150px" src="Factory2_InfluenceDiagram.png">
<caption>

Factory2 InfluenceDiagram.png
</caption>
</image>
</p>

</sec>
<sec>
<st>
Uses in teaching</st>

<p>

Decision trees, <link xlink:type="simple" xlink:href="../259/1194259.xml">
influence diagrams</link>, <link>
utility functions</link>, and other <link xlink:type="simple" xlink:href="../842/1190842.xml">
decision analysis</link> tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of <link xlink:type="simple" xlink:href="../476/43476.xml">
operations research</link> or <link xlink:type="simple" xlink:href="../200/20200.xml">
management science</link> methods.</p>

</sec>
<sec>
<st>
Creation of decision nodes</st>

<p>

Three popular rules are applied in the automatic creation of classification trees. The Gini rule splits off a single group of as large a size as possible, whereas the entropy and twoing rules find multiple groups comprising as close to half the samples as possible. Both algorithms proceed recursively down the tree until stopping criteria are met. </p>
<p>

The Gini rule is typically used by programs that build ('induce') decision trees using the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../563/4141563.xml#xpointer(//*[./st=%22Classification+and+regression+trees%22])">
CART algorithm</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
.  Entropy (or information gain) is used by programs that are based on the <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../814/1966814.xml">
C4.5 algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
.  A brief comparison of these two criterion can be seen under <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../003/577003.xml#xpointer(//*[./st=%22formulae%22])">
Decision tree formulae</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
.</p>
<p>

More information on automatically building ('inducing') decision trees can be found under <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../003/577003.xml">
Decision tree learning</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
.</p>

</sec>
<sec>
<st>
Advantages</st>

<p>

Amongst decision support tools, decision trees (and <link xlink:type="simple" xlink:href="../259/1194259.xml">
influence diagrams</link>) have several advantages:</p>
<p>

Decision trees:
<list>
<entry level="1" type="bullet">

 <b>are simple to understand and interpret.</b> People are able to understand decision tree models after a brief explanation.</entry>
<entry level="1" type="bullet">

 <b>have value even with little hard data.</b> Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.</entry>
<entry level="1" type="bullet">

<b>use a <link xlink:type="simple" xlink:href="../308/2288308.xml">
white box</link> model.</b> If a given result is provided by a model, the explanation for the result is easily replicated by simple math.</entry>
<entry level="1" type="bullet">

<b>can be combined with other decision techniques.</b> The following example uses Net Present Value calculations, PERT 3-point estimations (decision #1) and a linear distribution of expected outcomes (decision #2):</entry>
</list>
</p>
<p>

<image width="150px" src="Investment_decision_Insight.png">
<caption>

Investment decision Insight.png
</caption>
</image>
</p>
<p>

<list>
<entry level="1" type="bullet">

<b>can be used to optimize an investment portfolio.</b> The following example shows a portfolio of 7 investment options (projects). The organization has $10,000,000 available for the total investment. Bold lines mark the best selection 1, 3, 5, 6, and 7 which will cost $9,750,000 and create a payoff of 16,175,000. All other combinations would either exceed the budget or yield a lower payoff:</entry>
</list>
</p>



</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <system wordnetid="108435388" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<network wordnetid="108434259" confidence="0.8">
<link xlink:type="simple" xlink:href="../259/1194259.xml">
Influence diagram</link></network>
</group>
</system>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../476/43476.xml">
Operations research</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../572/433572.xml">
Decision tables</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../820/640820.xml">
Morphological analysis</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../980/11845980.xml">
Recursive partitioning</link></entry>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../003/577003.xml">
Decision tree learning</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../579/17633579.xml">
Topological combinatorics</link></entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../563/4141563.xml#xpointer(//*[./st=%22Classification+and+regression+trees%22])">
CART</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 a common way of automatically building decision trees</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../546/18475546.xml">
MARS</link> extends decision trees to better handle numerical data</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

Y. Yuan and M.J. Shaw, <weblink xlink:type="simple" xlink:href="http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6V05-4007D5X-C&amp;_user=793840&amp;_coverDate=01%2F27%2F1995&amp;_fmt=summary&amp;_orig=search&amp;_cdi=5637&amp;view=c&amp;_acct=C000043460&amp;_version=1&amp;_urlVersion=0&amp;_userid=793840&amp;md5=b66b56153f6780c30e07201eadd454cf&amp;ref=full">
Induction of fuzzy decision trees</weblink>. Fuzzy Sets and Systems 69 (1995), pp. 125–139</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.projectsphinx.com/decision_trees/index.html">
Decision Tree Primer</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mindtools.com/pages/article/newTED_04.htm">
Decision Tree Analysis</weblink> mindtools.com</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.autonlab.org/tutorials/dtree.html">
Decision Trees Tutorial Slides by Andrew Moore</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://gunston.gmu.edu/healthscience/730/default.asp">
Decision Analysis open course at George Mason University</weblink></entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</tree>
</article>
