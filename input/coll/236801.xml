<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:35:06[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<know-how  confidence="0.8" wordnetid="105616786">
<method  confidence="0.8" wordnetid="105660268">
<header>
<title>Markov chain Monte Carlo</title>
<id>236801</id>
<revision>
<id>229715982</id>
<timestamp>2008-08-04T05:24:05Z</timestamp>
<contributor>
<username>Cydebot</username>
<id>1215485</id>
</contributor>
</revision>
<categories>
<category>Monte Carlo methods</category>
<category>Computational statistics</category>
</categories>
</header>
<bdy>

"MCMC" redirects here. For the organization, see <link xlink:type="simple" xlink:href="../125/5374125.xml">
Malaysian Communications and Multimedia Commission</link>.<p>

<b><physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo</link></method>
</know-how>
</technique>
 (MCMC)</b> methods (which include <b><link xlink:type="simple" xlink:href="../451/235451.xml">
random walk</link> Monte Carlo</b> methods), are a class of <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>s for sampling from <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link>s based on constructing a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 that has the desired distribution as its <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml#xpointer(//*[./st=%22Steady-state_analysis_and_limiting_distributions%22])">
equilibrium distribution</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
. The state of the chain after a large number of steps is then used as a sample from the desired distribution. The quality of the sample improves as a function of the number of steps.</p>
<p>

Usually it is not hard to construct a Markov Chain with the desired properties. The more difficult problem is to determine how many steps are needed to converge to the stationary distribution within an acceptable error. A good chain will have <it>rapid mixing</it>—the stationary distribution is reached quickly starting from an arbitrary position—described further under <link xlink:type="simple" xlink:href="../938/6101938.xml">
Markov chain mixing time</link>.</p>
<p>

Typical use of MCMC sampling can only approximate the target distribution, as there is always some residual effect of the starting position. More sophisticated MCMC-based algorithms such as <link xlink:type="simple" xlink:href="../637/17318637.xml">
coupling from the past</link> can produce exact samples, at the cost of additional computation and an unbounded (though finite in expectation) <link xlink:type="simple" xlink:href="../ury/23rd_century.xml">
running time</link>.</p>
<p>

The most common application of these algorithms is numerically calculating multi-dimensional <link xlink:type="simple" xlink:href="../532/15532.xml">
integral</link>s. In these methods, an <link xlink:type="simple" xlink:href="../052/59052.xml">
ensemble</link> of "walkers" moves around randomly. At each point where the walker steps, the integrand value at that point is counted towards the integral. The walker then may make a number of tentative steps around the area, looking for a place with reasonably high contribution to the integral to move into next. Random walk methods are a kind of random simulation or <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo method</link></method>
</know-how>
</technique>
. However, whereas the random samples of the integrand used in a conventional Monte Carlo integration are <link xlink:type="simple" xlink:href="../593/27593.xml">
statistically independent</link>, those used in MCMC are <it><link xlink:type="simple" xlink:href="../057/157057.xml">
correlated</link></it>. A <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 is constructed in such a way as to have the integrand as its <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml#xpointer(//*[./st=%22Steady-state_analysis_and_limiting_distributions%22])">
equilibrium distribution</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
. Surprisingly, this is often easy to do.</p>
<p>

Multi-dimensional integrals often arise in <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian statistics</link>, <link xlink:type="simple" xlink:href="../418/106418.xml">
computational physics</link> and <link xlink:type="simple" xlink:href="../353/149353.xml">
computational biology</link>, so Markov chain Monte Carlo methods are widely used in those fields. For example, see Gill<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> and Robert &amp; Casella<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

<sec>
<st>
 Random walk algorithms </st>

<p>

Many Markov chain Monte Carlo methods move around the equilibrium distribution in relatively small steps, with no tendency for the steps to proceed in the same direction. These methods are easy to implement and analyse, but unfortunately it can take a long time for the walker to explore all of the space. The walker will often double back and cover ground already covered.  Here are some random walk MCMC methods:</p>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../107/56107.xml">
Metropolis-Hastings algorithm</link>: Generates a <link xlink:type="simple" xlink:href="../451/235451.xml">
random walk</link> using a proposal density and a method for rejecting proposed moves.</entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../709/509709.xml">
Gibbs sampling</link></method>
</know-how>
: Requires that all the <link xlink:type="simple" xlink:href="../458/504458.xml">
conditional distribution</link>s of the target distribution can be sampled exactly.  Popular partly because when this is so, the method does not require any `tuning'.</entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../788/7149788.xml">
Slice sampling</link></method>
</know-how>
: Depends on the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function.  This method alternates uniform sampling in the vertical direction with uniform sampling from the horizontal `slice' defined by the current vertical position.</entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../630/17238630.xml">
Multiple-try Metropolis</link></method>
</know-how>
: A variation of the <link>
Metropolis–Hastings algorithm</link> that allows multiple trials at each point. This allows the algorithm to generally take larger steps at each iteration, which helps combat problems intrinsic to large dimensional problems.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Avoiding random walks </st>

<p>

More sophisticated algorithms use some method of preventing the walker from doubling back. These algorithms may be harder to implement, but may exhibit faster convergence (i.e. fewer steps for an accurate result). </p>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../447/4068447.xml">
Successive over-relaxation</link>: A Monte Carlo version of this technique can be seen as a variation on <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../709/509709.xml">
Gibbs sampling</link></method>
</know-how>
; it sometimes avoids random walks.</entry>
<entry level="1" type="bullet">

 <link>
Hybrid Monte Carlo (HMC)</link> (Would be better called `Hamiltonian Monte Carlo'): Tries to avoid random walk behaviour by introducing an auxiliary <link xlink:type="simple" xlink:href="../431/20431.xml">
momentum</link> vector and implementing <link xlink:type="simple" xlink:href="../319/198319.xml">
Hamiltonian dynamics</link> where the potential function is the target density.  The momentum samples are discarded after sampling.  The end result of Hybrid MCMC is that proposals move across the sample space in larger steps and are therefore less correlated and converge to the target distribution more rapidly.</entry>
<entry level="1" type="bullet">

 Some variations on slice sampling also avoid random walks.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></entry>
</list>
</p>

</sec>
<sec>
<st>
 Changing dimension </st>

<p>

The <link xlink:type="simple" xlink:href="../215/6069215.xml">
Reversible Jump</link> method is a variant of Metropolis-Hastings that allows proposals that change the dimensionality of the space. This method was proposed in 1995 by <link xlink:type="simple" xlink:href="../150/929150.xml">
Peter Green</link> of <link xlink:type="simple" xlink:href="../991/60991.xml">
Bristol University</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>.  Markov chain Monte Carlo methods that change dimensionality have also long been used in <link xlink:type="simple" xlink:href="../518/29518.xml">
statistical physics</link> applications, where for some problems a distribution that is a <link xlink:type="simple" xlink:href="../074/1129074.xml">
grand canonical ensemble</link> is used (eg, when the number of molecules in a box is variable).</p>

</sec>
<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian inference</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
 
 <cite style="font-style:normal" class="book">Jeff Gill&#32;(2008). <weblink xlink:type="simple" xlink:href="http://worldcat.org/isbn/1-58488-562-7">
Bayesian methods: a social and behavioral sciences approach</weblink>, Second Edition,&#32;London:&#32;Chapman and Hall/CRC. ISBN 1-58488-562-7.</cite>&nbsp;
</entry>
<entry id="2">
 
 <cite style="font-style:normal" class="book">Christian P Robert &amp; Casella G&#32;(2004). <weblink xlink:type="simple" xlink:href="http://worldcat.org/isbn/0-387-21239-6">
Monte Carlo statistical methods</weblink>, Second Edition,&#32;New York:&#32;Springer. ISBN 0-387-21239-6.</cite>&nbsp;
</entry>
<entry id="3">
Radford M. Neal, "Slice Sampling". <it>The Annals of Statistics</it>, 31(3):705-767, 2003.</entry>
<entry id="4">
P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika, 82(4):711–732, 1995</entry>
</reflist>

<list>
<entry level="1" type="bullet">

 Christophe Andrieu et al, <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/andrieu03introduction.html">
"An Introduction to MCMC for Machine Learning"</weblink>, 2003</entry>
<entry level="1" type="bullet">

 Bernd A. Berg. "Markov Chain Monte Carlo Simulations and Their Statistical Analysis". Singapore, World Scientific 2004.</entry>
<entry level="1" type="bullet">

 George Casella and Edward I. George. "Explaining the Gibbs sampler". <it>The American Statistician</it>, 46:167-174, 1992. <it>(Basic summary and many references.)''</it></entry>
<entry level="1" type="bullet">

 A.E. Gelfand and A.F.M. Smith. "Sampling-Based Approaches to Calculating Marginal Densities". <it>J. American Statistical Association</it>, 85:398-409, 1990.</entry>
<entry level="1" type="bullet">

 Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. <it>Bayesian Data Analysis</it>. London: Chapman and Hall. First edition, 1995. <it>(See Chapter 11.)''</it></entry>
<entry level="1" type="bullet">

 S. Geman and D. Geman. "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images". <it>IEEE Transactions on Pattern Analysis and Machine Intelligence</it>, 6:721-741, 1984.</entry>
<entry level="1" type="bullet">

 Radford M. Neal, <weblink xlink:type="simple" xlink:href="http://www.cs.utoronto.ca/~radford/review.abstract.html">
Probabilistic Inference Using Markov Chain Monte Carlo Methods</weblink>, 1993.</entry>
<entry level="1" type="bullet">

 Gilks W.R., Richardson S. and Spiegelhalter D.J. "Markov Chain Monte Carlo in Practice". <it>Chapman &amp; Hall/CRC</it>, 1996.</entry>
<entry level="1" type="bullet">

 C.P. Robert and G. Casella. "Monte Carlo Statistical Methods" (second edition). New York: Springer-Verlag, 2004.</entry>
<entry level="1" type="bullet">

 R. Y. Rubinstein and D. P. Kroese. "Simulation and the Monte Carlo Method" (second edition). New York: John Wiley &amp; Sons, 2007.</entry>
<entry level="1" type="bullet">

 R. L. Smith "Efficient Monte Carlo Procedures for Generating Points Uniformly Distributed Over Bounded Regions", <it>Operations Research</it>, Vol. 32, pp. 1296-1308, 1984.</entry>
<entry level="1" type="bullet">

 Asmussen and Glynn "Stochastic Simulation: Algorithms and Analysis", Springer.  Series: Stochastic Modelling and Applied Probability, Vol. 57,  2007.</entry>
<entry level="1" type="bullet">

 P. Atzberger, "An Introduction to Monte-Carlo Methods." <weblink xlink:type="simple" xlink:href="http://www.math.ucsb.edu/~atzberg/spring2006/monteCarloMethod.pdf">
http://www.math.ucsb.edu/~atzberg/spring2006/monteCarloMethod.pdf</weblink>.</entry>
</list>
</p>


</sec>
</bdy>
</method>
</know-how>
</article>
