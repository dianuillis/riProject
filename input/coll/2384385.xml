<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:29:55[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Cascade correlation algorithm</title>
<id>2384385</id>
<revision>
<id>194021652</id>
<timestamp>2008-02-25T21:45:27Z</timestamp>
<contributor>
<username>Andreas Kaufmann</username>
<id>72502</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Cascade-Correlation</b> is an architecture and <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> for <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link>s developed by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../172/507172.xml">
Scott Fahlman</link></associate>
</research_worker>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
.
Instead of just adjusting the weights in a network of fixed topology, Cascade-Correlation begins with a
minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer
structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit
then becomes a permanent feature-detector in the network, available for producing outputs or for creating
other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over
existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the
structures it has built even if the training set changes, and it requires no <link xlink:type="simple" xlink:href="../091/1360091.xml">
backpropagation</link> of error signals
through the connections of the network.
<sec>
<st>
 External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.iastate.edu/~honavar/fahlman.pdf">
The Cascade-Correlation Learning Architecture</weblink> Scott E. Fahlman and Christian Lebiere, August 29, 1991.  Article created for <agency wordnetid="108337324" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../427/157427.xml">
National Science Foundation</link></agency>
 under Contract Number EET-8716324  and <link xlink:type="simple" xlink:href="../957/8957.xml">
Defense Advanced Research Projects Agency</link> (DOD), ARPA Order No. 4976 under Contract F33615-87-C-1499.</entry>
</list>
</p>

</sec>
</bdy>
</article>
