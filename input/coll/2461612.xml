<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:34:08[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<system  confidence="0.8" wordnetid="108435388">
<group  confidence="0.8" wordnetid="100031264">
<network  confidence="0.8" wordnetid="108434259">
<header>
<title>Variational message passing</title>
<id>2461612</id>
<revision>
<id>234622558</id>
<timestamp>2008-08-27T18:46:34Z</timestamp>
<contributor>
<username>Ezubaric</username>
<id>3573</id>
</contributor>
</revision>
<categories>
<category>Bayesian networks</category>
</categories>
</header>
<bdy>

<b>Variational message passing</b> (VMP) is an approximate <link xlink:type="simple" xlink:href="../465/317465.xml">
inference</link> technique for continuous-valued <link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian networks</link>, with <link>
conjugate-exponential</link> parents, developed by John Winn in his PhD thesis. John Winn is now at <physical_entity wordnetid="100001930" confidence="0.8">
<military_unit wordnetid="108198398" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<division wordnetid="108213205" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<army_unit wordnetid="108190754" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../522/2163522.xml">
Microsoft Research</link></research_worker>
</army_unit>
</scientist>
</causal_agent>
</division>
</person>
</military_unit>
</physical_entity>
.  VMP was developed as a means of generalizing the approximate <link xlink:type="simple" xlink:href="../882/171882.xml">
variational methods</link> used by such techniques as <link xlink:type="simple" xlink:href="../351/4605351.xml">
Latent Dirichlet allocation</link> and works by updating an approximate distribution at each node through messages in the node's <system wordnetid="108435388" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<network wordnetid="108434259" confidence="0.8">
<link xlink:type="simple" xlink:href="../984/1169984.xml">
Markov blanket</link></network>
</group>
</system>
.
<sec>
<st>
Likelihood Lower Bound</st>

<p>

Given some set of hidden variables <math>H</math> and observed variables <math>V</math>, the goal of approximate inference is to lower-bound the probability that a graphical model is in the configuration <math>V</math>.  Over some probability distribution <math>Q</math> (to be defined later), </p>
<p>

<indent level="1">

<math> \ln P(V) = \sum_H Q(H) \ln \frac{P(H,V)}{P(H|V)} = \sum_{H} Q(H) [ \ln \frac{P(H,V)}{Q(H)} - \ln \frac{P(H|V)}{Q(H)} ]  </math>.
</indent>

So, if we define our lower bound to be </p>
<p>

<indent level="1">

<math> L(Q) = \sum_{H} Q(H) \ln \frac{P(H,V)}{Q(H)} </math>,
</indent>

then the likelihood is simply this bound plus the <link xlink:type="simple" xlink:href="../527/467527.xml">
relative entropy</link> between <math>P</math> and <math>Q</math>.  Because the relative entropy is non-negative, the function <math>L</math> defined above is indeed a lower bound of the log likelihood of our observation <math>V</math>.  The distribution <math>Q</math> will have a simpler character than that of <math>P</math> because marginalizing over <math>P</math> is intractable for all but the simplest of <link xlink:type="simple" xlink:href="../298/447298.xml">
graphical models</link>.  In particular, VMP uses a factorized distribution <math>Q</math>:</p>
<p>

<indent level="1">

<math> Q(H) = \prod_i Q_i(H_i) </math>
</indent>

where <math>H_i</math> is a disjoint part of the graphical model.</p>

</sec>
<sec>
<st>
Determining the Update Rule</st>

<p>

The likelihood estimate needs to be as large as possible; because it's a lower bound, getting closer <math>\log P</math> improves the approximation of the log likelihood.  By substituting in the factorized version of <math>Q</math>, <math>L(Q)</math>, parameterized over the hidden nodes <math>H_i</math> as above, is simply the negative <link xlink:type="simple" xlink:href="../527/467527.xml">
relative entropy</link> between <math>Q_j</math> and <math>Q_j^*</math> plus other terms independent of <math>Q_j</math> if <math>Q_j^*</math> is defined as as</p>
<p>

<indent level="1">

<math>Q_j^*(H_j) = \frac{1}{Z} e^{\mathbb{E}_{-j}\{\ln P(H,V)\}} </math>,
</indent>

where <math>\mathbb{E}_{-j}\{\ln P(H,V)\}</math> is the expectation over all distributions <math>Q_i</math> except <math>Q_j</math>.  Thus, if we set <math>Q_j</math> to be <math>Q_j^*</math>, the bound <math>L</math> is maximized.</p>

</sec>
<sec>
<st>
Messages in Variational Message Passing</st>

<p>

Parents send their children the expectation of their <link>
sufficient statistic</link> while children send their parents their <link xlink:type="simple" xlink:href="../403/493403.xml">
natural parameter</link>, which also requires messages to be sent from the co-parents of the node.  </p>

</sec>
<sec>
<st>
Relationship to Exponential Families</st>

<p>

Because all nodes in VMP come from <structure wordnetid="105726345" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../174/339174.xml">
exponential families</link></kind>
</distribution>
</type>
</arrangement>
</category>
</concept>
</idea>
</structure>
 and all parents of nodes are <link xlink:type="simple" xlink:href="../412/846412.xml">
conjugate</link> to their children nodes, the expectation of the <link>
sufficient statistic</link> can be computed from the <link xlink:type="simple" xlink:href="../189/398189.xml">
normalization factor</link>.  </p>

</sec>
<sec>
<st>
VMP Algorithm</st>

<p>

The algorithm begins by computing the expected value of the sufficient statistics for that vector.  Then, until the likelihood converges to a stable value (this is usually accomplished by setting a small threshold value and running the algorithm until it increases by less than that threshold value), do the following at each node:</p>

<p>

Get all messages from parents
Get all messages from children (this might require the children to get messages from the co-parents)
Compute the expected value of the nodes sufficient statistics</p>


</sec>
<sec>
<st>
Constraints</st>

<p>

Because every child must be conjugate to its parent, this limits the types of distributions that can be used in the model.  For example, the parents of a <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian distribution</link> must be a <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian distribution</link> (corresponding to the <link xlink:type="simple" xlink:href="../192/19192.xml">
Mean</link>) and a <link xlink:type="simple" xlink:href="../079/207079.xml">
gamma distribution</link> (corresponding to the precision, or one over <math>\sigma</math> in more common paramaterizations).  Discrete variables can have <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../833/1117833.xml">
Dirichlet</link></distribution>
</arrangement>
</structure>
 parents, and <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../561/41561.xml">
Poisson</link></distribution>
</arrangement>
</structure>
 and <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../906/45906.xml">
exponential</link></distribution>
</arrangement>
</structure>
 nodes must have <link xlink:type="simple" xlink:href="../079/207079.xml">
gamma</link> parents.  However, if the data can be modeled in this manner, VMP offers a generalized framework for providing inference.</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 An <weblink xlink:type="simple" xlink:href="http://vibes.sourceforge.net/">
implementation</weblink> of VMP with usage examples</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 M. J. Beal (2003). <weblink xlink:type="simple" xlink:href="http://www.cs.toronto.edu/~beal/thesis/beal03.pdf">
Variational Algorithms for Approximate Bayesian Inference</weblink>. PhD Thesis, <weblink xlink:type="simple" xlink:href="http://www.gatsby.ucl.ac.uk">
Gatsby Computational Neuroscience Unit</weblink>, University College London, 2003.</entry>
<entry level="1" type="bullet">

 J. M. Winn and C. Bishop (2005). <weblink xlink:type="simple" xlink:href="http://research.microsoft.com/users/Cambridge/jwinn/papers/VMP2005.pdf">
Variational Message Passing</weblink>. Journal of Machine Learning Research 6: 661-694.</entry>
</list>
</p>

</sec>
</bdy>
</network>
</group>
</system>
</article>
