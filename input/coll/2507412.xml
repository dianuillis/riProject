<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:36:07[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<plant  confidence="0.8" wordnetid="100017222">
<tree  confidence="0.8" wordnetid="113104059">
<vascular_plant  confidence="0.8" wordnetid="113083586">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<woody_plant  confidence="0.8" wordnetid="113103136">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Information gain in decision trees</title>
<id>2507412</id>
<revision>
<id>227417052</id>
<timestamp>2008-07-23T14:19:04Z</timestamp>
<contributor>
<username>Mathiasl26</username>
<id>83113</id>
</contributor>
</revision>
<categories>
<category>Decision trees</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link> and <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, <b>information gain</b> is an alternative synonym for <it><link>
Kullbackâ€“Leibler divergence</link></it>.  <p>

In particular, the information gain about a random variable <it>X</it> obtained from an observation that a random variable <it>A</it> takes the value <it>A=a</it> is the Kullback-Leibler divergence <it>D</it>KL( <it>p</it>(<it>x</it>|<it>a</it>) || <it>p</it>(<it>x</it>|I) ) of the <link xlink:type="simple" xlink:href="../877/472877.xml">
prior distribution</link> <it>p</it>(<it>x</it>|I) for x from the <link xlink:type="simple" xlink:href="../672/357672.xml">
posterior distribution</link> <it>p</it>(<it>x</it>|<it>a</it>) for <it>x</it> given <it>a</it>.  </p>
<p>

The <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> of the information gain is the <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link> <it>I(X;A)</it> of <it>X</it> and <it>A</it> &mdash; i.e. the reduction in the <link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link> of <it>X</it> achieved by learning the state of the random variable <it>A</it>. </p>
<p>

In machine learning this concept can be used to define a preferred sequence of attributes to investigate to most rapidly narrow down the state of <it>X</it>.  Such a sequence (which depends on the outcome of the investigation of previous attributes at each stage) is called a <link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link>. Usually an attribute with high information gain should be preferred to other attributes.</p>

<sec>
<st>
 General definition </st>

<p>

In general terms, the <link xlink:type="simple" xlink:href="../653/9653.xml">
expected</link> information gain is the change in <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link> from a prior state to a state that takes some information as given: </p>
<p>

<math> IG(Ex,a) = H(Ex) - H(Ex|a) </math></p>

</sec>
<sec>
<st>
 Formal definition </st>

<p>

Let <math>Attr</math> be the set of all attributes and <math>Ex</math> the set of all training examples,
<math>value(x,a)</math> with 
<math>x\in Ex</math> defines the value of a specific example <math>x</math> for attribute <math>a\in Attr</math>, <math>H</math> specifies the entropy.
The information gain for an attribute <math>a\in Attr</math> is defined as follows:</p>
<p>

<math>IG(Ex,a)=H(Ex)-\sum_{v\in values(a)} \frac{|\{x\in Ex|value(x,a)=v\}|}{|Ex|} \bullet H(\{x\in Ex|value(x,a)=v\}) </math></p>
<p>

The information gain is equal to the total entropy for an attribute if for each of the attribute values a unique classification can be made for the result attribute. In this case the relative entropies subtracted from the total entropy are 0.</p>

</sec>
<sec>
<st>
Drawbacks</st>
<p>

Although information gain is usually a good measure for deciding the relevance of an attribute, it is not perfect. A notable problem occurs when information gain is applied to attributes that can take on a large number of distinct values. For example, suppose that we are building a decision tree for some data describing a business's customers. Information gain is often used to decide which of the attributes are the most relevant, so they can be tested near the root of the tree. One of the input attributes might be the customer's credit card number. This attribute has a high information gain, because it uniquely identifies each customer, but we do <it>not</it> want to include it in the decision tree: deciding how to treat a customer based on their credit card number is unlikely to generalize to customers we haven't seen before.</p>
<p>

<link>
Information gain ratio</link> is sometimes used instead. This biases the decision tree against considering attributes with a large number of distinct values.</p>

</sec>
<sec>
<st>
 Constructing a decision tree using information gain </st>

<p>

A decision tree can be constructed top-down using the information gain in the
following way:</p>
<p>

<list>
<entry level="1" type="number">

 begin at the root node</entry>
<entry level="1" type="number">

 determine the attribute with the highest information gain which is not used in an ancestor node </entry>
<entry level="1" type="number">

 add a child node for each possible value of that attribute</entry>
<entry level="1" type="number">

 attach all examples to the child node where the attribute values of the examples are identical to the attribute value attached to the node</entry>
<entry level="1" type="number">

 if all examples attached to the child node can be classified uniquely add that classification to that node and mark it as leaf node</entry>
<entry level="1" type="number">

 go back to step two if there is at least one more unused attribute left, otherwise add the classification of most of the examples attached to the child node</entry>
</list>
</p>

</sec>
<sec>
<st>
External sources</st>
<p>

<list>
<entry level="1" type="bullet">

[1] Mitchell, Tom M., Machine Learning.  The Mc-Graw-Hill Companies, Inc., 1997</entry>
</list>
</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</article>
