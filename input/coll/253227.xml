<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:39:20[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<algorithm  confidence="0.9511911446218017" wordnetid="105847438">
<header>
<title>Knuth–Morris–Pratt algorithm</title>
<id>253227</id>
<revision>
<id>238370799</id>
<timestamp>2008-09-14T15:00:09Z</timestamp>
<contributor>
<username>Tom Alsberg</username>
<id>284393</id>
</contributor>
</revision>
<categories>
<category>Search algorithms</category>
<category>Donald Knuth</category>
<category>Algorithms on strings</category>
</categories>
</header>
<bdy>

The <b>Knuth–Morris–Pratt <link xlink:type="simple" xlink:href="../648/28648.xml">
string searching algorithm</link></b> searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.<p>

The algorithm was conceived by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Donald Knuth</link></scientist>
</person>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../977/1597977.xml">
Vaughan Pratt</link></associate>
</scholar>
</scientist>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</person>
</peer>
</physical_entity>
 and independently by <link xlink:type="simple" xlink:href="../164/11040164.xml">
J. H. Morris</link> in <link xlink:type="simple" xlink:href="../549/34549.xml">
1977</link>, but the three published it jointly.</p>
<p>

In this article, we will be using <link xlink:type="simple" xlink:href="../052/2052.xml#xpointer(//*[./st=%22Index+of+the+first+element%22])">
zero-based arrays</link> for our strings; thus the 'C' in W (shown <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22A+worked+example+of+the+search+algorithm%22])">
below</link>) will be denoted W[2].  In general, notation follows <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
 syntax, when it is not too illogical.</p>

<sec>
<st>
The KMP algorithm</st>

<ss1>
<st>
A worked example of the search algorithm</st>
<p>

To illustrate the algorithm's details, we work through a (relatively artificial) run of the algorithm. At any given time, the algorithm is in a state determined by two integers, m and i, which denote respectively the position within S which is the beginning of a prospective <it>match</it> for W, and the <it>index</it> in W denoting the character currently under consideration.  This is depicted, at the start of the run, like:</p>
<p>

m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W: ABCDABD
i: 0123456</p>
<p>

We proceed by comparing successive characters of W to "parallel" characters of S, moving from one to the next if they match.  However, in the fourth step, we get S[3] is a space and W[3] = 'D', a mismatch.  Rather than beginning to search again at S[1], we note that no 'A' occurs between positions 0 and 3 in S except at 0; hence, having checked all those characters previously, we know there is no chance of finding the beginning of a match if we check them again.  Therefore we move on to the next character, setting m = 4 and i = 0.</p>
<p>

m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:     ABCDABD
i:     0123456</p>
<p>

We quickly obtain a nearly complete match "ABCDAB" when, at W[6] (S[10]), we again have a discrepancy.  However, just prior to the end of the current partial match, we passed an "AB" which could be the beginning of a new match, so we must take this into consideration.  As we already know that these characters match the two characters prior to the current position, we need not check them again; we simply reset m = 8, i = 2 and continue matching the current character.  Thus, not only do we omit previously matched characters of S, but also previously matched characters of W.</p>
<p>

m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:         ABCDABD
i:         0123456</p>
<p>

This search fails immediately, however, as the pattern still does not contain a space, so as in the first trial, we return to the beginning of W and begin searching at the next character of S: m = 11, reset i = 0.</p>
<p>

m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:            ABCDABD
i:            0123456</p>
<p>

Once again we immediately hit upon a match "ABCDAB" but the next character, 'C', does not match the final character 'D' of the word W.  Reasoning as before, we set m = 15, to start at the two-character string "AB" leading up to the current position, set i = 2, and continue matching from the current position.</p>
<p>

m: 01234567890123456789012
S: ABC ABCDAB ABCDABCDABDE
W:                ABCDABD
i:                0123456</p>
<p>

This time we are able to complete the match, whose first character is S[15].</p>

</ss1>
<ss1>
<st>
Description of and pseudocode for the search algorithm</st>

<p>

The above example contains all the elements of the algorithm.  For the moment, we assume the existence of a "partial match" table T, described <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22The+partial+match+table+(also+known+as+failure+function)%22])">
below</link>, which indicates where we need to look for the start of a new match in the event that the current one ends in a mismatch.  The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch).  This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, we cannot backtrack and must simply check the next character; and second, although the next possible match will <it>begin</it> at index m + i - T[i], as in the example above, we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]].  The following is a sample <link xlink:type="simple" xlink:href="../185/24185.xml">
pseudocode</link> implementation of the KMP search algorithm:</p>
<p>

<b>algorithm</b> <it>kmp_search</it>:
<b>input</b>:
an array of characters, S (the text to be searched)
an array of characters, W (the word sought)
<b>output</b>:
an integer (the <link xlink:type="simple" xlink:href="../052/2052.xml#xpointer(//*[./st=%22Indices+into+arrays%22])">
zero-based</link> position in S at which W is found)</p>
<p>

<b>define variables</b>:
an integer, m &amp;larr; 0 (the beginning of the current match in S)
an integer, i &amp;larr; 0 (the position of the current character in W)
an array of integers, T (the table, computed elsewhere)</p>
<p>

<b>while</b> m + i is less than the length of S, do:
<b>if</b> W[i] = S[m + i],
<b>let</b> i &amp;larr; i + 1
<b>if</b> i equals the length of W,
<b>return</b> m
<b>otherwise</b>,
<b>let</b> m &amp;larr; m + i - T[i],
<b>if</b> i is greater than 0,
<b>let</b> i &amp;larr; T[i]</p>
<p>

(if we reach here, we have searched all of S unsuccessfully)
<b>return</b> the length of S</p>

</ss1>
<ss1>
<st>
The efficiency of the search algorithm</st>
<p>

Assuming the prior existence of the table T, the search portion of the Knuth-Morris-Pratt algorithm has <link xlink:type="simple" xlink:href="../543/7543.xml">
complexity</link> O(k), where k is the length of S and the O is <link xlink:type="simple" xlink:href="../578/44578.xml">
big-O notation</link>.  As except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the <b>while</b> loop, we will calculate a bound on the number of iterations of this loop; in order to do this we first make a key observation about the nature of T.  By definition it is constructed so that if a match which had begun at S[m] fails while comparing S[m + i] to W[i], then the next possible match must begin at S[m + (i - T[i])].  In particular the next possible match must occur at a higher index than m, so that T[i]  i.</p>
<p>

Using this fact, we will show that the loop can execute at most 2k times.  For in each iteration, it executes one of the two branches in the loop.  The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased.  The second branch adds i - T[i] to m, and as we have seen, this is always a positive number.  Thus the location m of the beginning of the current potential match is increased.  Now, the loop ends if m + i = k; therefore each branch of the loop can be reached at most k times, since they respectively increase either m + i or m, and m &amp;le; m + i: if m = k, then certainly m + i &amp;ge; k, so that since it increases by unit increments at most, we must have had m + i = k at some point in the past, and therefore either way we would be done.</p>
<p>

Thus the loop executes at most 2k times, showing that the time complexity of the search algorithm is O(k).</p>

</ss1>
</sec>
<sec>
<st>
The "partial match" table (also known as "failure function")</st>
<p>

The goal of the table is to allow the algorithm not to match any character of S more than once.  The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an <it>initial segment</it> of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position.  In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.</p>
<p>

We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W[0] that just failed to match; this is how far we have to backtrack in finding the next match.  Hence T[i] is exactly the length of the longest possible <it>proper</it> initial segment of W which is also a segment of the substring ending at W[i - 1].  We use the convention that the empty string has length 0.  Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T[0] = -1, as discussed <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Description+of+and+pseudocode+for+the+search+algorithm%22])">
above</link>.</p>

<ss1>
<st>
A worked example of the table-building algorithm</st>
<p>

We consider the example of W = "ABCDABD" first.  We will see that it follows much the same pattern as the main search, and is efficient for similar reasons.  We set T[0] = -1.  To find T[1], we must discover a proper suffix of "A" which is also a prefix of W.  But there are no proper suffixes of "A", so we set T[1] = 0.  Likewise, T[2] = 0.</p>
<p>

Continuing to T[3], we note that there is a shortcut to checking <it>all</it> suffixes: let us say that we discovered a proper prefix ending at W[2] with length 2 (the maximum possible); then its first character is a proper prefix of a proper prefix of W, hence a proper prefix itself, and it ends at W[1], which we already determined cannot occur.  Therefore we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T[3] = 0.</p>
<p>

We pass to the subsequent W[4], 'A'.  The same logic shows that the longest substring we need consider has length 1, and although in this case 'A' <it>does</it> work, recall that we are looking for segments ending <it>before</it> the current character; hence T[4] = 0 as well.</p>
<p>

Considering now the next character, W[5], which is 'B', we exercise the following logic: if we were to find a subpattern beginning before the previous character W[4], yet continuing to the current one W[5], then in particular it would itself have a proper initial segment ending at W[4] yet beginning before it, which contradicts the fact that we already found that 'A' itself is the earliest occurrence of a proper segment ending at W[4].  Therefore we need not look before W[4] to find a terminal string for W[5]. Therefore T[5] = 1.</p>
<p>

Finally, we see that the next character in the ongoing segment starting at W[4] = 'A' would be 'B', and indeed this is also W[5].  Furthermore, the same argument as above shows that we need not look before W[4] to find a segment for W[6], so that this is it, and we take T[6] = 2.</p>
<p>

Therefore we compile the following table:</p>
<p>

<table style="background-color:white; font-family:monospace; text-align:right" class="wikitable">
<header>
i</header>
<row>
<col>
0</col>
<col>
1</col>
<col>
2</col>
<col>
3</col>
<col>
4</col>
<col>
5</col>
<col>
6</col>
</row>
<row>
<header>
W[i]</header>
<col>
A</col>
<col>
B</col>
<col>
C</col>
<col>
D</col>
<col>
A</col>
<col>
B</col>
<col>
D</col>
</row>
<row>
<header>
T[i]</header>
<col>
-1</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
2</col>
</row>
</table>
</p>

</ss1>
<ss1>
<st>
Description of and pseudocode for the table-building algorithm</st>

<p>

The example above illustrates the general technique for assembling the table with a minimum of fuss.  The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it.  The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning.  This necessitates some initialization code.</p>
<p>

<b>algorithm</b> <it>kmp_table</it>:
<b>input</b>:
an array of characters, W (the word to be analyzed)
an array of integers, T (the table to be filled)
<b>output</b>:
nothing (but during operation, it populates the table)</p>
<p>

<b>define variables</b>:
an integer, pos &amp;larr; 2 (the current position we are computing in T)
an integer, cnd &amp;larr; 0 (the <link xlink:type="simple" xlink:href="../052/2052.xml#xpointer(//*[./st=%22Indices+into+arrays%22])">
zero-based</link> index in W of the next character of the current candidate substring)</p>
<p>

(the first few values are fixed but different from what the algorithm might suggest)
<b>let</b> T[0] &amp;larr; -1, T[1] &amp;larr; 0</p>
<p>

<b>while</b> pos is less than the length of W, do:
(first case: the substring continues)
<b>if</b> W[pos - 1] = W[cnd], <b>let</b> T[pos] &amp;larr; cnd + 1, pos &amp;larr; pos + 1, cnd &amp;larr; cnd + 1</p>
<p>

(second case: it doesn't, but we can fall back)
<b>otherwise</b>, <b>if</b> cnd &amp;gt; 0, <b>let</b> cnd &amp;larr; T[cnd]</p>
<p>

(third case: we have run out of candidates.  Note cnd = 0)
<b>otherwise</b>, <b>let</b> T[pos] &amp;larr; 0, pos &amp;larr; pos + 1</p>

</ss1>
<ss1>
<st>
The efficiency of the table-building algorithm</st>
<p>

The complexity of the table algorithm is O(n), where n is the length of W.  As except for some initialization all the work is done in the <b>while</b> loop, it is sufficient to show that this loop executes in O(n) time, which will be done by simultaneously examining the quantities pos and pos - cnd.  In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased.  In the second branch, cnd is replaced by T[cnd], which we saw <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22The+efficiency+of+the+search+algorithm%22])">
above</link> is always strictly less than cnd, thus increasing pos - cnd.  In the third branch, pos is incremented and cnd is not, so both pos and pos - cnd increase. Since pos &amp;ge; pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = n, it must terminate after at most 2n iterations of the loop, since pos - cnd begins at 1.  Therefore the complexity of the table algorithm is O(n).</p>

</ss1>
</sec>
<sec>
<st>
The efficiency of the KMP algorithm</st>
<p>

Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).</p>
<p>

As is apparent from the <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22A+worked+example+of+the+search+algorithm%22])">
worked example</link>, the algorithm makes its greatest gains over a naive string-searching algorithm when it can skip the most characters at a time.  That is, the less it has to backtrack, the faster it goes, which is reflected in the table T by the presence of zeroes.  A word such as "ABCDEFG" works well with this algorithm because it has no repetitions of its beginning, so that its table is simply all zeroes with a -1 at the beginning.  On the opposite end of the spectrum, W = "AAAAAAA" works terribly, because its table is</p>
<p>

<table style="background-color:white; font-family:monospace; text-align:right" class="wikitable">
<header>
i</header>
<row>
<col>
0</col>
<col>
1</col>
<col>
2</col>
<col>
3</col>
<col>
4</col>
<col>
5</col>
<col>
6</col>
</row>
<row>
<header>
W[i]</header>
<col>
A</col>
<col>
A</col>
<col>
A</col>
<col>
A</col>
<col>
A</col>
<col>
A</col>
<col>
A</col>
</row>
<row>
<header>
T[i]</header>
<col>
-1</col>
<col>
0</col>
<col>
1</col>
<col>
2</col>
<col>
3</col>
<col>
4</col>
<col>
5</col>
</row>
</table>
</p>
<p>

This is the worst possible pattern for T, and it can be exploited by a word such as S = "AAAAAABAAAAAABAAAAAAA", in which the algorithm tries to match each 'A' against each 'B' before giving up; this results in the maximum possible number of iterations, verging on twice the number of characters of S as the number of repetitions of "AAAAAAB" increases.  Although the table-building routine is rapid for this word (since no backtracking ever occurs), this routine is run only once for a given W, while the search routine is potentially run many times.  If each time, this W is searched for in strings like S, the overall efficiency will be as poor as possible.  By way of comparison, this particular combination is a <it>best</it> case for the <link xlink:type="simple" xlink:href="../709/684709.xml">
Boyer-Moore string search algorithm</link>.</p>
<p>

Note that in practice, the KMP algorithm is not good at searching in natural language texts because it can only skip characters when the first part of the pattern actually matches a part of the text. In practice this only happens seldomly in natural language texts. For instance, consider how many times a prefix of the pattern "text" matches something in this paragraph.</p>

</sec>
<sec>
<st>
External links</st>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ics.uci.edu/~eppstein/161/960227.html">
An explanation of the algorithm</weblink> and <weblink xlink:type="simple" xlink:href="http://www.ics.uci.edu/~eppstein/161/kmp/">
sample C++ code</weblink> by <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../871/4015871.xml">
David Eppstein</link></scientist>
</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www-igm.univ-mlv.fr/~lecroq/string/node8.html">
Knuth-Morris-Pratt algorithm</weblink> description and C code by Christian Charras and Thierry Lecroq</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ics.uci.edu/~goodrich/dsa/11strings/demos/pattern/">
Interactive animation for Knuth-Morris-Pratt algorithm</weblink> by Mike Goodrich</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inf.fh-flensburg.de/lang/algorithmen/pattern/kmpen.htm">
Explanation of the algorithm from scratch</weblink> by FH Flensburg.</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Donald Knuth</link></scientist>
</person>
; <link>
James H. Morris, Jr</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../977/1597977.xml">
Vaughan Pratt</link></associate>
</scholar>
</scientist>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</person>
</peer>
</physical_entity>
&#32;(1977).&#32;"<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/context/23820/0">
Fast pattern matching in strings</weblink>". <it>SIAM Journal on Computing</it>&#32;<b>6</b>&#32;(2): 323–350. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1137%2F0206024">
10.1137/0206024</weblink>.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book"><scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../475/4108475.xml">
Thomas H. Cormen</link></scientist>
;&#32;<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../884/1400884.xml">
Charles E. Leiserson</link></scientist>
, <link xlink:type="simple" xlink:href="../057/68057.xml">
Ronald L. Rivest</link>, <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../993/3489993.xml">
Clifford Stein</link></scientist>
&#32;(2001).&#32;"Section 32.4: The Knuth-Morris-Pratt algorithm", <work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../226/3499226.xml">
Introduction to Algorithms</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
, Second edition,&#32;MIT Press and McGraw-Hill,&#32;923–931. ISBN 978-0-262-03293-3.</cite>&nbsp;</entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</article>
