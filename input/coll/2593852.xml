<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:49:57[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Lanczos algorithm</title>
<id>2593852</id>
<revision>
<id>240515457</id>
<timestamp>2008-09-23T20:00:43Z</timestamp>
<contributor>
<username>Edward</username>
<id>4261</id>
</contributor>
</revision>
<categories>
<category>Numerical linear algebra</category>
</categories>
</header>
<bdy>

The <b>Lanczos algorithm</b> is an iterative algorithm invented by <physical_entity wordnetid="100001930" confidence="0.8">
<expert wordnetid="109617867" confidence="0.8">
<analyst wordnetid="109790482" confidence="0.8">
<physicist wordnetid="110428004" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<immigrant wordnetid="110199489" confidence="0.8">
<migrant wordnetid="110314952" confidence="0.8">
<traveler wordnetid="109629752" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../748/1657748.xml">
Cornelius Lanczos</link></mathematician>
</scientist>
</causal_agent>
</traveler>
</migrant>
</immigrant>
</person>
</physicist>
</analyst>
</expert>
</physical_entity>
 that is an adaptation of <link xlink:type="simple" xlink:href="../550/5975550.xml">
power methods</link> to find <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalue</link>s and <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link>s of a <link>
square matrix</link> or the <link xlink:type="simple" xlink:href="../207/142207.xml">
singular value decomposition</link> of a rectangular matrix. It is particularly useful for finding decompositions of very large sparse matrices. In <link xlink:type="simple" xlink:href="../427/689427.xml">
Latent Semantic Indexing</link>, for instance, matrices relating millions of documents to hundreds of thousands of terms must be reduced to singular value form.<p>

<link xlink:type="simple" xlink:href="../146/14573146.xml">
Peter Montgomery</link> published in 1995 an algorithm, based on the Lanczos algorithm, for finding elements of the <link xlink:type="simple" xlink:href="../679/59679.xml">
nullspace</link> of a large sparse matrix over <region wordnetid="108630985" confidence="0.8">
<field wordnetid="108569998" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<link xlink:type="simple" xlink:href="../263/11137263.xml">
GF(2)</link></geographical_area>
</tract>
</location>
</field>
</region>
; since the set of people interested in large sparse matrices over finite fields and the set of people interested in large eigenvalue problems scarcely overlap, this is often also called the <it>block Lanczos algorithm</it> without causing unreasonable confusion. See <link xlink:type="simple" xlink:href="../979/14572979.xml">
Block Lanczos algorithm for nullspace of a matrix over a finite field</link>.</p>

<sec>
<st>
Power method for finding eigenvalues</st>
<p>
 
The power method for finding the largest eigenvalue of a matrix <math>A\,</math> can be summarized by noting that if <math>x_0\,</math> is a random vector and <math>x_{n+1} = A x_n\,</math>, then in the large <math>n</math> limit, <math>x_n/||x_n||</math> approaches the eigenvector corresponding to the largest eigenvalue. </p>
<p>

If <math>A = U \operatorname{diag}(\sigma_i) U' \,</math> is the eigenvalue decomposition of <math>A\,</math>, then <math>A^n = U \operatorname{diag}(\sigma_i^n) U'</math>. As <math>n\,</math> gets very large, the diagonal matrix of eigenvalues will be dominated by whichever eigenvalue is largest (neglecting the case where there are large equal eigenvalues, of course). As this happens, <math>|x_{n+1}| / |x_{n}|\,</math> will converge to the largest eigenvalue and <math> x_n / |x_n|\,</math> to the associated eigenvector. If the largest eigenvalue is multiple, then <math>x_n \,</math> will converge to a vector in the subspace spanned by the eigenvectors associated with those largest eigenvalues. After you get the first eigenvector/value, you can successively restrict the algorithm to the null space of the known eigenvectors to get the other eigenvector/values.</p>
<p>

In practice, this simple algorithm does not work very well for computing very many of the eigenvectors because any <link xlink:type="simple" xlink:href="../450/432450.xml">
round-off error</link> will tend to introduce slight components of the more significant eigenvectors back into the computation, degrading the accuracy of the computation. Pure power methods also can converge slowly, even for the first eigenvector.</p>

</sec>
<sec>
<st>
Lanczos method</st>
<p>

During the procedure of applying the power method, while getting the ultimate eigenvector <math>A^{n-1} v</math>, we also got a series of vectors <math>A^j v, \, j=0,1,\cdots,n-2</math> which were eventually discarded. That is clearly a huge waste. Some advanced algorithms, such as <link xlink:type="simple" xlink:href="../614/1134614.xml">
Arnoldi's algorithm</link> and the <link xlink:type="simple" xlink:href="../852/2593852.xml">
Lanczos algorithm</link> save this information and use the <link>
Gramâ€“Schmidt process</link> or <link xlink:type="simple" xlink:href="../424/485424.xml">
Householder algorithm</link> to reorthogonalize them into a basis spanning the <link xlink:type="simple" xlink:href="../615/2542615.xml">
Krylov subspace</link> corresponding to the matrix <math>A</math>.</p>

<ss1>
<st>
The algorithm</st>

<p>

The Lanczos algorithm can be viewed as a simplified <link xlink:type="simple" xlink:href="../614/1134614.xml">
Arnoldi's algorithm</link> in that it applies to Hermitian matrices. It transforms the original matrix into a <link xlink:type="simple" xlink:href="../218/519218.xml">
tridiagonal matrix</link> which is real and symmetric.</p>

<ss2>
<st>
Definitions</st>
<p>

We hope to calculate the tridiagonal and symmetric matrix <math>H_{mm} = V_m^* A V_m.</math></p>
<p>

The diagonal elements are denoted by <math>\alpha_j = h_{jj}</math>, and the off-diagonal elements are denoted by <math> \beta_j = h_{j-1,j} </math>.</p>
<p>

Note that <math> h_{j-1,j} = h_{j,j-1} </math>, due to its symmetry.</p>

</ss2>
<ss2>
<st>
Iteration</st>

<p>

(Note: Following these steps alone will <b>not</b> give you the correct eigenvalue and eigenvectors. More consideration must be applied to correct for the numerical errors. See the section <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Numerical_stability%22])">
Numerical stability</link> in the following.)</p>
<p>

There are in principle four ways to write the iteration procedure. Paige[1972] and other works show that the following procedure is the most numerically stable.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

<p>

<b>Algorithm</b> Lanczos
<math>v_1 \leftarrow </math> random vector with norm 1.
<math>v_0 \leftarrow 0</math> 
<math>\beta_1 \leftarrow 0</math>
<b>Iteration</b>: for <math>j = 1,2,\cdots,m</math>
<math> w_j \leftarrow A v_j - \beta_j v_{j-1} </math>
<math> \alpha_j \leftarrow (w_j, v_j) </math>
<math> w_j \leftarrow w_j - \alpha_j v_j </math>
<math> \beta_{j+1} \leftarrow \left\| w_j \right\| </math>
<math> v_{j+1} \leftarrow w_j / \beta_{j+1} </math>
<b>return</b></p>
<p>

<list>
<entry level="1" type="bullet">

"&amp;larr;" is a loose shorthand for "changes to".  For instance, "<it>largest</it> &amp;larr; <it>item</it>" means that the value of <it>largest</it> changes to the value of <it>item</it>.</entry>
<entry level="1" type="bullet">

"<b>return</b>" terminates the algorithm and outputs the value that follows.</entry>
</list>
</p>

<p>

Note that (x,y) represents the dot product of vectors x and y here.</p>
<p>

After the iteration, we get the <math>\alpha_j</math> and <math>\beta_j</math> which constructs a tridiagonal matrix </p>
<p>

<math>T_{mm} = \left( \begin{array}{ccccccc}
\alpha_1 &amp; \beta_2  &amp; 0        &amp; 0        &amp; \cdots &amp; 0       &amp; 0 \\
\beta_2  &amp; \alpha_2 &amp; \beta_3  &amp; 0        &amp; \cdots &amp; 0       &amp; 0 \\
0        &amp; \beta_3  &amp; \alpha_3 &amp; \beta_4  &amp; \cdots &amp; 0       &amp; 0 \\
\vdots   &amp; \vdots   &amp; \vdots   &amp; \vdots   &amp; \vdots &amp; \vdots  &amp; \vdots \\
0        &amp; 0        &amp; 0        &amp; 0        &amp; \cdots &amp; \alpha_{m-1}       &amp; \beta_m \\
0        &amp; 0        &amp; 0        &amp; 0        &amp; \cdots &amp; \beta_m &amp; \alpha_m \\
\end{array} \right) </math></p>
<p>

The vectors <math>v_j</math> (<b>Lanczos vectors</b>) generated on the fly constructs the transformation matrix </p>
<p>

<math>V_m = \left( v_1, v_2, \cdots, v_m \right)</math>, </p>
<p>

which is useful for calculating the eigenvectors (see below). In practice, it could be saved after generation (but takes a lot of memory), or could be regenerated when needed, as long as one keeps the first vector <math>v_1</math>.</p>

</ss2>
<ss2>
<st>
Solve for eigenvalue and eigenvectors</st>

<p>

After the matrix <math>T_{mm}</math> is calculated, one can solve its eigenvalues <math>\lambda_i^{(m)}</math> and their corresponding eigenvectors <math>u_i^{(m)}</math>. This process is pretty simple due to the nature of <math>T</math> being a <link xlink:type="simple" xlink:href="../218/519218.xml">
tridiagonal matrix</link>. </p>
<p>

It can be proved that the eigenvalues are approximate eigenvalues of the original matrix <math>A</math>.</p>
<p>

The Ritz eigenvectors <math>y_i</math> of <math>A</math> can be calculated by <math>y_i = V_m u_i^{(m)}</math>, where <math>V_m</math> is the transformation matrix whose column vectors are <math>v_1, v_2, \cdots, v_m</math>.</p>

</ss2>
</ss1>
<ss1>
<st>
Numerical stability</st>
<p>

Stability means how much the algorithm will be affected (i.e. will it produce the approximate result close to the original one) if there are small numerical errors introduced and accumulated. Numerical stability is the central criterion for judging the usefulness of implementing an algorithm on a computer with roundoff. </p>
<p>

For the Lanczos algorithm, it can be proved that with <it>exact arithmetic</it>, the set of vectors <math>v_1, v_2, \cdots, v_{m+1}</math> constructs an <it>orthogonal</it> basis, and the eigenvalues/vectors solved are good approximations to those of the original matrix. However, in practice (as we code it on to digital computers where round-off errors are inevitable), the orthogonality is quickly lost and in some cases the new vector could even be linearly dependent on the set that is already constructed. As a result, some of the eigenvalues of the resultant tridiagonal matrix may not be approximations to the original matrix. Therefore, the Lanczos algorithm is not very stable.</p>
<p>

Users of this algorithm must be able to find and remove those "spurious" eigenvalues. Practical implementations of the Lanczos algorithm go in three directions to fight this stability issue:
<list>
<entry level="1" type="number">

 Prevent the loss of orthogonality</entry>
<entry level="1" type="number">

 Recover the orthogonality after the basis is generated</entry>
<entry level="1" type="number">

 After the good and "spurious" eigenvalues are all identified, remove the spurious ones.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
Variations</st>
<p>

Variations on the Lanczos algorithm exist where the vectors involved are tall, narrow matrices instead of vectors and the normalizing constants are small square matrices. These are called "block" Lanczos algorithms and can be much faster on computers with large numbers of registers and long memory fetch times.</p>
<p>

Many implementations of the Lanczos algorithm restart after a certain number of iterations.  One of the most influential restarted variations is the implicitly restarted Lanczos method <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>, which is implemented in ARPACK<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>.  This has led into a number of other restarted variations such as restarted Lanczos bidiagonalization<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.  Another successful restarted variation is the Thick-Restart Lanczos method<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>, which has been implemented in a software package called TRLan<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>.</p>

</sec>
<sec>
<st>
Applications</st>
<p>

Lanczos algorithms are very attractive because the multiplication by <math>A\,</math> is the only large scale linear operation. Since weighted-term text retrieval engines implement just this operation, the Lanczos algorithm can be applied efficiently to text documents (see <link xlink:type="simple" xlink:href="../427/689427.xml">
Latent Semantic Indexing</link>). Eigenvectors are also important for the analysis of other large scale text retrieval methods such as the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../223/1851223.xml">
HITS algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 from IBM, or the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../724/23724.xml">
PageRank</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 algorithm used at one time by Google.</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
Cullum and Willoughby, <it>Lanczos Algorithms for Large Symmetric Eigenvalue Computations</it>, Vol. 1, ISBN 0-8176-3058-9(v.1)</entry>
<entry id="2">
Yousef Saad, <it>Numerical Methods for Large Eigenvalue Problems</it>,  ISBN 0-470-21820-7, http://www-users.cs.umn.edu/~saad/books.html</entry>
<entry id="3">
D. Calvetti, L. Reichel, and D.C. Sorensen&#32;(1994).&#32;"<weblink xlink:type="simple" xlink:href="http://etna.mcs.kent.edu/vol.2.1994/pp1-21.dir/pp1-21.ps">
An Implicitly Restarted Lanczos Method for Large Symmetric Eigenvalue Problems</weblink>".&#32;  ETNA.</entry>
<entry id="4">
R. B. Lehoucq, D. C. Sorensen, and C. Yang&#32;(1998).&#32;"<weblink xlink:type="simple" xlink:href="http://www.ec-securehost.com/SIAM/SE06.html">
ARPACK Users Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods</weblink>".&#32;  SIAM.</entry>
<entry id="5">
E. Kokiopoulou and C. Bekas and E. Gallopoulos&#32;(2004).&#32;"<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016/j.apnum.2003.11.011">
Computing smallest singular triplets with implicitly restarted Lanczos bidiagonalization</weblink>".&#32;  Appl. Numer. Math..</entry>
<entry id="6">
Kesheng Wu and Horst Simon&#32;(2000).&#32;"<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1137/S0895479898334605">
Thick-Restart Lanczos Method for Large Symmetric  Eigenvalue Problems</weblink>".&#32;  SIAM.</entry>
<entry id="7">
Kesheng Wu and Horst Simon&#32;(2001).&#32;"<weblink xlink:type="simple" xlink:href="http://crd.lbl.gov/~kewu/trlan.html">
TRLan software package</weblink>".</entry>
</reflist>
</p>



</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://books.google.com/books?vid=ISBN0801854148">
Golub and van Loan give very good descriptions of the various forms of Lanczos algorithms in their book <it>Matrix Computations''</it></weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://ai.stanford.edu/~ang/papers/ijcai01-linkanalysis.pdf">
Andrew Ng et al., an analysis of PageRank</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.farcaster.com/papers/crypto-solve/node3.html">
Lanczos and conjugate gradient methods</weblink> B. A. LaMacchia and A. M. Odlyzko, Solving Large Sparse Linear Systems Over Finite Fields</entry>
</list>
</p>



</sec>
</bdy>
</article>
