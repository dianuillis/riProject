<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:41:36[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>BRST algorithm</title>
<id>2619137</id>
<revision>
<id>215336855</id>
<timestamp>2008-05-27T19:28:55Z</timestamp>
<contributor>
<username>DOI bot</username>
<id>6652755</id>
</contributor>
</revision>
<categories>
<category>Optimization algorithms</category>
</categories>
</header>
<bdy>

<b>Boender-Rinnooy-Stougie-Timmer</b> algorithm (BRST) is an optimization algorithm suitable for finding <link xlink:type="simple" xlink:href="../285/914285.xml">
global optimum</link> of black box functions. In their paper Boender <it>et al</it> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> describe their method as a stochastic method involving a combination of sampling, clustering and local search, terminating with a range of confidence intervals on the value of the global minimum. <p>

The algorithm of Boender <it>et al</it> has been modified by Timmer <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>. Timmer considered several clustering methods. Based on experiments a method named "multi level single linkage" was deemed most accurate.</p>
<p>

Csendes' algorithms <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> are implementations of the algorithm of [Boender ''et al'']<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> and originated the public-domain software product <link>
GLOBAL</link>. The local algorithms used are a random direction, linear search algorithm also used by Törn, and a quasi--Newton algorithm not using the derivative of the function. The results show the dependence of the result on the auxiliary local algorithm used.</p>

<sec>
<st>
 Background </st>
<p>

Extending the class of functions to include multimodal functions makes the global optimization problem unsolvable in general. In order to be solvable some smoothness condition on the function in addition to continuity must be known.</p>
<p>

The existence of several local minima and unsolvability in general are important characteristics of global optimization. Unsolvability here means that a solution cannot be guaranteed in a finite number of steps.
There are two ways to deal with the unsolvability problem. First, "a priori" conditions on f and A are posed which turns the problem into a solvable one or at least makes it possible to tell for sure that a solution has been found. This restricts the function class that is considered.
The second approach which allows a larger class of objective functions to be considered is to give up the solvability requirement and only try to obtain an estimate of the global minimum. In this "probabilistic" approach it would be desirable also to obtain some results on the goodness of an obtained estimate. Some of the solvable problems may fall in this class because the number of steps required for a guaranteed solution might be prohibitively large.</p>
<p>

When relaxing the requirement on solvability it seems rational to require that the probability that a solution is obtained approaches 1 if the procedure is allowed to continue forever. An obvious probabilistic global search procedure is to use a local algorithm starting from several points distributed over the whole optimization region. This procedure is named "Multistart". Multistart is certainly one of the earliest global procedures used. It has even been used in local optimization for increasing the confidence in the obtained solution. One drawback of Multistart is that when many starting points are used the same minimum will eventually be determined several times. In order to improve the efficiency of Multistart this should be avoided.</p>
<p>

Clustering methods are used to avoid this repeated determination of local minima. This is realized in three steps which may be iteratively used. The three steps are:</p>
<p>

<list>
<entry level="1" type="bullet">

 (a) Sample points in the region of interest.</entry>
<entry level="1" type="bullet">

 (b) Transform the sample to obtain points grouped around the local minima.</entry>
<entry level="1" type="bullet">

 (c) Use a clustering technique to recognize these groups (i.e. neighbourhoods of the local minima).</entry>
</list>
</p>
<p>

If the procedure employing these steps is successful then starting a single local optimization from each cluster would determine the local minima and thus also the global minimum. The advantage in using this approach is that the work spared by computing each minimum just once can be spent on computations in (a) and (b), which will increase the probability that the global minimum will be found.</p>
<p>

Being a <link>
clustering method</link>, their effectiveness is higher for low dimensional problems and become less effective for problems having a few hundred variables.</p>

</sec>
<sec>
<st>
 References </st>
<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">Boender, C.G.E., A.H.G. Rinnooy Kan, L. Strougie and G.T. Timmer&#32;(1982).&#32;"A stochastic method for global optimization". <it>Mathematical Programming</it>&#32;<b>22</b>: 125–140. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF01581033">
10.1007/BF01581033</weblink>.</cite>&nbsp;</entry>
<entry id="2">
Timmer, G.T.&#32;(1984).&#32;"<it>Global optimization: A stochastic approach</it>"&#32;(Ph.D. Thesis). &#32;Erasmus University Rotterdam.</entry>
<entry id="3">
 <cite style="font-style:normal">Csendes, T.&#32;(1988).&#32;"Nonlinear parameter estimation by global optimization—Efficiency and reliability". <it>Acta Cybernetica</it>&#32;<b>8</b>: 361–370.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.abo.fi/~atorn/Globopt.html">
http://www.abo.fi/~atorn/Globopt.html</weblink> With the author's permission, text has been verbatim copied.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mat.univie.ac.at/~vpk/math/gopt_eng.html">
Janka</weblink> Compares various global optimization algorithms, of which BRST shows superior performance.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mat.univie.ac.at/~vpk/math/dix_sze_eng.html">
Janka</weblink> Presents the number of function-evaluations performed on the testset of Dixon-Szegö. Along with the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../980/2618980.xml">
MCS algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, the BRST requires the lowest number of evaluations.</entry>
</list>
</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
