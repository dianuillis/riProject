<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:52:22[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Semi-supervised learning</title>
<id>2829632</id>
<revision>
<id>244538103</id>
<timestamp>2008-10-11T09:16:53Z</timestamp>
<contributor>
<username>Jcarroll</username>
<id>513245</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>, <b>semi-supervised learning</b> is a class of <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> techniques that make use of both labeled and unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data.  Semi-supervised learning falls between <link xlink:type="simple" xlink:href="../497/233497.xml">
unsupervised learning</link> (without any labeled training data) and <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link> (with completely labeled training data).  Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent to manually classify training examples. The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value.<p>

One example of a semi-supervised learning technique is <link xlink:type="simple" xlink:href="../131/18586131.xml">
co-training</link>, in which two or possibly more learners are each trained on a set of examples, but with each learner using a different, and ideally independent, set of features for each example. </p>
<p>

An alternative approach is to model the joint probability distribution of the features and the labels. For the unlabelled data the labels can then be treated as 'missing data'. It is common to use the <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<numerical_quantity wordnetid="105856066" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<value wordnetid="105856388" confidence="0.8">
<quantity wordnetid="105855125" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../752/470752.xml">
EM algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</quantity>
</value>
</rule>
</event>
</numerical_quantity>
</concept>
</idea>
 to maximize the likelihood of the model.</p>

<sec>
<st>
See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../553/9303553.xml">
Constrained clustering</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../361/960361.xml">
Transductive learning</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="number">

 Abney, S., Semisupervised Learning for Computational Linguistics. Chapman &amp; Hall/CRC, 2008.</entry>
<entry level="1" type="number">

 Blum, A., Mitchell, T. <weblink xlink:type="simple" xlink:href="http://www.cs.wustl.edu/~zy/paper/cotrain.ps">
Combining labeled and unlabeled data with co-training</weblink>. <it>COLT: Proceedings of the Workshop on Computational Learning Theory</it>, Morgan Kaufmann, 1998, p. 92-100.</entry>
<entry level="1" type="number">

 Chapelle, O., B. Schölkopf and A. Zien: <it>Semi-Supervised Learning</it>. MIT Press, Cambridge, MA (2006). <weblink xlink:type="simple" xlink:href="http://www.kyb.tuebingen.mpg.de/ssl-book/">
Further information</weblink>.</entry>
<entry level="1" type="number">

 Huang T-M., Kecman V., Kopriva I. <weblink xlink:type="simple" xlink:href="http://www.learning-from-data.com">
http://www.learning-from-data.com</weblink>, "Kernel Based Algorithms for Mining Huge Data Sets, Supervised, Semisupervised and Unsupervised Learning", Springer-Verlag, Berlin, Heidelberg,  260 pp. 96 illus., Hardcover, ISBN 3-540-31681-7, 2006.</entry>
<entry level="1" type="number">

 O'Neill, T. J. (1978) Normal discrimination with unclassified observations. Journal of the American Statistical Association, 73, 821–826.</entry>
<entry level="1" type="number">

 Zhu, X. <weblink xlink:type="simple" xlink:href="http://www.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf">
Semi-supervised learning literature survey</weblink>.</entry>
</list>
</p>

</sec>
</bdy>
</article>
