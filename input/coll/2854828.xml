<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:53:32[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<method  confidence="0.9511911446218017" wordnetid="105660268">
<header>
<title>Multi-armed bandit</title>
<id>2854828</id>
<revision>
<id>238776433</id>
<timestamp>2008-09-16T09:52:47Z</timestamp>
<contributor>
<username>Melcombe</username>
<id>4682566</id>
</contributor>
</revision>
<categories>
<category>Sequential methods</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

A <b>multi-armed bandit</b>, also sometimes called a K-armed bandit, is a simple <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> problem based on an analogy with a traditional <link xlink:type="simple" xlink:href="../229/29229.xml">
slot machine</link> (one-armed bandit) but with more than one lever. When pulled, each lever provides a reward drawn from a distribution associated to that specific lever. The objective of the gambler is to maximize the collected reward sum through iterative pulls. It is classically assumed that the gambler has no initial knowledge about the levers. The crucial tradeoff the gambler faces at each trial is between "exploitation" of the lever that has the highest <link xlink:type="simple" xlink:href="../018/533018.xml">
expected payoff</link> and "exploration" to get more <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
information</link></proposition>
</theorem>
</message>
</statement>
 about the expected payoffs of the other levers.
<sec>
<st>
 Empirical motivation </st>

<p>

The multi-armed bandit problem, originally described by Robbins in 1952, is a simple model of an agent that simultaneously attempts to acquire new knowledge and to optimize its decisions based on existing knowledge. Practical examples include clinical trials where the effects of different experimental treatments need to be investigated while minimizing patient losses, and <link xlink:type="simple" xlink:href="../453/923453.xml">
adaptive routing</link> efforts for minimizing delays in a network. The questions arising in these cases are related to the problem of balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge. This is known as the <it>exploitation vs. exploration tradeoff</it> in <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link>.</p>
<p>

The model can also be used to control dynamic allocation of resources to different projects, answering the question "which project should I work on" given uncertainty about the difficulty and payoff of each possibility.</p>

</sec>
<sec>
<st>
 The multi-armed bandit model </st>

<p>

The multi-armed bandit (or just bandit for short) can be seen as a set of real distributions <math>B = \{R_1, \dots ,R_K\}</math>, each distribution being  associated with the rewards delivered by one of the <it>K</it> levers. Let <math>\mu_1, \dots , \mu_K</math> be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon <it>H</it> is the number of rounds that remain to be played. The bandit problem is formally equivalent to a one-state <link xlink:type="simple" xlink:href="../883/1125883.xml">
Markov decision process</link>. The regret <math>\rho</math> after <it>T</it> rounds is defined as the difference  between the reward sum associated with an optimal strategy and the sum of the collected rewards: <math>\rho = T \mu^* - \sum_{t=1}^T \widehat{r}_t</math>, where <math>\mu^*</math> is the maximal reward mean, <math>\mu^* = \max_k \{ \mu_k \}</math>, and <math>\widehat{r}_t</math> is the reward at time <it>t</it>. A strategy whose average regret per round <math>\rho / T</math> tends to zero with probability 1 when the number of played rounds tends to infinity is a <it>zero-regret strategy</it>. Intuitively, zero-regret strategies are guaranteed to converge to an optimal strategy, not necessarily unique, if enough rounds are played.</p>

</sec>
<sec>
<st>
 Variations </st>

<p>

Another formulation of the multi-armed bandit has each arm representing an independent markov machine. Each time a particular arm is played, the state of that machine advances to a new one, chosen according to the markov state evolution probabilities. There is also a reward depending on the current state of the machine.</p>
<p>

In a generalisation called the <link>
Restless Bandit Problem</link>, the states of non-played arms can also evolve over time.</p>
<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../214/2600214.xml">
Whittle</link></person>
 has also discussed systems where the number of choices (about which arm to play) increases over time: an <it>Arm Acquiring Bandit</it>.</p>

</sec>
<sec>
<st>
 Common bandit strategies </st>

<p>

Many strategies exist which provide an approximate solution to the bandit problem, and can be put into the three broad categories detailed below.</p>

<ss1>
<st>
 Semi-uniform strategies </st>

<p>

Semi-uniform strategies were the earliest (and simplest) strategies discovered to approximately solve the bandit problem. All those strategies have in common a <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../247/89247.xml">
greedy</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 behavior where the <it>best</it> lever (based on previous observations) is always pulled except when a (uniformly) random action is taken.</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Epsilon-greedy strategy</b>: The best lever is selected for a proportion <math>1 - \epsilon</math> of the trials, and another lever is randomly selected (with uniform probability) for a proportion <math>\epsilon</math>. A typical parameter value might be <math>\epsilon = 0.1</math>, but this can vary widely depending on circumstances and predilections.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Epsilon-first strategy</b>: A pure exploration phase is followed by a pure exploitation phase. For <math>N</math> trials in total, the exploration phase occupies <math>\epsilon N</math> trials and the exploitation phase <math>(1 - \epsilon) N</math> trials. During the exploration phase, a lever is randomly selected (with uniform probability); during the exploitation phase, the best lever is always selected. </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Epsilon-decreasing strategy</b>: Similar to the epsilon-greedy strategy, except that the value of <math>\epsilon</math> decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Probability matching strategies </st>

<p>

Probability matching strategies reflect the idea that the number of pulls for a given lever should <it>match</it> its actual probability of being the optimal lever.</p>

</ss1>
<ss1>
<st>
 Pricing strategies </st>

<p>

Pricing strategies establish a <it>price</it> for each lever. The lever of highest price is always pulled.</p>

</ss1>
</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

H. Robbins. Some Aspects of the Sequential Design of Experiments. In <it>Bulletin of the American Mathematical Society</it>, volume 58, pages 527–535, 1952.</entry>
<entry level="1" type="bullet">

Richard Sutton and Andrew Barto. <it>Reinforcement Learning</it>. MIT Press, 1998. (available <weblink xlink:type="simple" xlink:href="http://www-anw.cs.umass.edu/~rich/book/the-book.html">
online</weblink>)</entry>
<entry level="1" type="bullet">

Bandit project (<weblink xlink:type="simple" xlink:href="http://bandit.sourceforge.net">
bandit.sourceforge.net</weblink>), open source implementation of many bandit strategies.</entry>
<entry level="1" type="bullet">

S. Dayanik, W. Powell, and K. Yamazaki (2007). Index policies for discounted bandit problems with availability constraints (<weblink xlink:type="simple" xlink:href="http://www.princeton.edu/~sdayanik/papers/bandit.pdf">
http://www.princeton.edu/~sdayanik/papers/bandit.pdf</weblink>)</entry>
<entry level="1" type="bullet">

<person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../214/2600214.xml">
Peter Whittle</link></person>
. Arm-acquiring bandits. Ann. Probab., 9:284–292, 1981. (Available <weblink xlink:type="simple" xlink:href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aop/1176994469">
online</weblink>)</entry>
<entry level="1" type="bullet">

<person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../214/2600214.xml">
Peter Whittle</link></person>
. Restless bandits: Activity allocation in a changing world. Appl. Prob., 25(A):287–298, 1988.</entry>
<entry level="1" type="bullet">

Sudipto Guha, Kamesh Munagala, Peng Shi, <it>On Index Policies for Restless Bandit Problems</it>, 2007 <weblink xlink:type="simple" xlink:href="http://arxiv.org/abs/0711.3861">
arXiv:0711.3861v1</weblink></entry>
<entry level="1" type="bullet">

Warren B. Powell, <it>Approximate Dynamic Programming,</it> John Wiley and Sons, New York, 2007 (Chapter 10).</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../447/9267447.xml">
Gittins index</link></method>
</know-how>
 &mdash; a powerful, general strategy for analyzing bandit problems.</entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../886/7189886.xml">
Optimal stopping</link></method>
</know-how>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../279/12487279.xml">
Search theory</link></entry>
</list>
</p>

</sec>
</bdy>
</method>
</article>
