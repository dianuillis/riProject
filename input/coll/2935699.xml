<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 20:00:08[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Run-time algorithm specialisation</title>
<id>2935699</id>
<revision>
<id>189987137</id>
<timestamp>2008-02-08T17:04:23Z</timestamp>
<contributor>
<username>The Anomebot2</username>
<id>1979668</id>
</contributor>
</revision>
<categories>
<category>Algorithms</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>, <b>run-time algorithm specialisation</b> is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of <link xlink:type="simple" xlink:href="../546/2546.xml">
automated theorem proving</link> and, more specifically, in the <link xlink:type="simple" xlink:href="../408/720408.xml">
Vampire theorem prover</link> project.<p>

The idea is inspired by the use of <link xlink:type="simple" xlink:href="../428/62428.xml">
partial evaluation</link> in optimising program translation. 
Many core operations in theorem provers exhibit the following pattern.
Suppose that we need to execute some algorithm <math>\mathit{alg}(A,B)</math> in a situation where a value of <math>A</math> <it>is fixed for potentially many different values of</it> <math>B</math>. In order to do this efficiently, we can try to find a specialisation of <math>\mathit{alg}</math> for every fixed <math>A</math>, i.e., such an algorithm <math>\mathit{alg}_A</math>, that executing <math>\mathit{alg}_A(B)</math> is equivalent to executing <math>\mathit{alg}(A,B)</math>.</p>
<p>

The specialised algorithm may be more efficient than the generic one, since it can <it>exploit some particular properties</it> of the fixed value <math>A</math>. Typically, <math>\mathit{alg}_A(B)</math> can avoid some operations that <math>\mathit{alg}(A,B)</math> would have to perform, if they are known to be redundant for this particular parameter <math>A</math>. 
In particular, we can often identify some tests that are true or false for <math>A</math>, unroll loops and recursion, etc. </p>
<p>

The key difference between run-time specialisation and partial evaluation is that the values of <math>A</math> on which <math>\mathit{alg}</math> is specialised are not known statically, so the <it>specialisation takes place at run-time</it>.</p>
<p>

There is also an important technical difference. Partial evaluation is applied to algorithms explicitly represented as codes in some programming language. At run-time, we do not need any concrete representation of <math>\mathit{alg}</math>. We only have to <it>imagine</it> <math>\mathit{alg}</math> <it>when we program</it> the specialisation procedure.
All we need is a concrete representation of the specialised version <math>\mathit{alg}_A</math>. This also means that we cannot use any universal methods for specialising algorithms, which is usually the case with partial evaluation. Instead, we have to program a specialisation procedure for every particular algorithm <math>\mathit{alg}</math>. An important advantage of doing so is that we can use some powerful <it>ad hoc</it> tricks exploiting peculiarities of <math>\mathit{alg}</math> and the representation of <math>A</math> and <math>B</math>, which are beyond the reach of any universal specialisation methods.</p>

<sec>
<st>
Specialisation with compilation</st>

<p>

The specialised algorithm has to be represented in a form that can be interpreted.
In many situations, usually when <math>\mathit{alg}_A(B)</math> is to be computed on many values <math>B</math> in a row, we can write <math>\mathit{alg}_A</math> as a code of a special <link xlink:type="simple" xlink:href="../492/60492.xml">
abstract machine</link>, and we often say that <math>A</math> is <it>compiled</it>.  
Then the code itself can be additionally optimised by answer-preserving transformations that rely only on the semantics of instructions of the abstract machine. </p>
<p>

Instructions of the abstract machine can usually be represented as records. One field of such a record stores an integer tag that identifies the instruction type, other fields may be used for storing additional Parameters of the instruction, for example a pointer to another
instruction representing a label, if the semantics of the instruction requires a jump. All instructions of a code can be stored in an array, or list, or tree. </p>
<p>

Interpretation is done by fetching instructions in some order, identifying their type
and executing the actions associated with this type. 
In <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
 or C++ we can use a <b>switch</b> statement to associate 
some actions with different instruction tags. 
Modern compilers usually compile a <b>switch</b> statement with integer labels from a narrow range rather efficiently by storing the address of the statement corresponding to a value <math>i</math> in the <math>i</math>-th cell of a special array. One can exploit this
by taking values for instruction tags from a small interval of integers.</p>

</sec>
<sec>
<st>
Data-and-algorithm specialisation</st>

<p>

There are situations when many instances of <math>A</math> are intended for long-term storage and the calls of <math>\mathit{alg}(A,B)</math> occur with different <math>B</math> in an unpredicatable order.
For example, we may have to check <math>\mathit{alg}(A_1,B_1)</math> first, then <math>\mathit{alg}(A_2,B_2)</math>, then <math>\mathit{alg}(A_1,B_3)</math>, and so on.
In such circumstances, full-scale specialisation with compilation may not be suitable due to excessive memory usage.  
However, we can sometimes find a compact specialised representation <math>A^{\prime}</math>
for every <math>A</math>, that can be stored with, or instead of, <math>A</math>. 
We also define a variant <math>\mathit{alg}^{\prime}</math> that works on this representation 
and any call to <math>\mathit{alg}(A,B)</math> is replaced by <math>\mathit{alg}^{\prime}(A^{\prime},B)</math>, intended to do the same job faster.</p>

</sec>
<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

 <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../564/1561564.xml">
Psyco</link></software>
, a specializing run-time compiler for <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../862/23862.xml">
Python</link></programming_language>
</entry>
<entry level="1" type="bullet">

 <link>
multi-stage programming</link></entry>
</list>
</p>

</sec>
<sec>
<st>
Reading list</st>

<p>

<list>
<entry level="1" type="bullet">

 A. Voronkov, "The Anatomy of Vampire: Implementing Bottom-Up Procedures with Code Trees", <it>Journal of Automated Reasoning</it>, 15(2), 1995 (<it>original idea</it>)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 A. Riazanov and A. Voronkov, "Efficient Checking of Term Ordering Constraints", <it>Proc. IJCAR</it> 2004, Lecture Notes in Artificial Intelligence 3097, 2004 (<it>compact but self-contained illustration of the method</it>)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 A. Riazanov and A. Voronkov, <it>Efficient Instance Retrieval with Standard and Relational Path Indexing, Information and Computation</it>, 199(1-2), 2005 (<it>contains another illustration of the method</it>)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 A. Riazanov, "Implementing an Efficient Theorem Prover", PhD thesis, The University of Manchester, 2003 (<it>contains the most comprehensive description of the method and many examples</it>)</entry>
</list>
</p>

</sec>
</bdy>
</article>
