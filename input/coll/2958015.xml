<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 19:58:55[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Philosophy of artificial intelligence</title>
<id>2958015</id>
<revision>
<id>239085010</id>
<timestamp>2008-09-17T19:12:03Z</timestamp>
<contributor>
<username>CharlesGillingham</username>
<id>4604963</id>
</contributor>
</revision>
<categories>
<category>Philosophy of artificial intelligence</category>
</categories>
</header>
<bdy>

<table style="background:#f9f9f9; font-size:85%; line-height:110%; ">
<row>
<col>
 <image width="32x28px" src="Portal.svg">
</image>
</col>
<col style="padding:0 0.2em;">
 <b><it>
Artificial intelligence&#32;portal</it></b></col>
</row>
</table>

<p>

<table style="background:#f9f9f9; font-size:85%; line-height:110%; ">
<row>
<col>
 <image width="32x28px" src="Portal.svg">
</image>
</col>
<col style="padding:0 0.2em;">
 <b><it>
Mind and Brain&#32;portal</it></b></col>
</row>
</table>
</p>

<p>

<indent level="1">

<it>See also: <link xlink:type="simple" xlink:href="../583/13659583.xml">
ethics of artificial intelligence</link></it>
</indent>

The <b>philosophy of artificial intelligence</b> considers the relationship between <it>machines</it> and <it>thought</it> and attempts to answer such question as:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>
<list>
<entry level="1" type="bullet">

 Can a machine act intelligently? Can it solve <it>any</it> problem that a person would solve by thinking? </entry>
<entry level="1" type="bullet">

 Can a machine have a <link xlink:type="simple" xlink:href="../483/6880483.xml">
mind</link>, mental states and <link xlink:type="simple" xlink:href="../664/5664.xml">
consciousness</link> in the same sense humans do? Can it <it>feel</it>?</entry>
<entry level="1" type="bullet">

 Are human intelligence and machine intelligence the same?  Is the human brain essentially a computer? </entry>
</list>

These three questions reflect the divergent interests of <link xlink:type="simple" xlink:href="../164/1164.xml">
AI researchers</link>, <link xlink:type="simple" xlink:href="../155/13692155.xml">
philosopher</link>s and <link xlink:type="simple" xlink:href="../626/5626.xml">
cognitive scientists</link> respectively. The answers to these questions depend on how one defines "intelligence" or "consciousness" and exactly which "machines" are under discussion.</p>
<p>

Important <link xlink:type="simple" xlink:href="../094/81094.xml">
proposition</link>s in the philosophy of AI include:</p>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../840/43840.xml">
Turing's "polite convention"</link>: <it>If a machine acts as intelligently as a human being, then it is as intelligent as a human being.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></entry>
<entry level="1" type="bullet">

 The <link xlink:type="simple" xlink:href="../646/1124646.xml">
Dartmouth proposal</link>: <it>"Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it."</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../304/287304.xml">
Newell</link> and <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../205/14205.xml">
Simon</link></scientist>
</person>
's <link xlink:type="simple" xlink:href="../999/2685999.xml">
physical symbol system hypothesis</link>: <it>"A physical symbol system has the necessary and sufficient means of general intelligent action."</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></entry>
<entry level="1" type="bullet">

 <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
Searle</link></philosopher>
's <work wordnetid="100575741" confidence="0.8">
<argument wordnetid="106648724" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../216/6216.xml#xpointer(//*[./st=%22Strong+AI%22])">
strong AI hypothesis</link></activity>
</psychological_feature>
</research>
</act>
</investigation>
</evidence>
</experiment>
</indication>
</event>
</scientific_research>
</argument>
</work>
: <it>"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../823/29823.xml">
Hobbes</link>' <link xlink:type="simple" xlink:href="../331/382331.xml">
mechanism</link>: <it>"Reason is nothing but reckoning."</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></entry>
</list>
</p>


<sec>
<st>
 Can a machine display general intelligence?  </st>

<p>

Is it possible to create a machine that can solve <it>all</it> the problems humans solve using their intelligence? This is the question that AI researchers are most interested in answering. It defines the scope of what machines will be able to do in the future and guides the direction of AI research. It only concerns the <it>behavior</it> of machines and ignores the issues of interest to <link xlink:type="simple" xlink:href="../921/22921.xml">
psychologists</link>, <link xlink:type="simple" xlink:href="../626/5626.xml">
cognitive scientist</link>s and <link xlink:type="simple" xlink:href="../155/13692155.xml">
philosophers</link>; to answer this question, it doesn't matter whether a machine is <it>really</it> thinking (as a person thinks) or is just <it>acting like</it> it is thinking.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></p>
<p>

The basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the <link xlink:type="simple" xlink:href="../646/1124646.xml">
Dartmouth Conferences</link> of 1956:
<list>
<entry level="1" type="bullet">

 <it>Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></entry>
</list>

Arguments against the basic premise must show that building a working AI system is impossible, because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for thinking and yet can't be duplicated by a machine (or by the methods of current AI research).  Arguments in favor of the basic premise must show that such a system is possible.</p>
<p>

The first step to answering the question is to clearly define "intelligence."</p>

<ss1>
<st>
Intelligence</st>

<ss2>
<st>
 Turing test </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../840/43840.xml">
Turing test</link></it>
</indent>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Alan Turing</link></scientist>
</person>
, in a famous and seminal 1950 paper,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answer <it>any</it> question put to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an online <link xlink:type="simple" xlink:href="../892/38892.xml">
chat room</link>, where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> Turing notes that no one (except philosophers) ever asks the question "can people think?" He writes "instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref> Turing's test extends this polite convention to machines:
<list>
<entry level="1" type="bullet">

 <it>If a machine acts as intelligently as human being, then it is as intelligent as a human being.''</it></entry>
</list>
</p>

</ss2>
<ss2>
<st>
 Human intelligence vs. intelligence in general </st>

<p>

One criticism of the <link xlink:type="simple" xlink:href="../840/43840.xml">
Turing test</link> is that it is explicitly <link xlink:type="simple" xlink:href="../060/19009060.xml">
anthropomorphic</link>. If our ultimate goal is to create machines that are <it>more</it> intelligent than people, why should we insist that our machines must closely <it>resemble</it> people? <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../689/566689.xml">
Russell</link></scientist>
</person>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<employee wordnetid="110053808" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../666/566666.xml">
Norvig</link></associate>
</research_worker>
</employee>
</scientist>
</causal_agent>
</colleague>
</worker>
</person>
</peer>
</physical_entity>
 write that "aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons.'"<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref> Recent AI research defines intelligence in terms of <link xlink:type="simple" xlink:href="../268/1187268.xml">
rational agent</link>s or <link xlink:type="simple" xlink:href="../317/2711317.xml">
intelligent agent</link>s. An "agent" is something which perceives and acts in an environment. A "performance measure" defines what counts as success for the agent.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref> 
<list>
<entry level="1" type="bullet">

 <it>If an agent acts so as maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref></entry>
</list>

Definitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the <link xlink:type="simple" xlink:href="../840/43840.xml">
Turing test</link>, they don't also test for human traits that we may not want to consider intelligent, like the ability to be insulted or the temptation to lie. They have the disadvantage that they fail to make the commonsense differentiation between "things that think" and "things that don't". By this definition, even a thermostat has a rudimentary intelligence.</p>

</ss2>
</ss1>
<ss1>
<st>
Arguments that a machine can display general intelligence</st>

<ss2>
<st>
The brain can be simulated</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../395/1908395.xml">
artificial brain</link></it>
</indent>

<image width="240px" src="MRI.ogg" type="thumb">
<caption>

An <link xlink:type="simple" xlink:href="../450/19450.xml">
MRI</link> scan of a normal adult human brain
</caption>
</image>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../639/19639.xml">
Marvin Minsky</link></scientist>
</person>
 writes that "if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then .... we ... ought to be able to reproduce the behavior of the nervous system with some physical device."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref> This argument, first introduced as early as 1943<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref> and vividly described by <link xlink:type="simple" xlink:href="../556/298556.xml">
Hans Moravec</link> in 1988,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref> is now associated with futurist <link xlink:type="simple" xlink:href="../984/25984.xml">
Ray Kurzweil</link>, who estimates that computer power will be sufficient for a complete brain simulation by the year 2029.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref></p>
<p>

Few disagree that a brain simulation is possible in theory, even critics of AI such as <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../926/952926.xml">
Hubert Dreyfus</link></philosopher>
 and <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
John Searle</link></philosopher>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref>
However, Searle points out that, in principle, <it>anything</it> can be simulated by a computer, and so any process at all can be considered "computation", if you're willing to stretch the definition to the breaking point. "What we wanted to know is what distinguishes the mind from thermostats and livers," he writes.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2218%22])">18</ref> Any argument that involves simply copying a brain is an argument that admits that we know nothing about how intelligence works. "If we had to know how the brain worked to do AI, we wouldn't bother with AI."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2219%22])">19</ref></p>


</ss2>
<ss2>
<st>
Human thinking is symbol processing </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../999/2685999.xml">
physical symbol system</link></it>
</indent>

In 1963, <link xlink:type="simple" xlink:href="../304/287304.xml">
Alan Newell</link> and <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../205/14205.xml">
Herbert Simon</link></scientist>
</person>
 proposed that "symbol manipulation" was the essence of both human and machine intelligence. They wrote: 
<list>
<entry level="1" type="bullet">

 <it>A <link xlink:type="simple" xlink:href="../999/2685999.xml">
physical symbol system</link> has the necessary and sufficient means of general intelligent action.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></entry>
</list>

This claim is very strong: it implies both that human thinking is a kind of symbol manipulation (because a symbol system is <it>necessary</it> for intelligence) and that machines can be intelligent (because a symbol system is <it>sufficient</it> for intelligence).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2220%22])">20</ref> Another version of this position was described by philosopher <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../926/952926.xml">
Hubert Dreyfus</link></philosopher>
, who called it "the psychological assumption":
<list>
<entry level="1" type="bullet">

 <it>The mind can be viewed as a device operating on bits of information according to formal rules.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2221%22])">21</ref></entry>
</list>

A distinction is usually made between the kind of high level symbols that directly correspond with objects in the world, such as &amp;lt;dog&amp;gt; and &amp;lt;tail&amp;gt; and the more complex "symbols" that are present in a machine like a <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link>. Early research into AI, called "good old fashioned artificial intelligence" (<link xlink:type="simple" xlink:href="../417/339417.xml">
GOFAI</link>) by <skilled_worker wordnetid="110605985" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<volunteer wordnetid="110759331" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<serviceman wordnetid="110582746" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../423/339423.xml">
John Haugeland</link></scholar>
</serviceman>
</causal_agent>
</alumnus>
</worker>
</intellectual>
</volunteer>
</person>
</philosopher>
</physical_entity>
</skilled_worker>
, focused on these kind of high level symbols.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2222%22])">22</ref></p>

</ss2>
<ss2>
<st>
Arguments against symbol processing</st>
<p>

These arguments show that human thinking does not consist (solely) of high level symbol manipulation. They do <it>not</it> show that artificial intelligence is impossible, only that more than symbol processing is required.</p>

<ss3>
<st>
 Lucas, Penrose and Gödel </st>

<p>

In 1931 <link>
Kurt Gödel</link> proved that it is always possible to create <link xlink:type="simple" xlink:href="../094/81094.xml">
statements</link> that a <link xlink:type="simple" xlink:href="../102/396102.xml">
formal system</link> (such as an AI program) could not prove. A human being, however, can (with some thought) see the truth of these "Gödel statements". This proved to philosopher <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../188/311188.xml">
John Lucas</link></associate>
</scholar>
</causal_agent>
</colleague>
</intellectual>
</person>
</philosopher>
</peer>
</physical_entity>
  that human reason would always be superior to machines.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2223%22])">23</ref> He wrote "<link>
Gödel's theorem</link> seems to me to prove that <link xlink:type="simple" xlink:href="../331/382331.xml">
mechanism</link> is false, that is, that minds cannot be explained as machines."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2224%22])">24</ref> 
<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../193/26193.xml">
Roger Penrose</link></scientist>
</person>
 expanded on this argument in his 1989 book <work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../005/433005.xml">
The Emperor's New Mind</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
, where he speculated that <link xlink:type="simple" xlink:href="../202/25202.xml">
quantum mechanical</link> processes inside individual neurons gave humans this special advantage over machines.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2225%22])">25</ref></p>
<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<professor wordnetid="110480730" confidence="0.9173553029164789">
<writer wordnetid="110794014" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../758/8758.xml">
Douglas Hofstadter</link></writer>
</professor>
</person>
, in his <link xlink:type="simple" xlink:href="../230/24230.xml">
Pulitzer prize</link> winning book <it>,</it> explains that these "Gödel-statements" always refer to the system itself, similar to the way the <link xlink:type="simple" xlink:href="../638/9638.xml">
Epimenides paradox</link> uses statements that refer to themselves, such as "this statement is false" or "I am lying".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2226%22])">26</ref> But, of course, the <link xlink:type="simple" xlink:href="../638/9638.xml">
Epimenides paradox</link> applies to anything that makes statements, whether they are machines <it>or</it> humans, even Lucas himself. Consider:
<list>
<entry level="1" type="bullet">

 <it>Lucas can't assert the truth of this statement.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2227%22])">27</ref></entry>
</list>

This statement is true but can't be asserted by Lucas. This shows that Lucas himself is subject to the same limits that he describes for machines, as are all people, and so <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../188/311188.xml">
Lucas</link></associate>
</scholar>
</causal_agent>
</colleague>
</intellectual>
</person>
</philosopher>
</peer>
</physical_entity>
's argument is pointless.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2228%22])">28</ref></p>
<p>

Further, <link xlink:type="simple" xlink:href="../166/3837166.xml">
Russell</link> and <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<employee wordnetid="110053808" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../666/566666.xml">
Norvig</link></associate>
</research_worker>
</employee>
</scientist>
</causal_agent>
</colleague>
</worker>
</person>
</peer>
</physical_entity>
 note that Gödel's argument only applies to what can theoretically be proved, given an infinite amount of memory and time. In practice, real machines (including humans) have finite resources and will have difficulty proving many theorems. It is not necessary to prove everything in order to be intelligent.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2229%22])">29</ref></p>

</ss3>
<ss3>
<st>
 Dreyfus: the primacy of unconscious skills </st>

<p>

<indent level="1">

<it>Main article: <link>
Dreyfus' critique of artificial intelligence</link></it>
</indent>

<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../926/952926.xml">
Hubert Dreyfus</link></philosopher>
 argued that human intelligence and expertise depended primarily on unconscious instincts rather than conscious symbolic manipulation, and argued that these unconscious skills would never be captured in formal rules.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2230%22])">30</ref> </p>
<p>

<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../926/952926.xml">
Dreyfus</link></philosopher>
's argument had been anticipated by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Turing</link></scientist>
</person>
 in his 1950 paper <link xlink:type="simple" xlink:href="../048/404048.xml">
Computing machinery and intelligence</link>, where he had classified this as the "argument from the informality of behavior."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2231%22])">31</ref> Turing argued in response that, just because we don't know the rules that govern a complex behavior, this does not mean that no such rules exist. He wrote: "we cannot so easily convince ourselves of the absence of complete laws of behaviour ... The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, 'We have searched enough. There are no such laws.'" <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2232%22])">32</ref></p>
<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../689/566689.xml">
Russell</link></scientist>
</person>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<employee wordnetid="110053808" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../666/566666.xml">
Norvig</link></associate>
</research_worker>
</employee>
</scientist>
</causal_agent>
</colleague>
</worker>
</person>
</peer>
</physical_entity>
 point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the "rules" that govern unconscious reasoning.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2233%22])">33</ref> The <link xlink:type="simple" xlink:href="../720/3479720.xml">
situated</link> movement in <link xlink:type="simple" xlink:href="../673/46673.xml">
robotics</link> research attempts to capture our unconscious skills at perception and attention.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2234%22])">34</ref> <link xlink:type="simple" xlink:href="../306/1563306.xml">
Computational intelligence</link> paradigms, such as <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural net</link>s, <link xlink:type="simple" xlink:href="../837/190837.xml">
evolutionary algorithm</link>s and so on are mostly directed at simulated unconscious reasoning and learning. Research into <link xlink:type="simple" xlink:href="../339/2239339.xml">
commonsense knowledge</link> has focused on reproducing the "background" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation or "<link xlink:type="simple" xlink:href="../417/339417.xml">
GOFAI</link>", towards new models that are intended to capture more of our <it>unconscious</it> reasoning. Historian and AI researcher <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../005/11702005.xml">
Daniel Crevier</link></educator>
</professional>
</adult>
</academician>
</causal_agent>
</person>
</physical_entity>
 wrote that "time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2235%22])">35</ref></p>

</ss3>
</ss2>
</ss1>
</sec>
<sec>
<st>
Can a machine have a mind, consciousness and mental states?</st>
<p>

This is a philosophical question, related to the <link xlink:type="simple" xlink:href="../263/30263.xml">
problem of other minds</link> and the <link xlink:type="simple" xlink:href="../216/634216.xml">
hard problem of consciousness</link>. The question revolves around a position defined by <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
John Searle</link></philosopher>
 as "strong AI":
<list>
<entry level="1" type="bullet">

 <it>A physical symbol system can have a mind and mental states.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></entry>
</list>

Searle distinguished this position from what he called "weak AI":
<list>
<entry level="1" type="bullet">

 <it>A physical symbol system can act intelligently.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></entry>
</list>

<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
Searle</link></philosopher>
 introduced the terms to isolate strong AI from weak AI so he could focus on what he thought was the more interesting and debatable issue. He argued that <it>even if we assume</it> that we had a computer program that acted exactly like a human mind, there would still be a difficult philosophical question that needed to be answered.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>
<p>

Neither of Searle's two positions are of great concern to AI research, since they do not directly answer the question "can a machine display general intelligence?" (unless it can also be shown that consciousness is <it>necessary</it> for intelligence).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2236%22])">36</ref> <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Turing</link></scientist>
</person>
 wrote "I do not wish to give the impression that I think there is no mystery about consciousness ... [b]ut I do not think these mysteries necessarily need to be solved before we can answer the question [of whether machines can think]."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2237%22])">37</ref> <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../689/566689.xml">
Russell</link></scientist>
</person>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<employee wordnetid="110053808" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../666/566666.xml">
Norvig</link></associate>
</research_worker>
</employee>
</scientist>
</causal_agent>
</colleague>
</worker>
</person>
</peer>
</physical_entity>
 agree: "Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2238%22])">38</ref></p>
<p>

Before we can answer this question, we must be clear what we mean by "minds", "mental states" and "consciousness".
</p>
<ss1>
<st>
 Consciousness, minds, mental states, meaning </st>

<p>

<image width="150px" src="RobertFuddBewusstsein17Jh.png" type="thumb">
<caption>

Representation of consciousness from the 17th century.
</caption>
</image>

The words "<link xlink:type="simple" xlink:href="../378/19378.xml">
mind</link>" and "<link xlink:type="simple" xlink:href="../664/5664.xml">
consciousness</link>" are used by different communities in different ways. Some <link xlink:type="simple" xlink:href="../742/21742.xml">
new age</link> thinkers, for example, use the word "consciousness" to describe something similar to <link xlink:type="simple" xlink:href="../531/13531.xml">
Bergson</link>'s "<link>
élan vital</link>": an invisible, energetic fluid that permeates life and especially the mind. <link xlink:type="simple" xlink:href="../787/26787.xml">
Science fiction</link> writers use the word to describe some <link xlink:type="simple" xlink:href="../081/48081.xml">
essential</link> property that makes us human: a machine or alien that is "conscious" will be presented as a fully human character, with intelligence, desires, will, insight, pride and so on. (Science fiction writers also use the words "sentience", "sapience," "self-awareness" or "ghost" (as in the <it><link xlink:type="simple" xlink:href="../914/12914.xml">
Ghost in the Shell</link></it> manga and anime series) to describe this essential human property.) For others, the words "mind" or "consciousness" are used as a kind of secular synonym for the <link xlink:type="simple" xlink:href="../297/28297.xml">
soul</link>.</p>
<p>

For <link xlink:type="simple" xlink:href="../155/13692155.xml">
philosophers</link>, <link xlink:type="simple" xlink:href="../245/21245.xml">
neuroscientists</link> and <link xlink:type="simple" xlink:href="../626/5626.xml">
cognitive scientists</link>, the words are used in a way that is both more precise and more mundane: they refer to the familiar, everyday experience of having a "thought in your head", like a perception, a dream, an intention or a plan, and to the way we <it>know</it> something, or <it>mean</it> something or <it>understand</it> something. "It's not hard to give a commonsense definition of consciousness" observes philosopher <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
John Searle</link></philosopher>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2239%22])">39</ref> What is mysterious and fascinating is not so much <it>what</it> it is but <it>how</it> it is: how does a lump of fatty tissue and electricity give rise to this (familiar) experience of perceiving, meaning or thinking? </p>
<p>

<link xlink:type="simple" xlink:href="../155/13692155.xml">
Philosopher</link>s call this the <link xlink:type="simple" xlink:href="../216/634216.xml">
hard problem of consciousness</link>. It is the latest version of a classic problem in the <link xlink:type="simple" xlink:href="../483/6880483.xml">
philosophy of mind</link> called the "<link xlink:type="simple" xlink:href="../483/6880483.xml">
mind-body problem</link>."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2240%22])">40</ref> A related problem is the problem of <it>meaning</it> or <it>understanding</it> (which philosophers call "<link xlink:type="simple" xlink:href="../483/184483.xml">
intentionality</link>"): what is the connection between our <it>thoughts</it> (i.e. patterns of neurons) and <it>what we are thinking about</it> (i.e. objects and situations out in the world)? A third issue is the problem of <it>experience</it> (or "<link xlink:type="simple" xlink:href="../944/2798944.xml">
phenomenology</link>"): If two people see the same thing, do they have the same experience? Or are there things "inside their head" (called "<link xlink:type="simple" xlink:href="../965/165965.xml">
qualia</link>") that can be different from person to person?<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2241%22])">41</ref></p>
<p>

<link xlink:type="simple" xlink:href="../202/1307202.xml">
Neurobiologist</link>s believe all these problems will be solved as we begin to identify the <link xlink:type="simple" xlink:href="../264/18345264.xml">
neural correlates of consciousness</link>: the actual machinery in our heads that creates the mind, experience and understanding. Even the harshest critics of <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> agree that the brain is just a machine, and that consciousness and intelligence are the result of physical processes in the brain.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2242%22])">42</ref> The difficult philosophical question is this: can a computer program, running on a digital machine that shuffles the binary digits of zero and one, duplicate the ability of the <link xlink:type="simple" xlink:href="../264/18345264.xml">
neurons</link> to create <link xlink:type="simple" xlink:href="../378/19378.xml">
mind</link>s, with <link>
mental state</link>s (like understanding or perceiving), and ultimately, the experience of <link xlink:type="simple" xlink:href="../664/5664.xml">
consciousness</link>?</p>

</ss1>
<ss1>
<st>
 Arguments that a computer can't have a mind and mental states </st>

<ss2>
<st>
 Searle's Chinese room </st>

<p>

<indent level="1">

<it>Main article: <work wordnetid="100575741" confidence="0.8">
<argument wordnetid="106648724" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../216/6216.xml">
Chinese room</link></activity>
</psychological_feature>
</research>
</act>
</investigation>
</evidence>
</experiment>
</indication>
</event>
</scientific_research>
</argument>
</work>
</it>
</indent>
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
John Searle</link></philosopher>
 asks us to consider a thought experiment: suppose we have written a computer program that passes the Turing Test and demonstrates "general intelligent action." Suppose, specifically that the program can converse in fluent Chinese. Write the program on 3x5 cards and give them to an ordinary person. Lock the person into a room and have him follow the instructions on the cards. He will copy out Chinese characters and pass them in and out of the room through a slot. From the outside, it will appear that the Chinese room contains a fully intelligent person who speaks Chinese. The question is this: is there anyone (or anything) in the room that understands Chinese? That is, is there anything that has the <link>
mental state</link> of <link xlink:type="simple" xlink:href="../180/216180.xml">
understanding</link>, or which has <link xlink:type="simple" xlink:href="../664/5664.xml">
conscious</link> <link xlink:type="simple" xlink:href="../696/491696.xml">
awareness</link> of what is being discussed in Chinese? The man is clearly not aware. The room can't be aware. The <it>cards</it> certainly aren't aware. <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
Searle</link></philosopher>
 concludes that the <work wordnetid="100575741" confidence="0.8">
<argument wordnetid="106648724" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../216/6216.xml">
Chinese room</link></activity>
</psychological_feature>
</research>
</act>
</investigation>
</evidence>
</experiment>
</indication>
</event>
</scientific_research>
</argument>
</work>
, or <it>any</it> other <link xlink:type="simple" xlink:href="../999/2685999.xml">
physical symbol system</link>, cannot have a <link xlink:type="simple" xlink:href="../378/19378.xml">
mind</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2243%22])">43</ref></p>
<p>

<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
Searle</link></philosopher>
 goes on to argue that actual <link>
mental state</link>s and <link xlink:type="simple" xlink:href="../664/5664.xml">
consciousness</link> require (yet to be described) "actual physical-chemical properties of actual human brains."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2244%22])">44</ref> He argues there are special "causal properties" of <link xlink:type="simple" xlink:href="../717/3717.xml">
brain</link>s and <link xlink:type="simple" xlink:href="../120/21120.xml">
neuron</link>s that gives rise to <link xlink:type="simple" xlink:href="../378/19378.xml">
mind</link>s: in his words "brains cause minds."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2245%22])">45</ref></p>

</ss2>
<ss2>
<st>
 Related arguments: Leibniz' mill, Block's telephone exchange and blockhead  </st>

<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../281/12281.xml">
Gottfried Leibniz</link></philosopher>
</person>
 made essentially the same argument as <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
Searle</link></philosopher>
 in 1714, using the thought experiment of expanding the brain until it was the size of a <link xlink:type="simple" xlink:href="../784/166784.xml">
mill</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2246%22])">46</ref> In 1974, <link xlink:type="simple" xlink:href="../348/6359348.xml">
Lawrence Davis</link> imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../592/1134592.xml">
Ned Block</link></philosopher>
 envisioned the entire population of China involved in such a brain simulation. This thought experiment is called "the Chinese Nation" or "the Chinese Gym".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2247%22])">47</ref> <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../592/1134592.xml">
Ned Block</link></philosopher>
 also proposed his "<link xlink:type="simple" xlink:href="../969/738969.xml">
blockhead</link>" argument, which is a version of the <work wordnetid="100575741" confidence="0.8">
<argument wordnetid="106648724" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../216/6216.xml">
Chinese room</link></activity>
</psychological_feature>
</research>
</act>
</investigation>
</evidence>
</experiment>
</indication>
</event>
</scientific_research>
</argument>
</work>
 in which the program has been <link xlink:type="simple" xlink:href="../871/25871.xml">
re-factored</link> into a simple set of rules of the form "see this, do that", removing all mystery from the program.</p>

</ss2>
<ss2>
<st>
 Responses to the Chinese Room </st>
<p>

Responses to the Chinese room emphasize several different points. 
<list>
<entry level="1" type="number">

<b>The systems reply</b> and the <b>virtual mind reply</b>:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2248%22])">48</ref> This reply argues that <it>the system</it>, including the man, the program, the room, and the cards, is what understands Chinese. Searle claims that the man in the room is the only thing which could possibly "have a mind" or "understand", but others disagree, arguing that it is possible for there to be <it>two</it> minds in the same physical place, similar to the way a computer can simultaneously "be" two machines at once: one physical (like a <link xlink:type="simple" xlink:href="../979/19006979.xml">
Macintosh</link>) and one "<link xlink:type="simple" xlink:href="../353/32353.xml">
virtual</link>" (like a <link xlink:type="simple" xlink:href="../236/33236.xml">
word processor</link>).</entry>
<entry level="1" type="number">

<b>Speed, power and complexity replies</b>:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2249%22])">49</ref> Several critics point out that the man in the room would probably take millions of years to respond to a simple question, and would require "filing cabinets" of astronomical proportions. This brings the clarity of Searle's intuition into doubt.</entry>
<entry level="1" type="number">

<b>Robot reply</b>:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2250%22])">50</ref> To truly understand, some believe the Chinese Room needs eyes and hands. <link xlink:type="simple" xlink:href="../556/298556.xml">
Hans Moravec</link> writes: 'If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2251%22])">51</ref></entry>
<entry level="1" type="number">

<b>Brain simulator reply</b>:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2252%22])">52</ref> What if the program simulates the sequence of nerve firings at the synapses of an actual brain of an actual Chinese speaker? The man in the room would be simulating an actual brain. This is a variation on the "systems reply" that appears more plausible because "the system" now clearly operates like a human brain, which strengthens the intuition that there is something besides the man in the room that could understand Chinese.</entry>
<entry level="1" type="number">

<b>Other minds reply</b> and the <b>epiphenomena reply</b>:<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2253%22])">53</ref> Several people have noted that Searle's argument is just a version of the <link xlink:type="simple" xlink:href="../263/30263.xml">
problem of other minds</link>, applied to machines. Since it's difficult to decide if people are "actually" thinking, we shouldn't be surprised that it's difficult to answer the same question about machines. A related idea is that Searle's "causal properties" of neurons are <link xlink:type="simple" xlink:href="../604/59604.xml">
epiphenomenal</link>: they have no effect on the real world. Why would natural selection create them in the first place, if they make no difference to behavior?</entry>
</list>
</p>

</ss2>
</ss1>
</sec>
<sec>
<st>
Is thinking a kind of computation?</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../220/3951220.xml">
computational theory of mind</link></it>
</indent>

This issue is of primary importance to <link xlink:type="simple" xlink:href="../626/5626.xml">
cognitive scientists</link>, who study the nature of human thinking and problem solving. </p>
<p>

The <link xlink:type="simple" xlink:href="../220/3951220.xml">
computational theory of mind</link> or "<link xlink:type="simple" xlink:href="../220/3951220.xml">
computationalism</link>" claims that the relationship between mind and body is similar (if not identical) to the relationship between a <it>running program</it> and a computer. The idea has philosophical roots in <link xlink:type="simple" xlink:href="../823/29823.xml">
Hobbes</link> (who claimed reasoning was "nothing more than reckoning"), <link xlink:type="simple" xlink:href="../281/12281.xml">
Leibniz</link> (who attempted to create a logical calculus of all human ideas),  <link xlink:type="simple" xlink:href="../717/58717.xml">
Hume</link> (who thought perception could be reduced to "atomic impressions") and even <link xlink:type="simple" xlink:href="../631/14631.xml">
Kant</link> (who analyzed all experience as controlled by formal rules).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2254%22])">54</ref> The latest version is associated with philosophers <person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../233/75233.xml">
Hilary Putnam</link></philosopher>
</person>
 and <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../994/427994.xml">
Jerry Fodor</link></philosopher>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2255%22])">55</ref></p>
<p>

This question bears on our earlier questions: if the human brain is a kind of computer then computers can be both intelligent and conscious, answering both the practical and philosophical questions of AI. In terms of the practical question of AI ("Can a machine display general intelligence?"), some versions of <link xlink:type="simple" xlink:href="../220/3951220.xml">
computationalism</link> make the claim that (as <link xlink:type="simple" xlink:href="../823/29823.xml">
Hobbes</link> wrote):
<list>
<entry level="1" type="bullet">

 <it>Reasoning is nothing but reckoning</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></entry>
</list>

In other words, our intelligence derives from a form of <it>calculation</it>, similar to <link xlink:type="simple" xlink:href="../118/3118.xml">
arithmetic</link>. This is the <link xlink:type="simple" xlink:href="../999/2685999.xml">
physical symbol system</link> hypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI ("Can a machine have mind, mental states and consciousness?"), most versions of <link xlink:type="simple" xlink:href="../220/3951220.xml">
computationalism</link> claim that (as <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../426/179426.xml">
Stevan Harnad</link></educator>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
 characterizes it):
<list>
<entry level="1" type="bullet">

 <it>Mental states are just implementations of (the right) computer programs</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2256%22])">56</ref></entry>
</list>

This is <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
John Searle</link></philosopher>
's "strong AI" discussed above, and it is the real target of the <link>
Chinese Room</link> argument (according to <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../426/179426.xml">
Harnad</link></educator>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2256%22])">56</ref></p>

</sec>
<sec>
<st>
 Other related questions </st>

<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Alan Turing</link></scientist>
</person>
 noted that there are many arguments of the form "a machine will never do X", where X can be many things, such as:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2257%22])">57</ref>
Turing argues that these objections are often based on naive assumptions about the versatility of machines or are "disguised forms of the argument from consciousness". Writing a program that exhibits one of these behaviors "will not make much of an impression."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2257%22])">57</ref> All of these arguments are tangential to the basic premise of AI, unless it can be shown that one of these traits is essential for general intelligence.</p>

<ss1>
<st>
 Can a machine have emotions? </st>

<p>

<link xlink:type="simple" xlink:href="../556/298556.xml">
Hans Moravec</link> believes "I think robots in general will be quite emotional about being nice people"<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2258%22])">58</ref>  and describes emotions in terms of the behaviors they cause. Fear is a source of urgency. Empathy is a necessary component of good <link xlink:type="simple" xlink:href="../516/13516.xml">
human computer interaction</link>. He says robots "will try to please you in an apparently selfless manner because it will get a thrill out of this positive reinforcement. You can interpret this as a kind of love."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2258%22])">58</ref> <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../005/11702005.xml">
Daniel Crevier</link></educator>
</professional>
</adult>
</academician>
</causal_agent>
</person>
</physical_entity>
 writes "Moravec's point is that emotions are just devices for channeling behavior in a direction beneficial to the survival of one's species."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2259%22])">59</ref></p>
<p>

The question of whether the machine <it>actually feels</it> an emotion, or whether it merely acts as if feeling an emotion is the philosophical question, "can a machine be conscious?" in another form.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2237%22])">37</ref></p>

</ss1>
<ss1>
<st>
 Can a machine be self aware? </st>

<p>

"Self awareness", as noted above, is sometimes used by <link xlink:type="simple" xlink:href="../787/26787.xml">
science fiction</link> writers as a name for the <link xlink:type="simple" xlink:href="../081/48081.xml">
essential</link> human property that makes a character fully human. <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Turing</link></scientist>
</person>
 strips away all other properties of human beings and reduces the question to "can a machine be the subject of its own thought?" Can it <it>think about itself</it>? Viewed in this way, it is obvious that a program can be written that can report on its own internal states, such as a <link xlink:type="simple" xlink:href="../485/50485.xml">
debugger</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2257%22])">57</ref></p>

</ss1>
<ss1>
<st>
 Can a machine be original or creative? </st>

<p>

Turing reduces this to the question of whether a machine can “take us by surprise" and argues that this is obviously true, as any programmer can attest.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2260%22])">60</ref> He notes that, with enough storage capacity, a computer can behave in an astronomical number of different ways.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2261%22])">61</ref> It must be possible, even trivial, for a computer that can represent ideas to combine them in new ways. (<physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../991/99991.xml">
Douglas Lenat</link></associate>
</research_worker>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
's <link xlink:type="simple" xlink:href="../279/253279.xml">
Automated Mathematician</link>, as one example, combined ideas to discover new mathematical truths.)</p>

</ss1>
<ss1>
<st>
 Can a machine have a soul? </st>

<p>

Finally, those who believe in the existence of a soul would argue that
<list>
<entry level="1" type="bullet">

 <it>Thinking is a function of man’s immortal soul''</it></entry>
</list>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Alan Turing</link></scientist>
</person>
 called this “the theological objection” and writes
In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing mansions for the souls that he creates.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2262%22])">62</ref></p>

</ss1>
</sec>
<sec>
<st>
 See also </st>

<p>

<table style="background-color:transparent;table-layout:fixed;" width="100%" cellpadding="0" border="0" cellspacing="0">
<row valign="top">
<col>
<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../164/1164.xml">
Artificial intelligence</link></entry>
<entry level="1" type="bullet">

<division wordnetid="108220714" confidence="0.8">
<administrative_unit wordnetid="108077292" confidence="0.8">
<branch wordnetid="108401248" confidence="0.8">
<link xlink:type="simple" xlink:href="../868/4522868.xml">
Philosophy of information</link></branch>
</administrative_unit>
</division>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../483/6880483.xml">
Philosophy of mind</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../717/3717.xml#xpointer(//*[./st=%22Other+matters%22])">
Brain (other matters section)</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../220/3951220.xml">
Computational theory of mind</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../308/461308.xml">
Functionalism</link></entry>
</list>
</col>
<col>
<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../840/43840.xml">
Turing Test</link></entry>
<entry level="1" type="bullet">

<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../395/1908395.xml">
Artificial brain</link></instrumentality>
</artifact>
</system>
</entry>
<entry level="1" type="bullet">

<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../999/2685999.xml">
Physical symbol system</link></instrumentality>
</artifact>
</system>
</entry>
<entry level="1" type="bullet">

<link>
Dreyfus' critique of artificial intelligence</link></entry>
<entry level="1" type="bullet">

<work wordnetid="100575741" confidence="0.8">
<argument wordnetid="106648724" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<indication wordnetid="106797169" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<evidence wordnetid="106643408" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../216/6216.xml">
Chinese room</link></activity>
</psychological_feature>
</research>
</act>
</investigation>
</evidence>
</experiment>
</indication>
</event>
</scientific_research>
</argument>
</work>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../048/404048.xml">
Computing Machinery and Intelligence</link></entry>
</list>
</col>
</row>
</table>
</p>


</sec>
<sec>
<st>
 Notes </st>

<p>

<reflist>
<entry id="1">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;947 define the philosophy of AI as consisting of the first two questions, and the additional question of the <link xlink:type="simple" xlink:href="../583/13659583.xml">
ethics of artificial intelligence</link>. <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFFearn2007%22])">
Fearn 2007</link>, p.&nbsp;55 writes "In the current literature, philosophy has to chief roles: to determine whether or not such machines would be conscious, and, second, to predict whether or not such machines are possible." The last question bears on the first two.</entry>
<entry id="2">
This is a paraphrase of the essential point of the <link xlink:type="simple" xlink:href="../840/43840.xml">
Turing Test</link>. <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHaugeland1985%22])">
Haugeland 1985</link>, pp.&nbsp;6-9, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, p.&nbsp;24, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, pp.&nbsp;2-3 and 948</entry>
<entry id="3">
 <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFMcCarthyMinskyRochesterShannon1955%22])">
McCarthy et al. 1955</link>. This assertion was printed in the program for the <link xlink:type="simple" xlink:href="../646/1124646.xml">
Dartmouth Conference</link> of 1956, widely considered the "birth of AI."also <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, p.&nbsp;28</entry>
<entry id="4">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFNewellSimon1976%22])">
Newell &amp; Simon 1976</link> and <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;18</entry>
<entry id="5">
This version is from <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1999%22])">
Searle (1999)</link>, and is also quoted in <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDennett1991%22])">
Dennett 1991</link>, p.&nbsp;435.  Searle's original formulation was "The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states."   <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link>, p.&nbsp;1)</cite>.  Strong AI is defined similarly by <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig (2003</link>, p.&nbsp;947): "The assertion that machines could possibly act intelligently (or, perhaps better, act as if they were intelligent) is called the 'weak AI' hypothesis by philosophers, and the assertion that machines that do so are actually thinking (as opposed to simulating thinking) is called the 'strong AI' hypothesis."</entry>
<entry id="6">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHobbes1651%22])">
Hobbes 1651</link>, chpt. 5</entry>
<entry id="7">
See <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;3, where they make the distinction between <it>acting</it> rationally and <it>being</it> rational, and define AI as the study of the former.</entry>
<entry id="8">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> and see <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;948, where they call his paper "famous" and write "Turing examined a wide variety of possible objections to the possibility of intelligent machines, including virtually all of those that have been raised in the half century since his paper appeared."</entry>
<entry id="9">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under "The Argument from Consciousness"</entry>
<entry id="10">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;3</entry>
<entry id="11">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;4-5, 32, 35, 36 and 56</entry>
<entry id="12">
Russell and Norvig would prefer the word "rational" to "intelligent".</entry>
<entry id="13">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, p.&nbsp;125</entry>
<entry id="14">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFPittsMcCullough1943%22])">
Pitts &amp; McCullough 1943</link></entry>
<entry id="15">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFMoravec1988%22])">
Moravec 1988</link></entry>
<entry id="17">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../926/952926.xml">
Hubert Dreyfus</link></philosopher>
 writes: "In general, by accepting the fundamental assumptions that the nervous system is part of the physical world and that all physical processes can described in a mathematical formalism which can in turn be manipulated by a digital computer, one can arrive at the strong claim that the behavior which results from human 'information processing,' whether directly formalizable or not, can always be indirectly reproduced on a digital machine."  <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDreyfus1972%22])">
Dreyfus 1972</link>, pp.&nbsp;194-5)</cite>. <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
John Searle</link></philosopher>
 writes: "Could a man made machine think? Assuming it possible produce artificially a machine with a nervous system, ... the answer to the question seems to be obviously, yes ... Could a digital computer think? If by 'digital computer' you mean anything at all that has a level of description where it can be correctly described as the instantiation of a computer program, then again the answer is, of course, yes, since we are the instantiations of any number of computer programs, and we can think."  <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link>, p.&nbsp;11)</cite></entry>
<entry id="16">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFKurzweil2005%22])">
Kurzweil 2005</link>, p.&nbsp;262. Also see <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig%22])">
Russell Norvig</link>, p.&nbsp;957 and <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, pp.&nbsp;271 and 279. The most extreme form of this argument (the brain replacement scenario) was put forward by <link>
Clark Glymour</link> in the mid-70s and was touched on by <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../182/7644182.xml">
Zenon Pylyshyn</link></philosopher>
 and <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
John Searle</link></philosopher>
 in 1980</entry>
<entry id="19">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link>, p.&nbsp;14</entry>
<entry id="18">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link>, p.&nbsp;7</entry>
<entry id="21">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDreyfus1979%22])">
Dreyfus 1979</link>, p.&nbsp;156</entry>
<entry id="20">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
Searle</link></philosopher>
 writes "I like the straight forwardness of the claim." <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link>, p.&nbsp;4</entry>
<entry id="23">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFLucas1961%22])">
Lucas 1961</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, pp.&nbsp;949-950, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHofstadter1979%22])">
Hofstadter 1979</link>, pp.&nbsp;471-473,476-477, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under “The Argument from Mathematics”</entry>
<entry id="22">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHaugeland1985%22])">
Haugeland 1985</link>, p.&nbsp;5</entry>
<entry id="25">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFPenrose1989%22])">
Penrose 1989</link></entry>
<entry id="24">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFLucas1961%22])">
Lucas 1961</link>, p.&nbsp;57-9</entry>
<entry id="27">
According to <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHofstadter1979%22])">
Hofstadter 1979</link>, p.&nbsp;476-477, this statement was first proposed by <link>
C. H. Whitely</link></entry>
<entry id="26">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHofstadter1979%22])">
Hofstadter 1979</link></entry>
<entry id="29">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;950. They point out that real machines with finite memory can be modeled using <link xlink:type="simple" xlink:href="../983/10983.xml">
first order logic</link>, which is formally <link xlink:type="simple" xlink:href="../800/54800.xml">
decidable</link>, and Gödel's argument does not apply to them at all.</entry>
<entry id="28">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHofstadter1979%22])">
Hofstadter 1979</link>, pp.&nbsp;476-477, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;950, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under “The Argument from Mathematics” where he writes “although it is established that there are limitations to the powers of any particular machine, it has only been stated, without sort of proof, that no such limitations apply to the human intellect.”</entry>
<entry id="31">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;950-51</entry>
<entry id="30">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDreyfus1972%22])">
Dreyfus 1972</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDreyfus1979%22])">
Dreyfus 1979</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDreyfusDreyfus1986%22])">
Dreyfus &amp; Dreyfus 1986</link>. See also <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, pp.&nbsp;950-952, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993120-132%22])">
Crevier &amp; 1993 120-132</link> and <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHearn2007%22])">
Hearn 2007</link>, pp.&nbsp;50-51</entry>
<entry id="34">
See <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFBrooks1990%22])">
Brooks 1990</link> and <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFMoravec1988%22])">
Moravec 1988</link></entry>
<entry id="35">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, p.&nbsp;125</entry>
<entry id="32">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under "(8) The Argument from the Informality of Behavior"</entry>
<entry id="33">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;52</entry>
<entry id="38">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;947</entry>
<entry id="39">
"[P]eople always tell me it was very hard to define consciousness, but I think if you're just looking for the kind of commonsense definition that you get at the beginning of the investigation, and not at the hard nosed scientific definition that comes at the end, it's not hard to give commonsense definition of consciousness." <weblink xlink:type="simple" xlink:href="http://www.abc.net.au/rn/philosopherszone/stories/2006/1639491.htm">
The Philosopher's Zone: The question of consciousness</weblink>. Also see <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDennett1991%22])">
Dennett 1991</link></entry>
<entry id="36">
There are a few researchers who believe that consciousness is an essential element in intelligence, such as <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../583/632583.xml">
Igor Aleksander</link></associate>
</educator>
</research_worker>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../705/1456705.xml">
Stan Franklin</link></educator>
</research_worker>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../946/17651946.xml">
Ron Sun</link></psychologist>
</research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
 and <link>
Pentti Haikonen</link>, although their definition of "consciousness" strays very close to "intelligence." See <link xlink:type="simple" xlink:href="../552/195552.xml">
artificial consciousness</link>.</entry>
<entry id="37">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under “(4) The Argument from Consciousness”. See also <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig%22])">
Russell Norvig</link>, p.&nbsp;952-3, where they identify Searle's argument with Turing's "Argument from Consciousness."</entry>
<entry id="42">
For example, <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../079/147079.xml">
John Searle</link></philosopher>
 writes: "Can a machine think? The answer is, obvious, yes. We are precisely such machines."  <cite class="inline">(<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link>, p.&nbsp;11)</cite></entry>
<entry id="43">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link>. See also <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link>, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, pp.&nbsp;958-960, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, pp.&nbsp;269-272 and <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHearn2007%22])">
Hearn 2007</link>, pp.&nbsp;43-50</entry>
<entry id="40">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFBlackmore2005%22])">
Blackmore 2005</link>, p.&nbsp;2</entry>
<entry id="41">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;954-956</entry>
<entry id="46">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link>, 2.1, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFLeibniz1714%22])">
Leibniz 1714</link>, 17</entry>
<entry id="47">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link>, 2.3</entry>
<entry id="44">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link>, p.&nbsp;13</entry>
<entry id="45">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1984%22])">
Searle 1984</link></entry>
<entry id="51">
Quoted in <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, p.&nbsp;272</entry>
<entry id="50">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link> under "2. The Robot Reply (Yale)". <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link>, 4.3 ascribes this position to <link xlink:type="simple" xlink:href="../868/1499868.xml">
Margaret Boden</link>, <link>
Tim Crane</link>, <person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../756/8756.xml">
Daniel Dennett</link></philosopher>
</person>
, <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../994/427994.xml">
Jerry Fodor</link></philosopher>
, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../426/179426.xml">
Stevan Harnad</link></educator>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
, <link xlink:type="simple" xlink:href="../556/298556.xml">
Hans Moravec</link> and <link xlink:type="simple" xlink:href="../777/192777.xml">
Georges Rey</link></entry>
<entry id="49">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link>, 4.2 ascribes this position to <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../592/1134592.xml">
Ned Block</link></philosopher>
, <person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../756/8756.xml">
Daniel Dennett</link></philosopher>
</person>
, <link>
Tim Maudlin</link>, <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../221/258221.xml">
David Chalmers</link></philosopher>
, <person wordnetid="100007846" confidence="0.9508927676800064">
<writer wordnetid="110794014" confidence="0.9173553029164789">
<link xlink:type="simple" xlink:href="../340/44340.xml">
Steven Pinker</link></writer>
</person>
, <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../385/425385.xml">
Patricia Churchland</link></philosopher>
 and others.</entry>
<entry id="48">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link> under "1. THe Systems Reply (Berkeley)", <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, p.&nbsp;269, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRussellNorvig2003%22])">
Russell &amp; Norvig 2003</link>, p.&nbsp;959, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link>, 4.1. Among those who hold to the "system" position (according to Cole) are <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../592/1134592.xml">
Ned Block</link></philosopher>
, <physical_entity wordnetid="100001930" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<historian wordnetid="110177150" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../942/5717942.xml">
Jack Copeland</link></educator>
</scholar>
</professional>
</historian>
</adult>
</academician>
</causal_agent>
</intellectual>
</person>
</philosopher>
</physical_entity>
, <person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../756/8756.xml">
Daniel Dennett</link></philosopher>
</person>
, <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../994/427994.xml">
Jerry Fodor</link></philosopher>
, <skilled_worker wordnetid="110605985" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<volunteer wordnetid="110759331" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<serviceman wordnetid="110582746" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../423/339423.xml">
John Haugeland</link></scholar>
</serviceman>
</causal_agent>
</alumnus>
</worker>
</intellectual>
</volunteer>
</person>
</philosopher>
</physical_entity>
</skilled_worker>
, <link xlink:type="simple" xlink:href="../984/25984.xml">
Ray Kurzweil</link> and <link xlink:type="simple" xlink:href="../777/192777.xml">
Georges Rey</link>. Those who have defended the "virtual mind" reply include <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../639/19639.xml">
Marvin Minsky</link></scientist>
</person>
, <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../462/300462.xml">
Alan Perlis</link></scientist>
</person>
, <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../221/258221.xml">
David Chalmers</link></philosopher>
, <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../592/1134592.xml">
Ned Block</link></philosopher>
 and J. Cole (again, according to <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link>)</entry>
<entry id="55">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHorst2005%22])">
Horst 2005</link></entry>
<entry id="54">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFDreyfus1979%22])">
Dreyfus 1979</link>, p.&nbsp;156, <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHaugeland%22])">
Haugeland</link>, pp.&nbsp;15-44</entry>
<entry id="53">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link> under "5. The Other Minds Reply", <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link>, 4.4. <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> makes this reply under "(4) The Argument from Consciousness." Cole ascribes this position to <person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../756/8756.xml">
Daniel Dennett</link></philosopher>
</person>
 and <link xlink:type="simple" xlink:href="../556/298556.xml">
Hans Moravec</link>.</entry>
<entry id="52">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFSearle1980%22])">
Searle 1980</link> under "3. The Brain Simulator Reply (Berkeley and M.I.T.)" <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCole2004%22])">
Cole 2004</link> ascribes this position to <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../017/5341017.xml">
Paul</link></philosopher>
 and <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../385/425385.xml">
Patricia Churchland</link></philosopher>
 and <link xlink:type="simple" xlink:href="../984/25984.xml">
Ray Kurzweil</link></entry>
<entry id="59">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, p.&nbsp;266</entry>
<entry id="58">
Quoted in <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFCrevier1993%22])">
Crevier 1993</link>, p.&nbsp;266</entry>
<entry id="57">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under "(5) Arguments from Various Disabilities"</entry>
<entry id="56">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFHarnad2001%22])">
Harnad 2001</link></entry>
<entry id="62">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under "(1) The Theological Objection”, although it should be noted that he also writes “I am not very impressed with theological arguments whatever they may be used to support”</entry>
<entry id="61">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under "(5) Argument from Various Disabilities"</entry>
<entry id="60">
<link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFTuring1950%22])">
Turing 1950</link> under "(6) Lady Lovelace's Objection"</entry>
</reflist>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

  <cite id="CITEREFBlackmore2005" style="font-style:normal"><link>
Blackmore, Susan</link>&#32;(2005),&#32;<it>Consciousness: A Very Short Introduction</it>, Oxford University Press</cite>&nbsp; </entry>
<entry level="1" type="bullet">

  <cite id="CITEREFBrooks1990" style="font-style:normal"><link>
Brooks, Rodney</link>&#32;(1990),&#32;"<weblink xlink:type="simple" xlink:href="http://people.csail.mit.edu/brooks/papers/elephants.pdf">
Elephants Don't Play Chess</weblink>",&#32;<it>Robotics and Autonomous Systems</it>&#32;<b>6</b>:  3-15, .&#32;Retrieved on 29 August 2007</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFCole2004" style="font-style:normal">Cole, David&#32;(Fall 2004),&#32;<weblink xlink:type="simple" xlink:href="http://plato.stanford.edu/archives/fall2004/entries/chinese-room/">
"The Chinese Room Argument"</weblink>, in&#32;Zalta, Edward N.,&#32;<it>The Stanford Encyclopedia of Philosophy</it>, </cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFCrevier1993" style="font-style:normal"><link>
Crevier, Daniel</link>&#32;(1993),&#32;<it>AI: The Tumultuous Search for Artificial Intelligence</it>, New York, NY: BasicBooks, ISBN 0-465-02997-3</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFDennett1991" style="font-style:normal"><link>
Dennett, Daniel</link>&#32;(1991),&#32;<it><book wordnetid="106410904" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../212/157212.xml">
Consciousness Explained</link></book>
</it>, The Penguin Press, ISBN 0-7139-9037-6</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFDreyfus1972" style="font-style:normal"><link>
Dreyfus, Hubert</link>&#32;(1972),&#32;<it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../856/8919856.xml">
What Computers Can't Do</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
</it>, New York: MIT Press, ISBN 0060110821</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFDreyfus1979" style="font-style:normal"><link>
Dreyfus, Hubert</link>&#32;(1979),&#32;<it>What Computers Still Can't Do</it>, New York: MIT Press</cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFDreyfusDreyfus1986" style="font-style:normal"><link>
Dreyfus, Hubert</link>&#32;&amp;&#32;Dreyfus, Stuart&#32;(1986),&#32;<it>Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer</it>, Oxford, UK: Blackwell</cite>&nbsp; </entry>
<entry level="1" type="bullet">

  <cite id="CITEREFFearn2007" style="font-style:normal">Fearn, Nicholas&#32;(2007),&#32;<it>The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers</it>, New York: Grove Press</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFGladwell2005" style="font-style:normal"><link>
Gladwell, Malcolm</link>&#32;(2005),&#32;, Boston: Little, Brown, ISBN 0-316-17232-4</cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFHarnad2001" style="font-style:normal"><link>
Harnad, Stevan</link>&#32;(2001),&#32;<weblink xlink:type="simple" xlink:href="http://cogprints.org/4023/1/searlbook.htm">
"What's Wrong and Right About Searle's Chinese Room Argument?"</weblink>, in&#32;Bishop, M.&#32;&amp;&#32;Preston, J.,&#32;<it>Essays on Searle's Chinese Room Argument</it>, Oxford University Press, </cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFHobbes1651" style="font-style:normal"><link>
Hobbes</link>&#32;(1651),&#32;<it><link xlink:type="simple" xlink:href="../359/143359.xml">
Leviathan</link></it></cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFHofstadter1979" style="font-style:normal"><link>
Hofstadter, Douglas</link>&#32;(1979),&#32;</cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFHorst2005" style="font-style:normal">Horst, Steven&#32;(Fall 2005),&#32;<weblink xlink:type="simple" xlink:href="http://plato.stanford.edu/archives/fall2005/entries/computational-mind/">
"The Computational Theory of Mind"</weblink>, in&#32;Zalta, Edward N.,&#32;<it>The Stanford Encyclopedia of Philosophy</it>, </cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFKurzweil2005" style="font-style:normal"><link>
Kurzweil, Ray</link>&#32;(2005),&#32;<it><link xlink:type="simple" xlink:href="../123/767123.xml">
The Singularity is Near</link></it>, New York: Viking Press, ISBN 0-670-03384-7</cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFLucas1961" style="font-style:normal"><link>
Lucas, John</link>&#32;(1961),&#32;<weblink xlink:type="simple" xlink:href="http://users.ox.ac.uk/~jrlucas/Godel/mmg.html">
"Minds, Machines and Gödel"</weblink>, in&#32;Anderson, A.R.,&#32;<it>Minds and Machines</it>, </cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFMcCarthyMinskyRochesterShannon1955" style="font-style:normal"><link>
McCarthy, John</link>; <link>
Minsky, Marvin</link>; <link>
Rochester, Nathan</link>&#32;&amp;&#32;<link>
Shannon, Claude</link>&#32;(1955),&#32;<it><weblink xlink:type="simple" xlink:href="http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html">
A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence</weblink></it>, </cite>&nbsp;.</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFMcDermott1997" style="font-style:normal">McDermott, Drew&#32;(May 14, 1997),&#32;"<weblink xlink:type="simple" xlink:href="http://www.psych.utoronto.ca/~reingold/courses/ai/cache/mcdermott.html">
How Intelligent is Deep Blue</weblink>",&#32;<it>New York Times</it>, </cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFMoravec1988" style="font-style:normal"><link>
Moravec, Hans</link>&#32;(1988),&#32;<it>Mind Children</it>, Harvard University Press</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFNewellSimon1963" style="font-style:normal"><link>
Newell, Allen</link>&#32;&amp;&#32;Simon, H. A.&#32;(1963),&#32;"GPS: A Program that Simulates Human Thought", in&#32;Feigenbaum, E.A.&#32;&amp;&#32;Feldman, J.,&#32;<it>Computers and Thought</it>, McGraw-Hill</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFNewellSimon1976" style="font-style:normal"><link>
Newell, Allen</link>&#32;&amp;&#32;Simon, H. A.&#32;(1976),&#32;<weblink xlink:type="simple" xlink:href="http://www.rci.rutgers.edu/~cfs/472_html/AI_SEARCH/PSS/PSSH4.html">
"Computer Science as Empirical Inquiry: Symbols and Search"</weblink>,&#32;<it>Communications of the ACM</it>, <b>19</b>, </cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFRussellNorvig2003" style="font-style:normal"><link>
Russell, Stuart J.</link>&#32;&amp;&#32;<link>
Norvig, Peter</link>&#32;(2003),&#32;<it><weblink xlink:type="simple" xlink:href="http://aima.cs.berkeley.edu/">
</weblink></it>&#32;(2nd ed.), Upper Saddle River, NJ: Prentice Hall, ISBN 0-13-790395-2, </cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFPenrose1989" style="font-style:normal"><link>
Penrose, Roger</link>&#32;(1989),&#32;, Oxford University Press, ISBN 0-14-014534-6</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFSearle1980" style="font-style:normal"><link>
Searle, John</link>&#32;(1980),&#32;"<weblink xlink:type="simple" xlink:href="http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html">
Minds, Brains and Programs</weblink>",&#32;<it>Behavioral and Brain Sciences</it>&#32;<b>3</b>(3):  417-457, </cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFSearle1992" style="font-style:normal">Searle, John&#32;(1992),&#32;<it>The Rediscovery of the the Mind</it>, Cambridge, Massachusetts: M.I.T. Press</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFSearle1999" style="font-style:normal"><link>
Searle, John</link>&#32;(1999),&#32;<it>Mind, language and society</it>, New York, NY: Basic Books, ISBN 0465045219, <link xlink:type="simple" xlink:href="../885/883885.xml">
OCLC</link> <weblink xlink:type="simple" xlink:href="http://worldcat.org/oclc/231867665+43689264">
231867665 43689264</weblink></cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFTuring1950" style="font-style:normal"><link>
Turing, Alan</link>&#32;(October 1950),&#32;"<weblink xlink:type="simple" xlink:href="http://loebner.net/Prizef/TuringArticle.html">
Computing Machinery and Intelligence</weblink>",&#32;<it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../598/3995598.xml">
Mind</link></periodical>
</it>&#32;<b>LIX</b>(236):  433–460, <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1093%2Fmind%2FLIX.236.433">
10.1093/mind/LIX.236.433</weblink>, <symbol wordnetid="106806469" confidence="0.8">
<standard wordnetid="107260623" confidence="0.8">
<signal wordnetid="106791372" confidence="0.8">
<identifier wordnetid="107270601" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../930/234930.xml">
ISSN</link></system_of_measurement>
</identifier>
</signal>
</standard>
</symbol>
 <weblink xlink:type="simple" xlink:href="http://worldcat.org/issn/0026-4423">
0026-4423</weblink>, .&#32;Retrieved on 17 August 2008</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.shawnkilmer.com/?p=92">
Research Paper: Philosophy of Consciousness and Ethics In Artificial Intelligence</weblink></entry>
</list>
</p>

</sec>
</bdy>
</article>
