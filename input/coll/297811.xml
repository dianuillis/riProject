<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:45:01[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<inequality  confidence="0.9511911446218017" wordnetid="104752221">
<header>
<title>Jensen&apos;s inequality</title>
<id>297811</id>
<revision>
<id>244329419</id>
<timestamp>2008-10-10T08:03:43Z</timestamp>
<contributor>
<username>Europemayhem</username>
<id>8038154</id>
</contributor>
</revision>
<categories>
<category>Articles containing proofs</category>
<category>Inequalities</category>
<category>Statistical inequalities</category>
<category>Mathematical analysis</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link>, <b>Jensen's inequality</b>, named after the Danish mathematician <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../666/1430666.xml">
Johan Jensen</link></mathematician>
</scientist>
</causal_agent>
</engineer>
</person>
</physical_entity>
, relates the value of a <link xlink:type="simple" xlink:href="../568/245568.xml">
convex function</link> of an <link xlink:type="simple" xlink:href="../532/15532.xml">
integral</link> to the integral of the convex function. It was proved by Jensen in 1906<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. Given its generality, the inequality appears in many forms depending on the context, some of which are presented below. In its simplest form the inequality states,
"the convex transformation of a mean is less than or equal to the mean after convex transformation."<p>

The finite form of the equation was the logo of <link xlink:type="simple" xlink:href="../676/317676.xml">
Institute for Mathematical Sciences</link> at <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../767/176767.xml">
University of Copenhagen</link></university>
 until 2006.</p>

<sec>
<st>
Statements</st>

<p>

The classical form of Jensen's inequality involves several numbers and weights. The inequality can be stated quite generally using <link xlink:type="simple" xlink:href="../873/19873.xml">
measure theory</link>, or the equivalent probabilist notation. In this probabilistic setting the inequality can be further generalized to its <it>full strength</it>.</p>

<ss1>
<st>
Finite form</st>
<p>

For a real <link xlink:type="simple" xlink:href="../568/245568.xml">
convex function</link> φ, numbers  <it>xi</it> in its domain, and positive weights <it>ai</it>, Jensen's inequality can be stated as:</p>
<p>

<indent level="1">

<math>\varphi\left(\frac{\sum a_{i} x_{i}}{\sum a_{i}}\right) \le \frac{\sum a_{i} \varphi (x_{i})}{\sum a_{i}};</math>
</indent>

and the inequality is clearly reversed if φ is <link xlink:type="simple" xlink:href="../321/263321.xml">
concave</link>.</p>
<p>

As a particular case, if the weights <it>ai</it> are all equal to unity, then</p>
<p>

<indent level="1">

<math>\varphi\left(\frac{\sum x_{i}}{n}\right) \le \frac{\sum \varphi (x_{i})}{n}.</math>
</indent>

For instance, the log(<it>x</it>) function is <it>concave</it> (note that we can use Jensen's to prove convexity or concavity, if it holds for two real numbers whose functions are taken), so substituting <math>\scriptstyle\varphi(x)\,=\,-\log(x)</math> in the previous formula, this establishes the (logarithm of) the familiar <link xlink:type="simple" xlink:href="../011/605011.xml">
arithmetic mean-geometric mean inequality</link>:</p>
<p>

<indent level="1">

<math> \frac{x_1 + x_2 + \cdots + x_n}{n} \ge \sqrt[n]{x_1 x_2 \cdots x_n}.</math>
</indent>

The variable <it>x</it> may, if required, be a function of another variable (or set of variables) <it>t</it>, so that <it>xi</it> = <it>g</it>(<it>ti</it>). All of this carries directly over to the general continuous case: the weights <it>ai</it> are replaced by a non-negative integrable function <it>f</it>(<it>x</it>), such as a probability distribution, and the summations replaced by integrals.</p>

</ss1>
<ss1>
<st>
Measure-theoretic and probabilistic form</st>
<p>

Let (Ω,A,μ) be a <link xlink:type="simple" xlink:href="../873/19873.xml#xpointer(//*[./st=%22Formal+definitions%22])">
measure space</link>, such that μ(Ω) = 1. If <it>g</it> is a <link xlink:type="simple" xlink:href="../491/19725491.xml">
real</link>-valued function that is μ-<link xlink:type="simple" xlink:href="../520/602520.xml">
integrable</link>, and if φ is a <link xlink:type="simple" xlink:href="../055/44055.xml">
measurable</link> <link xlink:type="simple" xlink:href="../568/245568.xml">
convex function</link> on the real axis, then:</p>
<p>

<indent level="1">

<math>\varphi\left(\int_{\Omega} g\, d\mu\right) \le \int_\Omega \varphi \circ g\, d\mu. </math>
</indent>

The same result can be equivalently stated in a <link xlink:type="simple" xlink:href="../542/23542.xml">
probability theory</link> setting, by a simple change of notation. Let <math>\scriptstyle(\Omega, \mathfrak{F},\mathbb{P})</math> be a <link xlink:type="simple" xlink:href="../325/43325.xml">
probability space</link>, <it>X</it> an <link xlink:type="simple" xlink:href="../520/602520.xml">
integrable</link> real-valued <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> and φ a measurable <link xlink:type="simple" xlink:href="../568/245568.xml">
convex function</link>. Then:</p>
<p>

<indent level="1">

<math>\varphi\left(\mathbb{E}\{X\}\right) \leq \mathbb{E}\{\varphi(X)\}.</math>
</indent>

In this probability setting, the measure μ is intended as a probability <math>\scriptstyle\mathbb{P}</math>, the integral with respect to μ as an <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> <math>\scriptstyle\mathbb{E}</math>, and the function <it>g</it> as a <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> <it>X</it>.</p>

</ss1>
<ss1>
<st>
 General inequality in a probabilistic setting </st>

<p>

More generally, let <it>T</it> be a real <link xlink:type="simple" xlink:href="../752/45752.xml">
topological vector space</link>, and <it>X</it> a <it>T</it>-valued <link xlink:type="simple" xlink:href="../520/602520.xml">
integrable</link> random variable. In this general setting, <it>integrable</it> means that for any element <it>z</it> in the <link xlink:type="simple" xlink:href="../988/7988.xml">
dual space</link> of <it>T</it>: <math>\scriptstyle\mathbb{E}|\langle z, X \rangle|\,&amp;lt;\,\infty </math>, there exists an element <math>\scriptstyle\mathbb{E}\{X\}</math> in <it>T</it>, such that <math>\scriptstyle\langle z, \mathbb{E}\{X\}\rangle\,=\,\mathbb{E}\{\langle z, X \rangle\}</math>. Then, for any measurable convex function φ and any sub-<link xlink:type="simple" xlink:href="../586/29586.xml">
&amp;sigma;-algebra</link> <math>\scriptstyle\mathfrak{G}</math> of <math>\scriptstyle\mathfrak{F}</math>:</p>
<p>

<indent level="1">

<math>\varphi\left(\mathbb{E}\{X|\mathfrak{G}\}\right) \leq  \mathbb{E}\{\varphi(X)|\mathfrak{G}\}.</math>
</indent>

Here <math>\scriptstyle\mathbb{E}\{\cdot|\mathfrak{G} \}</math> stands for the <link xlink:type="simple" xlink:href="../099/435099.xml">
expectation conditioned</link> to the σ-algebra <math>\scriptstyle\mathfrak{G}</math>. This general statement reduces to the previous ones when the topological vector space <it>T</it> is the <link xlink:type="simple" xlink:href="../491/19725491.xml">
real axis</link>, and  <math>\scriptstyle\mathfrak{G}</math> is the trivial σ-algebra <math>\scriptstyle\{\varnothing, \Omega\}</math>.</p>

</ss1>
</sec>
<sec>
<st>
Proofs</st>
<p>

<image location="right" width="350px" src="Jensen_graph.png" type="thumb">
<caption>

A graphical "proof" of Jensen's inequality for the probabilistic case. The dashed curve along the <it>X</it> axis is the hypothetical distribution of <it>X</it>, while the dashed curve along the <it>Y</it> axis is the corresponding distribution of <it>Y</it> values. Note that the convex mapping <it>Y(X)</it> increasingly "stretches" the distribution for increasing values of <it>X</it>.
</caption>
</image>
</p>
<p>

A proof of Jensen's inequality can be provided in several ways, and three different proofs corresponding to the different statements above will be offered. Before embarking on these mathematical derivations, however, it is worth analyzing an intuitive graphical argument based on the probabilistic case where <it>X</it> is a real number (see figure). Assuming a hypothetical distribution of <it>X</it> values, one can immediately identify the position of <math>\scriptstyle\mathbb{E}\{X\}</math> and its image <math>\scriptstyle\varphi(\mathbb{E}\{X\})</math> in the graph. Noticing that for convex mappings <math>\scriptstyle Y\,=\,\varphi(X)</math> the corresponding distribution of <it>Y</it> values is increasingly "stretched out" for increasing values of <it>X</it>, it is easy to see that the distribution of <it>Y</it> is broader than that of <it>X</it> in the interval corresponding to <it>X</it>&nbsp;&amp;gt;&nbsp;<it>X</it>0 and narrower in <it>X</it>&nbsp;&nbsp;<it>X</it>0 for any <it>X</it>0; in particular, this is also true for <math>\scriptstyle X_0 \,=\, \mathbb{E}\{ X \}</math>. Consequently, in this picture the expectation of <it>Y</it> will always shift upwards with respect to the position of <math>\scriptstyle\varphi(\mathbb{E}\{ X \} )</math>, and this "proves" the inequality, i.e.</p>
<p>

<indent level="1">

<math> \mathbb{E}\{ Y(X) \} \geq Y(\mathbb{E}\{ X \} ), </math>
</indent>

the equality taking place when <math>\scriptstyle\varphi(X)</math> is not strictly convex, e.g. when it is a straight line.</p>
<p>

The proofs below formalize this intuitive notion.</p>

<ss1>
<st>
Proof 1 (finite form)</st>
<p>

If <it>λ</it>1 and <it>λ</it>2 are two arbitrary positive real numbers such that <it>λ</it>1&nbsp;+&nbsp;<it>λ</it>2&nbsp;=&nbsp;1 then convexity of <math>\scriptstyle\varphi</math> implies</p>
<p>

<indent level="1">

<math>\varphi(\lambda_1 x_1+\lambda_2 x_2)\leq \lambda_1\,\varphi(x_1)+\lambda_2\,\varphi(x_2)\text{ for any }x_1,\,x_2.</math> 
</indent>

This can be easily generalized: if <it>λ</it>1, <it>λ</it>2, ..., <it>λn</it> are positive real numbers such that <it>λ</it>1&nbsp;+&nbsp;...&nbsp;+&nbsp;<it>λn</it>&nbsp;=&nbsp;1, then</p>
<p>

<indent level="1">

<math>\varphi(\lambda_1 x_1+\lambda_2 x_2+\cdots+\lambda_n x_n)\leq \lambda_1\,\varphi(x_1)+\lambda_2\,\varphi(x_2)+\cdots+\lambda_n\,\varphi(x_n),</math>
</indent>

for any <it>x</it>1,&nbsp;...,&nbsp;<it>xn</it>. This <it>finite form</it> of the Jensen's inequality can be proved by <link xlink:type="simple" xlink:href="../881/18881.xml">
induction</link>: by convexity hypotheses, the statement is true for <it>n</it>&nbsp;=&nbsp;2. Suppose it is true also for some <it>n</it>, one needs to prove it for <it>n</it>&nbsp;+&nbsp;1. At least one of the <it>λi</it> is strictly positive, say <it>λ</it>1; therefore by convexity inequality:</p>
<p>

<indent level="1">

<math>\varphi\left(\sum_{i=1}^{n+1}\lambda_i x_i\right)= \varphi\left(\lambda_1 x_1+(1-\lambda_1)\sum_{i=2}^{n+1} \frac{\lambda_i}{1-\lambda_1} x_i\right)\leq \lambda_1\,\varphi(x_1)+(1-\lambda_1) \varphi\left(\sum_{i=2}^{n+1}\left( \frac{\lambda_i}{1-\lambda_1} x_i\right)\right).</math>
</indent>

Since <math>\scriptstyle \sum_{i=2}^{n+1} \lambda_i/(1-\lambda_1)\, =\,1</math>, one can apply the induction hypotheses to the last term in the previous formula to obtain the result, namely the finite form of the Jensen's inequality.</p>
<p>

In order to obtain the general inequality from this finite form, one needs to use a density argument. The finite form can  be re-written as:</p>
<p>

<indent level="1">

<math>\varphi\left(\int x\,d\mu_n(x) \right)\leq \int \varphi(x)\,d\mu_n(x),</math>
</indent>

where <it>μ</it>n<it> is a measure given by an arbitrary <link xlink:type="simple" xlink:href="../534/794534.xml">
convex combination</link> of <link xlink:type="simple" xlink:href="../021/37021.xml">
Dirac delta</link>s:</it></p>
<p>

<indent level="1">

<math>\mu_n=\sum_{i=1}^n \lambda_i \delta_{x_i}.</math>
</indent>

Since convex functions are <link xlink:type="simple" xlink:href="../122/6122.xml">
continuous</link>, and since convex combinations of Dirac deltas are <link xlink:type="simple" xlink:href="../662/33662.xml">
weakly</link> <link xlink:type="simple" xlink:href="../477/68477.xml">
dense</link> in the set of probability measures (as could be easily verified), the general statement is obtained simply by a limiting procedure.</p>

</ss1>
<ss1>
<st>
Proof 2 (measure-theoretic form)</st>
<p>

Let <it>g</it> be a real-valued μ-integrable function on a measure space Ω, and let <it>φ</it> be a convex function on the real numbers. Define the right-handed derivative of φ at <it>x</it> as</p>
<p>

<indent level="1">

<math>\varphi^\prime(x):=\lim_{t\to0^+}\frac{\varphi(x+t)-\varphi(x)}{t}.</math>
</indent>

Since φ is convex, the quotient of the right-hand side is decreasing when <it>t</it> approaches 0 from the right, and bounded below by any term of the form</p>
<p>

<indent level="1">

<math>\frac{\varphi(x+t)-\varphi(x)}{t}</math>
</indent>

where <it>t</it>  0, and therefore, the limit does always exist.</p>
<p>

Now, let us define the following:</p>
<p>

<indent level="1">

<math>x_0:=\int_\Omega g\, d\mu,</math>
</indent>

<indent level="1">

<math>a:=\varphi^\prime(x_0),</math>
</indent>

<indent level="1">

<math>b:=\varphi(x_0)-x_0\varphi^\prime(x_0).</math>
</indent>

Then for all <it>x</it>, <it>ax</it>&nbsp;+&nbsp;<it>b</it>&nbsp;≤&nbsp;<it>φ</it>(<it>x</it>). To see that, take <it>x</it>&nbsp;&amp;gt;&nbsp;<it>x</it>0, and define <it>t</it> = <it>x</it>&nbsp;&amp;minus;&nbsp;<it>x</it>0 &amp;gt; 0. Then,</p>
<p>

<indent level="1">

<math>\varphi^\prime(x_0)\leq\frac{\varphi(x_0+t)-\varphi(x_0)}{t}.</math>
</indent>

Therefore,</p>
<p>

<indent level="1">

<math>\varphi^\prime(x_0)(x-x_0)+\varphi(x_0)\leq\varphi(x)</math>
</indent>

as desired. The case for <it>x</it>  <it>x</it>0 is proven similarly, and clearly <it>ax</it>0&nbsp;+&nbsp;<it>b</it>&nbsp;=&nbsp;<it>φ</it>(<it>x</it>0).</p>
<p>

φ(<it>x</it>0) can then be rewritten as</p>
<p>

<indent level="1">

<math>ax_0+b=a\left(\int_\Omega g\,d\mu\right)+b.</math>
</indent>

But since μ(Ω) = 1, then for every real number <it>k</it> we have</p>
<p>

<indent level="1">

<math>\int_\Omega k\,d\mu=k.</math>
</indent>

In particular,</p>
<p>

<indent level="1">

<math>a\left(\int_\Omega g\,d\mu\right)+b=\int_\Omega(ag+b)\,d\mu\leq\int_\Omega\varphi\circ g\,d\mu.</math>
</indent>

</p>
</ss1>
<ss1>
<st>
 Proof 3 (general inequality in a probabilistic setting)</st>
<p>

Let <math>X</math> be an integrable random variable that takes value in a real topological vector space <it>T</it>. Since <math>\scriptstyle\varphi:T \mapsto \mathbb{R}</math> is convex, for any <math>x,y \in T</math>, the quantity</p>
<p>

<indent level="1">

<math>\frac{\varphi(x+\theta\,y)-\varphi(x)}{\theta},</math>
</indent>

is decreasing as θ approaches 0+. In particular, the <it>subdifferential</it> of <it>φ</it> evaluated at <it>x</it> in the direction <it>y</it> is well-defined by</p>
<p>

<indent level="1">

<math>(D\varphi)(x)\cdot y:=\lim_{\theta \downarrow 0} \frac{\varphi(x+\theta\,y)-\varphi(x)}{\theta}=\inf_{\theta \neq 0} \frac{\varphi(x+\theta\,y)-\varphi(x)}{\theta}.</math>
</indent>

It is easily seen that the subdifferential is linear in <it>y</it> and, since the infimum taken in the right-hand side of the previous formula is smaller than the value of the same term for <it>θ</it>&nbsp;=&nbsp;1, one gets</p>
<p>

<indent level="1">

<math>\varphi(x)\leq \varphi(x+y)-(D\varphi)(x)\cdot y.\,</math>
</indent>

In particular, for an arbitrary sub-σ-algebra <math>\scriptstyle\mathfrak{G}</math> we can evaluate the last inequality when <math>\scriptstyle x\,=\,\mathbb{E}\{X|\mathfrak{G}\},\,y=X-\mathbb{E}\{X|\mathfrak{G}\}</math> to obtain</p>
<p>

<indent level="1">

<math>\varphi(\mathbb{E}\{X|\mathfrak{G}\})\leq \varphi(X)-(D\varphi)(\mathbb{E}\{X|\mathfrak{G}\})\cdot (X-\mathbb{E}\{X|\mathfrak{G}\}).</math>
</indent>

Now, if we take the expectation conditioned to <math>\scriptstyle\mathfrak{G}</math> on both sides of the previous expression, we get the result since:</p>
<p>

<indent level="1">

<math>\mathbb{E}\{\left[(D\varphi)(\mathbb{E}\{X|\mathfrak{G}\})\cdot (X-\mathbb{E}\{X|\mathfrak{G}\})\right]|\mathfrak{G}\}=(D\varphi)(\mathbb{E}\{X|\mathfrak{G}\})\cdot \mathbb{E}\{ \left( X-\mathbb{E}\{X|\mathfrak{G}\} \right) |\mathfrak{G}\}=0,</math>
</indent>

by the linearity of the subdifferential in the <it>y</it> variable, and well-known properties of the <link xlink:type="simple" xlink:href="../099/435099.xml">
conditional expectation</link>.</p>

</ss1>
</sec>
<sec>
<st>
Applications and special cases</st>

<ss1>
<st>
Form involving a probability density function</st>

<p>

Suppose Ω is a measurable subset of the real line and <it>f</it>(<it>x</it>) is a non-negative function such that</p>
<p>

<indent level="1">

<math>\int_{-\infty}^\infty f(x)\,dx = 1.</math>
</indent>

In probabilistic language, <it>f</it> is a <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link>.</p>
<p>

Then Jensen's inequality becomes the following statement about convex integrals:</p>
<p>

If <it>g</it> is any real-valued measurable function and φ is convex over the range of <it>g</it>, then</p>
<p>

<indent level="1">

<math> \varphi\left(\int_{-\infty}^\infty g(x)f(x)\, dx\right) \le \int_{-\infty}^\infty \varphi(g(x)) f(x)\, dx. </math>
</indent>

If <it>g</it>(<it>x</it>) = <it>x</it>, then this form of the inequality reduces to a commonly used special case:</p>
<p>

<indent level="1">

<math>\varphi\left(\int_{-\infty}^\infty x\, f(x)\, dx\right) \le \int_{-\infty}^\infty \varphi(x)\,f(x)\, dx.</math>
</indent>

</p>
</ss1>
<ss1>
<st>
Alternative finite form</st>

<p>

If <math>\Omega</math> is some finite set <math>\{x_1,x_2,\ldots,x_n\}</math>, and if <math>\mu</math> is a <link xlink:type="simple" xlink:href="../786/191786.xml">
counting measure</link> on <math>\Omega</math>, then the general form reduces to a statement about sums:</p>
<p>

<indent level="1">

<math> \varphi\left(\sum_{i=1}^{n} g(x_i)\lambda_i \right) \le \sum_{i=1}^{n} \varphi(g(x_i))\lambda_i, </math>
</indent>

provided that <math> \lambda_1 + \lambda_2 + \cdots + \lambda_n = 1, \lambda_i \ge 0. </math></p>
<p>

There is also an infinite discrete form.</p>

</ss1>
<ss1>
<st>
Statistical physics</st>

<p>

Jensen's inequality is of particular importance in statistical physics when the convex function is an exponential, giving:</p>
<p>

<indent level="1">

<math> e^{\langle X \rangle} \leq \left\langle e^X \right\rangle, </math>
</indent>

where angle brackets denote <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link>s with respect to some <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> in the <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> <it>X</it>. </p>
<p>

The proof in this case is very simple (cf. Chandler, Sec. 5.5).  The desired inequality follows directly, by writing</p>
<p>

<indent level="1">

<math> \left\langle e^X \right\rangle
= e^{\langle X \rangle} \left\langle e^{X - \langle X \rangle} \right\rangle </math>
</indent>

and then applying the inequality
<indent level="1">

<math> e^X \geq 1+X \, </math>
</indent>

to the final exponential.</p>

</ss1>
<ss1>
<st>
Information theory</st>

<p>

If <it>p</it>(<it>x</it>) is the true probability distribution for <it>x</it>, and <it>q</it>(<it>x</it>) is another distribution, then applying Jensen's inequality for the random variable <it>Y</it>(<it>x</it>) = <it>q</it>(<it>x</it>)/<it>p</it>(<it>x</it>) and the function φ(<it>y</it>) = &amp;minus;log(<it>y</it>) gives </p>
<p>

<indent level="1">

<math>\Bbb{E}\{\varphi(Y)\} \ge \varphi(\Bbb{E}\{Y\})</math>
</indent>

<indent level="1">

<math>\Rightarrow  \int p(x) \log \frac{p(x)}{q(x)}dx  \ge  - \log \int p(x) \frac{q(x)}{p(x)}dx </math>
</indent>

<indent level="1">

<math>\Rightarrow \int p(x) \log \frac{p(x)}{q(x)}dx \ge 0  </math>
</indent>

<indent level="1">

<math>\Rightarrow  - \int p(x) \log q(x) \ge - \int p(x) \log p(x), </math>
</indent>

a result called <link xlink:type="simple" xlink:href="../678/2035678.xml">
Gibbs' inequality</link>.  </p>
<p>

It shows that the average message length is minimised when codes are assigned on the basis of the true probabilities <it>p</it> rather than any other distribution <it>q</it>. The quantity that is non-negative is called the <link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler distance</link> of <it>q</it> from <it>p</it>.</p>

</ss1>
<ss1>
<st>
Rao-Blackwell theorem</st>


<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../010/400010.xml">
Rao-Blackwell theorem</link></it>
</indent>

If <it>L</it> is a convex function, then from Jensen's inequality we get</p>
<p>

<indent level="1">

<math>L(\Bbb{E}\{\delta(X)\}) \le \Bbb{E}\{L(\delta(X))\} \quad \Rightarrow \quad \Bbb{E}\{L(\Bbb{E}\{\delta(X)\})\} \le \Bbb{E}\{L(\delta(X))\}.</math>
</indent>

So if δ(<it>X</it>) is some <link xlink:type="simple" xlink:href="../043/10043.xml">
estimator</link> of an unobserved parameter θ given a vector of observables <it>X</it>; and if <it>T</it>(<it>X</it>) is a <link>
sufficient statistic</link> for θ; then an improved estimator, in the sense of having a smaller expected loss <it>L</it>, can be obtained by calculating</p>
<p>

<indent level="1">

<math>\delta_{1}(X) = \Bbb{E}_{\theta}\{\delta(X') \,|\, T(X')= T(X)\},</math>
</indent>

the expected value of δ with respect to θ, taken over all possible vectors of observations <it>X</it> compatible with the same value of <it>T</it>(<it>X</it>) as that observed.</p>
<p>

This result is known as the <link xlink:type="simple" xlink:href="../010/400010.xml">
Rao-Blackwell theorem</link>.</p>

</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../340/18340.xml">
Law of averages</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Walter Rudin&#32;(1987). Real and Complex Analysis.&#32;McGraw-Hill. ISBN 0-07-054234-1.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book"><physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<chemist wordnetid="109913824" confidence="0.8">
<link xlink:type="simple" xlink:href="../963/17290963.xml">
David Chandler</link></chemist>
</scientist>
</causal_agent>
</person>
</physical_entity>
&#32;(1987). Introduction to Modern Statistical Mechanics.&#32;Oxford. ISBN 0-19-504277-8.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal"><physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../666/1430666.xml">
Jensen, Johan Ludwig William Valdemar</link></mathematician>
</scientist>
</causal_agent>
</engineer>
</person>
</physical_entity>
&#32;(1906).&#32;"<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/r55q1411g840j446/">
Sur les fonctions convexes et les inégalités entre les valeurs moyennes</weblink>". <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<periodical wordnetid="106593296" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../708/6860708.xml">
Acta Mathematica</link></publication>
</periodical>
</artifact>
</creation>
</product>
</work>
</it>&#32;<b>30</b>: 175–193. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF02418571">
10.1007/BF02418571</weblink>.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

  <cite id="Reference-Mathworld-Jensen's inequality"><physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<encyclopedist wordnetid="110055566" confidence="0.8">
<compiler wordnetid="109946957" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../189/836189.xml">
Eric W. Weisstein</link></scholar>
</mathematician>
</writer>
</scientist>
</causal_agent>
</alumnus>
</compiler>
</encyclopedist>
</intellectual>
</person>
</communicator>
</physical_entity>
, <it><weblink xlink:type="simple" xlink:href="http://mathworld.wolfram.com/JensensInequality.html">
Jensen's inequality</weblink></it> at <computer wordnetid="103082979" confidence="0.8">
<work wordnetid="104599396" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<reference_book wordnetid="106417598" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<encyclopedia wordnetid="106427387" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<web_site wordnetid="106359193" confidence="0.8">
<link xlink:type="simple" xlink:href="../235/374235.xml">
MathWorld</link></web_site>
</device>
</book>
</instrumentality>
</artifact>
</product>
</encyclopedia>
</publication>
</reference_book>
</machine>
</creation>
</work>
</computer>
.</cite></entry>
<entry level="1" type="bullet">

 Jensen's inequality served as the logo for the <weblink xlink:type="simple" xlink:href="http://www.math.ku.dk/ma/en/">
Mathematics department of Copenhagen University</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
Footnotes</st>

<p>

<reflist>
<entry id="1">
Jensen, J. <it>Sur les fonctions convexes et les inégalités entre les valeurs moyennes</it>.</entry>
</reflist>
</p>


</sec>
</bdy>
</inequality>
</article>
