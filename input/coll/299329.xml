<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:44:49[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<entity  confidence="0.9511911446218017" wordnetid="100001740">
<language  confidence="0.8" wordnetid="106282651">
<header>
<title>Stochastic context-free grammar</title>
<id>299329</id>
<revision>
<id>236520785</id>
<timestamp>2008-09-05T20:47:08Z</timestamp>
<contributor>
<username>Grm wnr</username>
<id>72203</id>
</contributor>
</revision>
<categories>
<category>Statistical natural language processing</category>
<category>Natural language parsing</category>
<category>Bioinformatics</category>
<category>Formal languages</category>
</categories>
</header>
<bdy>

A <b>stochastic context-free grammar</b> (<b>SCFG</b>; also <b>probabilistic context-free grammar</b>, <b>PCFG</b>) is a <link xlink:type="simple" xlink:href="../759/6759.xml">
context-free grammar</link> in which each production is augmented with a probability.  The probability of a derivation (parse) is then the product of the probabilities of the productions used in that derivation; thus some derivations are more consistent with the stochastic grammar than others.
SCFGs extend context-free grammars in the same way that <link xlink:type="simple" xlink:href="../770/98770.xml">
hidden Markov model</link>s extend <link xlink:type="simple" xlink:href="../855/25855.xml">
regular grammar</link>s.
SCFGs have application in areas as diverse as <link xlink:type="simple" xlink:href="../652/21652.xml">
Natural language processing</link> to the study of <link xlink:type="simple" xlink:href="../758/25758.xml">
RNA</link> molecules.  SCFGs are a specialized form of <link xlink:type="simple" xlink:href="../803/1819803.xml">
weighted context-free grammar</link>s.
<sec>
<st>
Techniques</st>

<p>

A variant of the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../929/53929.xml">
CYK algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 finds the <link xlink:type="simple" xlink:href="../015/228015.xml">
Viterbi parse</link> of a sequence for a given SCFG.  The Viterbi parse is the most likely derivation (parse) of the sequence by the given SCFG.</p>
<p>

The Inside and Outside algorithms are analogues of the <link xlink:type="simple" xlink:href="../015/228015.xml">
Forward algorithm</link> and <link>
Backward algorithm</link>, and can be used to compute the total probability of all derivations that are consistent with a given sequence, based on some SCFG.  This is equivalent to the probability of the SCFG generating the sequence, and is intuitively a measure of how consistent the sequence is with the given grammar.</p>
<p>

The <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../939/4492939.xml">
Inside/Outside algorithms</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 can also be used to compute the probabilities that a given production will be used in a random derivation of a sequence.  This is used as part of an <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<numerical_quantity wordnetid="105856066" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<value wordnetid="105856388" confidence="0.8">
<quantity wordnetid="105855125" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../752/470752.xml">
Expectation-maximization algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</quantity>
</value>
</rule>
</event>
</numerical_quantity>
</concept>
</idea>
 to learn the <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood</link> probabilities for an SCFG based on a set of training sequences that the SCFG should model.  The algorithm is analogous to that used by <link xlink:type="simple" xlink:href="../770/98770.xml">
hidden Markov models</link>.</p>

</sec>
<sec>
<st>
Applications</st>

<ss1>
<st>
Natural language processing</st>

<p>

Context-free grammars were originally conceived in an attempt to model natural languages, i.e. those normally spoken by humans.  Some research has extended this idea with SCFGs.</p>
<p>

Here is a tiny example of a 2-rule PCFG grammar. Each rule is preceded by a probability that reflects the relative frequency with which it occurs. 
<indent level="1">

0.7 VP → V NP
</indent>
:0.3 VP → V NP NP
Given this grammar, we can now say that the number of NPs expected while deriving VPs is 0.7 x 1 + 0.3 x 2 = 1.3. </p>
<p>

In particular, some <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link> systems use SCFGs to improve their probability estimate and thereby their performance.</p>
<p>

Recently, Probabilistic CFG's have played a role in explaining the 
<link xlink:type="simple" xlink:href="../834/8075834.xml">
Accessibility Hierarchy</link>, which seeks to explain why certain structures are more difficult to understand than others, e.g. those with relative clauses like "they had forgotten that the box which Pat brought with apples in was lost".  </p>
<p>

It turns out that if there is a probabilistic account of more likely constructions, then one can compute an <link xlink:type="simple" xlink:href="../773/14773.xml">
information theoretic</link> measure (<link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link>) for the constructs.  If the cognitive apparatus for syntax is based on information theoretic considerations, then it may very well employ something similar to PCFG.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

</ss1>
<ss1>
<st>
RNA</st>

<p>

Context-free grammars are adept at modeling the <link xlink:type="simple" xlink:href="../691/28691.xml">
secondary structure</link> of RNA<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.
Secondary structure involves <link xlink:type="simple" xlink:href="../505/21505.xml">
nucleotide</link>s within a single-stranded RNA molecule that are complementary to each other, and therefore base pair.  This base pairing is biologically important to the proper function of the RNA molecule.  Much of this base pairing can be represented in a context-free grammar (the major exception being <link xlink:type="simple" xlink:href="../614/2163614.xml">
pseudoknot</link>s).</p>
<p>

For example, consider the following grammar, where a,c,g,u represents nucleotides and S is the start symbol (and only non-terminal):
<indent level="1">

S → aSu | cSg | gSc | uSa
</indent>
This simple CFG represents an RNA molecule consisting entirely of two wholly complementary regions, in which only canonical complementary pairs are allowed (i.e. A-U and C-G).</p>
<p>

By attaching probabilities to more sophisticated CFGs, it is possible to model bases or base pairings that are more or less consistent with an expected RNA molecule pattern.  SCFGs are used to model the patterns in <link xlink:type="simple" xlink:href="../493/196493.xml">
RNA gene</link> families in the Rfam Database, and search genome sequences for likely additional members of these families.  SCFGs have also been used to find RNA genes using comparative genomics.  In this work, homologs of a potential RNA gene in two related organisms were inspected using SCFG techniques to see if their secondary structure is conserved.  If it is, the sequence is likely to be an RNA gene, and the secondary structure is presumed to be conserved because of the functional needs of that RNA gene.
It has been shown that SCFGs could predict the secondary structure of an RNA molecule similarly to existing techniques: such SCFGs are used, for example, by the <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../364/15286364.xml">
Stemloc</link></software>
 program.</p>

</ss1>
</sec>
<sec>
<st>
 Comparison with Generative Grammar </st>

<p>

With the publication of Gold's Theorem<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> 1967 it was claimed that grammars for natural languages governed by deterministic rules could not be learned based on positive instances alone. This was part of the argument from the <link xlink:type="simple" xlink:href="../458/2691458.xml">
poverty of stimulus</link>, presented in 1980<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>  and implicit since the early works by Chomsky of the 1950s. This led to the <link xlink:type="simple" xlink:href="../355/3719355.xml">
nativist</link> view, that a form of grammar (including a complete conceptual lexicon in certain versions) were hardwired from birth. This view is largely limited to GB and MP theories.</p>
<p>

A grammar is a description of the syntax of a language.  Theoretical models  focus on  a mental language or <link>
i-language</link>. In contrast, others <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> approach to syntax seeks to construct grammars that will describe language usage. </p>
<p>

A problem faced in any formal syntax is that often more than one production rule may apply to a structure, thus resulting in a conflict. The greater the coverage, the higher this conflict, and all grammarians (starting with <link xlink:type="simple" xlink:href="../422/5604422.xml">
Panini</link>) have spent considerable effort devising a prioritization for the rules, which usually turn out to be defeasible.  Another difficulty is overgeneration, where unlicensed structures are also generated.  Probabilistic grammars circumvent these problems by using the frequency of various productions to order them, resulting in a "most likely" (winner-take-all) interpretation, which by definition, is defeasible given additional data.  As usage patterns are altered in <link xlink:type="simple" xlink:href="../479/14266479.xml">
diachronic</link> shifts, these probabilistic rules can be re-learned, thus upgrading the grammar. </p>
<p>

One may construct a probabilistic grammar from a traditional formal syntax by assigning each non-terminal a probability taken from some distribution, to be eventually estimated from usage data.  On most samples of broad language, probabilistic grammars that tune these probabilities from data typically outperform hand-crafted grammars (although some rule-based grammars are now approaching the accuracies of PCFG).  </p>
<p>

Recently, probabilistic grammars appear to have gained some cognitive plausibility. It is well known that there are degrees of difficulty in accessing different syntactic structures (e.g. the <link xlink:type="simple" xlink:href="../834/8075834.xml">
Accessibility Hierarchy</link> for <link xlink:type="simple" xlink:href="../532/507532.xml">
relative clause</link>s). Probabilistic versions of <model wordnetid="105890249" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<hypothesis wordnetid="105888929" confidence="0.8">
<link xlink:type="simple" xlink:href="../439/58439.xml#xpointer(//*[./st=%22minimalism%22])">
minimalist</link></hypothesis>
</concept>
</idea>
</model>
 grammars have been used to compute information-theoretic <link xlink:type="simple" xlink:href="../891/9891.xml">
entropy</link> values which appear to correlate well with psycholinguistic data on understandability and production difficulty.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">John Hale&#32;(2006).&#32;"Uncertainty About the Rest of the Sentence". <it>Cognitive Science</it>&#32;<b>30</b>: 643–672. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1207%2Fs15516709cog0000_64">
10.1207/s15516709cog0000_64</weblink>.</cite>&nbsp;</entry>
<entry id="2">
Durbin, Eddy, Krogh, Mitchison, Biological sequence analysis, Cambridge University Press, 1998. This bioinformatics textbook includes an accessible introduction to the use of SCFGs for modelling RNA, as well as the history of this application to 1998.
</entry>
<entry id="3">
Sean R. Eddy and Richard Durbin (1994), "RNA sequence analysis 
using covariance models", <it>Nucleic Acids Research</it>, 22 (11): 2079-88. <weblink xlink:type="simple" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pubmed&amp;pubmedid=8029015">
http://www.pubmedcentral.nih.gov/articlerender.fcgi?tool=pubmed&amp;pubmedid=8029015</weblink>
</entry>
<entry id="4">
 Gold, E. (1967). Language identification in the limit. Information and Control 10, 447-474.</entry>
<entry id="5">
 Chomsky, N. (1980). Rules and representations Oxford: Basil Blackwell.</entry>
<entry id="6">
 <cite style="font-style:normal" class="book">George Lakoff and Mark Johnson&#32;(1999). Philosophy in the Flesh: The embodied mind and its challenge to Western thought. Part IV..&#32;New York:&#32;Basic Books..</cite>&nbsp;</entry>
<entry id="7">
 <cite style="font-style:normal">John Hale&#32;(2006).&#32;"Uncertainty About the Rest of the Sentence". <it>Cognitive Science</it>&#32;<b>30</b>: 643–672. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1207%2Fs15516709cog0000_64">
10.1207/s15516709cog0000_64</weblink>.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
<sec>
<st>
Further reading</st>
<p>

<list>
<entry level="1" type="bullet">

 Elena Rivas and Sean R. Eddy (2001), "Noncoding RNA gene detection using comparative sequence analysis", <it>BMC Bioinformatics</it>, 2 (1): 8. <weblink xlink:type="simple" xlink:href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=11801179&amp;dopt=Abstract">
http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=11801179&amp;dopt=Abstract</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../544/7024544.xml">
Statistical parsing</link></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://rfam.janelia.org/">
Rfam Database</weblink></entry>
</list>
</p>


</sec>
</bdy>
</language>
</entity>
</article>
