<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:45:36[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<know-how  confidence="0.8" wordnetid="105616786">
<method  confidence="0.8" wordnetid="105660268">
<header>
<title>Gaussian process</title>
<id>302944</id>
<revision>
<id>233928231</id>
<timestamp>2008-08-24T13:29:09Z</timestamp>
<contributor>
<username>Mebden</username>
<id>405869</id>
</contributor>
</revision>
<categories>
<category>Stochastic processes</category>
<category>Kernel methods for machine learning</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

A <b>Gaussian process</b> is a <link xlink:type="simple" xlink:href="../895/47895.xml">
stochastic process</link> which generates <link xlink:type="simple" xlink:href="../361/160361.xml">
samples</link> over time {<it>Xt</it>}<it>t</it> ∈<it>T</it> such that no matter which finite <link xlink:type="simple" xlink:href="../632/55632.xml">
linear combination</link> of the <it>Xt</it> one takes (or, more generally, any linear <link xlink:type="simple" xlink:href="../948/1374948.xml">
functional</link> of the sample function <it>Xt</it>), that <link xlink:type="simple" xlink:href="../591/91591.xml">
linear</link> combination will be <link xlink:type="simple" xlink:href="../462/21462.xml">
normally distributed</link>.<p>

Some authors <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> also assume the <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>s <it>Xt</it> have mean zero.</p>

<sec>
<st>
History</st>
<p>

The concept is named after <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../125/6125.xml">
Carl Friedrich Gauss</link></scientist>
</person>
 simply because the normal <link xlink:type="simple" xlink:href="../955/51955.xml">
distribution</link> is sometimes called the <it>Gaussian distribution</it>, although Gauss was not the first to study that distribution.</p>

</sec>
<sec>
<st>
Alternative definitions</st>
<p>

Alternatively, a process is Gaussian <link xlink:type="simple" xlink:href="../922/14922.xml">
if and only if</link> for every <link xlink:type="simple" xlink:href="../742/11742.xml">
finite set</link> of <link xlink:type="simple" xlink:href="../775/1603775.xml">
indices</link> <it>t</it>1, ..., <it>tk</it> in the index set <it>T</it></p>
<p>

<indent level="1">

<math> \vec{\mathbf{X}}_{t_1, \ldots, t_k} = (\mathbf{X}_{t_1}, \ldots, \mathbf{X}_{t_k}) </math>
</indent>

is a <link xlink:type="simple" xlink:href="../347/50347.xml">
vector-valued</link> Gaussian <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>. Using <link xlink:type="simple" xlink:href="../401/3229401.xml">
characteristic functions</link> of random variables, we can formulate the Gaussian property as follows:{<it>Xt</it>}<it>t</it> ∈ <it>T</it> is Gaussian if and only if for every finite set of indices <it>t</it>1, ..., <it>tk</it> there are positive reals σ<it>l j</it> and reals μ<it>j</it> such that</p>
<p>

<indent level="1">

<math> \operatorname{E}\left(\exp\left(i \ \sum_{\ell=1}^k t_\ell \ \mathbf{X}_{t_\ell}\right)\right) = \exp \left(-\frac{1}{2} \, \sum_{\ell, j} \sigma_{\ell j} t_\ell t_j + i \sum_\ell \mu_\ell t_\ell\right). </math>
</indent>

The numbers σ<it>l j</it> and μ<it>j</it> can be shown to be the <link xlink:type="simple" xlink:href="../059/157059.xml">
covariance</link>s and <link xlink:type="simple" xlink:href="../192/19192.xml">
means</link> of the variables in the process. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

</sec>
<sec>
<st>
Important Gaussian processes</st>
<p>

The <link xlink:type="simple" xlink:href="../984/149984.xml">
Wiener process</link> is perhaps the most widely studied Gaussian process. It is not <link xlink:type="simple" xlink:href="../898/329898.xml">
stationary</link>, but it has stationary increments.</p>
<p>

The <link xlink:type="simple" xlink:href="../186/2183186.xml">
Ornstein-Uhlenbeck process</link> is a <link xlink:type="simple" xlink:href="../957/310957.xml">
stationary</link> Gaussian process.</p>
<p>

The <link xlink:type="simple" xlink:href="../777/1843777.xml">
Brownian bridge</link> is a Gaussian process whose increments are not <link xlink:type="simple" xlink:href="../593/27593.xml">
independent</link>.</p>

</sec>
<sec>
<st>
Uses</st>
<p>

A Gaussian process can be used as a <link xlink:type="simple" xlink:href="../877/472877.xml">
prior probability distribution</link> over <mathematical_relation wordnetid="113783581" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../427/185427.xml">
functions</link></function>
</concept>
</idea>
</mathematical_relation>
 in <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian inference</link>. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> (Given any set of <math>N</math> points in the desired domain of your functions, take a <link xlink:type="simple" xlink:href="../347/50347.xml">
multivariate Gaussian</link> whose covariance <link xlink:type="simple" xlink:href="../728/19008728.xml">
matrix</link> parameter is the <link xlink:type="simple" xlink:href="../959/987959.xml">
Gram matrix</link> of your N points with some desired <link xlink:type="simple" xlink:href="../771/16771.xml">
kernel</link>, and <link xlink:type="simple" xlink:href="../361/160361.xml">
sample</link> from that Gaussian.) Inference of continuous values with a Gaussian process prior is known as <link xlink:type="simple" xlink:href="../026/477026.xml">
Gaussian process regression</link>, or <link xlink:type="simple" xlink:href="../026/477026.xml">
Kriging</link> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>.</p>

</sec>
<sec>
<st>
Notes</st>
<p>

<reflist>
<entry id="1">
 <cite id="Reference-Simon-1979" style="font-style:normal" class="book">Simon, Barry&#32;(1979). Functional Integration and Quantum Physics.&#32;Academic Press.</cite>&nbsp;</entry>
<entry id="2">
 <cite id="Reference-Dudley-1989" style="font-style:normal" class="book">Dudley, R.M.&#32;(1989). Real Analysis and Probability.&#32;Wadsworth and Brooks/Cole.</cite>&nbsp;</entry>
<entry id="3">
 <cite id="Reference-Rasmussen-2006" style="font-style:normal" class="book">Rasmussen, C.E.;&#32;Williams, C.K.I&#32;(2006). Gaussian Processes for Machine Learning.&#32;<company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../601/719601.xml">
MIT Press</link></company>
. ISBN 0-262-18253-X.</cite>&nbsp;</entry>
<entry id="4">
 <cite id="Reference-Stein-1999" style="font-style:normal" class="book">Stein, M.L.&#32;(1999). Interpolation of Spatial Data: Some Theory for Kriging.&#32;<link xlink:type="simple" xlink:href="../367/1098367.xml">
Springer</link>.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.GaussianProcess.org">
The Gaussian Processes Web Site</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf">
A gentle introduction to Gaussian processes</weblink></entry>
</list>
</p>


</sec>
</bdy>
</method>
</know-how>
</article>
