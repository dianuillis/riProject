<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:46:02[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<know-how  confidence="0.8" wordnetid="105616786">
<method  confidence="0.8" wordnetid="105660268">
<header>
<title>Kernel trick</title>
<id>303912</id>
<revision>
<id>224976903</id>
<timestamp>2008-07-11T07:42:55Z</timestamp>
<contributor>
<username>Harold f</username>
<id>1040921</id>
</contributor>
</revision>
<categories>
<category>Kernel methods for machine learning</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, the <b>kernel trick</b> is a method for using a <link xlink:type="simple" xlink:href="../974/98974.xml">
linear classifier</link> algorithm to solve a non-linear problem by mapping the original non-linear observations into a higher-dimensional space, where the linear classifier is subsequently used; this makes a linear classification in the new space equivalent to non-linear classification in the original space.<p>

This is done using <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../990/303990.xml">
Mercer's theorem</link></proposition>
</theorem>
</message>
</statement>
, which states that any continuous, symmetric, <link xlink:type="simple" xlink:href="../624/372624.xml">
positive semi-definite</link> <link>
 kernel function</link> <it>K</it>(<it>x</it>, <it>y</it>) can be expressed as a <link xlink:type="simple" xlink:href="../093/157093.xml">
dot product</link> in a high-dimensional <link xlink:type="simple" xlink:href="../667/27667.xml">
space</link>.  </p>
<p>

More specifically, if the arguments to the kernel are in a <link xlink:type="simple" xlink:href="../586/29586.xml">
measurable space</link> <it>X</it>, and if the kernel is <link xlink:type="simple" xlink:href="../624/372624.xml">
positive semi-definite</link> — i.e. </p>
<p>

<indent level="1">

<math>\sum_{i,j} K(x_i,x_j) c_i c_j \ge 0</math> 
</indent>

for any finite subset  {<it>x</it>1, ..., <it>x</it>n} of <it>X</it> and subset {<it>c</it>1, ..., <it>c</it>n} of objects (typically real numbers) — then there exists a function &amp;phi;(<it>x</it>) whose <link xlink:type="simple" xlink:href="../991/275991.xml">
range</link> is in an <link xlink:type="simple" xlink:href="../856/14856.xml">
inner product space</link> of possibly high dimension, such that</p>
<p>

<indent level="1">

<math>K(x,y) = \varphi(x)\cdot\varphi(y).</math>
</indent>

The kernel trick transforms any algorithm that solely depends on the dot product between two vectors. Wherever a dot product is used, it is replaced with the kernel function. Thus, a linear algorithm can easily be transformed into a non-linear algorithm. This non-linear algorithm is equivalent to the linear algorithm operating in the range space of &amp;phi;. However, because kernels are used, the &amp;phi; function is never explicitly computed. This is desirable, because the high-dimensional space may be infinite-dimensional (as is the case when the kernel is a <link xlink:type="simple" xlink:href="../552/245552.xml">
Gaussian</link>).</p>
<p>

The kernel trick was first published by Aizerman et al.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>
<p>

It has been applied to several kinds of algorithm in <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> and <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, including:
<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../777/172777.xml">
Perceptron</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
s</entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../309/65309.xml">
Support vector machine</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</method>
</rule>
</event>
</know-how>
s</entry>
<entry level="1" type="bullet">

 <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<component wordnetid="105868954" confidence="0.8">
<part wordnetid="105867413" confidence="0.8">
<link xlink:type="simple" xlink:href="../340/76340.xml">
Principal components analysis</link></part>
</component>
</concept>
</idea>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../900/363900.xml">
Canonical correlation analysis</link></entry>
<entry level="1" type="bullet">

 <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../824/140824.xml">
Fisher's</link></scientist>
</person>
 <link xlink:type="simple" xlink:href="../657/1470657.xml">
linear discriminant analysis</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../675/669675.xml">
Clustering</link></entry>
</list>
</p>
<p>

The origin of the term <it>kernel trick</it> is not known.</p>

<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">M. Aizerman, E. Braverman, and L. Rozonoer&#32;(1964).&#32;"Theoretical foundations of the potential function method in pattern recognition learning". <it>Automation and Remote Control</it>&#32;<b>25</b>: 821–837.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
<sec>
<st>
See also</st>

<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../576/3424576.xml">
Kernel methods</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../912/430912.xml">
Integral transforms</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../167/18994167.xml">
Hilbert space</link>, specifically <link xlink:type="simple" xlink:href="../196/651196.xml">
reproducing kernel Hilbert space</link></entry>
<entry level="1" type="bullet">

 <link>
Mercer kernel</link></entry>
</list>
</p>



</sec>
</bdy>
</method>
</know-how>
</article>
