<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:46:20[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>VC dimension</title>
<id>305846</id>
<revision>
<id>230320250</id>
<timestamp>2008-08-07T01:49:18Z</timestamp>
<contributor>
<username>Ubermammal</username>
<id>4421458</id>
</contributor>
</revision>
<categories>
<category>Statistical classification</category>
<category>Dimension</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../537/387537.xml">
computational learning theory</link>, the <b>VC dimension</b> (for <b>Vapnik-Chervonenkis dimension</b>) is a measure of the <link xlink:type="simple" xlink:href="../594/4379594.xml">
capacity</link> of a <link xlink:type="simple" xlink:href="../244/1579244.xml">
statistical classification</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>, defined as the <link xlink:type="simple" xlink:href="../er)/6174_(number).xml">
cardinality</link> of the largest set of points that the algorithm can <link xlink:type="simple" xlink:href="../659/1433659.xml">
shatter</link>. It is a core concept in <link xlink:type="simple" xlink:href="../843/305843.xml">
Vapnik-Chervonenkis theory</link>, and was originally defined by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../673/209673.xml">
Vladimir Vapnik</link></educator>
</mathematician>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../850/305850.xml">
Alexey Chervonenkis</link></mathematician>
</scientist>
</causal_agent>
</person>
</physical_entity>
.<p>

Informally, the capacity of a classification model is related to how complicated it can be. For example, consider the <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../299/87299.xml">
thresholding</link></function>
</mathematical_relation>
 of a high-<link xlink:type="simple" xlink:href="../352/480352.xml">
degree</link> <link xlink:type="simple" xlink:href="../000/23000.xml">
polynomial</link>: if the polynomial evaluates above zero, that point is classified as positive, otherwise as negative. A high-degree polynomial can be wiggly, so it can fit a given set of training points well. But one can expect that the classifier will make errors on other points, because it is too wiggly. Such a polynomial has a high capacity. A much simpler alternative is to threshold a linear function. This polynomial may not fit the training set well, because it has a low capacity. We make this notion of capacity more rigorous below.</p>

<sec>
<st>
 Shattering </st>

<p>

A classification model <math>f</math> with some parameter vector <math>\theta</math> is said to <it>shatter</it> a set of data points (<math>x_1,x_2,\ldots,x_n</math>) if, for all assignments of labels to those points, there exists a <math>\theta</math> such that the model <math>f</math> makes no errors when evaluating that set of data points.</p>
<p>

VC dimension of a model <math>f</math> is <math>h'</math> where <math>h'</math> is the maximum <math>h</math> such that some data point set of <link xlink:type="simple" xlink:href="../er)/6174_(number).xml">
cardinality</link> <math>h</math> can be shattered by <math>f</math>.  </p>
<p>

For example, consider a <link xlink:type="simple" xlink:href="../974/98974.xml">
straight line</link> as the classification model: the model used by a <link xlink:type="simple" xlink:href="../777/172777.xml">
perceptron</link>. The line should separate positive data points from negative data points.  When there are 3 points that are not collinear, the line can shatter them.  However, the line cannot shatter four points.  Thus, the VC dimension of this particular classifier is 3.  It is important to remember that one can choose the arrangement of points, but then cannot change it as the labels on the points are <link xlink:type="simple" xlink:href="../027/44027.xml">
permuted</link>.  Note, only 3 of the 8 possible permutations are shown for the 3 points.</p>
<p>

<table cellpadding="4" border="0" cellspacing="0">
<row>
<col align="center" bgcolor="#ddffdd">
 <image width="150px" src="VC1.png">
<caption>

VC1.png
</caption>
</image>
</col>
<col align="center" bgcolor="#ddffdd">
 <image width="150px" src="VC2.png">
<caption>

VC2.png
</caption>
</image>
</col>
<col align="center" bgcolor="#ddffdd">
 <image width="150px" src="VC3.png">
<caption>

VC3.png
</caption>
</image>
</col>
<col align="center" bgcolor="#ffdddd">
 <image width="150px" src="VC4.png">
<caption>

VC4.png
</caption>
</image>
</col>
</row>
<row>
<col colspan="3" align="center" bgcolor="#ddffdd">
<b>3 points shattered</b></col>
<col align="center" bgcolor="#ffdddd">
<b>4 points impossible</b></col>
</row>
</table>
</p>

</sec>
<sec>
<st>
 Uses </st>

<p>

The VC dimension has utility in statistical learning theory, because it can predict a <link xlink:type="simple" xlink:href="../934/22934.xml">
probabilistic</link> <link xlink:type="simple" xlink:href="../693/42693.xml">
upper bound</link> on the test error of a classification model.</p>
<p>

The bound on the test error of a classification model (on data that is drawn <link xlink:type="simple" xlink:href="../067/453067.xml">
i.i.d.</link> from the same distribution as the training set) is given by</p>
<p>

<indent level="1">

Training error + <math>\sqrt{h(\log(2N/h)+1)-\log(\eta/4)\over N}</math>
</indent>

with probability <math>1-\eta</math>, where <math>h</math> is the VC dimension of the classification model, and <math>N</math> is the size of the training set (restriction: this formula is valid when the VC dimension is small <math>h&amp;lt;N</math>).</p>

</sec>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

 <link>
Andrew Moore</link>'s <weblink xlink:type="simple" xlink:href="http://www-2.cs.cmu.edu/~awm/tutorials/vcdim.html">
VC dimension tutorial</weblink></entry>
<entry level="1" type="bullet">

 V. Vapnik and A. Chervonenkis. "On the uniform convergence of relative frequencies of events to their probabilities." <it>Theory of Probability and its Applications</it>, 16(2):264--280, 1971. </entry>
<entry level="1" type="bullet">

 A. Blumer, A. Ehrenfeucht, D. Haussler, and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../520/11076520.xml">
M. K. Warmuth</link></scientist>
. "Learnability and the Vapnik-Chervonenkis dimension." <it>Journal of the ACM</it>, 36(4):929--865, 1989.</entry>
<entry level="1" type="bullet">

 Christopher Burges Tutorial on SVMs for Pattern Recognition (containing information also for VC dimension) <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/burges98tutorial.html">
http://citeseer.ist.psu.edu/burges98tutorial.html</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
