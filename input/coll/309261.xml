<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:46:36[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Nonlinear dimensionality reduction</title>
<id>309261</id>
<revision>
<id>229108816</id>
<timestamp>2008-07-31T22:15:30Z</timestamp>
<contributor>
<username>Ahoerstemeier</username>
<id>7580</id>
</contributor>
</revision>
<categories>
<category>Multivariate statistics</category>
<category>Dimension</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<link xlink:type="simple" xlink:href="../401/2498401.xml">
High-dimensional</link> data, meaning data which requires more than two or three dimensions to represent, can be difficult to interpret. One approach to simplification is to assume that the data of interest lies on an embedded non-linear <link xlink:type="simple" xlink:href="../470/2073470.xml">
manifold</link> within the higher dimensional space. If the manifold is of low enough dimension then the data can be visualised in the low dimensional space.<p>

Below are summarized some important algorithms from the history of manifold learning and <b>nonlinear dimensionality reduction</b>. Many of these non-linear <link xlink:type="simple" xlink:href="../867/579867.xml">
dimensionality reduction</link> methods are related to linear methods which are listed below. The non-linear methods can be broadly classified into two groups: those which actually provide a mapping (either from the high dimensional space to the low dimensional embedding or vice versa), and those that just give a visualisation. Typically those that just give a visualisation are based on proximity data - that is, <link xlink:type="simple" xlink:href="../378/39378.xml">
distance</link> measurements.</p>

<sec>
<st>
Linear methods</st>

<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../031/598031.xml">
Independent component analysis</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 (ICA).</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../340/76340.xml">
Principal component analysis</link> (PCA) (also called <link>
Karhunen-Loève transform</link> &mdash; KLT).</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../207/142207.xml">
Singular value decomposition</link> (SVD).</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../492/253492.xml">
Factor analysis</link>.</entry>
</list>
</p>

</sec>
<sec>
<st>
Non-linear mappings</st>

<p>

Perhaps the principal method amongst those that provide a mapping from the high dimensional space to the embedded space is <link xlink:type="simple" xlink:href="../424/5978424.xml">
kernel PCA</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. This method provides a non-linear <link xlink:type="simple" xlink:href="../340/76340.xml">
principal components analysis</link> (PCA) by applying the <link xlink:type="simple" xlink:href="../912/303912.xml">
kernel trick</link>. Kernel PCA first (implicitly) construct a higher dimensional space, in which there are a large number of linear relations between the dimensions. Subsequently, the low-dimensional data representation is obtained by applying traditional PCA.</p>
<p>

<b><link>
Principal curve</link>s and manifolds</b> give the natural geometric framework for nonlinear dimensionality reduction and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold, and
by encoding using standard geometric projection onto the manifold. How to
define the "simplicity" of the manifold is problem-dependent, however, it is
commonly measured by the intrinsic dimensionality and/or the smoothness
of the manifold.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

<link>
Gaussian process latent variable model</link>s (GPLVM)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> are a probabilistic non-linear PCA. Like kernel PCA they use a kernel function to form the mapping (in the form of a <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../944/302944.xml">
Gaussian process</link></method>
</know-how>
). However in the GPLVM the mapping is from the embedded space to the data space (like density networks and GTM) whereas in kernel PCA it is in the opposite direction.</p>
<p>

Other nonlinear techniques include techniques for locally linear embedding (such as Locally Linear Embedding (LLE)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>, <link>
Hessian LLE</link>, <link>
Laplacian Eigenmaps</link>, and <link>
LTSA</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>). These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data (actually these techniques can be viewed upon as defining a graph-based kernel for Kernel PCA). In this way, the techniques are capable of unfolding datasets such as the <substance wordnetid="100020090" confidence="0.8">
<solid wordnetid="115046900" confidence="0.8">
<food wordnetid="107555863" confidence="0.8">
<food wordnetid="100021265" confidence="0.8">
<bread wordnetid="107679356" confidence="0.8">
<foodstuff wordnetid="107566340" confidence="0.8">
<starches wordnetid="107566863" confidence="0.8">
<baked_goods wordnetid="107622061" confidence="0.8">
<link xlink:type="simple" xlink:href="../683/1935683.xml">
Swiss roll</link></baked_goods>
</starches>
</foodstuff>
</bread>
</food>
</food>
</solid>
</substance>
. Techniques that employ neighborhood graphs in order to retain global properties of the data include <link xlink:type="simple" xlink:href="../623/14628623.xml">
Isomap</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> and <link>
Maximum Variance Unfolding</link>.</p>
<p>

A completely different approach to nonlinear dimensionality reduction is through the use of <link xlink:type="simple" xlink:href="../612/6836612.xml">
autoencoder</link>s, a special kind of feed-forward <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link>s. Although the idea of autoencoders is quite old, training of the encoders has only recently become possible through the use of Restricted <link xlink:type="simple" xlink:href="../059/1166059.xml">
Boltzmann machine</link>s. Related to autoencoders is the <link>
NeuroScale</link> algorithm, which uses stress functions inspired by <link xlink:type="simple" xlink:href="../786/398786.xml">
multidimensional scaling</link> and <link>
Sammon mapping</link>s (see below) to learn a non-linear mapping from the high dimensional to the embedded space. The mappings in NeuroScale are based on <link xlink:type="simple" xlink:href="../443/9651443.xml">
radial basis function network</link>s.</p>
<p>

<link xlink:type="simple" xlink:href="../996/76996.xml">
Kohonen maps</link> (also called <it>Self-Organizing Maps</it> or SOM) and its probabilistic variant <link xlink:type="simple" xlink:href="../054/1091054.xml">
generative topographic mapping</link> (GTM) use a point representation in the embedded space to form a latent variable model which is based around a non-linear mapping from the embedded space to the high dimensional space. These techniques are related to work on <link>
density networks</link>, which also are based around the same probabilistic model.</p>

</sec>
<sec>
<st>
Methods based on proximity matrices</st>

<p>

A method based on proximity matrices is one where the data is presented to the algorithm in the form of a <link xlink:type="simple" xlink:href="../743/1004743.xml">
similarity matrix</link> or a <link xlink:type="simple" xlink:href="../350/831350.xml">
distance matrix</link>. These methods all fall under the broader class of <link>
metric multidimensional scaling</link>. The variations tend to be differences in how the proximity data is computed; for example, <link xlink:type="simple" xlink:href="../623/14628623.xml">
Isomap</link>, <link>
locally linear embeddings</link>, <link xlink:type="simple" xlink:href="../410/1187410.xml">
maximum variance unfolding</link>, and <link>
 Sammon mapping</link> (which is not in fact a mapping) are examples of metric multidimensional scaling methods.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link>
pairwise distance methods</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../713/70713.xml">
topographic map</link></entry>
<entry level="1" type="bullet">

 <link>
elastic net</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../996/76996.xml">
self-organizing map</link>s (SOM)</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../657/1470657.xml">
discriminant analysis</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
B. Schölkopf, A. Smola, K.-R. Muller, Kernel Principal Component Analysis, In: Bernhard Schölkopf, Christopher J. C. Burges, Alexander J. Smola (Eds.), Advances in Kernel Methods-Support Vector Learning, 1999, MIT Press  Cambridge, MA, USA, 327-352. ISBN 0-262-19416-3</entry>
<entry id="2">
A. Gorban, B. Kegl, D. Wunsch, A. Zinovyev  (Eds.), 
<weblink xlink:type="simple" xlink:href="http://pca.narod.ru/contentsgkwz.htm">
Principal Manifolds for Data Visualisation and Dimension Reduction,</weblink> 
LNCSE 58, Springer, Berlin - Heidelberg - New York, 2007. ISBN 978-3-540-73749-0</entry>
<entry id="3">
<weblink xlink:type="simple" xlink:href="http://www.gaussianprocess.org/">
The Gaussian Processes Web Site</weblink></entry>
<entry id="4">
S. T. Roweis and L. K. Saul, Nonlinear Dimensionality Reduction by Locally Linear Embedding, Science Vol 290, 22 December 2000, 2323-2326. </entry>
<entry id="5">
Zhenyue Zhang and Hongyuan Zha, Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment, SIAM Journal on Scientific Computing 26 (1) (2005), 313 - 338.</entry>
<entry id="6">
J. B. Tenenbaum, V. de Silva, J. C. Langford, A Global Geometric Framework for Nonlinear Dimensionality Reduction, Science 290, (2000), 2319-2323.</entry>
<entry id="7">
<weblink xlink:type="simple" xlink:href="http://bioinfo-out.curie.fr/projects/elmap/">
ELastic MAPs</weblink></entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://isomap.stanford.edu/">
Isomap</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ncrg.aston.ac.uk/GTM/">
Generative Topographic Mapping</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://research.microsoft.com/users/mtipping/pages/thesis.htm">
Mike Tipping's Thesis</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dcs.shef.ac.uk/~neil/gplvm/">
Gaussian Process Latent Variable Model</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.toronto.edu/~roweis/lle/">
Locally Linear Embedding</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.seas.upenn.edu/~kilianw/sde">
Semidefinite Embedding</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.visumap.net/index.aspx?p=Resources">
Relational Perspective Map</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cse.msu.edu/~lawhiu/manifold/">
Manifold Learning Resource Page</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.unimaas.nl/l.vandermaaten/dr">
Matlab Toolbox for Dimensionality Reduction</weblink></entry>
</list>
</p>

</sec>
</bdy>
</article>
