<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 20:08:50[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Concept drift</title>
<id>3118600</id>
<revision>
<id>239377627</id>
<timestamp>2008-09-18T21:07:17Z</timestamp>
<contributor>
<username>Deathmagnetic08</username>
<id>7330749</id>
</contributor>
</revision>
<categories>
<category>Data mining</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../563/4141563.xml">
predictive analytics</link> and <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, the <b>concept drift</b> means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as the times passes.<p>

The term <it>concept</it> refers to the quantity you are looking to predict. More generally, it can also refer to other phenomena of interest besides the target concept, such as an input, but, in the context of concept drift, the term commonly refers to the target variable.</p>

<sec>
<st>
Examples</st>
<p>

In a <link xlink:type="simple" xlink:href="../790/58790.xml">
fraud detection</link> application the target concept may be a <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../686/238686.xml">
binary</link></instrumentality>
</artifact>
</system>
 attribute FRAUDULENT with values "yes" or "no" that indicates whether a given transaction is fraudulent. Or, in a <link xlink:type="simple" xlink:href="../231/73231.xml">
weather prediction</link> application, there may be several target concepts such as TEMPERATURE, PRESSURE, and HUMIDITY.</p>
<p>

The behavior of the customers in an <link xlink:type="simple" xlink:href="../379/1118379.xml">
online shop</link> may change over time. Let's say you want to predict weekly merchandise sales, and you have developed a predictive model that works to your satisfaction. The model may use inputs such as the amount of money spent on <link xlink:type="simple" xlink:href="../861/2861.xml">
advertising</link>, <link xlink:type="simple" xlink:href="../852/239852.xml">
promotions</link> you are running, and other metrics that may affect sales. What you are likely to experience is that the model will become less and less accurate over time - you will be a victim of concept drift. In the merchandise sales application, one reason for concept drift may be seasonality, which means that shopping behavior changes seasonally. You will likely have higher sales in the winter holiday season than during the summer.</p>

</sec>
<sec>
<st>
Possible remedies</st>
<p>

To prevent <link xlink:type="simple" xlink:href="../388/1500388.xml">
deterioration</link> in <link xlink:type="simple" xlink:href="../066/246066.xml">
prediction</link> accuracy over time the model has to be refreshed periodically. One approach is to retrain the model using only the most recently observed samples (Widmer and Kubat, 1996). Another approach is to add new inputs which may be better at explaining the causes of the concept drift. For our sales prediction application you may be able to reduce concept drift by adding information about the season to your model. By providing information about the time of the year you will likely reduce rate of deterioration of your model, but you likely will never be able to prevent concept drift altogether. This is because actual shopping behavior does not follow any static, <link>
finite model</link>. New factors may arise at any time that influence shopping behavior, the influence of the known factors or their interactions may change.</p>
<p>

Concept drift cannot be avoided if you are looking to predict a complex phenomenon that is not governed by fixed <link xlink:type="simple" xlink:href="../098/39098.xml">
laws of nature</link>. All processes that arise from human activity, such as <link xlink:type="simple" xlink:href="../119/503119.xml">
socioeconomic</link> processes, and <link xlink:type="simple" xlink:href="../450/5652450.xml">
biological processes</link> are likely to experience concept drift. Therefore, periodic retraining, also known as refreshing of your model is inescapable.</p>

</sec>
<sec>
<st>
 Bibliographic references </st>
<p>

<list>
<entry level="1" type="bullet">

 Carroll J. and Rosson M. B. The paradox of the active user. In J.M. Carroll (Ed.), Interfacing Thought: Cognitive Aspects of Human-Computer Interaction. Cambridge, MA, MIT Press, 1987.</entry>
<entry level="1" type="bullet">

 Grabtree I. Soltysiak S. Identifying and Tracking Changing Interests. International Journal of Digital Libraries, Springer Verlag, vol. 2, 38-53.</entry>
<entry level="1" type="bullet">

 Harries M. B., Sammut C., Horn K. Extracting Hidden Context, Machine Learning 32, 1998, pp. 101-126.</entry>
<entry level="1" type="bullet">

 Klinkenberg, Ralf: <it>Learning Drifting Concepts: Example Selection vs. Example Weighting</it>. In Intelligent Data Analysis (IDA), Special Issue on Incremental Learning Systems Capable of Dealing with Concept Drift, Vol. 8, No. 3, pages 281--300, 2004.</entry>
<entry level="1" type="bullet">

 Klinkenberg, Ralf. Predicting Phases in Business Cycles Under Concept Drift. In Hotho, Andreas and Stumme, Gerd (editors), Proceedings of LLWA-2003 / FGML-2003, pages 3--10, Karlsruhe, Germany, 2003.</entry>
<entry level="1" type="bullet">

 Klinkenberg, Ralf and Rüping, Stefan: <it>Concept Drift and the Importance of Examples</it>. In Franke, Jürgen and Nakhaeizadeh, Gholamreza and Renz, Ingrid (editors), Text Mining -- Theoretical Aspects and Applications, pages 55--77, Berlin, Germany, Physica-Verlag, 2003.</entry>
<entry level="1" type="bullet">

 Klinkenberg, Ralf: <it>Using Labeled and Unlabeled Data to Learn Drifting Concepts</it>. In Kubat, Miroslav and Morik, Katharina (editors), Workshop notes of the IJCAI-01 Workshop on \em Learning from Temporal and Spatial Data, pages 16--24, IJCAI, Menlo Park, CA, USA, AAAI Press, 2001.</entry>
<entry level="1" type="bullet">

 Klinkenberg, Ralf and Joachims, Thorsten: <it>Detecting Concept Drift with Support Vector Machines</it>. In Langley, Pat (editor), Proceedings of the Seventeenth International Conference on Machine Learning (ICML), pages 487--494, San Francisco, CA, USA, Morgan Kaufmann, 2000.</entry>
<entry level="1" type="bullet">

 Klinkenberg, Ralf and Renz, Ingrid: <it>Adaptive Information Filtering: Learning in the Presence of Concept Drifts</it>. In Sahami, Mehran and Craven, Mark and Joachims, Thorsten and McCallum, Andrew (editors), Workshop Notes of the ICML/AAAI-98 Workshop \em Learning for Text Categorization, pages 33--40, Menlo Park, CA, USA, AAAI Press, 1998.</entry>
<entry level="1" type="bullet">

 Kolter, J.Z. and Maloof, M.A. Dynamic Weighted Majority: A new ensemble method for tracking concept drift. Proceedings of the Third International IEEE Conference on Data Mining, pages 123-130, Los Alamitos, CA: IEEE Press, 2003.</entry>
<entry level="1" type="bullet">

 Kolter J.Z. and Maloof, M.A. Using additive expert ensembles to cope with concept drift. In Proceedings of the Twenty-second International Conference on Machine Learning, pages 449-456. New York, NY: ACM Press, 2005.</entry>
<entry level="1" type="bullet">

 Kolter, J.Z. and Maloof, M.A. <weblink xlink:type="simple" xlink:href="http://jmlr.csail.mit.edu/papers/volume8/kolter07a/kolter07a.pdf">
Dynamic Weighted Majority: An ensemble method for drifting concepts.</weblink> Journal of Machine Learning Research 8:2755--2790, 2007.</entry>
<entry level="1" type="bullet">

 Koychev I. Gradual Forgetting for Adaptation to Concept Drift. In Proceedings of ECAI 2000 Workshop Current Issues in Spatio-Temporal Reasoning. Berlin, Germany, 2000, pp. 101-106.</entry>
<entry level="1" type="bullet">

 Koychev I. and Schwab I., Adaptation to Drifting User’s Interests, Proc. of ECML2000 Workshop: Machine Learning in New Information Age, Barcelona, Spain, 2000, pp. 39-45.</entry>
<entry level="1" type="bullet">

 Maloof M.A. and Michalski R.S. Selecting examples for partial memory learning. Machine Learning, 41(11), 2000, pp. 27-52.</entry>
<entry level="1" type="bullet">

 Maloof M.A. and Michalski R.S. Incremental learning with partial instance memory. Artificial Intelligence 154, 2004, pp. 95-126.</entry>
<entry level="1" type="bullet">

 Mitchell T., Caruana R., Freitag D., McDermott, J. and Zabowski D. Experience with a Learning Personal Assistant. Communications of the ACM 37(7), 1994, pp. 81-91.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://webmining.spd.louisville.edu/Websites/PAPERS/journal/Computer-Networks-Jnl-Spec-Issue-Web-Dynamics-2006-Mining-Evolving-Streams-Retrospective-Validation.pdf">
Nasraoui O. , Rojas C., and Cardona C., “ A Framework for Mining Evolving Trends in Web Data Streams using Dynamic Learning and Retrospective Validation ”, Journal of Computer Networks- Special Issue on Web Dynamics, 50(10), 1425-1652, July 2006</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://webmining.spd.louisville.edu/Websites/PAPERS/conference/CIKM-2006-Collaborative-Filtering-Recommender-Sys-in-Evolving-Web-Clickstreams.pdf">
Nasraoui O. , Cerwinske J., Rojas C., and Gonzalez F., "Collaborative Filtering in Dynamic Usage Environments", in Proc. of CIKM 2006 – Conference on Information and Knowledge Management, Arlington VA , Nov. 2006</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://users.rowan.edu/~polikar/RESEARCH/PUBLICATIONS/icmlc07.pdf">
Mulhbaier D., and Polikar, R. "Multiple Classifiers Based Incremental Learning Algorithm for Learning in Nonstationary Environments," IEEE International Conference on Machine Learning and Cybernetics, Volume 6, Page(s):3618 - 3623, 19-22 August 2007.</weblink> </entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://users.rowan.edu/~polikar/RESEARCH/PUBLICATIONS/ijcnn08.pdf">
Karnick M., Ahiskali M., Muhlbaier, M.D., and Polikar R., "Learning Concept Drift in Nonstationary Environments Using an Ensemble of Classifiers Based Approach," World Congress on Computational Intelligence / IEEE International Joint Conference on Neural Networks, Hong Kong, 1-6 June 2008.</weblink> </entry>
<entry level="1" type="bullet">

 Núñez M., Fidalgo R., and Morales R., <weblink xlink:type="simple" xlink:href="http://www.jmlr.org/papers/volume8/nunez07a/nunez07a.pdf">
Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</weblink>, Journal of Machine Learning Research, 8, (2007) 2595-2628</entry>
<entry level="1" type="bullet">

 Schlimmer J., and Granger R. Incremental Learning from Noisy Data, Machine Learning, 1(3), 1986, 317-357.</entry>
<entry level="1" type="bullet">

 Scholz, Martin and Klinkenberg, Ralf: <it>Boosting Classifiers for Drifting Concepts</it>. In Intelligent Data Analysis (IDA), Special Issue on Knowledge Discovery from Data Streams, Vol. 11, No. 1, pages 3-28, March 2007.</entry>
<entry level="1" type="bullet">

 Scholz, Martin and Klinkenberg, Ralf: <it>An Ensemble Classifier for Drifting Concepts</it>. In Gama, J. and Aguilar-Ruiz, J. S. (editors), Proceedings of the Second International Workshop on Knowledge Discovery in Data Streams, pages 53--64, Porto, Portugal, 2005.</entry>
<entry level="1" type="bullet">

 Schwab I., Pohl W. and Koychev I. Learning to Recommend from Positive Evidence, Proceedings of Intelligent User Interfaces 2000, ACM Press, 241 - 247.</entry>
<entry level="1" type="bullet">

 Widmer G. Tracking Changes through Meta-Learning, Machine Learning 27, 1997, pp. 256-286.</entry>
<entry level="1" type="bullet">

 Widmer G. and Kubat M. Learning in the presence of concept drift and hidden contexts. Machine Learning 23, 1996, pp. 69-101.</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../301/1760301.xml">
Data stream mining</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../253/42253.xml">
Data mining</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link></entry>
</list>
</p>

</sec>
<sec>
<st>
Software</st>
<p>

<list>
<entry level="1" type="bullet">

 <work wordnetid="100575741" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../865/5100865.xml">
RapidMiner</link></activity>
</psychological_feature>
</act>
</undertaking>
</event>
</work>
 (<weblink xlink:type="simple" xlink:href="http://rapid-i.com/">
RapidMiner, formerly YALE (Yet Another Learning Environment)</weblink>): free open-source software for knowledge discovery, data mining, and machine learning also featuring data stream mining, learning time-varying concepts, and tracking drifting concept (if used in combination with its data stream mining plugin (formerly: concept drift plugin))</entry>
<entry level="1" type="bullet">

 EDDM (<weblink xlink:type="simple" xlink:href="http://iaia.lcc.uma.es/Members/mbaena/papers/eddm/">
EDDM (Early Drift Detection Method)</weblink>): free open-source implementation of drift detection methods in <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../034/3829034.xml">
Weka (machine learning)</link></software>
.</entry>
</list>
</p>

</sec>
<sec>
<st>
Researchers working on concept drift problems</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.liaad.up.pt/~jgama/">
Joao Gama</weblink>, University of Porto, Portugal</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www-ai.cs.uni-dortmund.de/PERSONAL/klinkenberg.html">
Ralf Klinkenberg</weblink>, University of Dortmund, Germany</entry>
<entry level="1" type="bullet">

 Ivan Koychev, Aberdeen, UK</entry>
<entry level="1" type="bullet">

 Miroslav Kubat, FL, USA</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://users.rowan.edu/~polikar/RESEARCH">
Robi Polikar</weblink>, Rowan University, Glassboro, NJ, USA</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.informatics.bangor.ac.uk/~kuncheva/">
Ludmila Kuncheva</weblink>, University of Wales, Bangor, UK</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.georgetown.edu/~maloof/">
Mark Maloof</weblink>, Georgetown University, USA</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://louisville.edu/~o0nasr01/">
Olfa Nasraoui</weblink>, University of Louisville, USA</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://lis2.huie.hokudai.ac.jp/~knishida/index_en.html">
Kyosuke Nishida</weblink>, Hokkaido University, Japan</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cp.jku.at/people/widmer/">
Gerhard Widmer</weblink>, Johannes Kepler University (JKU) Linz, Austria</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.mat.ua.pt/gladys">
Gladys Castillo</weblink>, University of Aveiro, Portugal</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://zliobaite.googlepages.com/">
Indre Zliobaite</weblink>, Vilnius University, Lithuania</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://iaia.lcc.uma.es/~rfm">
Raúl Fidalgo</weblink>, University of Málaga, Spain</entry>
</list>
</p>

</sec>
</bdy>
</article>
