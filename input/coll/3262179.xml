<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 20:13:36[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Cobham&apos;s thesis</title>
<id>3262179</id>
<revision>
<id>242627057</id>
<timestamp>2008-10-03T00:12:03Z</timestamp>
<contributor>
<username>Dcoetzee</username>
<id>13476</id>
</contributor>
</revision>
<categories>
<category>Computational complexity theory</category>
<category>Articles that may contain original research since May 2008</category>
</categories>
</header>
<bdy>

<b>Cobham's thesis</b> asserts that computational problems can be feasibly computed on some computational device only if they can be computed in <link xlink:type="simple" xlink:href="../576/44576.xml">
polynomial time</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><p>

Formally, to say that a problem can be solved in polynomial time is to say that there exists an algorithm that, given an <it>n</it>-bit instance of the problem as input, can produce a solution in time <link xlink:type="simple" xlink:href="../578/44578.xml">
O</link>(nc), where <it>c</it> is a constant that depends on the problem but not the particular instance of the problem.</p>
<p>

Alan Cobham's 1965 paper entitled "The intrinsic computational difficulty of functions"<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> is one of the earliest mentions of the concept of the <link xlink:type="simple" xlink:href="../426/502426.xml">
complexity class</link> <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../550/658550.xml">
P</link></group>
</collection>
</class>
, consisting of problems decidable in polynomial time.  Cobham theorized that this complexity class was a good way to describe the set of feasibly <link xlink:type="simple" xlink:href="../136/442136.xml">
computable</link> problems. Any problem that cannot be contained in P is not feasible, but if a real-world problem can be solved by an algorithm existing in P, generally such an algorithm will eventually be discovered.</p>
<p>

The class P is a useful object of study because it is not sensitive to the details of the model of computation: for example, a change from a single-tape <invention wordnetid="105633385" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../403/30403.xml">
Turing machine</link></method>
</know-how>
</invention>
 to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.</p>

<sec>
<st>
Reasons behind Cobham's Thesis</st>
<p>

The thesis is widely considered to be a good rule of thumb for real-life problems. Typical input lengths that users and programmers are interested in are between 100 and 1,000,000, approximately. Consider an input length of n=100 and a polynomial algorithm whose running time is n2. This is a typical running time for a polynomial algorithm. (See the "Objections" section for a discussion of atypical running times.) The number of steps that it will require, for n=100, is 1002=10000. A typical CPU will be able to do 109 operations per second. So this algorithm will finish in (10000 ÷109) = .00001 seconds. A running time of .00001 seconds is reasonable, and that's why this is called a practical algorithm. The same algorithm with an input length of 1,000,000 will take about 17 minutes, which is also a reasonable time.</p>
<p>

Meanwhile, an algorithm that runs in exponential time might have a running time of 2n. The number of operations that it will require, for n=100, is 2100. It will take (2100 ÷ 109) ≈ 1.3×1021 seconds, which is (1.3×1021 ÷ 31556926) ≈ 4.1×1013 years.  The largest problem this algorithm could solve in a day would have n=46, which seems very small.</p>

</sec>
<sec>
<st>
Objections</st>

<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-content" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40px" src="Ambox_question.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>This section may contain  or  claims.</b> 
Please help Wikipedia by adding references. See the for details.<it>(May 2008)''</it></col>
</row>
</table>


There are many lines of objection to Cobham's thesis. The thesis essentially states that "<b>P</b>" means "easy, fast, and practical," while "not in <b>P</b>" means "hard, slow, and impractical." But this is not always true, for the following reasons:</p>
<p>

<list>
<entry level="1" type="bullet">

 It ignores constant factors and lower-order terms.</entry>
<entry level="1" type="bullet">

 It ignores the size of the exponent.</entry>
<entry level="1" type="bullet">

 It ignores the typical size of the input.</entry>
</list>
</p>
<p>

All three are related, and are general complaints about <link xlink:type="simple" xlink:href="../ury/23rd_century.xml">
analysis of algorithms</link>, but they particularly apply to Cobham's thesis since it makes an explicit claim about practicality.  Under Cobham's thesis, we are to call a problem for which the best algorithm takes 10100n instructions feasible, and a problem with an algorithm that takes 20.00001 n infeasible&mdash;even though we could never solve an instance of size n=1 with the former algorithm, whereas we could solve instance of the latter problem that have size n=106 without difficulty.  As we saw, it takes a day on a typical modern machine to process 2n operations when n=46; this may be the size of inputs we have, and the amount of time we have to solve a typical problem, making the 2n-time algorithm feasible in practice on the inputs we have.</p>
<p>

The typical response to this objection is that in general, algorithms for computational problems that naturally occur in computer science and in other fields have neither enormous nor minuscule constants and exponents.  Even when a polynomial-time algorithm has a large exponent, historically, other algorithms are subsequently developed that improve the exponent.  Also, typically, when computers are finally able to solve problems of size n fast enough, users of computers ask them to solve problems of size n+1.  These are all subjective statements.</p>
<p>

Another objection is that Cobham's thesis only considers the worst-case inputs.  Some <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../681/54681.xml">
NP-hard</link></group>
</collection>
</class>
 problems can be solved quite effectively on many of the large inputs that appear in practice; in particular, the <link xlink:type="simple" xlink:href="../248/31248.xml">
Travelling Salesman Problem</link> and <link xlink:type="simple" xlink:href="../715/4715.xml">
Satisfiability</link> can often be solved exactly even with input sizes in the tens or hundreds of thousands.  Even though the same algorithms can be made to fail (take too long, use too much memory, or trigger an error condition) on some inputs, users may be content with only sometimes getting solutions.  In some cases, in fact, an algorithm that might require exponential time is the technique of choice because it only does so on highly contrived examples; the <link xlink:type="simple" xlink:href="../458/349458.xml">
simplex method</link> is one such example.</p>
<p>

Cobham's thesis also ignores other models of computation.  A problem that requires taking exponential time to find the exact solution might allow for a fast <link xlink:type="simple" xlink:href="../105/563105.xml">
approximation algorithm</link> that returns a solution that is almost correct.  Allowing the algorithm to make random choices, or to sometimes make mistakes, might allow an algorithm to run in polynomial time rather than exponential time.  Though this is currently believed to be unlikely (see <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../771/54771.xml">
RP</link></group>
</collection>
</class>
, <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../079/4079.xml">
BPP</link></group>
</collection>
</class>
), in practice, randomized algorithms are often the fastest algorithms available for a problem (<link xlink:type="simple" xlink:href="../249/3268249.xml">
quicksort</link>, for example, or the <link>
Miller–Rabin primality test</link>).  Finally, <link xlink:type="simple" xlink:href="../220/25220.xml">
quantum computers</link> are able to solve in polynomial time some problems that have no known polynomial time algorithm on current computers, such as <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../674/42674.xml">
Shor's algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 for <link xlink:type="simple" xlink:href="../491/15491.xml">
integer factorization</link>, but this is not currently a practical concern since large-scale quantum computers are not yet available.</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal" class="book">Steven Homer and Alan L. Selman&#32;(1992).&#32;"<weblink xlink:type="simple" xlink:href="http://www.cse.buffalo.edu/tech-reports/91-04.ps">
Complexity Theory</weblink>",&#32;in Alan Kent and James G. Williams: Encyclopedia of Computer Science and Technology&#32;<b>26</b>.&#32;CRC Press.</cite>&nbsp;</entry>
<entry id="2">
 <cite style="font-style:normal" class="book">Alan Cobham&#32;(1965).&#32;"The intrinsic computational difficulty of functions", Proc. Logic, Methodology, and Philosophy of Science II.&#32;North Holland.</cite>&nbsp;</entry>
</reflist>

<list>
<entry level="1" type="bullet">

Steven Homer and Alan L. Selman (1992). <weblink xlink:type="simple" xlink:href="http://www.cse.buffalo.edu/tech-reports/91-04.ps">
"Complexity Theory"</weblink>, in Alan Kent and James G. Williams: Encyclopedia of Computer Science and Technology 26. CRC Press. </entry>
<entry level="1" type="bullet">

Alan Cobham (1965). "The intrinsic computational difficulty of functions", Proc. Logic, Methodology, and Philosophy of Science II. North Holland.</entry>
<entry level="1" type="bullet">

Lloyd, Seth. <weblink xlink:type="simple" xlink:href="http://arxiv.org/abs/quant-ph/0110141v1">
"Computational capacity of the universe"</weblink>. Accessed <link xlink:type="simple" xlink:href="../587/20587.xml">
March 31</link>, <link xlink:type="simple" xlink:href="../825/35825.xml">
2008</link>.</entry>
</list>
</p>

</sec>
</bdy>
</article>
