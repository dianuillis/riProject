<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 20:22:31[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Asymptotically optimal</title>
<id>3287619</id>
<revision>
<id>175975497</id>
<timestamp>2007-12-05T18:55:58Z</timestamp>
<contributor>
<username>Giftlite</username>
<id>37986</id>
</contributor>
</revision>
<categories>
<category>Computational complexity theory</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>, an <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> is said to be <b>asymptotically optimal</b> if, roughly speaking, for large inputs it performs at worst a constant factor (independent of the input size) worse than the <link>
best possible algorithm</link>. It is a term commonly encountered in computer science research as a result of widespread use of <link xlink:type="simple" xlink:href="../578/44578.xml">
big-O notation</link>.<p>

More formally, an algorithm is asymptotically optimal with respect to a particular resource if the problem has been proven to require &amp;Omega;(f(n)) of that resource, and the algorithm has been proven to use only O(f(n)).</p>
<p>

These proofs require an assumption of a particular <link xlink:type="simple" xlink:href="../278/1773278.xml">
model of computation</link>, i.e., certain restrictions on operations allowable with the input data.</p>
<p>

As a simple example, it's known that all <link xlink:type="simple" xlink:href="../304/3189304.xml">
comparison sort</link>s require at least &amp;Omega;(<it>n</it> log <it>n</it>) comparisons in the average and worst cases. <link xlink:type="simple" xlink:href="../039/20039.xml">
Mergesort</link> and <link xlink:type="simple" xlink:href="../995/13995.xml">
heapsort</link> are comparison sorts which perform O(<it>n</it> log <it>n</it>) comparisons, so they are asymptotically optimal in this sense. </p>
<p>

If the input data have some <it>a priori</it> properties which can be exploited in construction of algorithms, in addition to comparisons, then asymptotically faster algorithms may be possible. For example, if it is known that the N objects are <link xlink:type="simple" xlink:href="../563/14563.xml">
integer</link>s from the range [1..N], then they may be sorted O(N) time, e.g., by the <link xlink:type="simple" xlink:href="../592/97592.xml">
bucket sort</link>.</p>
<p>

A consequence of an algorithm being asymptotically optimal is that, for large enough inputs, no algorithm can outperform it by more than a fixed constant factor, such as 30%. For this reason, asymptotically optimal algorithms are often seen as the "end of the line" in research, the attaining of a result that cannot be dramatically improved upon. Conversely, if an algorithm is not asymptotically optimal, this implies that as the input grows in size, the algorithm performs increasingly worse than the best possible algorithm.</p>
<p>

In practice it's useful to find algorithms that perform better, even if they do not enjoy any asymptotic advantage. New algorithms may also present advantages such as better performance on specific inputs, decreased use of resources, or being simpler to describe and implement. Thus asymptotically optimal algorithms are not always the "end of the line".</p>
<p>

Although asymptotically optimal algorithms are important theoretical results, an asymptotically optimal algorithm might not be used in a number of practical situations:
<list>
<entry level="1" type="bullet">

 It only outperforms more commonly-used methods for <it>n</it> beyond the range of practical input sizes, such as inputs with 101000 bits.</entry>
<entry level="1" type="bullet">

 It is too complex, so that the difficulty of comprehending and implementing it outweighs its potential benefit in the range of input sizes under consideration.</entry>
<entry level="1" type="bullet">

 The inputs encountered in practice fall into special cases that have more efficient algorithms or that heuristic algorithms with bad worst-case times can nevertheless solve efficiently.</entry>
<entry level="1" type="bullet">

 On modern computers, hardware optimizations such as memory cache and parallel processing may be "broken" by an asymptotically optimal algorithm (assuming the analysis did not take these hardware optimizations into account).  In this case, there could be sub-optimal algorithms that make better use of these features and outperform an optimal algorithm on realistic data.</entry>
</list>
</p>
<p>

An example of an asymptotically-optimal algorithm not used in practice is <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../825/1051825.xml">
Bernard Chazelle</link></associate>
</research_worker>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
's linear-time algorithm for <link xlink:type="simple" xlink:href="../174/207174.xml">
triangulation</link> of a <link xlink:type="simple" xlink:href="../530/1059530.xml">
simple polygon</link>. Another is the <link xlink:type="simple" xlink:href="../434/1456434.xml">
resizable array</link> data structure published in "Resizable Arrays in Optimal Time and Space", which can index in constant time but on many machines carries a heavy practical penalty compared to ordinary array indexing.</p>

<sec>
<st>
 Formal definitions </st>

<p>

Formally, suppose that we have a lower-bound theorem showing that a problem requires &amp;Omega;(f(<it>n</it>)) time to solve for an instance (input) of size <it>n</it> (see  for the definition of &amp;Omega;). Then, an algorithm which solves the problem in O(f(<it>n</it>)) time is said to be asymptotically optimal. This can also be expressed using limits: suppose that b(<it>n</it>) is a lower bound on the running time, and a given algorithm takes time t(<it>n</it>). Then the algorithm is asymptotically optimal if:</p>
<p>

<indent level="1">

<math>\lim_{n\rightarrow\infty} \frac{t(n)}{b(n)} &amp;lt; \infty.</math>
</indent>

Note that this limit, if it exists, is always at least 1, as t(<it>n</it>) &amp;ge; b(<it>n</it>).</p>
<p>

Although usually applied to time efficiency, an algorithm can be said to use asymptotically optimal space, random bits, number of processors, or any other resource commonly measured using big-O notation.</p>
<p>

Sometimes vague or implicit assumptions can make it unclear whether an algorithm is asymptotically optimal. For example, a lower bound theorem might assume a particular <link xlink:type="simple" xlink:href="../492/60492.xml">
abstract machine</link> model, as in the case of comparison sorts, or a particular organization of memory. By violating these assumptions, a new algorithm could potentially asymptotically outperform the lower bound and the "asymptotically optimal" algorithms.</p>

</sec>
<sec>
<st>
 Speedup </st>

<p>

The nonexistence of an asymptotically optimal algorithm is called speedup. <link xlink:type="simple" xlink:href="../528/2757528.xml">
Blum's speedup theorem</link> shows that there exist artificially constructed problems with speedup. However, it is an <link xlink:type="simple" xlink:href="../335/1333335.xml">
open problem</link> whether many of the most well-known algorithms today are asymptotically optimal or not. For example, there is an O(<it>n</it>&amp;alpha;(<it>n</it>)) algorithm for finding <link xlink:type="simple" xlink:href="../795/41795.xml">
minimum spanning tree</link>s, where &amp;alpha;(<it>n</it>) is the very slowly-growing inverse of the <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../ury/30th_century.xml">
Ackermann function</link></function>
</mathematical_relation>
, but the best known lower bound is the trivial &amp;Omega;(<it>n</it>). Whether this algorithm is asymptotically optimal is unknown, and would be likely to be hailed as a significant result if it were resolved either way. Coppersmith and Winograd (1982) proved that matrix multiplication has a weak form of speedup among a restricted class of algorithms (Strassen-type bilinear identities with lambda-computation).</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<condition wordnetid="113920835" confidence="0.8">
<state wordnetid="100024720" confidence="0.8">
<problem wordnetid="114410605" confidence="0.8">
<difficulty wordnetid="114408086" confidence="0.8">
<link xlink:type="simple" xlink:href="../350/9312350.xml">
Element uniqueness problem</link></difficulty>
</problem>
</state>
</condition>
</entry>
</list>
</p>

</sec>
</bdy>
</article>
