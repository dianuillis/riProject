<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 20:21:33[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Fountain code</title>
<id>3410547</id>
<revision>
<id>243634920</id>
<timestamp>2008-10-07T11:54:14Z</timestamp>
<contributor>
<username>Garion96</username>
<id>397881</id>
</contributor>
</revision>
<categories>
<category>Coding theory</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../869/321869.xml">
Coding Theory</link> and <link xlink:type="simple" xlink:href="../810/248810.xml">
Communication Theory</link>, <b>fountain codes</b> (also known as <b>rateless erasure codes</b>) are a class of <link xlink:type="simple" xlink:href="../188/1146188.xml">
erasure codes</link> with the property that a potentially limitless sequence of encoding symbols can be generated from a given set of source symbols such that the original source symbols can be recovered from any subset of the encoding symbols of size equal to or only slightly larger than the number of source symbols.<p>

A fountain code is optimal if the original <it>k</it> source symbols can be recovered from any <it>k</it> encoding symbols. Fountain codes are known that have efficient encoding and decoding algorithms and that allow the recovery of the original <it>k</it> source symbols from any <it>k’</it> of the encoding symbols with high probability, where <it>k’</it> is just slightly larger than k.</p>

<sec>
<st>
 Practical considerations </st>
<p>

In practical simulations, for relatively short <math>K</math>, say less than <math>3,000</math>, the overhead <math>\gamma</math> defined by </p>
<p>

<indent level="1">

<math>\ K'=(1+\gamma)K</math> 
</indent>

is non trivial. With the short <math>K'</math>, both <link xlink:type="simple" xlink:href="../840/2172840.xml">
LT</link> and <link xlink:type="simple" xlink:href="../191/3340191.xml">
raptor code</link>s using the sparse <link xlink:type="simple" xlink:href="../431/244431.xml">
bipartite graph</link> (BP) algorithm never achieve an overhead <math>\gamma</math> of less than 0.10.  The raptor and online codes have an advantage (over what? Most likely: pure LT code) if and only if the BP algorithm over a check matrix <math>H</math> (or triangularization of a check matrix <math>H</math>) can recover most of input symbols.  This is a critical drawback of fountain codes.  If a raptor or online code is going to recover most of the input symbols, say more than 95%, then pre-encoding is not necessary, because transmitting a few tens of dense encoding symbols can cover up all the input symbols with an extremely high probability (called the <b>union bound</b>). The dense encoding symbols also contribute to the remaining matrix of <math>H</math> having its full column rank.  Thus, the unrecovered symbols can be solved by <link xlink:type="simple" xlink:href="../035/13035.xml">
Gaussian elimination</link> over the remaining graph.  The check equations of dense encoding symbols can be communicated to receivers by applying the same random degree generators of the sender.</p>
<p>

An accessible exposition of fountain codes can be found in the final chapter of David MacKay's free online textbook, <it><weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">
Information Theory, Inference, and Learning Algorithms</weblink></it></p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../191/3340191.xml">
Raptor codes</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../840/2172840.xml">
LT codes</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../820/1145820.xml">
Online codes</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../275/9552275.xml">
M. Luby</link></person>
, "LT Codes," In Proc. IEEE Symposium on the Foundations of Computer Science (FOCS), 2002, pp. 271-280.</entry>
<entry level="1" type="bullet">

 A. Shokrollahi, "Raptor Codes",  IEEE Transactions on Information Theory, 52(6), pp. 2551-2567, 2006. <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/iel5/18/34354/01638543.pdf?isnumber=34354&amp;prod=JNL&amp;arnumber=1638543&amp;arSt=2551&amp;ared=2567&amp;arAuthor=Shokrollahi%2C+A.">
PDF</weblink></entry>
<entry level="1" type="bullet">

 David J. C. MacKay. <it><weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">
Information Theory, Inference, and Learning Algorithms</weblink></it> Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1</entry>
</list>
</p>

</sec>
</bdy>
</article>
