<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:51:32[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Artificial neuron</title>
<id>349771</id>
<revision>
<id>210310750</id>
<timestamp>2008-05-05T12:32:46Z</timestamp>
<contributor>
<username>Betacommand</username>
<id>509520</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
</categories>
</header>
<bdy>

An <b>artificial neuron</b> is a mathematical function conceived as a crude model, or abstraction of biological <link xlink:type="simple" xlink:href="../120/21120.xml">
neuron</link>s. Artificial neurons are the constitutive units in an <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link>.  Depending on the specific model used, it can receive different names, such as <b>semi-linear unit</b>,  <b>Nv neuron</b>, <b>binary neuron</b>, <b>linear threshold function</b> or <b>McCulloch-Pitts neuron</b>. The artificial neuron receives one or more inputs (representing the one or more <link xlink:type="simple" xlink:href="../131/8131.xml">
dendrite</link>s) and sums them to produce an output (<link xlink:type="simple" xlink:href="../809/27809.xml">
synapse</link>). Usually the sums of each node are weighted, and the sum is passed through a <link xlink:type="simple" xlink:href="../103/146103.xml">
non-linear</link> function known as an <link xlink:type="simple" xlink:href="../835/14179835.xml">
activation function</link> or <link xlink:type="simple" xlink:href="../146/31146.xml">
transfer function</link>. The transfer functions usually have a <link xlink:type="simple" xlink:href="../210/87210.xml">
sigmoid shape</link>, but they may also take the form of other non-linear functions, <link xlink:type="simple" xlink:href="../130/404130.xml">
piecewise</link> linear functions, or <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Step+function%22])">
step functions</link>. They are also often  <link xlink:type="simple" xlink:href="../260/48260.xml">
monotonically increasing</link>, <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../122/6122.xml">
continuous</link></function>
</mathematical_relation>
, <link xlink:type="simple" xlink:href="../921/7921.xml">
differentiable</link> and <link xlink:type="simple" xlink:href="../509/311509.xml">
bounded</link>.<p>

The artificial neuron transfer function should not be confused with a linear system's <link xlink:type="simple" xlink:href="../146/31146.xml">
transfer function</link>.
</p>
<sec>
<st>
 Basic structure </st>

<p>

For a given artificial neuron, let there be <math>m+1</math> inputs with signals <math>x_0</math> through <math>x_m</math> and weights <math>w_0</math> through <math>w_m</math>. Usually, the <math>x_0</math> input is assigned the value <math>+1</math>, which makes it a <it>bias</it> input with <math>w_{k0}=b_k</math>. This leaves only <math>m</math> actual inputs to the neuron: from <math>x_1</math> to <math>x_m</math>.</p>
<p>

The output of <math>k</math>-th neuron is:</p>
<p>

<indent level="1">

<math>y_k =  \varphi \left( \sum_{j=0}^m w_{kj} x_j \right)</math>
</indent>

Where <math>\varphi</math> (Phi) is the transfer function.</p>
<p>

<image width="150px" src="artificial_neuron.png">
<caption>

artificial neuron.png
</caption>
</image>
</p>
<p>

The output is analogous to the <link xlink:type="simple" xlink:href="../958/958.xml">
axon</link> of a biological neuron, and its value propagates to input of the next layer, through a synapse. It may also exit the system, possibly as part of an output vector.</p>

</sec>
<sec>
<st>
History</st>

<p>

The first artificial neuron was the Threshold Logic Unit first proposed by <link xlink:type="simple" xlink:href="../508/44508.xml">
Warren McCulloch</link> and <link xlink:type="simple" xlink:href="../949/302949.xml">
Walter Pitts</link> in <link xlink:type="simple" xlink:href="../630/34630.xml">
1943</link>. As a transfer function, it employed a threshold, equivalent to using the <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../299/87299.xml">
Heaviside step function</link></function>
</mathematical_relation>
. Initially, only a simple model was considered, with binary inputs and outputs, some restrictions on the possible weights, and a more flexible threshold value. Since the beginning it was already noticed that any boolean function could be implemented by networks of such devices, what is easily seen from the fact that one can implement the AND and OR functions, and use them in the disjunctive or the <link xlink:type="simple" xlink:href="../342/73342.xml">
conjunctive normal form</link>.</p>
<p>

Researchers also soon realized that cyclic networks, with <link xlink:type="simple" xlink:href="../545/11545.xml">
feedback</link>s through neurons, could define dynamical systems with memory, but most of the research concentrated (and still does) on strictly feed-forward networks because of the smaller difficulty they present.</p>
<p>

One important and pioneering artificial neural network that used the linear threshold function was the <link xlink:type="simple" xlink:href="../777/172777.xml">
perceptron</link>, developed by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../462/1945462.xml">
Frank Rosenblatt</link></psychologist>
</scientist>
</causal_agent>
</person>
</physical_entity>
. This model already considered more flexible weight values in the neurons, and was used in machines with adaptive capabilities. The representation of the threshold values as a bias term was introduced by <link xlink:type="simple" xlink:href="../636/4793636.xml">
Widrow</link> in 1960.</p>
<p>

In the late 1980s, when research on neural networks regained strength, neurons with more continuous shapes started to be considered. The possibility of differentiating the activation function allows the direct use of the <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link> and other optimization algorithms for the adjustment of the weights. Neural networks also started to be used as a general function approximation model.</p>

</sec>
<sec>
<st>
Types of transfer functions</st>

<p>

The transfer function of a neuron is chosen to have a number of properties which either enhance or simplify the network containing the neuron.  Crucially, for instance, any <link xlink:type="simple" xlink:href="../644/2266644.xml">
multilayer perceptron</link> using a <it>linear</it> transfer function has an equivalent single-layer network; a non-linear function is therefore necessary to gain the advantages of a multi-layer network.</p>
<p>

Below, <it>u</it> refers in all cases to the weighted sum of all the inputs to the neuron, i.e. for <it>n</it> inputs, </p>
<p>

<indent level="1">

<math>
u = \sum_{i = 1}^n w_{i} x_{i}
</math>
</indent>

where <b>w</b> is a vector of <it>synaptic weights</it> and <b>x</b> is a vector of inputs.</p>

<ss1>
<st>
Step function</st>

<p>

The output <it>y</it> of this transfer function is binary, depending on whether the input meets a specified threshold, <it>θ</it>. The "signal" is sent, i.e. the output is set to one, if the activation meets the threshold.</p>
<p>

<indent level="1">

<math>y = \left\{ \begin{matrix} 1 &amp; \mbox{if }u \ge \theta \\ 0 &amp; \mbox{if }u &amp;lt; \theta \end{matrix} \right.</math>
</indent>

This function is used in <link xlink:type="simple" xlink:href="../777/172777.xml">
perceptron</link>s and often shows up in many other models. It performs a division of the <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../370/32370.xml">
space</link></concept>
</idea>
 of inputs by a <link xlink:type="simple" xlink:href="../862/99862.xml">
hyperplane</link>. It is specially useful in the last layer of a network intended to perform binary classification of the inputs. It can be approximated from other sigmoidal functions by assigning large values to the weights.</p>
<p>

See: <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../299/87299.xml">
Heaviside step function</link></function>
</mathematical_relation>
</p>

</ss1>
<ss1>
<st>
<link xlink:type="simple" xlink:href="../632/55632.xml">
Linear combination</link></st>

<p>

In this case, the output unit is simply the weighted sum of its inputs plus a <it>bias</it> term. A number of such linear neurons perform a linear transformation of the input vector. This is usually more useful in the first layers of a network. A number of analysis tools exist based on linear models, such as <link xlink:type="simple" xlink:href="../147/14147.xml">
harmonic analysis</link>, and they can all be used in neural networks with this linear neuron. The bias term allows us to make <link xlink:type="simple" xlink:href="../316/243316.xml">
affine transformations</link> to the data.</p>
<p>

See: <link xlink:type="simple" xlink:href="../102/18102.xml">
Linear Transformation</link>, <link>
Harmonic Analysis</link>, <link>
Linear Filter</link>, <link xlink:type="simple" xlink:href="../903/50903.xml">
Wavelets</link>, <link xlink:type="simple" xlink:href="../340/76340.xml">
Principal Component Analysis</link>, <link xlink:type="simple" xlink:href="../031/598031.xml">
Independent Component Analysis</link>, <link xlink:type="simple" xlink:href="../626/275626.xml">
Deconvolution</link>.</p>

</ss1>
<ss1>
<st>
<mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../210/87210.xml">
Sigmoid</link></function>
</mathematical_relation>
</st>

<p>

A fairly simple non-linear function, the <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../210/87210.xml">
logistic function</link></function>
</mathematical_relation>
 also has an easily calculated derivative, which can be important when calculating the weight updates in the network. It thus makes the network more easily manipulable mathematically, and was attractive to early computer scientists who needed to minimize the computational load of their simulations. It is commonly seen in <link>
multilayer perceptrons</link> using a <link xlink:type="simple" xlink:href="../091/1360091.xml">
backpropagation</link> algorithm.</p>


<p>

See: <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../210/87210.xml">
Sigmoid function</link></function>
</mathematical_relation>
</p>

</ss1>
</sec>
<sec>
<st>
<message wordnetid="106598915" confidence="0.8">
<subject wordnetid="106599788" confidence="0.8">
<link xlink:type="simple" xlink:href="../185/24185.xml">
Pseudocode</link></subject>
</message>
 Algorithm</st>
<p>

The following is a simple pseudocode implementation of a single TLU which takes <link xlink:type="simple" xlink:href="../335/212335.xml">
boolean</link> inputs (true or false), and returns a single <link xlink:type="simple" xlink:href="../335/212335.xml">
boolean</link> output when activated. An <link xlink:type="simple" xlink:href="../757/22757.xml">
object oriented</link> model is used. No method of training is defined, since several exist. If a purely functional model were used, the class TLU below would be replaced with a function TLU with input parameters threshold, weights, and inputs that returned a <link xlink:type="simple" xlink:href="../335/212335.xml">
boolean</link> value.</p>
<p>

<b>class</b> TLU <b>defined as:</b>
<b>data member</b> threshold <b>:</b> number
<b>data member</b> weights <b>: list of</b> numbers <b>of size</b> X
<b>function member</b> fire( inputs <b>: list of</b> booleans <b>of size</b> X ) <b>:</b> boolean <b>defined as:</b>
<b>variable</b> T <b>:</b> number
T <b>←</b> 0
<b>for each</b> i <b>in</b> 1 <b>to</b> X <b>:</b>
<b>if</b> inputs(i) <b>is</b> true <b>:</b>
T <b>←</b> T + weights(i)
<b>end if</b>
<b>end for each</b>
<b>if</b> T &amp;gt; threshold <b>:</b>
<b>return</b> true
<b>else:</b>
<b>return</b> false
<b>end if</b>
<b>end function</b>
<b>end class</b></p>

</sec>
<sec>
<st>
 Spreadsheet Example </st>

<p>

<table class="wikitable">
<row>
<col height="13" width="47" valign="bottom"></col>
<col width="42" valign="bottom"></col>
<col colspan="3" align="center" width="37" valign="bottom">
Input</col>
<col colspan="2" align="center" width="21" valign="bottom">
Initial</col>
<col colspan="2" align="center" width="39" valign="bottom">
Output</col>
<col width="35" valign="bottom"></col>
<col width="64" valign="bottom"></col>
<col width="38" valign="bottom"></col>
<col width="50" valign="bottom"></col>
<col colspan="2" align="center" width="41" valign="bottom">
Final</col>
</row>
<row>
<col height="38" valign="bottom">
Threshold</col>
<col valign="bottom">
Learning Rate</col>
<col colspan="2" align="center" valign="bottom">
Sensor values</col>
<col valign="bottom">
Desired output</col>
<col colspan="2" align="center" valign="bottom">
Weights</col>
<col colspan="2" align="center" valign="bottom">
Calculated</col>
<col align="center" valign="bottom">
Sum</col>
<col align="center" valign="bottom">
Network</col>
<col align="center" valign="bottom">
Error</col>
<col align="center" valign="bottom">
Correction</col>
<col colspan="2" align="center" valign="bottom">
Weights</col>
</row>
<row align="center" valign="bottom">
<col height="13">
TH</col>
<col>
LR</col>
<col>
X1</col>
<col>
X2</col>
<col>
Z</col>
<col>
w1</col>
<col>
w2</col>
<col>
C1</col>
<col>
C2</col>
<col>
S</col>
<col>
N</col>
<col>
E</col>
<col>
R</col>
<col>
W1</col>
<col>
W2</col>
</row>
<row align="center" valign="bottom">
<col height="13"></col>






<col>
X1 x w1</col>
<col>
X2 x w2</col>
<col>
C1+C2</col>
<col>
IF(S&amp;gt;TH,1,0)</col>
<col>
Z-N</col>
<col>
LR x E</col>
<col>
R+w1</col>
<col>
R+w2</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0.1</col>
<col>
0.3</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0.1</col>
<col>
0.3</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.1</col>
<col>
0.3</col>
<col>
0</col>
<col>
0.3</col>
<col>
0.3</col>
<col>
0</col>
<col>
1</col>
<col>
0.2</col>
<col>
0.3</col>
<col>
0.5</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.3</col>
<col>
0.5</col>
<col>
0.3</col>
<col>
0</col>
<col>
0.3</col>
<col>
0</col>
<col>
1</col>
<col>
0.2</col>
<col>
0.5</col>
<col>
0.7</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0.5</col>
<col>
0.7</col>
<col>
0.5</col>
<col>
0.7</col>
<col>
1.2</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0.7</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0.7</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0.7</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.5</col>
<col>
0.7</col>
<col>
0</col>
<col>
0.7</col>
<col>
0.7</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.5</col>
<col>
0.7</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.5</col>
<col>
0.7</col>
<col>
0.5</col>
<col>
0</col>
<col>
0.5</col>
<col>
0</col>
<col>
1</col>
<col>
0.2</col>
<col>
0.7</col>
<col>
0.9</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0.7</col>
<col>
0.9</col>
<col>
0.7</col>
<col>
0.9</col>
<col>
1.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
0.9</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
0.9</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
0.9</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0.7</col>
<col>
0.9</col>
<col>
0</col>
<col>
0.9</col>
<col>
0.9</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
0.9</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0.7</col>
<col>
0.9</col>
<col>
0.7</col>
<col>
0</col>
<col>
0.7</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
0.9</col>
</row>
<row align="center" valign="bottom">
<col height="13">
0.5</col>
<col>
0.2</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0.7</col>
<col>
0.9</col>
<col>
0.7</col>
<col>
0.9</col>
<col>
1.6</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0.7</col>
<col>
0.9</col>
</row>
</table>
</p>
<p>

Supervised neural network training for an OR gate.</p>
<p>

Note: Initial weight equals final weight of previous iteration.</p>

</sec>
<sec>
<st>
Criticism</st>
<p>

The artificial neuron is criticized by Izhikevich for not being biologically realistic.  Although this argument is technically correct, it is largely academic, as artificial neurons are not intended to perfectly model <link xlink:type="simple" xlink:href="../479/14408479.xml">
biological neurons</link>, but rather to perform complex non-linear computations.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../542/1729542.xml">
Neural network</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../777/172777.xml">
Perceptron</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../913/8220913.xml">
ADALINE</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../479/14408479.xml">
Biological neuron models</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../636/263636.xml">
Connectionism</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Bibliography </st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../508/44508.xml">
McCulloch, W</link>. and <link xlink:type="simple" xlink:href="../949/302949.xml">
Pitts, W</link>. (1943). <it>A logical calculus of the ideas immanent in nervous activity.</it> Bulletin of Mathematical Biophysics, 7:115 - 133.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.mind.ilstu.edu/curriculum/modOverview.php?modGUI=212">
http://www.mind.ilstu.edu/curriculum/modOverview.php?modGUI=212</weblink> (A good general overview, especially for teaching)</entry>
</list>
</p>


</sec>
</bdy>
</article>
