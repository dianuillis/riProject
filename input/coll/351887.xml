<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:52:53[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Friendly artificial intelligence</title>
<id>351887</id>
<revision>
<id>238489755</id>
<timestamp>2008-09-15T01:57:03Z</timestamp>
<contributor>
<username>Adam Bishop</username>
<id>13008</id>
</contributor>
</revision>
<categories>
<category>Futurology</category>
<category>Philosophy of artificial intelligence</category>
<category>Singularitarianism</category>
<category>Transhumanism</category>
</categories>
</header>
<bdy>

A <b>Friendly Artificial Intelligence</b> or <b>FAI</b> is an <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> (AI) that has a positive rather than negative effect on humanity.  Friendly AI also refers to the field of knowledge required to build such an AI.  This term particularly applies to AIs which have the potential to significantly impact humanity, such as those with intelligence comparable to or exceeding that of humans ("superintelligence"; see <link xlink:type="simple" xlink:href="../357/586357.xml">
strong AI</link> and <process wordnetid="105701363" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<explanation wordnetid="105793000" confidence="0.8">
<theory wordnetid="105989479" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../245/54245.xml">
technological singularity</link></higher_cognitive_process>
</theory>
</explanation>
</thinking>
</process>
.)  This specific term was coined by researcher <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../858/65858.xml">
Eliezer Yudkowsky</link></scientist>
</person>
 of the <association wordnetid="108049401" confidence="0.8">
<link xlink:type="simple" xlink:href="../564/596564.xml">
Singularity Institute for Artificial Intelligence</link></association>
 as a <link xlink:type="simple" xlink:href="../201/207201.xml">
technical term</link> distinct from the everyday meaning of the word "friendly"; however, the concern is much older.
<sec>
<st>
Goals and Definitions of Friendly AI</st>
<p>

Many experts have argued that AI systems with goals that are not perfectly identical to or very closely aligned with our own are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity.  Decades ago, Ryszard Michalski, one of the pioneers of Machine Learning, taught his Ph.D. students that any truly alien mind, to include machine minds, was unknowable and therefore dangerous.  More recently, Eliezer Yudkowsky has called for the creation of “Friendly AI” to mitigate the existential threat of hostile intelligences.  Stephen Omohundro argues that all advanced AI systems will, unless explicitly counteracted, exhibit a number of basic drives/tendencies/desires because of the intrinsic nature of goal-driven systems and that these drives will, “without special precautions”, cause the AI to act in ways that range from the disobedient to the dangerously unethical.  </p>
<p>

According to the proponents of Friendliness, the goals of future AIs will be more arbitrary and alien than commonly depicted in <link xlink:type="simple" xlink:href="../787/26787.xml">
science fiction</link> and earlier futurist speculation, in which AIs are often <link xlink:type="simple" xlink:href="../060/19009060.xml">
anthropomorphised</link> and assumed to share universal human modes of thought. Because AI is not guaranteed to see the "obvious" aspects of morality and sensibility that most humans see so effortlessly, the theory goes, AIs with intelligences or at least physical capabilities greater than our own may concern themselves with endeavours that humans would see as pointless or even laughably bizarre. One example Yudkowsky provides is that of an AI initially designed to solve the <condition wordnetid="113920835" confidence="0.8">
<state wordnetid="100024720" confidence="0.8">
<problem wordnetid="114410605" confidence="0.8">
<difficulty wordnetid="114408086" confidence="0.8">
<link xlink:type="simple" xlink:href="../125/19344125.xml">
Riemann hypothesis</link></difficulty>
</problem>
</state>
</condition>
, which, upon being upgraded or upgrading itself with superhuman intelligence, tries to develop <link xlink:type="simple" xlink:href="../637/19637.xml">
molecular nanotechnology</link> because it wants to convert all matter in the <link xlink:type="simple" xlink:href="../903/26903.xml">
Solar System</link> into computing material to solve the problem, killing the humans who asked the question. For humans, this would seem ridiculously absurd, but as Friendliness theory stresses, this is only because we evolved to have certain instinctive sensibilities which a robot, not sharing our evolutionary history, may not necessarily comprehend unless we design it to.</p>
<p>

Friendliness proponents stress less the danger of superhuman AIs that actively seek to <it>harm</it> humans, but more of AIs that are disastrously <it>indifferent</it> to them. Superintelligent AIs may be harmful to humans if steps are not taken to specifically design them to be benevolent. Doing so effectively is the primary goal of Friendly AI. Designing an <link xlink:type="simple" xlink:href="../357/586357.xml">
AI</link>, whether deliberately or semi-deliberately, without such "<it>Friendliness safeguards</it>", would therefore be seen as highly immoral, approximately equivalent to a parent raising a child with absolutely no regard for whether that child grows up to be a <link xlink:type="simple" xlink:href="../430/7753430.xml#xpointer(//*[./st=%22What_is_a_psychopath=3F%22])">
psychopath</link>.</p>
<p>

<link xlink:type="simple" xlink:href="../585/13585.xml">
Hugo de Garis</link> is noted for his belief that a major war between the supporters and opponents of intelligent machines, resulting in billions of deaths, is almost inevitable before the end of the 21st century.[2]:234 This prediction has attracted debate and criticism from the AI research community, and some of its more notable members, such as <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9173553029164789">
<link xlink:type="simple" xlink:href="../453/17453.xml">
Kevin Warwick</link></scientist>
</person>
, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<honoree wordnetid="110183757" confidence="0.8">
<pioneer wordnetid="110434725" confidence="0.8">
<programmer wordnetid="110481268" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<acquirer wordnetid="109764201" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<recipient wordnetid="109627906" confidence="0.8">
<creator wordnetid="109614315" confidence="0.8">
<computer_user wordnetid="109951274" confidence="0.8">
<laureate wordnetid="110249011" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<originator wordnetid="110383816" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../965/3965.xml">
Bill Joy</link></research_worker>
</originator>
</causal_agent>
</laureate>
</computer_user>
</creator>
</recipient>
</scientist>
</acquirer>
</engineer>
</programmer>
</pioneer>
</honoree>
</person>
</physical_entity>
, <writer wordnetid="110794014" confidence="0.9173553029164789">
<link xlink:type="simple" xlink:href="../269/17269.xml">
Ken MacLeod</link></writer>
, <link xlink:type="simple" xlink:href="../984/25984.xml">
Ray Kurzweil</link>, <link xlink:type="simple" xlink:href="../556/298556.xml">
Hans Moravec</link>, and <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../193/26193.xml">
Roger Penrose</link></scientist>
</person>
, have voiced their opinions on whether or not this future is likely.</p>
<p>

This belief that human goals are so arbitrary derives heavily from modern advances in <link xlink:type="simple" xlink:href="../703/9703.xml">
evolutionary psychology</link>. Friendliness theory claims that most AI speculation is clouded by analogies between AIs and humans, and assumptions that all possible minds must exhibit characteristics that are actually <link xlink:type="simple" xlink:href="../181/2232181.xml">
psychological adaptation</link>s that exist in humans (and other animals) only because they were once beneficial and perpetuated by <link xlink:type="simple" xlink:href="../147/21147.xml">
natural selection</link>. This idea is expanded on greatly in section two of Yudkowsky's <it>Creating Friendly AI</it>, <weblink xlink:type="simple" xlink:href="http://www.singinst.org/CFAI/anthro.html">
"Beyond anthropomorphism"</weblink>.</p>
<p>

Many supporters of FAI speculate that an AI able to reprogram and improve itself, <link xlink:type="simple" xlink:href="../455/1167455.xml">
Seed AI</link>, is likely to create a huge power disparity between itself and statically intelligent human minds; that its ability to enhance itself would very quickly outpace the human ability to exercise any meaningful control over it. While many doubt such scenarios are likely, if they were to occur, it would be important for AI to act benevolently towards humans. As <village wordnetid="108672738" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../308/22308.xml">
Oxford</link></village>
 philosopher <physical_entity wordnetid="100001930" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../292/408292.xml">
Nick Bostrom</link></scholar>
</causal_agent>
</intellectual>
</person>
</philosopher>
</physical_entity>
 puts it:</p>
<p>

<indent level="1">

"<it>Basically we should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.</it>'"
</indent>

It is important to stress that Yudkowsky's Friendliness Theory is very different from ideas relating to the concept that AIs may be made safe by including specifications or strictures into their programming or hardware architecture, often exemplified by <person wordnetid="100007846" confidence="0.9508927676800064">
<humorist wordnetid="110191943" confidence="0.9173553029164789">
<biochemist wordnetid="109854915" confidence="0.9173553029164789">
<writer wordnetid="110794014" confidence="0.9173553029164789">
<historian wordnetid="110177150" confidence="0.9173553029164789">
<novelist wordnetid="110363573" confidence="0.9173553029164789">
<essayist wordnetid="110064405" confidence="0.9173553029164789">
<link xlink:type="simple" xlink:href="../573/14573.xml">
Isaac Asimov</link></essayist>
</novelist>
</historian>
</writer>
</biochemist>
</humorist>
</person>
's <law wordnetid="108441203" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../136/60136.xml">
Three Laws of Robotics</link></group>
</collection>
</law>
, which would, in principle, force a machine to do nothing which might harm a human, or destroy it if it does attempt to do so. Friendliness Theory rather holds that the inclusion of such laws would be futile, because no matter how such laws are phrased or described, a truly intelligent machine with genuine (human-level or greater) creativity and resourcefulness could potentially design infinitely many ways of circumventing such laws, no matter how broadly or narrowly defined they were, or otherwise how categorically comprehensive they were formulated to be.</p>
<p>

Rather, Yudkowsky's Friendliness Theory relates, through the fields of <link xlink:type="simple" xlink:href="../657/350657.xml">
biopsychology</link>, that if a truly intelligent mind feels motivated to carry out some function, the result of which would violate some constraint imposed against it, then given enough time and resources, it will develop methods of defeating all such constraints (as humans have done repeatedly throughout the history of technological civilization). Therefore, the appropriate response to the threat posed by such intelligence, is to attempt to ensure that such intelligent minds specifically feel motivated <it>to not</it> harm other intelligent minds (in any sense of the word "harm"), and to that end will deploy their resources towards devising better methods of keeping them from harm. In this scenario, an AI would be <it>free</it> to murder, injure, or enslave a human being, but it would strongly desire not to do so and would only do so if it judged, according to that same desire, that some vastly greater good to that human or to human beings in general would result (though this particular idea is explored in Asimov's <book wordnetid="106410904" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../545/14545.xml">
I, Robot</link></book>
 stories, via the Zeroth Law).
Therefore, an AI designed with Friendliness safeguards would do everything in its power to ensure humans do not come to "harm", and to ensure that any other AIs that are built would also want humans not to come to harm, and to ensure that any upgraded or modified AIs, whether itself or others, would also never want humans to come to harm - it would try to minimize the harm done to all intelligent minds in perpetuity. As Yudkowsky puts it:</p>
<p>

<indent level="1">

"<it><link xlink:type="simple" xlink:href="../379/19379.xml">
Gandhi</link> does not want to commit murder, and does not want to modify himself to commit murder.</it>"
</indent>

One of the more contentious, recent hypotheses in Friendliness theory is the <link xlink:type="simple" xlink:href="../081/19215081.xml">
Coherent Extrapolated Volition</link> model, also developed by Yudkowsky. According to him our coherent extrapolated volition is our choices and the actions we would collectively take if we knew more, thought faster, were more the people we wished we were, and had grown up farther together. Yudkowsky believes a Friendly AI should initially seek to determine the coherent extrapolated volition of humanity, with which it can then alter its goals accordingly. Many other researchers believe, however, that the collective will of humanity will not converge to a single coherent set of goals even if "we knew more, thought faster, were more the people we wished we were, and had grown up farther together."</p>

</sec>
<sec>
<st>
Requirements for FAI and effective FAI</st>
<p>

The requirements for FAI to be effective, both internally, to protect humanity against <link xlink:type="simple" xlink:href="../517/97517.xml">
unintended consequence</link> of the AI in question and externally to protect against other non-FAIs arising from whatever source are:
<list>
<entry level="1" type="number">

 Friendliness - <it>that an AI feel <link xlink:type="simple" xlink:href="../023/806023.xml">
sympathetic</link> towards humanity and all life, and seek for their best interests''</it></entry>
<entry level="1" type="number">

 Conservation of Friendliness - <it>that an AI must desire to pass on its value system to all of its offspring and inculcate its values into others of its kind''</it></entry>
<entry level="1" type="number">

 Intelligence - <it>that an AI be smart enough to see how it might engage in altruistic behaviour to the greatest degree of equality, so that it is not kind to some but more cruel to others as a consequence, and to balance interests effectively''</it></entry>
<entry level="1" type="number">

 Self-improvement - <it>that an AI feel a sense of longing and striving for improvement both of itself and of all life as part of the consideration of wealth, while respecting and sympathising with the informed choices of lesser intellects not to improve themselves''</it></entry>
<entry level="1" type="number">

 <link xlink:type="simple" xlink:href="../468/2770468.xml">
First mover advantage</link> - <it>the first goal-driven general self-improving AI "wins" in the memetic sense, because it is powerful enough to prevent any other AI emerging, which might compete with its own goals.''</it></entry>
</list>
</p>

</sec>
<sec>
<st>
Promotion and support</st>
<p>

Promoting Friendly AI is one of the primary goals of the <association wordnetid="108049401" confidence="0.8">
<link xlink:type="simple" xlink:href="../564/596564.xml">
Singularity Institute for Artificial Intelligence</link></association>
, along with obtaining funding for, and ultimately creating a <link xlink:type="simple" xlink:href="../455/1167455.xml">
seed AI</link> program implementing the ideas of Friendliness theory.</p>
<p>

Several notable <link xlink:type="simple" xlink:href="../301/1508301.xml">
futurists</link> have voiced support for Friendly AI, including author and inventor <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9173553029164789">
<link xlink:type="simple" xlink:href="../984/25984.xml">
Raymond Kurzweil</link></scientist>
</person>
, medical life-extension advocate <link xlink:type="simple" xlink:href="../517/748517.xml">
Aubrey de Grey</link>, and <link xlink:type="simple" xlink:href="../852/410852.xml">
World Transhumanist Association</link> founder <physical_entity wordnetid="100001930" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../292/408292.xml">
Dr. Nick Bostrom</link></scholar>
</causal_agent>
</intellectual>
</person>
</philosopher>
</physical_entity>
 of <link xlink:type="simple" xlink:href="../797/31797.xml">
Oxford University</link>.</p>

</sec>
<sec>
<st>
Criticism</st>
<p>

One notable critic of Friendliness theory is <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../646/976646.xml">
Bill Hibbard</link></scientist>
</causal_agent>
</person>
</physical_entity>
, author of <it><link xlink:type="simple" xlink:href="../646/976646.xml">
Super-Intelligent Machines</link></it>, who considers the theory incomplete. Hibbard writes there should be broader <link xlink:type="simple" xlink:href="../986/22986.xml">
political</link> involvement in the design of AI and AI morality. He also believes that initially seed AI could only be created by powerful <link xlink:type="simple" xlink:href="../540/251540.xml">
private sector</link> interests (a view not shared by Yudkowsky), and that <link xlink:type="simple" xlink:href="../491/214491.xml">
multinational corporation</link>s and the like would have no <link xlink:type="simple" xlink:href="../023/422023.xml">
incentive</link> to implement Friendliness theory.</p>
<p>

In his criticism of the Singularity Institute's Friendly AI guidelines, he suggests an AI goal architecture in which <link xlink:type="simple" xlink:href="../482/682482.xml">
human</link> happiness is determined by human behaviors indicating happiness: "Any artifact implementing 'learning' [...] must have 'human happiness' as its only initial reinforcement value [...] and 'human happiness' values are produced by an algorithm produced by supervised learning, to recognize happiness in human facial expressions, voices and body language, as trained by human behavior experts." Yudkowsky later criticized this proposal by remarking that such a utility function would be better satisfied by filling the Solar System with microscopic smiling mannequins than by making existing humans happier.</p>
<p>

Others, like <link>
Ben Goertzel</link>, an <link xlink:type="simple" xlink:href="../357/586357.xml">
Artificial General Intelligence</link> researcher and now Director of research at the <association wordnetid="108049401" confidence="0.8">
<link xlink:type="simple" xlink:href="../564/596564.xml">
Singularity Institute</link></association>
, support the basic principles of the Friendly Artificial Intelligence concept but believe that guaranteed Friendliness is not possible.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../455/1167455.xml">
Seed AI</link> - a theory related to Friendly AI</entry>
<entry level="1" type="bullet">

<word wordnetid="106286395" confidence="0.8">
<part wordnetid="113809207" confidence="0.8">
<neologism wordnetid="106294441" confidence="0.8">
<language_unit wordnetid="106284225" confidence="0.8">
<link xlink:type="simple" xlink:href="../138/516138.xml">
Singularitarianism</link></language_unit>
</neologism>
</part>
</word>
 - a moral philosophy advocated by proponents of Friendly AI</entry>
<entry level="1" type="bullet">

<process wordnetid="105701363" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<explanation wordnetid="105793000" confidence="0.8">
<theory wordnetid="105989479" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../245/54245.xml">
Technological singularity</link></higher_cognitive_process>
</theory>
</explanation>
</thinking>
</process>
</entry>
</list>
</p>

</sec>
<sec>
<st>
Further reading</st>
<p>

<list>
<entry level="1" type="bullet">

 Yudkowsky, E.  2006. <weblink xlink:type="simple" xlink:href="http://www.singinst.org/upload/artificial-intelligence-risk.pdf">
Artificial Intelligence as a Positive and Negative Factor in Global Risk</weblink>. To appear in Global Catastrophic Risks, Oxford University Press, 2007. </entry>
</list>
</p>
<p>

Discusses Artificial Intelligence from the perspective of <link>
Existential risk</link>, introducing the term "Friendly AI".  In particular, Sections 1-4 give background to the definition of Friendly AI in Section 5.  Section 6 gives two classes of mistakes (technical and philosophical) which would both lead to the accidental creation of non-Friendly AIs.  Sections 7-13 discuss further related issues.</p>
<p>

<list>
<entry level="1" type="bullet">

 Omohundro, S.  2008 <weblink xlink:type="simple" xlink:href="http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">
The Basic AI Drives</weblink> Appeared in AGI-08 - Proceedings of the First Conference on Artificial General Intelligence</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.nickbostrom.com/ethics/ai.html">
Ethical Issues in Advanced Artificial Intelligence</weblink> by Nick Bostrom</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://singinst.org/friendly/whatis.html">
What is Friendly AI?</weblink> &mdash; A brief explanation of Friendly AI by the Singularity Institute.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.singinst.org/ourresearch/publications/guidelines.html">
SIAI Guidelines on Friendly AI</weblink> &mdash; The Singularity Institute's Official Guidelines</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.singinst.org/ourresearch/publications/CFAI/index.html">
Creating Friendly AI</weblink> &mdash; A near book-length explanation from the SIAI</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.ssec.wisc.edu/~billh/g/SIAI_critique.html">
Critique of the SIAI Guidelines on Friendly AI</weblink> &mdash; by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../646/976646.xml">
Bill Hibbard</link></scientist>
</causal_agent>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.optimal.org/peter/siai_guidelines.htm">
Commentary on SIAI's Guidelines on Friendly AI</weblink> &mdash; by <link>
Peter Voss</link>.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.fungible.com/respect/index.html">
Respectful AI Project Page</weblink> by <link>
Tim Freeman</link></entry>
</list>
</p>

</sec>
</bdy>
</article>
