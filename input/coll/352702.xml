<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:52:11[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Cooley-Tukey FFT algorithm</title>
<id>352702</id>
<revision>
<id>225025539</id>
<timestamp>2008-07-11T14:57:16Z</timestamp>
<contributor>
<username>Jennavecia</username>
<id>3080968</id>
</contributor>
</revision>
<categories>
<category>FFT algorithms</category>
</categories>
</header>
<bdy>

The <b>Cooley-Tukey <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link></b>, named after <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<employee wordnetid="110053808" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../795/1565795.xml">
J.W. Cooley</link></scholar>
</mathematician>
</employee>
</scientist>
</causal_agent>
</alumnus>
</worker>
</intellectual>
</person>
</physical_entity>
 and <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../974/415974.xml">
John Tukey</link></scientist>
</person>
, is the most common <link xlink:type="simple" xlink:href="../512/11512.xml">
fast Fourier transform</link> (FFT) algorithm.  It re-expresses the <link xlink:type="simple" xlink:href="../811/8811.xml">
discrete Fourier transform</link> (DFT) of an arbitrary <link xlink:type="simple" xlink:href="../289/82289.xml">
composite</link> size <it>N</it> = <it>N</it>1<it>N</it>2 in terms of smaller DFTs of sizes <it>N</it>1 and <it>N</it>2, <link xlink:type="simple" xlink:href="../407/25407.xml">
recursively</link>, in order  to reduce the computation time to O(<it>N</it> log <it>N</it>) for highly-composite <it>N</it> (<link xlink:type="simple" xlink:href="../039/1035039.xml">
smooth number</link>s). Because of the algorithm's importance, specific variants and implementation styles have become known by their own names, as described below.<p>

Because the Cooley-Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT.  For example, <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../408/241408.xml">
Rader's</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 or <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../431/241431.xml">
Bluestein's</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 algorithm can be used to handle large prime factors that cannot be decomposed by Cooley-Tukey, or the <link xlink:type="simple" xlink:href="../490/241490.xml">
prime-factor algorithm</link> can be exploited for greater efficiency in separating out <link xlink:type="simple" xlink:href="../556/6556.xml">
relatively prime</link> factors.</p>
<p>

See also the <link xlink:type="simple" xlink:href="../512/11512.xml">
fast Fourier transform</link> for information on other FFT algorithms, specializations for real and/or symmetric data, and accuracy in the face of finite <link xlink:type="simple" xlink:href="../376/11376.xml">
floating-point</link> precision.</p>

<sec>
<st>
 History </st>
<p>

This algorithm, including its recursive application, was invented around <link xlink:type="simple" xlink:href="../074/35074.xml">
1805</link> by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../125/6125.xml">
Carl Friedrich Gauss</link></scientist>
</person>
, who used it to interpolate the trajectories of the <link xlink:type="simple" xlink:href="../791/791.xml">
asteroid</link>s <planet wordnetid="109394007" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../262/47262.xml">
Pallas</link></planet>
 and <planet wordnetid="109394007" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../130/80130.xml">
Juno</link></planet>
, but his work was not widely recognized (being published only posthumously and in <language wordnetid="106282651" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../983/21983.xml">
neo-Latin</link></language>
) <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. Gauss did not analyze the asymptotic computational time, however. Various limited forms were also rediscovered several times throughout the 19th and early 20th centuries<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.  FFTs became popular after <link xlink:type="simple" xlink:href="../795/1565795.xml">
J. W. Cooley</link> of <link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link> and <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../974/415974.xml">
John W. Tukey</link></scientist>
</person>
 of <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../922/23922.xml">
Princeton</link></university>
 published a paper in 1965 reinventing the algorithm and describing how to perform it conveniently on a computer <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>. </p>
<p>

Tukey reportedly came up with the idea during a meeting of a US presidential advisory committee discussing ways to detect <link xlink:type="simple" xlink:href="../775/337775.xml">
nuclear-weapon tests</link> in the <system wordnetid="108435388" confidence="0.8">
<economy wordnetid="108366753" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../779/26779.xml">
Soviet Union</link></group>
</economy>
</system>
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.  Another participant at that meeting, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<physicist wordnetid="110428004" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../415/5189415.xml">
Richard Garwin</link></associate>
</scientist>
</causal_agent>
</colleague>
</person>
</physicist>
</peer>
</physical_entity>
 of IBM, recognized the potential of the method and put Tukey in touch with Cooley, who implemented it for a different (and less-classified) problem: analyzing 3d crystallographic data (see also: <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../512/11512.xml#xpointer(//*[./st=%22Multidimensional+FFT+algorithms%22])">
multidimensional FFTs</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
).  Cooley and Tukey subsequently published their joint paper, and wide adoption quickly followed.</p>
<p>

The fact that Gauss had described the same algorithm (albeit without analyzing its asymptotic cost) was not realized until several years after Cooley and Tukey's 1965 paper.  Their paper cited as inspiration only work by I. J. Good on what is now called the <link xlink:type="simple" xlink:href="../490/241490.xml">
prime-factor FFT algorithm</link> (PFA), but it was not realized until later that PFA is a quite different algorithm (only working for sizes that have <link xlink:type="simple" xlink:href="../556/6556.xml">
relatively prime</link> factors, unlike any composite size for Cooley-Tukey).</p>

</sec>
<sec>
<st>
 The radix-2 DIT case </st>
<p>

A <b>radix-2</b> decimation-in-time (<b>DIT</b>) FFT is the simplest and most common form of the Cooley-Tukey algorithm, although highly optimized Cooley-Tukey implementations typically use other forms of the algorithm as described below. Radix-2 DIT divides a DFT of size <it>N</it> into two <link xlink:type="simple" xlink:href="../495/177495.xml">
interleaved</link> DFTs (hence the name "radix-2") of size <it>N</it>/2 with each recursive stage. </p>
<p>

The DFT is defined by the formula:
<indent level="1">

<math> X_k =  \sum_{n=0}^{N-1} x_n e^{-\frac{2\pi i}{N} nk}</math>
</indent>
where <math>k</math> is an integer ranging from <math>0</math> to <math>N-1</math>.</p>
<p>

Radix-2 DIT first computes the Fourier transforms of the even-indexed numbers
<math>x_{2m} \ </math> (<math>x_0, x_2, \ldots, x_{N-2}</math>)
and of the odd-indexed numbers <math>x_{2m+1} \ </math> (<math>x_1, x_3, \ldots, x_{N-1}</math>), and then combines those two results to produce the Fourier transform of the whole sequence. This idea can then be performed <link xlink:type="simple" xlink:href="../407/25407.xml">
recursively</link> to reduce the overall runtime to O(<it>N</it> log <it>N</it>).  This simplified form assumes that <it>N</it> is a <link xlink:type="simple" xlink:href="../948/376948.xml">
power of two</link>; since the number of sample points <it>N</it> can usually be chosen freely by the application, this is often not an important restriction.  </p>
<p>

More explicitly, let us write <math>M=N/2</math> and denote the DFT of the even-indexed numbers <math>x_{2m}</math> by <math>E_j</math> and the DFT of the odd-indexed numbers <math>x_{2m+1}</math> by <math>O_j</math> (<math>m=0,...,M-1</math>, <math>j=0,...,M-1</math>). Then it follows:</p>
<p>

<indent level="1">

<math> \begin{matrix}

X_k &amp; = &amp;  \sum_{m=0}^{\frac{N}{2}-1} x_{2m} e^{-\frac{2\pi i}{N} (2m)k}
 +  \sum_{m=0}^{\frac{N}{2}-1} x_{2m+1} e^{-\frac{2\pi i}{N} (2m+1)k} \\ \\

&amp; = &amp; \sum_{m=0}^{M-1} x_{2m} e^{-\frac{2\pi i}{M} mk}
 +  e^{-\frac{2\pi i}{N}k} \sum_{m=0}^{M-1} x_{2m+1} e^{-\frac{2\pi i}{M} mk} \\ \\

&amp; = &amp; \left\{ \begin{matrix}
 E_k +  e^{-\frac{2\pi i}{N}k} O_k &amp; \mbox{if } k&amp;lt;M \\ \\

 E_{k-M} -  e^{-\frac{2\pi i}{N}(k-M)} O_{k-M} &amp; \mbox{if } k \geq M. \end{matrix} \right.

\end{matrix}
</math>
</indent>
Here we have used the critical fact that <math>E_{k+M}=E_k</math> and <math>O_{k+M}=O_k</math>, so that these DFTs, in addition to having only <it>M</it> sample points, needs only be evaluated for <it>M</it> values of <it>k</it>. The original DFT has thus been divided into two DFTs of size <it>N/2</it>.</p>
<p>

This process is an example of the general technique of <link xlink:type="simple" xlink:href="../154/201154.xml">
divide and conquer algorithm</link>s; in many traditional implementations, however, the explicit recursion is avoided, and instead one traverses the computational tree in <link xlink:type="simple" xlink:href="../026/97026.xml">
breadth-first</link> fashion.</p>
<p>

The above re-expression of a size-<it>N</it> DFT as two size-<it>N</it>/2 DFTs is sometimes called the <b>Danielson-<physical_entity wordnetid="100001930" confidence="0.8">
<expert wordnetid="109617867" confidence="0.8">
<analyst wordnetid="109790482" confidence="0.8">
<physicist wordnetid="110428004" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<immigrant wordnetid="110199489" confidence="0.8">
<migrant wordnetid="110314952" confidence="0.8">
<traveler wordnetid="109629752" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../748/1657748.xml">
Lanczos</link></mathematician>
</scientist>
</causal_agent>
</traveler>
</migrant>
</immigrant>
</person>
</physicist>
</analyst>
</expert>
</physical_entity>
</b> <link xlink:type="simple" xlink:href="../634/18634.xml">
lemma</link>, since the identity was noted by those two authors in 1942<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> (influenced by <link>
Runge's</link> 1903 work).  They applied their lemma in a "backwards" recursive fashion, repeatedly <it>doubling</it> the DFT size until the transform spectrum converged (although they apparently didn't realize the <link xlink:type="simple" xlink:href="../768/239768.xml">
linearithmic</link> asymptotic complexity they had achieved).  The Danielson-Lanczos work predated widespread availability of <link xlink:type="simple" xlink:href="../457/7878457.xml">
computer</link>s and required hand calculation (possibly with mechanical aides such as <link xlink:type="simple" xlink:href="../133/74133.xml">
adding machine</link>s); they reported a computation time of 140 minutes for a size-64 DFT operating on <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../512/11512.xml#xpointer(//*[./st=%22FFT+algorithms+specialized+for+real+and/or+symmetric+data%22])">
real inputs</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 to 3-5 significant digits.  Cooley and Tukey's 1965 paper reported a running time of 0.02 minutes for a size-2048 complex DFT on an <computer wordnetid="103082979" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<link xlink:type="simple" xlink:href="../324/147324.xml">
IBM 7094</link></machine>
</device>
</instrumentality>
</artifact>
</computer>
 (probably in 36-bit <link xlink:type="simple" xlink:href="../376/11376.xml">
single precision</link>, ~8 digits).  Rescaling the time by the number of operations, this corresponds roughly to a speedup factor of around 800,000.  (140 minutes for size 64 may sound like a long time, but it corresponds to an average of at most 16 seconds per floating-point operation, around 20% of which are multiplications...this is a fairly impressive rate for a human being to sustain for over two hours, especially considering the bookkeeping overhead.)</p>

</sec>
<sec>
<st>
 General factorizations </st>
<p>

More generally, Cooley-Tukey algorithms recursively re-express  a DFT of a composite size <it>N</it> = <it>N</it>1<it>N</it>2 as<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>:</p>
<p>

<list>
<entry level="1" type="number">

 Perform <it>N</it>1 DFTs of size <it>N</it>2.</entry>
<entry level="1" type="number">

 Multiply by complex <link xlink:type="simple" xlink:href="../950/171950.xml">
roots of unity</link> called <link xlink:type="simple" xlink:href="../398/2707398.xml">
twiddle factor</link>s.</entry>
<entry level="1" type="number">

 Perform <it>N</it>2 DFTs of size <it>N</it>1.</entry>
</list>
</p>
<p>

Typically, either <it>N</it>1 or <it>N</it>2 is a small factor (<it>not</it> necessarily prime), called the <b>radix</b> (which can differ between stages of the recursion).  If <it>N</it>1 is the radix, it is called a <b>decimation in time</b> (DIT) algorithm, whereas if <it>N</it>2 is the radix, it is <b>decimation in frequency</b> (DIF, also called the Sande-Tukey algorithm). The version presented above was a radix-2 DIT algorithm; in the final expression, the phase multiplying the odd transform is the twiddle factor, and the +/- combination (<it>butterfly</it>) of the even and odd transforms is a size-2 DFT.  (The radix's small DFT is sometimes known as a <link xlink:type="simple" xlink:href="../212/2707212.xml">
butterfly</link>, so-called because of the shape of the <link xlink:type="simple" xlink:href="../164/1344164.xml">
dataflow diagram</link> for the radix-2 case.)</p>
<p>

There are many other variations on the Cooley-Tukey algorithm.  <b>Mixed-radix</b> implementations handle composite sizes with a variety of (typically small) factors in addition to two, usually (but not always) employing the O(<it>N</it>2) algorithm for the prime base cases of the recursion.  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../890/5283890.xml">
Split radix</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 merges radices 2 and 4, exploiting the fact that the first transform of radix 2 requires no twiddle factor, in order to achieve the lowest known arithmetic operation count for power-of-two sizes.  (On present-day computers, performance is determined more by <link xlink:type="simple" xlink:href="../181/849181.xml">
cache</link> and <link xlink:type="simple" xlink:href="../187/1236187.xml">
CPU pipeline</link> considerations than by strict operation counts; well-optimized FFT implementations often employ larger radices and/or hard-coded base-case transforms of significant size.)  Another way of looking at the Cooley-Tukey algorithm is that it re-expresses a size <it>N</it> one-dimensional DFT as an <it>N</it>1 by <it>N</it>2 two-dimensional DFT (plus twiddles), where the output matrix is <link xlink:type="simple" xlink:href="../844/173844.xml">
transposed</link>. The net result of all of these transpositions, for a radix-2 algorithm, corresponds to a bit reversal of the input (DIF) or output (DIT) indices.  If, instead of using a small radix, one employs a radix of roughly √<it>N</it> and explicit input/output matrix transpositions, it is called a <b>four-step</b> algorithm (or <it>six-step</it>, depending on the number of transpositions), initially proposed to improve memory locality,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref> e.g. for cache optimization or <link xlink:type="simple" xlink:href="../722/1881722.xml">
out-of-core</link> operation, and was later shown to be an optimal <link xlink:type="simple" xlink:href="../377/1773377.xml">
cache-oblivious algorithm</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></p>
<p>

The general Cooley-Tukey factorization rewrites the indices <it>k</it> and <it>n</it> as <math>k = N_2 k_1 + k_2</math> and <math>n = N_1 n_2 + n_1</math>, respectively, where the indices <it>k</it>a and <it>n</it>a run from 0..<it>N</it>a-1 (for <it>a</it> of 1 or 2).  That is, it re-indexes the input (<it>n</it>) and output (<it>k</it>) as <it>N</it>1 by <it>N</it>2 two-dimensional arrays in <link xlink:type="simple" xlink:href="../786/1620786.xml">
column-major</link> and <link xlink:type="simple" xlink:href="../786/1620786.xml">
row-major order</link>, respectively; the difference between these indexings is a transposition, as mentioned above.  When this re-indexing is substituted into the DFT formula for <it>nk</it>, the <math>N_1 n_2 N_2 k_1</math> cross term vanishes (its exponential is unity), and the remaining terms give</p>
<p>

<indent level="1">

<math>X_{N_2 k_1 + k_2} =
      \sum_{n_1=0}^{N_1-1} \sum_{n_2=0}^{N_2-1}
         x_{N_1 n_2 + n_1}
         e^{-\frac{2\pi i}{N_1 N_2} \cdot (N_1 n_2 + n_1) \cdot (N_2 k_1 + k_2) }</math>
</indent>
::<math>= 
    \sum_{n_1=0}^{N_1-1} 
      \left[ e^{-\frac{2\pi i}{N} n_1 k_2 } \right]
      \left( \sum_{n_2=0}^{N_2-1} x_{N_1 n_2 + n_1}  
              e^{-\frac{2\pi i}{N_2} n_2 k_2 } \right)
      e^{-\frac{2\pi i}{N_1} n_1 k_1 }
</math></p>
<p>

where the inner sum is a DFT of size <it>N</it>2, the outer sum is a DFT of size <it>N</it>1, and the [...] bracketed term is the twiddle factor.</p>
<p>

An arbitrary radix <it>r</it> (as well as mixed radices) can be employed, as was shown by both Cooley and Tukey<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> as well as Gauss (who gave examples of radix-3 and radix-6 steps).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>  Cooley and Tukey originally assumed that the radix butterfly required O(<it>r</it>2) work and hence reckoned the complexity for a radix <it>r</it> to be O(<it>r</it>2&nbsp;<it>N</it>/<it>r</it>&nbsp;log<it>rN</it>) = O(<it>N</it>&nbsp;log2(<it>N</it>)&nbsp;<it>r</it>/log2<it>r</it>); from calculation of values of <it>r</it>/log2<it>r</it> for integer values of <it>r</it> from 2 to 12 the optimal radix is found to be 3 (the closest integer to <it><link xlink:type="simple" xlink:href="../633/9633.xml">
e</link></it>, which minimizes <it>r</it>/log2<it>r</it>).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref>  This analysis was erroneous, however: the radix-butterfly is also a DFT and can be performed via an FFT algorithm in O(<it>r</it>  log <it>r</it>) operations, hence the radix <it>r</it> actually cancels in the complexity O(<it>r</it>&nbsp;log(<it>r</it>)&nbsp;<it>N</it>/<it>r</it>&nbsp;log<it>rN</it>), and the optimal <it>r</it> is determined by more complicated considerations.  In practice, quite large <it>r</it> (32 or 64) are important in practice in order to effectively exploit e.g. the large number of <link xlink:type="simple" xlink:href="../432/486432.xml">
processor register</link>s on modern processors, and even a unbounded radix <it>r</it>=&amp;radic;<it>N</it> also achieves O(<it>N</it>&nbsp;log&nbsp;<it>N</it>) complexity and has theoretical and practical advantages for large <it>N</it> as mentioned above.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></p>

</sec>
<sec>
<st>
 Data reordering, bit reversal, and in-place algorithms </st>
<p>

Although the abstract Cooley-Tukey factorization of the DFT, above, applies in some form to all implementations of the algorithm, much greater diversity exists in the techniques for ordering and accessing the data at each stage of the FFT. Of special interest is the problem of devising an <link xlink:type="simple" xlink:href="../861/219861.xml">
in-place algorithm</link> that overwrites its input with its output data using only O(1) auxiliary storage.</p>
<p>

The most well-known reordering technique involves explicit <b>bit reversal</b> for in-place radix-2 algorithms.  Bit reversal is the <link xlink:type="simple" xlink:href="../027/44027.xml">
permutation</link> where the data at an index <it>n</it>, written in <link xlink:type="simple" xlink:href="../686/238686.xml">
binary</link> with digits <it>b</it>4<it>b</it>3<it>b</it>2<it>b</it>1<it>b</it>0 (e.g. 5 digits for <it>N</it>=32 inputs), is transferred to the index with reversed digits <it>b</it>0<it>b</it>1<it>b</it>2<it>b</it>3<it>b</it>4 . Consider the last stage of a radix-2 DIT algorithm like the one presented above, where the output is written in-place over the input: when <math>E_k</math> and <math>O_k</math> are combined with a size-2 DFT, those two values are overwritten by the outputs.  However, the two output values should go in the first and second <it>halves</it> of the output array, corresponding to the <it>most</it> significant bit <it>b</it>4 (for <it>N</it>=32); whereas the two inputs <math>E_k</math> and <math>O_k</math> are interleaved in the even and odd elements, corresponding to the <it>least</it> significant bit <it>b</it>0.  Thus, in order to get the output in the correct place, these two bits must be swapped in the input. If you include all of the recursive stages of a radix-2 DIT algorithm, <it>all</it> the bits must be swapped and thus one must pre-process the <it>input</it> with a bit reversal to get in-order output. Correspondingly, the reversed (dual) algorithm is radix-2 DIF, and this takes in-order input and produces bit-reversed <it>output</it>, requiring a bit-reversal post-processing step.  Alternatively, some applications (such as convolution) work equally well on bit-reversed data, so one can do radix-2 DIF without bit reversal, followed by processing, followed by the radix-2 DIT inverse DFT without bit reversal to produce final results in the natural order.</p>
<p>

Many FFT users, however, prefer natural-order outputs, and a separate, explicit bit-reversal stage can have a non-negligible impact on the computation time, even though bit reversal can be done in O(<it>N</it>) time and has been the subject of much research<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref>). Also, while the permutation is a bit reversal in the radix-2 case, it is more generally an arbitrary (mixed-base) digit reversal for the mixed-radix case, and the permutation algorithms become more complicated to implement. Moreover, it is desirable on many hardware architectures to re-order intermediate stages of the FFT algorithm so that they operate on consecutive (or at least more localized) data elements. To these ends, a number of alternative implementation schemes have been devised for the Cooley-Tukey algorithm that do not require separate bit reversal and/or involve additional permutations at intermediate stages.</p>
<p>

The problem is greatly simplified if it is <b>out-of-place</b>: the output array is distinct from the input array or, equivalently, an equal-size auxiliary array is available.  The <b>Stockham auto-sort</b> algorithm<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref> performs every stage of the FFT out-of-place, typically writing back and forth between two arrays, transposing one "digit" of the indices with each stage, and has been especially popular on <link xlink:type="simple" xlink:href="../359/55359.xml">
SIMD</link> architectures<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref>.  Even greater potential SIMD advantages (more consecutive accesses) have been proposed for the <b>Pease</b> algorithm<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref>, which also reorders out-of-place with each stage, but this method requires separate bit/digit reversal and O(<it>N</it> log <it>N</it>) storage.  One can also directly apply the Cooley-Tukey factorization definition with explicit (<link xlink:type="simple" xlink:href="../034/97034.xml">
depth-first</link>) recursion and small radices, which produces natural-order out-of-place output with no separate permutation step and can be argued to have <link xlink:type="simple" xlink:href="../377/1773377.xml">
cache-oblivious</link> locality benefits on systems with <link xlink:type="simple" xlink:href="../829/6829.xml">
hierarchical memory</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2218%22])">18</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2219%22])">19</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2220%22])">20</ref></p>
<p>

A typical strategy for in-place algorithms without auxiliary storage and without separate digit-reversal passes involves small matrix transpositions (which swap individual pairs of digits) at intermediate stages, which can be combined with the radix butterflies to reduce the number of passes over the data<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2221%22])">21</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2222%22])">22</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2223%22])">23</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2224%22])">24</ref></p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
 Gauss, Carl Friedrich,  "Nachlass: Theoria interpolationis methodo nova tractata", Werke, Band 3, 265&ndash;327 (Königliche Gesellschaft der Wissenschaften, Göttingen, 1866)</entry>
<entry id="2">
Heideman, M. T., D. H. Johnson, and C. S. Burrus, "Gauss and the history of the fast Fourier transform," IEEE ASSP Magazine, 1, (4), 14&ndash;21 (1984)</entry>
<entry id="3">
Cooley, James W., and John W. Tukey, "An algorithm for the machine calculation of complex Fourier series," <it>Math. Comput.</it> <b>19</b>, 297&ndash;301 (1965).</entry>
<entry id="4">
Cooley, James W., Peter A. W. Lewis, and Peter D. Welch, "Historical notes on the fast Fourier transform," <it>IEEE Trans. on Audio and Electroacoustics</it> <b>15</b> (2), 76&ndash;79 (1967).
</entry>
<entry id="5">
Rockmore, Daniel N. , <it>Comput. Sci. Eng.</it> <b>2</b> (1), 60 (2000). <weblink xlink:type="simple" xlink:href="http://www.cs.dartmouth.edu/~rockmore/cse-fft.pdf">
The FFT &mdash; an algorithm the whole family can use</weblink> Special issue on "top ten algorithms of the century "<weblink xlink:type="simple" xlink:href="http://amath.colorado.edu/resources/archive/topten.pdf">
 </weblink></entry>
<entry id="6">
Danielson, G. C., and C. Lanczos, "Some improvements in practical Fourier analysis and their application to X-ray scattering from liquids," <it>J. Franklin Inst.</it> <b>233</b>, 365&ndash;380 and 435&ndash;452 (1942).</entry>
<entry id="7">
Duhamel, P., and M. Vetterli, "Fast Fourier transforms: a tutorial review and a state of the art," <it>Signal Processing</it> <b>19</b>, 259&ndash;299 (1990)</entry>
<entry id="8">
Gentleman W. M., and G. Sande, "Fast Fourier transforms&mdash;for fun and profit," <it>Proc. AFIPS</it> <b>29</b>, 563&ndash;578 (1966).</entry>
<entry id="9">
 Bailey, David H., "FFTs in external or hierarchical memory," <it>J. Supercomputing</it> <b>4</b> (1), 23&ndash;35 (1990)</entry>
<entry id="10">
M. Frigo, C.E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In <it>Proceedings of the 40th IEEE Symposium on Foundations of Computer Science</it> (FOCS 99), p.285-297. 1999. <weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/iel5/6604/17631/00814600.pdf?arnumber=814600">
Extended abstract at IEEE</weblink>, <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/307799.html">
at Citeseer</weblink>.</entry>
<entry id="11">
Cooley, J. W., P. Lewis and P. Welch, "The Fast Fourier Transform and its Applications", <it>IEEE Trans on Education</it> <b>12</b>, 1, 28-34 (1969)</entry>
<entry id="12">
 Karp, Alan H., "Bit reversal on uniprocessors," <it>SIAM Review</it> <b>38</b> (1), 1&ndash;26 (1996)</entry>
<entry id="13">
 Carter, Larry and Kang Su Gatlin, "Towards an optimal bit-reversal permutation program," <it>Proc. 39th Ann. Symp. on Found. of Comp. Sci. (FOCS)</it>, 544&ndash;553 (1998).</entry>
<entry id="14">
 Rubio, M., P. Gómez, and K. Drouiche, "A new superfast bit reversal algorithm," <it>Intl. J. Adaptive Control and Signal Processing</it> <b>16</b>, 703&ndash;707 (2002)</entry>
<entry id="15">
Stockham, T. G., "High speed convolution and correlation", <it>Spring Joint Computer Conference, Proc. AFIPS</it> <b>28</b>, 229&ndash;233 (1966)</entry>
<entry id="17">
Pease, M. C."An adaptation of the fast Fourier transform for parallel processing", <it>J. ACM</it> <b>15</b> (2), 252&ndash;264 (1968)</entry>
<entry id="16">
Swarztrauber, P. N., "Vectorizing the FFTs", in G. Rodrigue (Ed.), <it>Parallel Computations</it> (Academic Press, New York, 1982), pp. 51&ndash;83.</entry>
<entry id="19">
Frigo, Matteo and Steven G. Johnson: <it>FFTW</it>, http://www.fftw.org/. A free (<link xlink:type="simple" xlink:href="../683/18938683.xml">
GPL</link>) C library for computing discrete Fourier transforms in one or more dimensions, of arbitrary size, using the Cooley-Tukey algorithm</entry>
<entry id="18">
Singleton, Richard C., "On computing the fast Fourier transform", <it>Commun. of the ACM</it> <b>10</b> (1967), 647&ndash;654</entry>
<entry id="21">
Johnson, H. W. and C. S. Burrus, "An in-place in-order radix-2 FFT," <it>Proc. ICASSP</it>, 28A.2.1&ndash;28A.2.4 (1984)</entry>
<entry id="20">
Frigo, M. and S. G. Johnson,  <it>Proceedings of the IEEE</it> <b>93</b> (2), 216–231 (2005)<weblink xlink:type="simple" xlink:href="http://fftw.org/fftw-paper-ieee.pdf">
The Design and Implementation of FFTW3</weblink></entry>
<entry id="23">
Qian, Z., C. Lu, M. An, and R. Tolimieri, "Self-sorting in-place FFT algorithm with minimum working space," <it>IEEE Trans. ASSP</it> <b>52</b> (10), 2835&ndash;2836 (1994)</entry>
<entry id="22">
 Temperton, C., "Self-sorting in-place fast Fourier transform," <it>SIAM J. Sci. Stat. Comput.</it> <b>12</b> (4), 808&ndash;823 (1991)</entry>
<entry id="24">
Hegland, M., "A self-sorting in-place fast Fourier transform algorithm suitable for vector and parallel processing," <it>Numerische Mathematik</it> <b>68</b> (4), 507&ndash;547 (1994)</entry>
</reflist>
</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
