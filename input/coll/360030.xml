<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:55:18[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<software  confidence="0.8" wordnetid="106566077">
<application  confidence="0.8" wordnetid="106570110">
<program  confidence="0.8" wordnetid="106568978">
<written_communication  confidence="0.8" wordnetid="106349220">
<writing  confidence="0.8" wordnetid="106359877">
<code  confidence="0.8" wordnetid="106355894">
<coding_system  confidence="0.8" wordnetid="106353757">
<header>
<title>Question answering</title>
<id>360030</id>
<revision>
<id>233363567</id>
<timestamp>2008-08-21T17:23:46Z</timestamp>
<contributor>
<username>LaaknorBot</username>
<id>6917606</id>
</contributor>
</revision>
<categories>
<category>Information retrieval</category>
<category>Artificial intelligence applications</category>
<category>Natural language processing</category>
<category>Computational linguistics</category>
</categories>
</header>
<bdy>

<b>Question answering</b> (QA) is a type of <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link>. Given a collection of documents (such as the <invention wordnetid="105633385" confidence="0.8">
<link xlink:type="simple" xlink:href="../139/33139.xml">
World Wide Web</link></invention>
 or a local collection) the system should be able to retrieve answers to <link xlink:type="simple" xlink:href="../038/502038.xml">
questions</link> posed in <link xlink:type="simple" xlink:href="../173/21173.xml">
natural language</link>. QA is regarded as requiring more complex <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link> (NLP) techniques than other types of information retrieval such as <link xlink:type="simple" xlink:href="../640/731640.xml">
document retrieval</link>, and it is sometimes regarded as the next step beyond <link xlink:type="simple" xlink:href="../023/4059023.xml">
search engine</link>s.<p>

QA research attempts to deal with a wide range of question types including: fact, list, definition, <it>How</it>, <it>Why</it>, hypothetical, semantically-constrained, and cross-lingual questions. Search collections vary from small local document collections, to internal organization documents, to compiled <link xlink:type="simple" xlink:href="../917/47917.xml">
newswire</link> reports, to the world wide web.</p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Closed-domain</it> question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in <link xlink:type="simple" xlink:href="../681/49681.xml">
ontologies</link>. </entry>
<entry level="1" type="bullet">

 <it><link xlink:type="simple" xlink:href="../750/16431750.xml#xpointer(//*[./st=%22References%22])">
Open-domain</link></it> question answering deals with questions about nearly everything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.</entry>
</list>
</p>
<p>

(Alternatively, <it>closed-domain</it> might refer to a situation where only a limited type of questions are accepted, such as questions asking for <link xlink:type="simple" xlink:href="../664/5046664.xml">
descriptive</link> rather than <link xlink:type="simple" xlink:href="../507/334507.xml">
procedural</link> information.)</p>

<sec>
<st>
Architecture</st>

<p>

The first QA systems were developed in the 1960s and they were
basically natural-language interfaces to <link xlink:type="simple" xlink:href="../136/10136.xml">
expert system</link>s that were
tailored to specific domains. In contrast, current QA systems use text
documents as their underlying knowledge source and combine various
<link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link> techniques to search for the answers.</p>
<p>

Current QA systems typically include a <b>question classifier</b> module that
determines the type of question and the type of answer. After the
question is analysed, the system typically uses several modules that
apply increasingly complex NLP techniques on a gradually reduced
amount of text. Thus, a <b>document retrieval module</b> uses 
<link xlink:type="simple" xlink:href="../023/4059023.xml">
search engine</link>s to identify the documents or paragraphs in the document set
that are likely to contain the answer. Subsequently a <b>filter</b>
preselects small text fragments that contain strings of the same type
as the expected answer. For example, if the question is "Who invented
Penicillin" the filter returns text that contain names of people.
Finally, an <b>answer extraction</b> module looks for further clues in
the text to determine if the answer candidate can indeed answer the
question.</p>

</sec>
<sec>
<st>
Question answering methods</st>

<p>

QA is very dependent on a good search <link xlink:type="simple" xlink:href="../887/53887.xml">
corpus</link> - for without documents containing 
the answer, there is little any QA system can do. It thus makes sense that 
larger collection sizes generally lend well to better QA performance, unless 
the question domain is orthogonal to the collection. The notion of 
<link xlink:type="simple" xlink:href="../838/5380838.xml">
data redundancy</link> in massive collections, such as the web, means that nuggets 
of information are likely to be phrased in many different ways in differing 
contexts and documents, leading to two benefits: 
<indent level="1">

<b>(1)</b> By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.
</indent>
:<b>(2)</b> Correct answers can be filtered from <link>
false positive</link>s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.</p>

<ss1>
<st>
Shallow</st>

<p>

Some methods of QA use <link xlink:type="simple" xlink:href="../940/6118940.xml">
keyword</link>-based techniques to locate interesting passages 
and sentences from the retrieved documents and then filter based on the presence 
of the desired answer type within that candidate text. Ranking is then done 
based on <link xlink:type="simple" xlink:href="../860/26860.xml">
syntactic</link> features such as word order or location and similarity to query.</p>
<p>

When using massive collections with good data redundancy, some systems use 
templates to find the final answer in the hope that the answer is just a reformulation of the question. If you posed the
question "What is a dog?", the system would detect the substring "What
is a X" and look for documents which start with "X is a Y".  This often works well on simple "<link xlink:type="simple" xlink:href="../655/11655.xml">
factoid</link>" questions seeking factual 
tidbits of information such as names, dates, locations, and quantities.</p>

</ss1>
<ss1>
<st>
Deep</st>

<p>

However, in the cases where simple question reformulation or keyword techniques 
will not suffice, more sophisticated syntactic, semantic and contextual 
processing must be performed to extract or construct the answer. These 
techniques might include <link xlink:type="simple" xlink:href="../608/1906608.xml">
named-entity recognition</link>, 
relation detection, <link xlink:type="simple" xlink:href="../857/1625857.xml">
coreference</link> resolution, <link xlink:type="simple" xlink:href="../396/67396.xml">
syntactic alternations</link>, 
<link xlink:type="simple" xlink:href="../065/67065.xml">
word sense disambiguation</link>, <link xlink:type="simple" xlink:href="../537/1936537.xml">
logic form</link> transformation,
logical <link xlink:type="simple" xlink:href="../465/317465.xml">
inference</link>s (<link xlink:type="simple" xlink:href="../459/60459.xml">
abduction</link>) and commonsense 
reasoning, temporal or spatial reasoning and so on. These systems will also very 
often utilize world knowledge that can be found in <link xlink:type="simple" xlink:href="../261/22261.xml">
ontologies</link> such as <link xlink:type="simple" xlink:href="../955/33955.xml">
WordNet</link>, 
or the <link xlink:type="simple" xlink:href="../601/247601.xml">
Suggested Upper Merged Ontology</link> (SUMO) to augment the available 
reasoning resources through semantic connections and definitions. </p>
<p>

More difficult queries such as <it>Why</it> or <it>How</it> questions, hypothetical 
postulations, spatially or temporally constrained questions, <link xlink:type="simple" xlink:href="../601/3192601.xml">
dialog</link> queries, 
badly-worded or ambiguous questions will all need these types of deeper 
understanding of the question. Complex or ambiguous document passages likewise 
need more NLP techniques applied to understand the text. </p>
<p>

Statistical QA, which introduces statistical question processing and answer 
extraction modules, is also growing in popularity in the research community. 
Many of the lower-level NLP tools used, such as <link xlink:type="simple" xlink:href="../912/746912.xml">
part-of-speech tagging</link>, 
<link xlink:type="simple" xlink:href="../015/310015.xml">
parsing</link>, named-entity detection, sentence boundary detection, and 
<link xlink:type="simple" xlink:href="../640/731640.xml">
document retrieval</link>, are already available as probabilistic applications.</p>
<p>

AQ (Answer Questioning) Methodology; introduces a working cycle to the QA methods. 
This method may be used in conjunction with any of the known or newly founded 
methods. AQ Method may be used upon perception of a posed question or answer.
The means by which it is utilized can be manipulated beyond its primary usage;
however, the primary usage is taking an answer and questioning it turning 
that very answer into a question. Example; 
A"I like sushi." Q"(Why do) I like sushi(?)" A"The flavor." Q"(What about) the flavor
of sushi (do) I like?"
Inadvertently, this may unveil different methods of thinking and perception as well.
While most would agree that this seems to be the endall stratagem, it is only a starting point with endless possibilities. Any number of question methods may be used 
to derive the number of WHY as in, A = ∞(Q), the answer may yield any number of questions 
to be asked; thereby unveiling an ongoing process constantly being reborn into the 
research being performed. The QA methodology utilizes just the opposite where, 
1(Q) = ((∞(A)-∞) = 1(A), supposedly there is only one true answer in reality everything 
else is perception or plausibility. 
Utilized alongside other forms of communication; debate may be greatly improved. Even 
this methodology should be questioned.</p>

</ss1>
</sec>
<sec>
<st>
Issues</st>

<p>

In 2002 a group of researchers wrote a roadmap
of research in question answering (see external links). The following
issues were identified.</p>

<p>

<list>
<entry level="1" type="definition">

Question classes : Different types of questions require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Question processing : The same information request can be expressed in various ways - some interrogative, some assertive. A semantic model of question understanding and processing is needed, one that would recognize equivalent questions, regardless of the speech act or of the words, syntactic inter-relations or idiomatic forms. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Context and Q&amp;A : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Data sources for Q&amp;A : Before a question can be answered, it must be known what knowledge sources are available. If the answer to a question is not present in the data sources, no matter how well we perform question processing, retrieval and extraction of the answer, we shall not obtain a correct result.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Answer extraction : Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context. Given that answer processing depends on such a large number of factors, research for answer processing should be tackled with a lot of care and given special importance.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Answer formulation : The result of a Q&amp;A system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc), a quantity (monetary value, length, size, distance, etc) or a date (e.g. the answer to the question "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Real time question answering : There is need for developing Q&amp;A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Multi-lingual question answering : The ability of developing Q&amp;A systems for other languages than English is very important. Moreover, the ability of finding answers in texts written in languages other than English, when an English question is asked is very important.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Interactive Q&amp;A : It is often the case that the information need is not well captured by a Q&amp;A system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but (s)he might want to have a dialogue with the system.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

Advanced reasoning for Q&amp;A : More sophisticated questioners expect answers which are outside the scope of written texts or structured databases. To upgrade a Q&amp;A system with such capabilities, we need to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms as well as knowledge specific to a variety of domains.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

User profiling for Q&amp;A : The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user etc. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.</entry>
</list>
</p>

</sec>
<sec>
<st>
History</st>

<p>

Some of the early <link xlink:type="simple" xlink:href="../164/1164.xml">
AI</link> systems were question answering systems. Two of the most famous QA systems of that time are BASEBALL and LUNAR, both of which were developed in the 1960s. BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain.</p>
<p>

Some of the early <link xlink:type="simple" xlink:href="../164/1164.xml">
AI</link> systems included question-answering abilities. Two of the most famous early systems are SHRDLU and ELIZA. <software wordnetid="106566077" confidence="0.8">
<application wordnetid="106570110" confidence="0.8">
<program wordnetid="106568978" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106359877" confidence="0.8">
<code wordnetid="106355894" confidence="0.8">
<coding_system wordnetid="106353757" confidence="0.8">
<link xlink:type="simple" xlink:href="../791/98791.xml">
SHRDLU</link></coding_system>
</code>
</writing>
</written_communication>
</program>
</application>
</software>
 simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program. <link xlink:type="simple" xlink:href="../235/10235.xml">
ELIZA</link>, in contrast, simulated a conversation with a psychologist. ELIZA was able to converse on any topic by resorting to very simple rules that detected important words in the person's input. It had a very rudimentary way to answer questions, and on its own it lead to a series of <link xlink:type="simple" xlink:href="../349/148349.xml">
chatterbot</link>s such as the ones that participate in the annual <symbol wordnetid="106806469" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<award wordnetid="106696483" confidence="0.8">
<signal wordnetid="106791372" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../725/238725.xml">
Loebner prize</link></psychological_feature>
</signal>
</award>
</contest>
</social_event>
</event>
</symbol>
.</p>
<p>

The 1970s and 1980s saw the development of comprehensive theories in <link xlink:type="simple" xlink:href="../561/5561.xml">
computational linguistics</link>, which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), a system that answered questions pertaining to the <family wordnetid="108078020" confidence="0.8">
<link xlink:type="simple" xlink:href="../642/31642.xml">
Unix</link></family>
 operating system. The system had a comprehensive hand-crafted knowledge base of  its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.</p>
<p>

In the late 1990s the annual <event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../206/1897206.xml">
Text Retrieval Conference</link></psychological_feature>
</contest>
</social_event>
</event>
 (TREC) included a question-answering track which has been running until the present. Systems participating in this competition were expected to answer questions on any topic by searching a corpus of text that varied from year to year. This competition fostered research and development in open-domain text-based question answering. The best system of the 2004 competition achieved 77% correct fact-based questions.</p>
<p>

In 2007 the annual <event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../206/1897206.xml">
Text Retrieval Conference</link></psychological_feature>
</contest>
</social_event>
</event>
 (TREC) included a blog data corpus for question answering. The blog data corpus contained both "clean" English as well as <link xlink:type="simple" xlink:href="../739/17193739.xml">
noisy text</link> that includes badly-formed English and spam. The introduction of <link xlink:type="simple" xlink:href="../739/17193739.xml">
noisy text</link> moved the question answering to a more realistic setting. Real-life data is inherently noisy as people are less careful when writing in spontaneous media like blogs. In earlier years the TREC data corpus consisted of only newswire data that was very clean.  </p>
<p>

An increasing number of systems include the <invention wordnetid="105633385" confidence="0.8">
<link xlink:type="simple" xlink:href="../139/33139.xml">
World Wide Web</link></invention>
 as one more <link xlink:type="simple" xlink:href="../887/53887.xml">
corpus</link> of text. Currently there is an increasing interest in the integration of question answering with web search. <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../624/429624.xml">
Ask.com</link></company>
 is an early example of a such a system, and <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../923/1092923.xml">
Google</link></company>
 and <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../001/19001.xml">
Microsoft</link></company>
 have started to integrate question-answering facilities in their <link xlink:type="simple" xlink:href="../023/4059023.xml">
search engine</link>s. One can only expect to see an even tighter integration in the near future.</p>

</sec>
<sec>
<st>
External links</st>

<p>

QA systems regularly compete in the <event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../206/1897206.xml">
TREC competition</link></psychological_feature>
</contest>
</social_event>
</event>
 and in the <link>
CLEF</link> evaluation campaign and some of them
have demos available on the World Wide Web.</p>


<ss1>
<st>
Evaluation Forums</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://trec.nist.gov/">
TREC competition</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.clef-campaign.org/">
CLEF evaluation campaign</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://research.nii.ac.jp/ntcir/">
NTCIR project</weblink></entry>
</list>
</p>

</ss1>
<ss1>
<st>
QA Systems &amp; Demos</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ask.com/">
Ask Jeeves search engine</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.brainboost.com/">
Automatic question answering engine</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://start.csail.mit.edu/">
START Web-based Question Answering system at MIT</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://demos.inf.ed.ac.uk:8080/qualim/">
University of Edinburgh QA system - Search Wikipedia</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://sourceforge.net/projects/openephyra/">
OpenEphyra open source question answering system</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.answerbus.com/">
AnswerBus</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://experimental-quetal.dfki.de/">
DFKI Experimental Open Domain Web QA system</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://qa.wpcarey.asu.edu/">
ASU-QA prototype Web-based QA system</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://wikiferret.com">
askEd! - a multilingual question answering system</weblink> (<weblink xlink:type="simple" xlink:href="http://wikiferret.com">
English</weblink>, <weblink xlink:type="simple" xlink:href="http://wikiferret.com/edw/pc/index_j.html">
Japanese</weblink>, <weblink xlink:type="simple" xlink:href="http://wikiferret.com/edw/pc/index_cn.html">
Chinese</weblink>, <weblink xlink:type="simple" xlink:href="http://wikiferret.com/edw/pc/index_ru.html">
Russian</weblink> and <weblink xlink:type="simple" xlink:href="http://wikiferret.com/edw/pc/index_sw.html">
Swedish</weblink>)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ics.mq.edu.au/~pizzato/tellme">
TellMe QA: A prototype QA system</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.laancor.com/technology/quadra/">
QUADRA: Question Answering Digital Research Assistant</weblink></entry>
</list>
</p>

</ss1>
<ss1>
<st>
Domain-specific QA Systems</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://eagl.unige.ch/EAGLi/">
EAGLi: MEDLINE question answering engine</weblink></entry>
</list>
</p>

</ss1>
<ss1>
<st>
Miscellaneous</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc">
QA roadmap (Word file)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.languagecomputer.com/">
Language Computer Corporation (LCC)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.laancor.com/">
LAANCOR, the Language Analytic Corporation</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://questsin.blogspot.com/2005/06/algorithm-for-generic-question.html">
Questsin, Blog on a simple do it yourself algorithm you could implement</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.linckels.lu/chest/">
CHESt, an e-Librarian Service that can be used as virtual private teacher</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://1aiway.com/nlp4net/services/enparser/question.aspx">
Natural Language Question-Answer</weblink> QA demo and code for <platform wordnetid="103961939" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<surface wordnetid="104362025" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<tool wordnetid="104451818" confidence="0.8">
<horizontal_surface wordnetid="103536348" confidence="0.8">
<implement wordnetid="103563967" confidence="0.8">
<link xlink:type="simple" xlink:href="../402/60402.xml">
.NET Framework</link></implement>
</horizontal_surface>
</tool>
</instrumentality>
</surface>
</artifact>
</platform>
 developers.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cnlp.org">
Center for Natural Language Processing at Syracuse University</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ephyra.info/">
Ephyra question answering project at Carnegie Mellon</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://thesis.liljenback.com/">
Thesis on Restricted-Domain Question Answering</weblink></entry>
</list>
</p>


</ss1>
</sec>
</bdy>
</coding_system>
</code>
</writing>
</written_communication>
</program>
</application>
</software>
</article>
