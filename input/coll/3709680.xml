<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 20:43:50[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Pattern theory</title>
<id>3709680</id>
<revision>
<id>220676995</id>
<timestamp>2008-06-21T00:58:59Z</timestamp>
<contributor>
<username>Eubot</username>
<id>231599</id>
</contributor>
</revision>
<categories>
<category>Applied mathematics</category>
</categories>
</header>
<bdy>

For other meanings of "pattern", see <link xlink:type="simple" xlink:href="../724/3666724.xml">
Pattern (disambiguation)</link>.
<b>Pattern theory</b>, formulated by <link>
Ulf Grenander</link>, is a mathematical <link xlink:type="simple" xlink:href="../700/13447700.xml">
formalism</link> to describe knowledge of the world as patterns. It differs from other approaches to <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.  <p>

In addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:
<list>
<entry level="1" type="bullet">

 Identify the hidden variables of a data set using real world data rather than artificial stimuli, which was commonplace at the time.</entry>
<entry level="1" type="bullet">

 Formulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.</entry>
<entry level="1" type="bullet">

 Study the randomness and variability of these graphs.</entry>
<entry level="1" type="bullet">

 Create the basic classes of stochastic models applied by listing the deformations of the patterns.</entry>
<entry level="1" type="bullet">

 Synthesize (sample) from the models, not just analyze signals with it.</entry>
</list>

Broad in its mathematical coverage, Pattern Theory spans algebra and statistics, as well as local topological and global entropic properties.</p>
<p>

The Brown University Pattern Theory Group was formed in <link xlink:type="simple" xlink:href="../671/34671.xml">
1972</link> by <link>
Ulf Grenander</link>. Many mathematicians are currently working in this group, noteworthy among them being the <symbol wordnetid="106806469" confidence="0.8">
<award wordnetid="106696483" confidence="0.8">
<signal wordnetid="106791372" confidence="0.8">
<link xlink:type="simple" xlink:href="../859/10859.xml">
Fields Medal</link></signal>
</award>
</symbol>
ist <peer wordnetid="109626238" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<contestant wordnetid="109613191" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<winner wordnetid="110782940" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<geometer wordnetid="110128016" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<medalist wordnetid="110305062" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../594/394594.xml">
David Mumford</link></scholar>
</medalist>
</mathematician>
</causal_agent>
</alumnus>
</geometer>
</associate>
</scientist>
</winner>
</colleague>
</intellectual>
</contestant>
</person>
</physical_entity>
</peer>
.  Mumford regards Grenander as his "guru" in this subject.</p>

<sec>
<st>
 Algebraic foundations </st>

<p>

We begin with an example to motivate the algebraic definitions that follows.</p>
<p>

<indent level="1">

<b>Example 1</b>  Grammar
</indent>

<table border="0">
<row>
<col>
<image location="right" width="200px" src="grammar_automaton.jpg" type="thumb">
<caption>

Grammar automaton
</caption>
</image>

<image location="right" width="200px" src="generator_list.jpg" type="thumb">
<caption>

Grammar generators
</caption>
</image>

<image location="right" width="200px" src="grammar_bond_table.jpg" type="thumb">
<caption>

Grammar bond table
</caption>
</image>
</col>
<col>
<indent level="1">

If we want to represent language patterns, the most immediate candidate for primitives might be words.  However, such phrases as “in order to” immediately obviates the inappropriateness of words as atomics.  In searching for other primitives, we might try the rules of grammar.  We can represent grammars as Finite State Automata or Context-Free Grammars.  Below is a sample Finite State grammar automaton
</indent>

<indent level="1">

The following phrases are generated from a few simple rules of the automaton and programming code in pattern theory:
</indent>
:: <it>the boy who owned the small cottage went to the deep forest</it>
<indent level="2">

 <it>the prince walked to the lake</it>
</indent>
:: <it>the girl walked to the lake and the princess went to the lake</it>
<indent level="2">

 <it>the pretty prince walked to the dark forest</it>
</indent>
:To create such sentences, rewriting rules in Finite State Automatons act as "generators" to create the sentences as follows: if a machine starts in state 1, it goes to state 2 and writes the word “the”.  From state 2, it writes one of 4 words:  prince, boy, princess, girl.  Such a simplistic automaton occasionally generates more awkward sentences 
<indent level="2">

 <it>the evil evil prince walked to the lake</it>
</indent>
:: <it>the prince walked to the dark forest and the prince walked to a forest and the princess who lived in some big small big cottage who owned the small big small house went to a forest</it><p>

<indent level="1">

From the finite state diagram we can infer the following <it>generators</it> (left) that creates the  signal.  A generator is a 4-tuple:  current state, next state, word written, probability of written word when there are multiple choices.  
</indent>

<indent level="1">

Imagine that a "configuration" of generators are strung together linearly so its output forms a sentence, so each generator "bonds" to the generators before and after it.  Denote these bonds as 1a,1b,2a,2b,…12a,12b.  Each numerical label corresponds to the automaton's state and each letter "a" and "b" corresponds to the inbound and outbound bonds.  Then the following bond table (right) is equivalent to the automaton diagram. For the sake of simplicity, only half of the bond table is shown -- the table is actually symmetric.</indent>
</p>
</col>
</row>
</table>
</p>

<p>

As one can tell from this example, and typical of signals we study, identifying the primitives and bond tables require some thought.  The example highlights another important fact not readily apparent in other signals modalities: that a configuration is not the signal we observe; rather, we observe its image as a sentence.  Herein lies a significant justification to distinguish an observable from a non-observable construct.  Additionally, it gives us an algebraic structure to associate our Hidden Markov Models with.  In sensory modalities such as the vision example below, the hidden configurations and observed images are much more similar that such a distinction may not seem justified.  Fortunately, we have the Grammar example to remind us of this distinction.</p>
<p>

Motivated by the example, we have the following definitions:</p>
<p>

1.  A <b>generator</b> <math>g{\in}G</math>, drawn as 
<indent level="1">

<image width="300px" src="generators.jpg">
<caption>

1 and 2 dimensional generators
</caption>
</image>

</indent>
is the primitive of Pattern Theory that generates the observed signal.   Structurally, it is a value with interfaces, called bonds <math>b{\in}B</math>, which connects the <math>g</math>'s to form a signal generator.  2 neighboring generators are connected when their bond values are the same.  Similarity self-maps <math>s{\in}S</math> s: G -&amp;gt; G express the invariances of the world we are looking at, such as rigid body transformations, or scaling.</p>
<p>

2.  Bonds glue generators into a <b>configuration</b>, c, which creates the signal against a backdrop <b>Σ</b>, with global features described locally by a bond coupling table called <math>\rho</math>.  The boolean function <math>\rho</math> is the principal component of the regularity 4-tuple \rho,Σ&amp;gt;, which is defined as</p>
<p>

<indent level="1">

 <math> \rho(c) = \prod_{ \text{neighboring bonds }b',b'' \in c } \rho(b',b'' ). </math>
</indent>

Regularity is designed to capture the notion of the global feature of interest on a local scale.</p>
<p>

3.  An <b>image</b> (C mod R) captures the notion of an observed Configuration, as opposed to one which exists independently from any perceptual apparatus.  Images are configurations distinguished only by their external bonds, inheriting the configuration’s composition and similarities transformations.  Formally, images are equivalence classes partitioned by an Identification Rule "~" with 3 properties:
<list>
<entry level="1" type="number">

 ext(c) = ext(c') whenever c~c'</entry>
<entry level="1" type="number">

 sc~sc' whenever c~c'</entry>
<entry level="1" type="number">

 sigma(c1,c2) ~ sigma(c1',c2') whenever c1~c1', c2~c2' are all regular.</entry>
</list>

A configuration corresponding to a physical stimulus may have many images, corresponding to many observer perception's identification rule.</p>
<p>

4.  A <b>pattern</b>  is the repeatable components of an image, defined as the S-invariant subset of an image.  Similarities are reference transformations we use to define patterns, eg. rigid body transformations.  At first glance, this definition seems suited for only texture patterns where the minimal sub-image is repeated over and over again.  If we were to view an image of an object such as a dog, its is not repeated, yet seem like it seems familiar and should be a pattern.  (Help needed here).</p>
<p>

5.  A <b>deformation</b> is a transformation of the original image that accounts for the noise in the environment and error in the perceptual apparatus.  Grenander identifies 4 types of deformations:  noise and blur, multi-scale superposition, domain warping, and interruptions.</p>
<p>

<indent level="1">

<b>Example 2</b>  Directed boundary
</indent>

<table border="0">
<row>
<col>
<image location="right" width="100px" src="directed_boundary_configuration.jpg" type="thumb">
<caption>

Configuration
</caption>
</image>

<image location="right" width="100px" src="directed_boundary_image.jpg" type="thumb">
<caption>

Image
</caption>
</image>

<image location="right" width="100px" src="directed_boundary_generators.jpg" type="thumb">
<caption>

Generators
</caption>
</image>

<image location="right" width="100px" src="directed_boundary_bond_table.jpg" type="thumb">
<caption>

Bond Table
</caption>
</image>
</col>
<col valign="top">
<indent level="1">

This configuration of generators generating the image is created by primitives woven together by the bonding table, and perceived by an observer with the identification rule that maps non "0" &amp; "1" generators to a single boundary element.  Nine other undepicted generators are created by rotating each of the non-"0"&amp;"1" generators by 90 degrees.  Keeping the feature of "directed boundaries" in mind, the generators are cooked with some thought and is interpreted as follows:  the "0" generator corresponds to interior elements, "1" to the exterior, "2" and its rotations are straight elements, and the remainder are the turning elements.
</indent>

<indent level="1">

With Boolean regularity defined as Product (all nbr bonds), any configurations with even a single generator violating the bond table is discarded from consideration.  Thus only features in its purest form with all neighboring generators adhering to the bond table are allowed.  This stringent condition can be relaxed using probability measures instead of Boolean bond tables.
</indent>

<indent level="2">

<math>p(c) = \prod_{ \text{neighboring bonds }b',b'' \in c } A(b',b'' ) </math>
</indent>

<indent level="1">

The new regularity no longer dictates a perfect directed boundary, but it defines a probability of a configuration in terms of the Acceptor function A().  Such configurations are allowed to have impurities and imperfections with respect to the feature of interest.</indent>
</col>
</row>
</table>
</p>

<p>

With the benefit of being given generators and complete bond tables, a difficult part of pattern analysis is done.  In tackling a new class of signals and features, the task of devising the generators and bond table is much more difficult</p>
<p>

Again, just as in grammars, identifying the generators and bond tables require some thought.  Just as subtle is the fact that a configuration is not the signal we observe. Rather, we observe its image as silhouette projections of the identification rule.</p>

</sec>
<sec>
<st>
 Topology </st>
<p>

to be expanded !</p>

</sec>
<sec>
<st>
 Entropy </st>
<p>

PT defines order in terms of the feature of interest given by <it>p</it>(<it>c</it>).</p>
<p>

<indent level="1">

 Energy(<it>c</it>) = &amp;minus;log <it>P</it>(<it>c</it>)
</indent>

</p>
</sec>
<sec>
<st>
 Statistics </st>

<p>

Grenander’s Pattern Theory treatment of <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian inference</link> in seems to be skewed towards on image reconstruction (eg. content addressable memory).  That is given image I-deformed, find I.  However, Mumford’s interpretation of Pattern Theory is broader and he defines PT to include many more well-known statistical methods.  Mumford’s criteria for inclusion of a topic as Pattern Theory are those methods "characterized by common techniques and motivations", such as the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../770/98770.xml">
HMM</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
, <link xlink:type="simple" xlink:href="../752/470752.xml">
EM algorithm</link>, <link xlink:type="simple" xlink:href="../297/125297.xml">
dynamic programming</link> circle of ideas.  Topics in this section will reflect Mumford's treatment of Pattern Theory.  His principle of statistical Pattern Theory are the following:
<list>
<entry level="1" type="bullet">

 Use real world signals rather than constructed ones to infer the hidden states of interest.</entry>
<entry level="1" type="bullet">

 Such signals contain too much complexity and artifacts to succumb to a purely deterministic analysis, so employ stochastic methods too.</entry>
<entry level="1" type="bullet">

 Respect the natural structure of the signal, including any symmetries, independence of parts, marginals on key statistics.  Validate by sampling from the derived models by and infer hidden states with Bayes’ rule.  </entry>
<entry level="1" type="bullet">

 Across all modalities, a limited family of deformations distort the <it>pure patterns</it> into real world signals.</entry>
<entry level="1" type="bullet">

 Stochastic factors affecting an observation show strong conditional independence.</entry>
</list>
</p>
<p>

Statistical PT makes ubiquitous use of conditional probability in the form of <link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes theorem</link> and <link xlink:type="simple" xlink:href="../770/98770.xml">
Markov</link> Models.  Both these concepts are used to express the relation between hidden states (configurations) and observed states (images).  Markov Models also captures the local properties of the stimulus, reminiscent of the purpose of bond table for regularity.  </p>
<p>

The generic set up is the following:
Let <it>s</it> = the hidden state of the data that we wish to know.  <it>i</it> = observed image.  Bayes theorem gives </p>
<p>

<indent level="2">

 <it>p</it> (<it>s</it> | <it>i</it> ) <it>p</it>(<it>i</it>) = <it>p</it> (<it>s</it>, <it>i</it> ) = <it>p</it> (<it>i</it>|<it>s</it> ) <it>p</it>(<it>s</it>)
</indent>

<indent level="1">

To analyze the signal (recognition):  fix i, maximize p, infer s.
</indent>
:To synthesize signals (sampling): fix s, generate i's, compare w/ real world images</p>
<p>

The following conditional probability examples illustrates these methods in action:</p>

<ss1>
<st>
 Conditional probability for local properties </st>

<p>

N-gram Text Strings:  See Mumford's Pattern Theory by Examples, Chapter 1.</p>
<p>

MAP ~ MDL (MDL offers a glimpse of why the MAP probabilistic formulation make sense analytically)</p>

</ss1>
<ss1>
<st>
 Conditional probability for hidden-observed states </st>


<p>

<table border="=">
<row>
<col valign="top">
<b>Bayes Theorem for Machine translation</b></col>
<col >
<it>f</it>)<it>p</it>(<it>f</it>) = <it>p</it>(<it>e</it>, <it>f</it>) = <it>p</it>(<it>f</it>|<it>e</it>)<it>p</it>(<it>e</it>) and reduces to the fundamental equation of machine translation: maximize <it>p</it>(<it>e</it>|<it>f</it>) = <it>p</it>(<it>f</it>|<it>e</it>)<it>p</it>(<it>e</it>) over the appropriate <it>e</it> (note that <it>p</it>(<it>f</it>) is independent of <it>e</it>, and so drops out when we maximize over <it>e</it>).  This reduces the problem to three main calculations for:
<list>
<entry level="1" type="number">

 <it>p</it>(<it>e</it>) for any given <it>e</it>, using the <it>N</it>-gram method and dynamic programming</entry>
<entry level="1" type="number">

 <it>p</it>(<it>f</it>|<it>e</it>) for any given <it>e</it> and <it>f</it>, using alignments and an <link xlink:type="simple" xlink:href="../752/470752.xml">
expectation-maximization (EM) algorithm</link></entry>
<entry level="1" type="number">

 <it>e</it> that maximizes the product of 1 and 2, again, using dynamic programming </entry>
</list>

The analysis seems to be symmetric with respect to the two languages, and if we think can calculate <it>p</it>(<it>f</it>|<it>e</it>), why not turn the analysis around and calculate <it>p</it>(<it>e</it>|<it>f</it>) directly?  The reason is that during the calculation of <it>p</it>(<it>f</it>|<it>e</it>) the asymmetric assumption is made that source sentence be well formed and we cannot make any such assumption about the target translation because we do not know what it will translate into.<p>

We now focus on <it>p</it>(<it>f</it>|<it>e</it>) in the three-part decomposition above.  The other two parts, <it>p</it>(<it>e</it>) and maximizing <it>e</it>, uses similar techniques as the <it>N</it>-gram model.  Given a French-English translation from a large training data set (such data sets exists from the Canadian parliament),
NULL   And    the    program      has    been    implemented
Le     programme    a ete     mis en application
the sentence pair can be encoded as an <it>alignment</it> (2, 3, 4, 5, 6, 6, 6) that reads as follows: the first word in French comes from the second English word, the second word in French comes from the 3rd English word, and so forth.  Although an alignment is a straight forward encoding of the translation, a more computationally convenient approach to an alignment is to break it down into four parameters: 
<list>
<entry level="1" type="number">

 <it>Fertility</it>:  the number of words in the French string that will be connected to it.  Eg. <it>n</it>( 3 | implemented ) = probability that “implemented” translates into three words – the word’s fertility</entry>
<entry level="1" type="number">

 <it>Spuriousness</it>:  we introduce the artifact NULL as a word to represent the probability of tossing in a spurious French word.  Eg.  p1 and its complement will be <it>p''</it></entry>
</list>

0 = 1&nbsp;−&nbsp;<it>p</it>1.
<list>
<entry level="1" type="number">

 <it>Translation</it>:  the translated version of each word.  Eg. <it>t</it>(<it>a</it> | has ) = translation probability that the English "has" translates into the French "a".</entry>
<entry level="1" type="number">

 <it>Distortion</it>:  the actual positions in the French string that these words will occupy.  Eg.  <it>d</it>( 5 | 2, 4, 6 ) = distortion of second position French word moving into the fifth position English word for a four-word English sentence and a six-word French sentence.  We encode the alignments this way to  easily represent and extract priors from our training data and the new formula becomes</entry>
</list>

<it>p</it>(<it>f</it>|<it>e</it> ) = Sum over all possible alignments an of <it>p</it>(<it>a</it>, <it>f</it> |<it>e</it> ) = </p>
<p>

<indent level="2">

 <math> = n_0(v_0 | \sum_{j=1}^{l}{v_j} )
\cdot \prod_{j=1}^{l} n(v_j | e_j)v_j! 
\cdot \prod_{j=1}^{m} t(f_j | e_{a_j}) 
\cdot \prod_{j:a_j\not =0}^{m} d(j | a_j,l,m).  \,</math>
</indent>

For the sake of simplicity in demonstrating an EM algorithm, we shall go through a simple calculation involving only translation probabilities <it>t</it>(), but needless to say that it the method applies to all parameters in their full glory.  Consider the simplified case (1) without the NULL word (2) where  every word has fertility 1 and (3) there are no distortion probabilities.  Our training data corpus will contain two-sentence pairs: <it>bc</it>&nbsp;→&nbsp;<it>xy</it> and <it>b</it>&nbsp;→&nbsp;<it>y</it>.  The translation of a two-word English sentence “b c” into the French sentence “<it>x y</it>” has two possible alignments, and including the one-sentence words, the alignments are:
b c   b c   b</p>
</col>
<col >
    x    |
x y   x y   y
called Parallel, Crossed, and Singleton respectively.<p>

To illustrate an EM algorithm, first set the desired parameter uniformly, that is </p>
<p>

<indent level="1">

 <it>t</it>(<it>x</it> | <it>b</it> ) = <it>t</it>(<it>y</it> | <it>b</it> ) = <it>t</it>(<it>x</it> | <it>c</it> ) = <it>t</it>(<it>y</it> | <it>c</it> ) = ½
</indent>

Then EM iterates as follows
<image width="150px" src="em_iterations.jpg">
<caption>

Iterations of an EM algorithm
</caption>
</image>

The alignment probability for the “crossing alignment” (where <it>b</it> connects to <it>y</it>) got a boost from the second sentence pair <it>b</it>/<it>y</it>.  That further solidified <it>t</it>(<it>y</it> | <it>b</it>), but as a side effect also boosted <it>t</it>(<it>x</it> | <it>c</it>), because <it>x</it> connects to <it>c</it> in that same “crossing alignment.”  The effect of boosting <it>t</it>(<it>x</it> | <it>c</it>) necessarily means downgrading <it>t</it>(<it>y</it> |<it>c</it>) because they sum to one.  So, even though <it>y</it> and <it>c</it> co-occur, analysis reveals that they are not translations of each other.  With real data, EM also is subject to the usual local extremum traps.</p>
</col>
</row>
<row>
<col valign="top">
<b>HMM’s for speech recognition</b></col>
<col>
For decades, speech recognition seemed to hit an impasse as scientists sought descriptive and analytic solution.  The sound wave p(t) below is produced by speaking the word “ski”.   <p>

<image location="left" width="400px" src="ski_recording.jpg" type="thumb">
<caption>

Vibrational breakdown of "ski"
</caption>
</image>
</p>
<p>

Its four distinct segments has very different characteristics.   One can choose from many levels of generators (hidden variables): the intention of the speaker’s brain, the state of the mouth and vocal cords, or the ‘phones’ themselves.  Phones are the generator of choice to be inferred and it encodes the word in a noisy, highly variable way.  Early work on speech recognition attempted to make this inference deterministically using logical rules based on binary features extracted from p(t).  For instance, the table below shows some of the features used to distinguish English consonants. </p>
<p>

<image location="left" width="400px" src="consonant_table.jpg" type="thumb">
<caption>

Deterministic approach to speech recognition
</caption>
</image>
In real situations, the signal is further complicated by background noises such as cars driving by or artifacts such as a cough in mid sentence (mumford’s 2nd underpinning).  The deterministic rule-based approach failed and the state of the art (eg. Dragon Naturally Speaking) is to use  a family of precisely tuned HMM’s and a Bayesian MAP estimators to do better.  Similar stories played out in vision, and other stimulus categories.</p>
<p>

(See Mumford's Pattern Theory: the mathematics of perception)
The Markov stochastic process is diagrammed as follows: </p>
<p>

<indent level="1">

 <math>\dots, x_k, x_{k+1}, \dots \Pr(x,s) = \prod_k p_1(x_k | x_{k-1}) p_2(s_k | x_k) </math>
</indent>

exponentials, EM algorithm</p>
</col>
</row>
</table>
</p>


</ss1>
</sec>
<sec>
<st>
Further reading</st>
<p>

<list>
<entry level="1" type="bullet">

 2007. Ulf Grenander and Michael Miller . Oxford University Press. Paperback. (ISBN 9780199297061)  </entry>
<entry level="1" type="bullet">

 1994. Ulf Grenander <it><link>
General Pattern Theory</link></it>. Oxford Science Publications. (ISBN 978-0198536710)</entry>
<entry level="1" type="bullet">

 1996. Ulf Grenander <it><link>
Elements of Pattern Theory</link></it>.  Johns Hopkins University Press. (ISBN 978-0801851889)</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dam.brown.edu/ptg">
Pattern Theory Group at Brown University</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dam.brown.edu/people/mumford/Papers/IHPBook/intro.pdf">
David Mumford, Pattern Theory By Example (in progress)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/brown93mathematics.html">
Brown et al 1993, The Mathematics of Statistical Machine Translation:  Parameter Estimation</weblink></entry>
</list>

</p>
</sec>
</bdy>
</article>
