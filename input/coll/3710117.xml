<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 20:38:49[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Semantic neural network</title>
<id>3710117</id>
<revision>
<id>161346677</id>
<timestamp>2007-09-30T16:56:45Z</timestamp>
<contributor>
<username>SmackBot</username>
<id>433328</id>
</contributor>
</revision>
<categories>
<category>Natural language processing</category>
<category>Neural networks</category>
</categories>
</header>
<bdy>
<p>

<b>Semantic neural network</b> (SNN) is based on <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../942/15942.xml">
John von Neumann</link></scientist>
</person>
's neural network [von Neumann, 1966] and <physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<philosopher wordnetid="110423589" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<inventor wordnetid="110214637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<creator wordnetid="109614315" confidence="0.8">
<link xlink:type="simple" xlink:href="../878/3031878.xml">
Nikolai Amosov</link></creator>
</scholar>
</inventor>
</writer>
</scientist>
</causal_agent>
</intellectual>
</person>
</philosopher>
</communicator>
</physical_entity>
 M-Network. There are limitations to a link topology for the von Neumann’s network but SNN accept a case without these limitations. Only logical values can be processed, but SNN accept that fuzzy values can be processed too. All neurons into the von Neumann network are synchronized by tacts. For further use of self-synchronizing circuit technique SNN accepts neurons can be self-running or synchronized.</p>
<p>

In contrast to the von Neumann network there are no limitations for topology of neurons for semantic networks. It leads to the impossibility of relative addressing of neurons as it was done by von Neumann. In this case an absolute readdressing should be used. Every neuron should have a unique identifier that would provide a direct access to another neuron. Of course, neurons interacting by axons-dendrites should have each others identifiers. An absolute readdressing can be modulated by using neuron specificity as it was realized for biological neural networks.</p>
<p>

There’s no description for self-reflectiveness and self-modification abilities into the initial description of semantic networks [Dudar Z.V., Shuklin D.E., 2000]. But in [Shuklin D.E. 2004] a conclusion had been drawn about the necessity of introspection and self-modification abilities in the system. For maintenance of these abilities a concept of pointer to neuron is provided. Pointers represent virtual connections between neurons. In this model, bodies and signals transferring through the neurons connections represent a physical body, and virtual connections between neurons are representing an astral body. It is proposed to create models of artificial neuron networks on the basis of virtual machine supporting the opportunity for paranormal effects.</p>
<p>

SNN is generally used for natural language processing.</p>

<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

 Neumann, J., 1966. <weblink xlink:type="simple" xlink:href="http://www.walenz.org/vonNeumann/">
Theory of self-reproducing automata, edited and completed by Arthur W. Burks.</weblink> - University of Illinois press, Urbana and London </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Dudar Z.V., Shuklin D.E., 2000. Implementation of neurons for semantic neural nets that’s understanding texts in natural language. In Radio-electronika i informatika KhTURE, 2000. No 4. Р. 89-96.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Shuklin D.E., 2004. <weblink xlink:type="simple" xlink:href="http://www.shuklin.com/ai/ht/en/ai04001f.aspx">
The further development of semantic neural network models.</weblink> In Artificial Intelligence, Donetsk, "Nauka i obrazovanie" Institute of Artificial Intelligence, Ukraine, 2004, No 3. P. 598-606 </entry>
</list>
</p>
<p>

----</p>
<p>

<list>
<entry level="1" type="bullet">

 Shuklin D.E. The Structure of a Semantic Neural Network Extracting the Meaning from a Text, In Cybernetics and Systems Analysis, Volume 37, Number 2, 4 March 2001, pp. 182-186(5) <weblink xlink:type="simple" xlink:href="http://www.ingentaconnect.com/content/klu/casa">
http://www.ingentaconnect.com/content/klu/casa</weblink></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Shuklin D.E. The Structure of a Semantic Neural Network Realizing Morphological and Syntactic Analysis of a Text, In Cybernetics and Systems Analysis, Volume 37, Number 5, September 2001, pp. 770-776(7)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Shuklin D.E. Realization of a Binary Clocked Linear Tree and Its Use for Processing Texts in Natural Languages, In Cybernetics and Systems Analysis, Volume 38, Number 4, July 2002, pp. 503-508(6)</entry>
</list>
</p>

</sec>
</bdy>
</article>
