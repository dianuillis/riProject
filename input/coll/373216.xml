<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:56:57[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Kahan summation algorithm</title>
<id>373216</id>
<revision>
<id>235906945</id>
<timestamp>2008-09-02T22:19:38Z</timestamp>
<contributor>
<username>Stemonitis</username>
<id>156441</id>
</contributor>
</revision>
<categories>
<category>Numerical analysis</category>
<category>Computer arithmetic</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../506/21506.xml">
numerical analysis</link>, the <b>Kahan summation algorithm</b> (also known as <b>compensated summation</b>) significantly reduces the <link xlink:type="simple" xlink:href="../692/12076692.xml">
numerical error</link> in the total obtained by adding a <link xlink:type="simple" xlink:href="../838/27838.xml">
sequence</link> of finite <link xlink:type="simple" xlink:href="../110/552110.xml">
precision</link> <link xlink:type="simple" xlink:href="../376/11376.xml">
floating point number</link>s, compared to the obvious approach. This <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> is attributed to <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../413/303413.xml">
William Kahan</link></scientist>
</person>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>
<sec>
<st>
The algorithm</st>
<p>

In <link xlink:type="simple" xlink:href="../185/24185.xml">
pseudocode</link>, the algorithm is:</p>
<p>

<b>function</b> kahanSum(input, n)
<b>var</b> sum = input[1]
<b>var</b> c = 0.0          //A running compensation for lost low-order bits.
<b>for</b> i = 2 <b>to</b> n
y = input[i] - c    //So far, so good: <it>c</it> is zero.
t = sum + y         //Alas, <it>sum</it> is big, <it>y</it> small, so low-order digits of <it>y</it> are lost.
c = (t - sum) - y   //<it>(t - sum)</it> recovers the high-order part of <it>y</it>; subtracting <it>y</it> recovers -(low part of <it>y</it>)
sum = t             //Algebraically, <it>c</it> should always be zero. Beware eagerly optimising compilers!
<b>next</b> i               //Next time around, the lost low part will be added to <it>y</it> in a fresh attempt.
<b>return</b> sum</p>
<p>

An example in six-digit floating-point decimal arithmetic. Suppose <it>sum</it> has attained the value 100000 and the next value of <it>input(i)</it> is 3.14159 (a six-digit floating point number) and <it>c</it> has the value zero.
y = 3.14159 - 0
t = 100000 + 3.14159
= 100003                      Many digits have been lost!
c = (100003 - 100000) - 3.14159 This <b>must</b> be evaluated as written! 
= 3.00000 - 3.14159           The assimilated part of <it>y</it> recovered, vs. the original full <it>y</it>.
= -0.141590                   The trailing zero because this is six-digit arithmetic.
sum = 100003                      Thus, few digits from <it>input(i</it>) met those of <it>sum</it>. </p>
<p>

The sum is so large that only the high-order digits of the input numbers are being accumulated. But on the next step, suppose <it>input(i)</it> has the value 2.71828, and this time, <it>c</it> is not zero...
y = 2.71828 - -0.141590         The shortfall from the previous stage has another chance.
= 2.85987                     It is of a size similar to <it>y</it>: most digits meet.
t = 100003 + 2.85987            But few meet the digits of <it>sum</it>. 
= 100006                      Rounding is good, but even with truncation,
c = (100006 - 100003) - 2.85987 This extracts whatever went in.
= 3.00000 - 2.85987           In this case, too much.
= .140130                     But no matter, the excess will be subtracted off next time.
sum = 100006</p>
<p>

So the summation is performed with two accumulators: <it>sum</it> holds the sum, and <it>c</it> accumulates the parts not assimilated into <it>sum</it>, to nudge the low-order part of <it>sum</it> the next time around. Thus the summation proceeds with "guard digits" in <it>c</it> which is better than not having any but is not as good as performing the calculations with double the precision of the input. However, if <it>input</it> is already in double precision, few systems supply <link xlink:type="simple" xlink:href="../752/1068752.xml">
quadruple precision</link>, and if they did, what if <it>input</it> were quadruple precision...</p>
<p>

Another approach is to perform the summation on differences from a working mean (in the hope that the value of <it>sum</it> never becomes much larger than individual differences), except that some of the values might be quite different from the working mean and thus suffer significant truncation. Alternatively, sort the values and pair positive and negative values so that the accumulated sum remains as close to zero as possible, at great cost in computational effort. Even less practical might be to perform the summation using some sort of <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../892/600892.xml">
multi-precision arithmetic</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 - perhaps as a part of checking the behaviour of another method.</p>
<p>

Remember that if data are available to limited precision, say six digits, then although 100000 + 2.71828 + 0.00000123456 = 100002.71828123456, the six-digit precision sum of 100003 is a more valid representation of their sum in that it does not present a spurious implication of seventeen-digit precision.</p>

</sec>
<sec>
<st>
Optimising compilers</st>
<p>

One of the optimizations performed by some <link xlink:type="simple" xlink:href="../739/5739.xml">
compiler</link>s is to attempt to spot and remove <link xlink:type="simple" xlink:href="../016/3822016.xml">
redundant code</link>.  A sophisticated optimizer might perform symbolic <link xlink:type="simple" xlink:href="../708/938708.xml">
expression</link> simplification and given the code 
t:=sum + y;
c:=(t - sum) - y;
deduce that 
c = 0
which is constant and need not be computed within the loop; further, since it is initialised to zero, the statement
y:=input[i] - c;
can be contracted so that the loop becomes
y:=input[i];
t:=sum + y;
sum:=t;
and finally, the variables <it>y</it> and <it>t</it> are just waystations, so the loop content is reduced to
sum:=sum + input[i];
Which has entirely removed the desired features. Some optimising compilers might carry their analyses so far as to deduce that a summation of <it>input</it> is intended, and then generate code employing maximum precision. It is far more likely that their workings will result in code that ruins the algorithm.</p>
<p>

The algorithm's execution can also be affected by non-mathematical optimisations. For instance, it is quite common for the floating-point computations to be carried out in machine <link xlink:type="simple" xlink:href="../996/25996.xml">
register</link>s that have a precision higher than that of the variables held in main storage, as in the IBM-PC and clones where some of the floating-point registers hold 80-bit floating-point numbers while in main storage they might be held only as 32-bit, or 64-bit as well as 80-bit. The sequence
y:=input[i] - c;
t:=sum + y;
c:=(t - sum) - y;
might be compiled without any of the unwanted mathematical transformations, but, notice that after the code for the first statement is executed, the register holding the result that was stored in <it>y</it> still has (or could still have: the registers might be organised as a stack with overflow to memory) that result and as the next statement refers to <it>y</it>, perhaps the code for it could be arranged so that the value of <it>y</it> need not be fetched from memory; similarly for <it>t</it> in the third statement. If the stored values are held to the same precision as the registers, then there will be no problem: the registers and main storage are thus equivalent except for the speed of access. But if not, the working of the algorithm will be ruined. Optimisation options helpful for some parts of the program will not necessarily be good for all parts of a program.</p>

</sec>
<sec>
<st>
Computer language features</st>

<p>

Some computer languages offer summation features:
+/input       <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../451/1451.xml">
APL</link></programming_language>
 (read this as "Plus, over <it>input</it>")
SUM(input)    <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../168/11168.xml">
Fortran</link></programming_language>
, a semi-function SUM supplied as a compiler intrinsic.
And it might be that the implementation will indeed use the best method. Alas, the Fortran manual to hand offers no details of its internal workings, merely that the result will be in the same precision as <it>input</it>. Inspection of the code generated by the Compaq Visual Fortran v6.6 compiler for a simple usage reveals the equivalent of
Load sum
Add  input(i)
Store sum
There is no attempt to hold the value of <it>sum</it> in a register, therefore, even if the addition were performed with a precision greater than that of <it>sum</it>, that will be lost when the result is stored into <it>sum</it> and later retrieved. So there is nothing beyond a straight summation.</p>
<p>

In some programming languages (C,C++,D), there exist a "volatile" keyword which ensures that all registers are written/read again to/from memory, so one can use this to disable optimisations for particular section of code.</p>

</sec>
<sec>
<st>
When all else fails</st>
<p>

Via careful testing and scrutiny of the code generated, it may be found that no rearrangement of the algorithm nor selection of compiler options delivers good results. In this situation the final word may be obtained through the manual preparation of embedded assembler-like statements to ensure the generation of exactly the machine code desired, if this facility is offered by the compiler, or of course the invocation of a utility routine written separately in machine code.</p>

</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
 <cite id="CITEREFKahan1970" style="font-style:normal">Kahan, William&#32;(Jan. 1965),&#32;"Further remarks on reducing truncation errors",&#32;<it>Communications of the ACM</it>&#32;<b>8</b>(1):  40, <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1145%2F363707.363723">
10.1145/363707.363723</weblink></cite>&nbsp;
</entry>
</reflist>

<list>
<entry level="1" type="bullet">

  <cite id="CITEREFHigham1993" style="font-style:normal">Higham, Nicholas J.&#32;(1993),&#32;"The accuracy of floating point summation",&#32;<it>SIAM Journal of Scientific Computing</it>&#32;<b>14</b>(4):  783-799, <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1137%2F0914050">
10.1137/0914050</weblink></cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite id="CITEREFGoldberg1991" style="font-style:normal">Goldberg, David&#32;(Mar. 1991),&#32;"<weblink xlink:type="simple" xlink:href="http://www.validlab.com/goldberg/paper.pdf">
What every computer scientist should know about floating-point arithmetic</weblink>",&#32;<it>ACM Computing Surveys</it>&#32;<b>23</b>(1):  5-48, <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1145%2F103162.103163">
10.1145/103162.103163</weblink>, </cite>&nbsp;</entry>
</list>
</p>


</sec>
</bdy>
</article>
