<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:54:27[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Static single assignment form</title>
<id>373371</id>
<revision>
<id>237994088</id>
<timestamp>2008-09-12T19:27:33Z</timestamp>
<contributor>
<username>Jplevyak</username>
<id>753590</id>
</contributor>
</revision>
<categories>
<category>Compiler theory</category>
<category>Articles with example pseudocode</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../739/5739.xml">
compiler</link> design, <b>static single assignment form</b> (often abbreviated as <b>SSA form</b> or <b>SSA</b>) is an <link xlink:type="simple" xlink:href="../091/485091.xml">
intermediate representation</link> (IR) in which every variable is assigned exactly once. Existing variables in the original IR are split into <it>versions</it>, new variables typically indicated by the original name with a subscript, so that every definition gets its own version. In SSA form, <link xlink:type="simple" xlink:href="../768/1286768.xml">
use-def chains</link> are explicit and each contains a single element.<p>

SSA was developed by <link>
Ron Cytron</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../372/8908372.xml">
Jeanne Ferrante</link></associate>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
, <link>
Barry Rosen</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<employee wordnetid="110053808" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../756/6664756.xml">
Mark Wegman</link></associate>
</employee>
</scientist>
</causal_agent>
</colleague>
</worker>
</person>
</peer>
</physical_entity>
, and <link>
Ken Zadeck</link>, researchers at <link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link> in the 1980s.</p>
<p>

In <link xlink:type="simple" xlink:href="../933/10933.xml">
functional language</link> compilers, such as those for <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../119/28119.xml">
Scheme</link></programming_language>
, <link xlink:type="simple" xlink:href="../607/20607.xml">
ML</link> and <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../630/13630.xml">
Haskell</link></programming_language>
, <link xlink:type="simple" xlink:href="../852/749852.xml">
continuation passing style</link> (CPS) is generally used where one might expect to find SSA in a compiler for <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../168/11168.xml">
Fortran</link></programming_language>
 or <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
. SSA and CPS are formally equivalent, so optimizations and transformations formulated in terms of one immediately apply to the other.</p>

<sec>
<st>
Benefits</st>
<p>

The primary usefulness of SSA comes from how it simultaneously simplifies and improves the results of a variety of <link xlink:type="simple" xlink:href="../355/40355.xml">
compiler optimization</link>s, by simplifying the properties of variables. For example, consider this piece of code:</p>
<p>

y := 1
y := 2
x := y</p>
<p>

As humans, we can see that the first assignment is not necessary, and that the value of y being used in the third line comes from the second assignment of y. A program would have to perform <link xlink:type="simple" xlink:href="../665/1837665.xml">
reaching definition analysis</link> to determine this. But if the program is in SSA form, both of these are immediate:</p>
<p>

y1 := 1
y2 := 2
x1 := y2</p>
<p>

<link xlink:type="simple" xlink:href="../355/40355.xml">
Compiler optimization</link> algorithms which are either enabled or strongly enhanced by the use of SSA include:
<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../347/113347.xml">
constant propagation</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../610/1836610.xml">
sparse conditional constant propagation</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../793/464793.xml">
dead code elimination</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../807/547807.xml">
global value numbering</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../268/2209268.xml">
partial redundancy elimination</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../907/475907.xml">
strength reduction</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../122/485122.xml">
register allocation</link></entry>
</list>
</p>

</sec>
<sec>
<st>
Converting to SSA</st>
<p>

Converting ordinary code into SSA form is primarily a simple matter of replacing the target of each assignment with a new variable, and replacing each use of a variable with the "version" of the variable <link xlink:type="simple" xlink:href="../665/1837665.xml">
reaching</link> that point. For example, consider the following <link xlink:type="simple" xlink:href="../653/43653.xml">
control flow graph</link>:</p>

<p>

<image width="150px" src="SSA_example1.1.png">
<caption>

An example control flow graph, before conversion to SSA
</caption>
</image>
</p>

<p>

Notice that we could change the name on the left side of "x <math>\leftarrow</math> x - 3", and change the following uses of x to use that new name, and the program would still do the same thing. We exploit this in SSA by creating two new variables, x1 and x2, each of which is assigned only once. We likewise give distinguishing subscripts to all the other variables, and we get this:</p>

<p>

<image width="150px" src="SSA_example1.2.png">
<caption>

An example control flow graph, partially converted to SSA
</caption>
</image>
</p>

<p>

We've figured out which definition each use is referring to, except for one thing: the uses of y in the bottom block could be referring to <it>either</it> y1 or y2, depending on which way the control flow came from. So how do we know which one to use?</p>
<p>

The answer is that we add a special statement, called a <it>Φ (Phi) function</it>, to the beginning of the last block. This statement will generate a new definition of y, y3, by "choosing" either y1 or y2, depending on which arrow control arrived from:</p>

<p>

<image width="150px" src="SSA_example1.3.png">
<caption>

An example control flow graph, fully converted to SSA
</caption>
</image>
</p>

<p>

Now, the uses of y in the last block can simply use y3, and they'll obtain the correct value either way. You might ask at this point, do we need to add a Φ function for x too? The answer is no; only one version of x, namely x2 is reaching this place, so there's no problem.</p>
<p>

A more general question along the same lines is, given an arbitrary control flow graph, how can I tell where to insert Φ functions, and for what variables? This is a difficult question, but one that has an efficient solution that can be computed using a concept called <it>dominance frontiers</it>.</p>
<p>

Note: the Φ functions are not actually implemented; instead, they're just markers for the compiler to place the value of all the variables, which are grouped together by the Φ function, in the same location in memory (or same register).</p>

<ss1>
<st>
Computing minimal SSA using dominance frontiers</st>
<p>

First, we need the concept of a <link xlink:type="simple" xlink:href="../116/396116.xml">
<it>dominator''</it></link>: we say that a node A <it>strictly dominates</it> a different node B in the control flow graph if it's impossible to reach B without passing through A first. This is useful, because if we ever reach B we know that any code in A has run. We say that A <it>dominates</it> B if either A strictly dominates B or A = B. </p>
<p>

Now we can define the <link xlink:type="simple" xlink:href="../116/396116.xml">
<it>dominance frontier''</it></link>: a node B is in the dominance frontier of a node A if A does <it>not</it> strictly dominate B, but does dominate some immediate predecessor of B (possibly A itself if A is the immediate predecessor of B). From A's point of view, these are the nodes at which other control paths, which don't go through A, make their earliest appearance.</p>


<p>

Dominance frontiers capture the precise places at which we need Φ functions: if the node A defines a certain variable, then that definition and that definition alone (or redefinitions) will reach every node A dominates. Only when we leave these nodes and enter the dominance frontier must we account for other flows bringing in other definitions of the same variable. Moreover, no other Φ functions are needed in the control flow graph to deal with A's definitions, and we can do with no less.</p>


<p>

One algorithm for computing the dominance frontier set is:</p>
<p>

<b>for each</b> node b
<b>if</b> the number of predecessors of b ≥ 2
<b>for each</b> p <b>in</b> predecessors of b
runner := p
<b>while</b> runner ≠ idom(b)
add b to runner’s dominance frontier set
runner := idom(runner)</p>
<p>

Note: in the code above, a predecessor of node n is any node from which control is transferred to node n, and idom(n) is the immediate dominator of node n.</p>
<p>

There is an efficient algorithm for finding dominance frontiers of each node. This algorithm was originally described in the paper "Efficiently computing static single assignment form and the control dependence graph", by R. Cytron, J. Ferrante, B. Rosen, M. Wegman and F. Zadeck, <it>ACM Trans. on Programming Languages and Systems</it> 13(4) 1991 pp.451&ndash;490. Also useful is chapter 19 of the book "Modern compiler implementation in Java" by Andrew Appel (Cambridge University Press, 2002). See the paper for more details.</p>
<p>

Keith D. Cooper, Timothy J. Harvey, and Ken Kennedy of <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../813/25813.xml">
Rice University</link></university>
 describe an algorithm in their paper titled <weblink xlink:type="simple" xlink:href="http://www.hipersoft.rice.edu/grads/publications/dom14.pdf">
<it>A Simple, Fast Dominance Algorithm''</it></weblink>. The algorithm uses well engineered data structures to improve performance.</p>

</ss1>
</sec>
<sec>
<st>
 Variations that reduce the number of Φ functions </st>
<p>

"Minimal" SSA inserts the minimal number of Φ functions required to ensure that each name is assigned a value exactly 
once and that each reference (use) of a name in the original program can still refer to a unique name.  (The latter requirement
is needed to ensure that the compiler can write down a name for each operand in each operation.)</p>
<p>

However, some of these Φ functions could be <it><link xlink:type="simple" xlink:href="../793/464793.xml">
dead</link></it>.  For this reason, minimal SSA 
does not necessarily produce the fewest number of Φ functions that are needed by a specific procedure.  For some types of analysis, these Φ functions are superfluous and can cause the analysis to run less efficiently.</p>

<ss1>
<st>
Pruned SSA</st>
<p>

Pruned SSA form is based on a simple observation: Φ functions are only needed for variables that are "live" after the Φ function.
(Here, "live" means that the value is used along some path that begins at the Φ function in question.)  If a variable is not live,
the result of the Φ function cannot be used and the assignment by the Φ function is dead.</p>
<p>

Construction of pruned SSA form uses <link xlink:type="simple" xlink:href="../356/4051356.xml">
live variable information</link> in the Φ function insertion phase to decide whether a given Φ function is needed.  If the original variable name isn't live at the Φ function insertion point, the Φ function isn't inserted.</p>
<p>

Another possibility is to treat pruning as a <link xlink:type="simple" xlink:href="../793/464793.xml">
dead code elimination</link> problem.  Then, a Φ function is live only if any use in the input program will be rewritten to it, or if it will be used as an argument in another Φ function.   When entering SSA form, each use is rewritten to the nearest definition that dominates it.  A Φ function will then be considered live as long as it is the nearest definition that dominates at least one use, or at least one argument of a live Φ.</p>

</ss1>
<ss1>
<st>
Semi-pruned SSA</st>
<p>

Semi-pruned SSA form <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> is an attempt to reduce the number of Φ functions without incurring the relatively high cost of computing live variable information.  It is based on the following observation: if a variable is never live upon entry into a basic block, it never needs a Φ function.  During SSA construction, Φ functions for any "block-local" variables are omitted.</p>
<p>

Computing the set of block-local variables is a simpler and faster procedure than full live variable analysis, making semi-pruned SSA form more efficient to compute than pruned SSA form.  On the other hand, pruned SSA form will contain fewer unnecessary Φ functions.</p>

</ss1>
</sec>
<sec>
<st>
Converting out of SSA form</st>
<p>

As SSA form is no longer useful for direct execution, it is frequently used "on top of" another IR with which it remains in direct correspondence.  This can be accomplished by "constructing" SSA as a set of functions which map between parts of the existing IR (basic blocks, instructions, operands, <it>etc.</it>) and its SSA counterpart.  When the SSA form is no longer needed, these mapping functions may be discarded, leaving only the now-optimized IR.</p>
<p>

Performing optimizations on SSA form usually leads to entangled SSA-Webs, meaning there are phi instructions whose operands do not all have the same root operand. In such cases color-out algorithms are used to come out of SSA. Naive algorithms introduce a copy along each predecessor path which caused a source of different root symbol to be put in phi than the destination of phi. There are multiple algorithms for coming out of SSA with fewer copies, most use interference graphs or some approximation of it to do copy coalescing.</p>

</sec>
<sec>
<st>
Extensions</st>
<p>

Extensions to SSA form can be divided into two categories.</p>
<p>

<it>Renaming scheme</it> extensions alter the renaming criterion. Recall that SSA form renames each variable when it is assigned a value. Alternative schemes include static single use form (which renames each variable at each statement when it is used) and static single information form (which renames each variable when it is assigned a value, and in each conditional context in which that variable is used).</p>
<p>

<it>Feature-specific</it> extensions retain the single assignment property for variables, but incorporate new semantics to model additional features. Some feature-specific extensions model high-level programming language features like arrays, objects and aliased pointers. Other feature-specific extensions model low-level architectural features like speculation and predication.</p>

</sec>
<sec>
<st>
Compilers using SSA form</st>
<p>

SSA form is a relatively recent development in the compiler community.  As such, many older compilers only use SSA form for some part of the compilation or optimization process, but most do not rely on it.  Examples of compilers that rely heavily on SSA form include:</p>
<p>

<list>
<entry level="1" type="bullet">

The ETH <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../019/449019.xml">
Oberon-2</link></programming_language>
 compiler was one of the first public projects to incorporate "GSA", a variant of SSA.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

The <link>
LLVM</link> Compiler Infrastructure uses SSA form for all scalar register values (everything except memory) in its primary code representation.  SSA form is only eliminated once register allocation occurs, late in the compile process (often at link time).</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

The open source SGI compiler <weblink xlink:type="simple" xlink:href="http://ipf-orc.sourceforge.net/">
ORC</weblink> uses SSA form in its global scalar optimizer, though the code is brought into SSA form before and taken out of SSA form afterwards.  ORC uses extensions to SSA form to represent memory in SSA form as well as scalar values.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

As of version 4 (released in April 2005) GCC, the <physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<compiler wordnetid="109946957" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<link xlink:type="simple" xlink:href="../323/12323.xml">
GNU Compiler Collection</link></writer>
</causal_agent>
</compiler>
</person>
</communicator>
</physical_entity>
, makes extensive use of SSA. The <link xlink:type="simple" xlink:href="../625/428625.xml">
frontend</link>s generate <link xlink:type="simple" xlink:href="../278/901278.xml">
GENERIC</link> code which is then converted into SSA form by the "<link>
gimplifier</link>" and optimized by the "<link>
middle-end</link>". The <link xlink:type="simple" xlink:href="../625/428625.xml">
backend</link> eventually translates the optimized intermediate code into <link xlink:type="simple" xlink:href="../344/26344.xml">
RTL</link>, executes some more low-level optimizations and finally turns RTL into <link xlink:type="simple" xlink:href="../368/1368.xml">
assembly language</link>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

<company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link></company>
's open source adaptive <link xlink:type="simple" xlink:href="../389/16389.xml">
Java virtual machine</link>, <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../117/1557117.xml">
Jikes</link></software>
 RVM, uses extended Array SSA, an extension of SSA that allows analysis of scalars, arrays, and object fields in a unified framework. Extended Array SSA analysis is only enabled at the maximum optimization level, which is applied to the most frequently executed portions of code.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

In 2002, <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/721276.html">
researchers modified</weblink> IBM's JikesRVM (named Jalapeño at the time) to run both standard Java <link xlink:type="simple" xlink:href="../997/89997.xml">
byte-code</link> and a typesafe SSA (<link xlink:type="simple" xlink:href="../092/3183092.xml">
SafeTSA</link>) byte-code class files, and demonstrated significant performance benefits to using the SSA byte-code.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

<company wordnetid="108058098" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../980/26980.xml">
Sun Microsystems</link></institution>
</company>
' <link xlink:type="simple" xlink:href="../920/1800920.xml">
Java HotSpot Virtual Machine</link> uses an SSA-based intermediate language in its JIT compiler.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mono-project.com/Main_Page">
Mono</weblink> uses SSA in its JIT compiler called Mini.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://jackcc.sf.net">
jackcc</weblink> is an open-source compiler for the academic instruction set Jackal 3.0.  It uses a simple 3-operand code with SSA for its intermediate representation.  As an interesting variant, it replaces Φ functions with a so-called SAME instruction, which instructs the register allocator to place the two live ranges into the same physical register.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

Although not a compiler, the <weblink xlink:type="simple" xlink:href="http://boomerang.sourceforge.net/">
Boomerang</weblink> <link xlink:type="simple" xlink:href="../636/18938636.xml">
decompiler</link> uses SSA form in its internal representation. SSA is used to simplify expression propagation, identifying parameters and returns, preservation analysis, and more.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.dotgnu.org">
Portable.NET</weblink> uses SSA in its JIT compiler.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.info.uni-karlsruhe.de/software/libfirm">
libFirm</weblink> a completely graph based SSA intermediate representation for compilers.  libFirm uses SSA form for all scalar register values until code generation by use of a SSA-aware register allocator.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

The Illinois Concert Compiler circa 1994 <weblink xlink:type="simple" xlink:href="http://www-csag.ucsd.edu/projects/concert.html">
http://www-csag.ucsd.edu/projects/concert.html</weblink> used a variant of SSA called SSU (Static Single Use) which renames each variable when it is assigned a value, and in each conditional context in which that variable is used; essentially the static single information form mentioned above.  The SSU form is documented in <weblink xlink:type="simple" xlink:href="http://www-csag.ucsd.edu/papers/jplevyak-thesis.ps">
John Plevyak's Ph.D Thesis</weblink>.</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
Practical Improvements to the Construction and Destruction of Static Single Assignment Form (1998),  Preston Briggs, Keith D. Cooper, Timothy J. Harvey, L. Taylor Simpson.</entry>
</reflist>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Appel, Andrew W.&#32;(1999). Modern Compiler Implementation in ML.&#32;Cambridge University Press. ISBN 0-521-58274-1.</cite>&nbsp; Also available in <message wordnetid="106598915" confidence="0.8">
<request wordnetid="106513366" confidence="0.8">
<link xlink:type="simple" xlink:href="../881/15881.xml">
Java</link></request>
</message>
 (ISBN 0-521-82060-X 2002) and <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
 (ISBN 0-521-60765-5, 199 8) versions.</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Cooper, Keith D.; &amp; Torczon, Linda.&#32;(2003). Engineering a Compiler.&#32;Morgan Kaufmann. ISBN 1-55860-698-X.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Muchnick, Steven S.&#32;(1997). Advanced Compiler Design and Implementation.&#32;Morgan Kaufmann. ISBN 1-55860-320-4.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Richard A. Kelsey&#32;(March 1995).&#32;"A Correspondence between Continuation Passing Style and Static Single Assignment Form". <it>ACM SIGPLAN Notices</it>&#32;<b>30</b>&#32;(3): 13–22. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1145%2F202530.202532">
10.1145/202530.202532</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Andrew W. Appel&#32;(April 1998).&#32;"SSA is Functional Programming". <it>ACM SIGPLAN Notices</it>&#32;<b>33</b>&#32;(4): 17–20. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1145%2F278283.278285">
10.1145/278283.278285</weblink>.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../355/40355.xml">
Compiler optimization</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../412/428412.xml">
Valgrind</link></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

Steven Bosscher and Diego Novillo. <weblink xlink:type="simple" xlink:href="http://lwn.net/Articles/84888/">
GCC gets a new Optimizer Framework</weblink>. An article about GCC's use of SSA and how it improves over older IRs.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.man.ac.uk/~jsinger/ssa.html">
The SSA Bibliography</weblink>. Extensive catalogue of SSA research papers.</entry>
</list>
</p>


</sec>
</bdy>
</article>
