<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 20:47:13[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>BLEU</title>
<id>3772276</id>
<revision>
<id>241854451</id>
<timestamp>2008-09-29T20:41:58Z</timestamp>
<contributor>
<username>Wiki alf</username>
<id>303874</id>
</contributor>
</revision>
<categories>
<category>Evaluation of machine translation</category>
</categories>
</header>
<bdy>

<table style="clear: right; margin-bottom: .5em; float: right; padding: .5em 0 .8em 1.4em; background: none; width: auto;" cellpadding="0" cellspacing="0">
<row>
<col>
__TOC__</col>
</row>
</table>
<p>

This article is about the evaluation metric for machine translation.&#32;&#32;For other meanings, see <link xlink:type="simple" xlink:href="../751/2300751.xml">
Bleu</link>.&#32;&#32;
<b>BLEU</b> (<b>Bilingual Evaluation Understudy</b>) is a method for evaluating the quality of text which has been translated from one <link xlink:type="simple" xlink:href="../173/21173.xml">
natural language</link> to another using <link xlink:type="simple" xlink:href="../980/19980.xml">
machine translation</link>. BLEU was one of the first <link xlink:type="simple" xlink:href="../442/187442.xml">
software metric</link>s to report high <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation</link> with human judgements of quality. The metric is currently one of the most popular in the field. The central idea behind the metric is that, "the closer a machine translation is to a professional human translation, the better it is".<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002a">
http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002a</weblink></p>
<p>

The metric calculates scores for individual segments, generally <unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link xlink:type="simple" xlink:href="../352/870352.xml">
sentence</link></definite_quantity>
</unit_of_measurement>
s, and then averages these scores over the whole <link xlink:type="simple" xlink:href="../244/2890244.xml">
corpus</link> in order to reach a final score. It has been shown to correlate highly with human judgements of quality at the corpus level.<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002b">
http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002b</weblink><weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Coughlin2003a">
http://localhost:18088/wiki/index.php/BLEU#endnote_Coughlin2003a</weblink> The quality of translation is indicated as a number between 0 and 1 and is measured as statistical closeness to a given set of good quality human reference translations. Therefore, it does not directly take into account translation intelligibility or grammatical correctness. </p>
<p>

The metric works by measuring the <link xlink:type="simple" xlink:href="../182/986182.xml">
n-gram</link> co-occurrence between a given translation and the set of reference translations and then taking the weighted <link xlink:type="simple" xlink:href="../046/13046.xml">
geometric mean</link>. BLEU is specifically designed to approximate human judgement on a <link xlink:type="simple" xlink:href="../244/2890244.xml">
corpus</link> level and performs badly if used to evaluate the quality of isolated sentences.</p>

<sec>
<st>
Algorithm</st>

<p>

BLEU uses a modified form of <link xlink:type="simple" xlink:href="../572/41572.xml">
precision</link> to compare a candidate translation against multiple reference translations. The metric modifies simple precision since machine translation systems have been known to generate more words than appear in a reference text. This is illustrated in the following example from Papineni et al. (2002),</p>
<p>

<table class="wikitable">
<caption>
Example of poor machine translation output with high precision</caption>
<row>
<col>
Candidate</col>
<col>
the</col>
<col>
the</col>
<col>
the</col>
<col>
the</col>
<col>
the</col>
<col>
the</col>
<col>
the</col>
</row>
<row>
<col>
Reference 1</col>
<col>
the</col>
<col>
cat</col>
<col>
is</col>
<col>
on</col>
<col>
the</col>
<col>
mat</col>
</row>
<row>
<col>
Reference 2</col>
<col>
there</col>
<col>
is</col>
<col>
a</col>
<col>
cat</col>
<col>
on</col>
<col>
the</col>
<col>
mat</col>
</row>
</table>
</p>
<p>

In this example, the candidate text is given a unigram precision of,</p>
<p>

<indent level="1">

<math>P = \frac{m}{w_{t}} = \frac{7}{7} = 1</math>
</indent>

Of the seven words in the candidate translation, all of them appear in the reference translations. This presents a problem for a metric, as the candidate translation above is complete nonsense, retaining none of the content of either of the references. The modification that BLEU makes is fairly straightforward.</p>
<p>

For each word in the candidate translation, the algorithm takes the maximum total count in the reference translations. Taking the example above, the word 'the' appears twice in reference 1, and once in reference 2. The largest value is taken, in this case '2' as the "maximum reference count".</p>
<p>

For each of the words in the candidate translation, the count of the word is compared against the maximum reference count, and the lowest value is taken. In this case, the count of the word 'the' in the candidate translation is '7', while the maximum reference count for the word is '2'. This "modified count" is then divided by the total number of words in the candidate translation. In the above example, the modified unigram precision score would be,</p>
<p>

<indent level="1">

<math>P = \frac{2}{7}</math>
</indent>

The above method is used to calculate scores for each <math>n</math>. The value of <math>n</math> which has the "highest correlation with monolingual human judgements"<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002c">
http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002c</weblink> was found to be 4. The unigram scores are found to account for the adequacy of the translation, in other words, how much information is retained in the translation. The longer <math>n</math>-gram scores account for the fluency of the translation, or to what extent it reads like "good English".</p>
<p>

The modification made to precision does not solve the problem of short translations. Short translations can produce very high precision scores, even using modified precision. An example of a candidate translation for the same references as above might be:</p>
<p>

<indent level="1">

the cat
</indent>

In this example, the modified unigram precision would be,</p>
<p>

<indent level="1">

<math>P = \frac{1}{2} + \frac{1}{2} = \frac{2}{2}</math>
</indent>

as the word 'the' and the word 'cat' appear once each in the candidate, and the total number of words is two. The modified bigram precision would be <math>1 / 1</math> as the bigram, "the cat" appears once in the candidate. It has been pointed out that precision is usually twinned with <link xlink:type="simple" xlink:href="../499/159499.xml">
recall</link> to overcome this problem <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002d">
http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002d</weblink>, as the unigram recall of this example would be <math>2 / 6</math> or <math>2 / 7</math>. The problem being that as there are multiple reference translations, a bad translation could easily have an inflated recall, such as a translation which consisted of all the words in each of the references.<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002e">
http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002e</weblink></p>
<p>

In order to produce a score for the whole corpus, the modified precision scores for the segments are combined using the <link xlink:type="simple" xlink:href="../046/13046.xml">
geometric mean</link>, multiplied by a brevity penalty, whose purpose is to prevent very short candidates from receiving too high a score. Let <math>r</math> be the total length of the reference corpus, and <math>c</math> the total length of the translation corpus. If <math>c \leq r</math>, the brevity penalty applies and is defined to be <math>e^{(1-r/c)}</math>. (In the case of multiple reference sentences, <math>r</math> is taken to be the sum of the lengths of the sentences whose lengths are closest to the lengths of the candidate sentences. However, in the version of the metric used by <link xlink:type="simple" xlink:href="../888/21888.xml">
NIST</link>, the short reference sentence is used.)</p>

</sec>
<sec>
<st>
Performance</st>

<p>

BLEU has frequently been reported as correlating well with human judgement,<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002f">
http://localhost:18088/wiki/index.php/BLEU#endnote_Papineni2002f</weblink><weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Coughlin2003b">
http://localhost:18088/wiki/index.php/BLEU#endnote_Coughlin2003b</weblink><weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Doddington2002a">
http://localhost:18088/wiki/index.php/BLEU#endnote_Doddington2002a</weblink> and certainly remains a benchmark for any new evaluation metric to beat. There are however a number of criticisms that have been voiced. It has been noted that while in theory capable of evaluating any language, BLEU does not in the present form work on languages without word boundaries.<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Denoul2005a">
http://localhost:18088/wiki/index.php/BLEU#endnote_Denoul2005a</weblink></p>
<p>

It has been argued that although BLEU certainly has significant advantages, there is no guarantee that an increase in BLEU score is an indicator of improved translation quality.<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Callison2006a">
http://localhost:18088/wiki/index.php/BLEU#endnote_Callison2006a</weblink> As BLEU scores are taken at the corpus level, it is difficult to give a textual example. Nevertheless, they highlight two instances where BLEU seriously underperformed. These were the 2005 <link xlink:type="simple" xlink:href="../888/21888.xml">
NIST</link> evaluations<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Lee2005a">
http://localhost:18088/wiki/index.php/BLEU#endnote_Lee2005a</weblink> where a number of different machine translation systems were tested, and their study of the <link xlink:type="simple" xlink:href="../593/29593.xml">
SYSTRAN</link> engine versus two engines using <link xlink:type="simple" xlink:href="../491/4558491.xml">
statistical machine translation</link> (SMT) techniques.<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Callison2006b">
http://localhost:18088/wiki/index.php/BLEU#endnote_Callison2006b</weblink></p>
<p>

In the 2005 NIST evaluation, they report that the scores generated by BLEU failed to correspond to the scores produced in the human evaluations. The system which was ranked highest by the human judges was only ranked 6th by BLEU. In their study, they compared SMT systems with SYSTRAN, a knowledge based system. The scores from BLEU for SYSTRAN were substantially worse than the scores given to SYSTRAN by the human judges. They note that the SMT systems were trained using BLEU minimum error rate training,<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Och2004a">
http://localhost:18088/wiki/index.php/BLEU#endnote_Och2004a</weblink> and point out that this could be one of the reasons behind the
difference. They conclude by recommending that BLEU be used in a more restricted manner, for comparing the results from two similar systems, and for tracking "broad, incremental changes to a single system".<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/BLEU#endnote_Callison2006c">
http://localhost:18088/wiki/index.php/BLEU#endnote_Callison2006c</weblink></p>

<ss1>
<st>
 BLEU and real applications of MT: criticism </st>

<p>

Another possible criticism of evaluation measures such as BLEU is that they are far from measuring the performance of machine translation in real situations, which may be grouped in two categories: <it>assimilation</it> (use of machine translation output "as is" as an aid to understanding) and <it>dissemination</it> (use of machine translation as a way to produce drafts that will be corrected or <it>postedited</it> before publishing). This is because BLEU tries to measure how close the result of machine is from a reference translation or a set of reference translations produced by human translators, which may or may not correlate with indicators of quality in those two groups of real situations. </p>
<p>

Indeed, one of the underlying assumptions of BLEU is that quality equals human likeness. But this may be one reason for criticism. For instance, "human-unlikely" translations such as English text without definite articles but otherwise "correct" may be very close to being adequate for assimilation purposes, but far from being adequate for dissemination purposes (too many words to insert). Conversely, "human-unlikely" translations with obvious errors affecting understandability of the text (for instance, lexical selection errors caused by ambiguity) may be easily rendered adequate by a human posteditor.</p>

</ss1>
</sec>
<sec>
<st>
See also</st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../595/4993595.xml">
NIST (metric)</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../043/5855043.xml">
METEOR</link></entry>
</list>
</p>

</sec>
<sec>
<st>
Notes</st>

<p>

<list>
<entry level="1" type="number">

  <cite id="endnote_Papineni2002a" style="font-style: normal;">
<b>^</b></cite>&nbsp; Papineni, K., et al. (2002)</entry>
<entry level="1" type="number">

  <cite id="endnote_Papineni2002b" style="font-style: normal;">
<b>^</b></cite>&nbsp; Papineni, K., et al. (2002)</entry>
<entry level="1" type="number">

  <cite id="endnote_Coughlin2003a" style="font-style: normal;">
<b>^</b></cite>&nbsp; Coughlin, D. (2003)</entry>
<entry level="1" type="number">

  <cite id="endnote_Papineni2002c" style="font-style: normal;">
<b>^</b></cite>&nbsp; Papineni, K., et al. (2002)</entry>
<entry level="1" type="number">

  <cite id="endnote_Papineni2002d" style="font-style: normal;">
<b>^</b></cite>&nbsp; Papineni, K., et al. (2002)</entry>
<entry level="1" type="number">

  <cite id="endnote_Papineni2002e" style="font-style: normal;">
<b>^</b></cite>&nbsp; Papineni, K., et al. (2002)</entry>
<entry level="1" type="number">

  <cite id="endnote_Papineni2002e" style="font-style: normal;">
<b>^</b></cite>&nbsp; Papineni, K., et al. (2002)</entry>
<entry level="1" type="number">

  <cite id="endnote_Coughlin2003b" style="font-style: normal;">
<b>^</b></cite>&nbsp; Coughlin, D. (2003)</entry>
<entry level="1" type="number">

  <cite id="endnote_Doddington2002a" style="font-style: normal;">
<b>^</b></cite>&nbsp; Doddington, G. (2002)</entry>
<entry level="1" type="number">

  <cite id="endnote_Denoul2005a" style="font-style: normal;">
<b>^</b></cite>&nbsp; Denoul, E. and Lepage, Y. (2005)</entry>
<entry level="1" type="number">

  <cite id="endnote_Callison2006a" style="font-style: normal;">
<b>^</b></cite>&nbsp; Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</entry>
<entry level="1" type="number">

  <cite id="endnote_Lee2005a" style="font-style: normal;">
<b>^</b></cite>&nbsp; Lee, A. and Przybocki, M. (2005)</entry>
<entry level="1" type="number">

  <cite id="endnote_Callison2006b" style="font-style: normal;">
<b>^</b></cite>&nbsp; Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</entry>
<entry level="1" type="number">

  <cite id="endnote_Och2004a" style="font-style: normal;">
<b>^</b></cite>&nbsp; Lin, C. and Och, F. (2004)</entry>
<entry level="1" type="number">

  <cite id="endnote_Callison2006c" style="font-style: normal;">
<b>^</b></cite>&nbsp; Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</entry>
</list>
</p>


</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. (2002). "BLEU: a method for automatic evaluation of machine translation" in <it>ACL-2002: 40th Annual meeting of the Association for Computational Linguistics</it> pp. 311--318</entry>
<entry level="1" type="bullet">

 Callison-Burch, C., Osborne, M. and Koehn, P. (2006) "Re-evaluating the Role of BLEU in Machine Translation Research" in <it>11th Conference of the European Chapter of the Association for Computational Linguistics: EACL 2006</it> pp. 249--256</entry>
<entry level="1" type="bullet">

 Doddington, G. (2002) "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics" in <it>Proceedings of the Human Language Technology Conference (HLT), San Diego, CA</it> pp. 128--132</entry>
<entry level="1" type="bullet">

 Coughlin, D. (2003) "Correlating Automated and Human Assessments of Machine Translation Quality" in <it>MT Summit IX, New Orleans, USA</it> pp. 23--27</entry>
<entry level="1" type="bullet">

 Denoul, E. and Lepage, Y. (2005) "BLEU in characters: towards automatic MT evaluation in languages without word delimiters" in <it>Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing</it> pp. 81--86</entry>
<entry level="1" type="bullet">

 Lee, A. and Przybocki, M. (2005) NIST 2005 machine translation evaluation official results</entry>
<entry level="1" type="bullet">

 Lin, C. and Och, F. (2004) "Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics" in <it>Proceedings of the 42nd Annual Meeting of the Association of Computational Linguistics</it>.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>



</sec>
</bdy>
</article>
