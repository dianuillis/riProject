<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:40:11[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Software performance testing</title>
<id>39294</id>
<revision>
<id>240664279</id>
<timestamp>2008-09-24T12:00:01Z</timestamp>
<contributor>
<username>MrOllie</username>
<id>6908984</id>
</contributor>
</revision>
<categories>
<category>Software testing</category>
<category>Software performance optimization</category>
</categories>
</header>
<bdy>

<table style="background:#f9f9f9; font-size:85%; line-height:110%; ">
<row>
<col>
 <image width="32x28px" src="Portal.svg">
</image>
</col>
<col style="padding:0 0.2em;">
 <b><it>
Software Testing&#32;portal</it></b></col>
</row>
</table>
<p>

In <link xlink:type="simple" xlink:href="../010/27010.xml">
software engineering</link>, <b>performance testing</b> is <link xlink:type="simple" xlink:href="../090/29090.xml">
testing</link> that is performed, from one perspective,  to determine how fast some aspect of a system performs under a particular workload.  It can also serve to validate and verify other quality attributes of the system, such as scalability, reliability and resource usage. Performance testing is a subset of <link xlink:type="simple" xlink:href="../610/6615610.xml">
Performance engineering</link>, an emerging computer science practice which strives to build performance into the design and architecture of a system, prior to the onset of actual coding effort.</p>
<p>

Performance testing can serve different purposes.  It can demonstrate that the system meets performance criteria.  It can compare two systems to find which performs better.  Or it can measure what parts of the system or workload cause the system to perform badly.  In the diagnostic case, software engineers use tools such as <link xlink:type="simple" xlink:href="../080/2310080.xml">
profiler</link>s to measure what parts of a device or software contribute most to the poor performance or to establish throughput levels (and thresholds) for maintained acceptable response time. It is critical to the cost performance of a new system, that performance test efforts begin at the inception of the development project and extend through to deployment. The later a performance defect is detected, the higher the cost of remediation. This is true in the case of functional testing, but even more so with performance testing, due to the end-to-end nature of its scope.</p>
<p>

In performance testing, it is often crucial (and often difficult to arrange) for the test conditions to be similar to the expected actual use.  This is, however, not entirely possible in actual practice.  The reason is that production systems have a random nature of the workload and while the test workloads do their best to mimic what may happen in the production environment, it is impossible to exactly replicate this workload variability - except in the most simple system.</p>
<p>

Loosely-coupled architectural implementations (e.g.: <work wordnetid="100575741" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<service wordnetid="100577525" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link>
SOA</link></activity>
</psychological_feature>
</act>
</service>
</event>
</work>
) have created additional complexities with performance testing. Enterprise services or assets (that share common infrastructure or platform) require coordinated performance testing (with all consumers creating production-like transaction volumes and load on shared infrastructures or platforms) to truly replicate production-like states. Due to the complexity and financial and time requirements around this activity, some organizations now employ tools that can monitor and create production-like conditions (also referred as "noise") in their performance testing environments (<link xlink:type="simple" xlink:href="../859/386859.xml">
PTE</link>) to understand capacity and resource requirements and verify / validate quality attributes.</p>
<p>

<b>Load Testing</b></p>
<p>

This is the simplest form of performance testing. A load test is usually conducted to understand the behavior of the application under a specific expected load. This load can be the expected concurrent number of users on the application performing a specific number of transaction within the set duration. This test will give out the response times of all the important business critical transactions. If the database, application server, etc are also monitored, then this simple test can itself point towards the bottleneck in the application.</p>
<p>

<b>Stress Testing</b></p>
<p>

This testing is normally used to break the application. Double the number of users are added to the application and the test is run again until the application breaks down. This kind of test is done to determine the application's robustness in times of extreme load and helps application administrators to determine if the application will perform sufficiently if the current load goes well above the expected load.</p>
<p>

<b>Endurance Testing (Soak Testing)</b></p>
<p>

This test is usually done to determine if the application can sustain the continuous expected load. Generally this test is done to determine if there are any memory leaks in the application.</p>
<p>

<b>Spike Testing</b> </p>
<p>

Spike testing, as the name suggests is done by spiking the number of users and understanding the behavior of the application whether it will go down or will it be able to handle dramatic changes in load. </p>
<p>

<b>Pre-requisites for Performance Testing</b></p>
<p>

A stable build of the application which must resemble the Production environment as close to possible.</p>
<p>

The performance testing environment should not be clubbed with UAT or development envrironment. This is dangerous as if an UAT or Integration testing or other testing is going on the same environment, then the results obtained from the performance testing may not be reliable. As a best practice it is always advisable to have a separate performance testing environment resembling the production environment as much as possible.</p>
<p>

<b>Conclusion</b></p>
<p>

Performance testing is evolving as a separate sciences, with the number of performance testng tools such as HP's LoadRunner, JMeter, OpenSTA, WebLoad, SilkPerformer. Also each of the tests are done catering to the specific requirements of the application.</p>

<sec>
<st>
 <b>Myths of Performance Testing</b> </st>
<p>

Some of the very common myths are given below.
<b>1. Performance Testing is done to break the system.</b></p>
<p>

Stress Testing is done to understand the break point of the system. Otherwise normal load testing is generally done to understand the behavior of the application under the expected user load. Depending on other requirements, such as expectation of spike load, continued load for an extended period of time would demand spike, endurance soak or stress testing.</p>
<p>

<b>2. Performance Testing should only be done after the System Integration Testing</b></p>
<p>

Although this is mostly the norm in the industry, performance testing can also be done while the initial development of the application is taking place. This kind of approach is known as the Early Performance Testing. This approach would ensure a holistic development of the application keeping the performance parameters in mind. Thus the finding of a performance bug just before the release of the application and the cost involved in rectifying the bug is reduced to a great extend.</p>
<p>

<b>3. Performance Testing only involves creation of scripts and any application changes would cause a simple refactoring of the scripts.</b></p>
<p>

Performance Testing in itself is an evolving science in the Software Industry. Scripting itself although important, is only one of the components of the performance testing. The major challenge for any performance tester is to determine the type of tests needed to execute and analyzing the various performance counters to determine the performance bottleneck.</p>
<p>

The other segment of the myth concerning the change in application would result only in little refactoring in the scripts is also untrue as any form of change on the UI espescially in Web protocol would entail complete re-development of the scripts from the scratch. This problem becomes bigger if the protocols involved include Web Services, Siebel, Web Click n Script, Citrix, SAP.</p>

</sec>
<sec>
<st>
Technology</st>
<p>

Performance testing technology employs one or more PCs or Unix servers to act as injectors – each emulating the presence of numbers of users and each running an automated sequence of interactions (recorded as a script, or as a series of scripts to emulate different types of user interaction) with the host whose performance is being tested. Usually, a separate PC acts as a test conductor, coordinating and gathering metrics from each of the injectors and collating performance data for reporting purposes. The usual sequence is to ramp up the load – starting with a small number of virtual users and increasing the number over a period to some maximum. The test result shows how the performance varies with the load, given as number of users vs response time. Various tools, are available to perform such tests. Tools in this category usually execute a suite of tests which will emulate real users against the system. Sometimes the results can reveal oddities, e.g., that while the average response time might be acceptable, there are outliers of a few key transactions that take considerably longer to complete – something that might be caused by inefficient database queries,pictures etc.</p>
<p>

Performance testing can be combined with <link xlink:type="simple" xlink:href="../754/8445754.xml">
stress testing</link>, in order to see what happens when an acceptable load is exceeded –does the system crash? How long does it take to recover if a large load is reduced? Does it fail in a way that causes collateral damage?</p>
<p>

Analytical Performance Modeling is a method to model the behaviour of an application in a spreadsheet. The model is fed with measurements of transaction resource demands (CPU, DIO, LAN, WAN), weighted by the transaction-mix (business transactions per hour). The weighted transaction resource demands are added-up to obtain the hourly resource demands and divided by the hourly resource capacity to obtain the resource loads. Using the responsetime formula (R=S/(1-U), R=responsetime, S=servicetime, U=load), responsetimes can be calculated and calibrated with the results of the performance tests. Analytical performance modelling allows evaluation of design options and system sizing based on actual or anticipated business usage. It is therefore much faster and cheaper than performance testing, though it requires thorough understanding of the hardware platforms.</p>

</sec>
<sec>
<st>
Performance specifications</st>
<p>

It is critical to detail performance specifications (requirements) and document them in any performance test plan. Ideally, this is done during the requirements development phase of any system development project, prior to any design effort. See <link xlink:type="simple" xlink:href="../610/6615610.xml">
Performance Engineering</link> for more details. </p>
<p>

However, performance testing is frequently not performed against a specification i.e. no one will have expressed what the maximum acceptable response time for a given population of users should be. Performance testing is frequently used as part of the process of performance profile tuning. The idea is to identify the “weakest link” – there is inevitably a part of the system which, if it is made to respond faster, will result in the overall system running faster. It is sometimes a difficult task to identify which part of the system represents this critical path, and some test tools include (or can have add-ons that provide) instrumentation that runs on the server (agents) and report transaction times, database access times, network overhead, and other server monitors, which can be analyzed together with the raw performance statistics. Without such instrumentation one might have to have someone crouched over Windows Task Manager at the server to see how much CPU load the performance tests are generating (assuming a Windows system under test).</p>
<p>

There is an apocryphal story of a company that spent a large amount optimizing their software without having performed a proper analysis of the problem. They ended up rewriting the system’s ‘idle loop’, where they had found the system spent most of its time, but even having the most efficient idle loop in the world obviously didn’t improve overall performance one iota!</p>
<p>

<b>Performance testing</b> can be performed across the web, and even done in different parts of the country, since it is known that the response times of the internet itself vary regionally. It can also be done in-house, although routers would then need to be configured to introduce the lag what would typically occur on public networks. Loads should be introduced to the system from realistic points. For example, if 50% of a system's user base will be accessing the system via a 56K modem connection and the other half over a T1, then the load injectors (computers that simulate real users) should either inject load over the same connections (ideal) or simulate the network latency of such connections, following the same user profile.</p>
<p>

It is always helpful to have a statement of the likely peak numbers of users that might be expected to use the system at peak times. If there can also be a statement of what constitutes the maximum allowable 95 percentile response time, then an injector configuration could be used to test whether the proposed system met that specification.</p>
<p>

Performance specifications should ask the following questions, at a minimum:
<list>
<entry level="1" type="bullet">

 In detail, what is the performance test scope? What subsystems, interfaces, components, etc are in and out of scope for this test?</entry>
<entry level="1" type="bullet">

 For the user interfaces (UI's) involved, how many concurrent users are expected for each (specify peak vs. nominal)?</entry>
<entry level="1" type="bullet">

 What does the target system (hardware) look like (specify all server and network appliance configurations)?</entry>
<entry level="1" type="bullet">

 What is the Application Workload Mix of each application component? (for example: 20% login, 40% search, 30% item select, 10% checkout). </entry>
<entry level="1" type="bullet">

 What is the System Workload Mix? [Multiple workloads may be simulated in a single performance test] (for example: 30% Workload A, 20% Workload B, 50% Workload C)</entry>
<entry level="1" type="bullet">

 What are the time requirements for any/all backend batch processes (specify peak vs. nominal)?</entry>
</list>
</p>

</sec>
<sec>
<st>
Tasks to undertake</st>
<p>

Tasks to perform such a test would include:
<list>
<entry level="1" type="bullet">

 Decide whether to use internal or external resources to perform the tests, depending on inhouse expertise (or lack thereof)</entry>
<entry level="1" type="bullet">

 Gather or elicit performance requirements (specifications) from users and/or business analysts</entry>
<entry level="1" type="bullet">

 Develop a high-level plan (or project charter), including requirements, resources, timelines and milestones</entry>
<entry level="1" type="bullet">

 Develop a detailed performance test plan (including detailed scenarios and test cases, workloads, environment info, etc)</entry>
<entry level="1" type="bullet">

 Choose test tool(s)</entry>
<entry level="1" type="bullet">

 Specify test data needed and charter effort (often overlooked, but often the death of a valid performance test)</entry>
<entry level="1" type="bullet">

 Develop proof-of-concept scripts for each application/component under test, using chosen test tools and strategies</entry>
<entry level="1" type="bullet">

 Develop detailed performance test project plan, including all dependencies and associated timelines</entry>
<entry level="1" type="bullet">

 Install and configure injectors/controller</entry>
<entry level="1" type="bullet">

 Configure the test environment (ideally identical hardware to the production platform), router configuration, quiet network (we don’t want results upset by other users), deployment of server instrumentation, database test sets developed, etc.</entry>
<entry level="1" type="bullet">

 Execute tests – probably repeatedly (iteratively) in order to see whether any unaccounted for factor might affect the results</entry>
<entry level="1" type="bullet">

 Analyze the results - either pass/fail, or investigation of critical path and recommendation of corrective action</entry>
</list>
</p>

</sec>
<sec>
<st>
 Methodology </st>


<ss1>
<st>
 Performance Testing Web Applications Methodology </st>
<p>

According to the Microsoft Developer Network the <weblink xlink:type="simple" xlink:href="http://msdn2.microsoft.com/en-us/library/bb924376.aspx">
Performance Testing Methodology</weblink> consists of the following activities:
<list>
<entry level="1" type="bullet">

 <b>Activity 1. Identify the Test Environment.</b>  Identify the physical test environment and the production environment as well as the tools and resources available to the test team. The physical environment includes hardware, software, and network configurations. Having a thorough understanding of the entire test environment at the outset enables more efficient test design and planning and helps you identify testing challenges early in the project. In some situations, this process must be revisited periodically throughout the project’s life cycle. </entry>
<entry level="1" type="bullet">

 <b>Activity 2. Identify Performance Acceptance Criteria.</b>  Identify the response time, throughput, and resource utilization goals and constraints. In general, response time is a user concern, throughput is a business concern, and resource utilization is a system concern. Additionally, identify project success criteria that may not be captured by those goals and constraints; for example, using performance tests to evaluate what combination of configuration settings will result in the most desirable performance characteristics. </entry>
<entry level="1" type="bullet">

 <b>Activity 3. Plan and Design Tests.</b>  Identify key scenarios, determine variability among representative users and how to simulate that variability, define test data, and establish metrics to be collected. Consolidate this information into one or more models of system usage to be implemented, executed, and analyzed.    </entry>
<entry level="1" type="bullet">

 <b>Activity 4. Configure the Test Environment.</b>  Prepare the test environment, tools, and resources necessary to execute each strategy as features and components become available for test. Ensure that the test environment is instrumented for resource monitoring as necessary. </entry>
<entry level="1" type="bullet">

 <b>Activity 5. Implement the Test Design.</b>  Develop the performance tests in accordance with the test design. </entry>
<entry level="1" type="bullet">

 <b>Activity 6. Execute the Test.</b>  Run and monitor your tests. Validate the tests, test data, and results collection. Execute validated tests for analysis while monitoring the test and the test environment. </entry>
<entry level="1" type="bullet">

 <b>Activity 7. Analyze Results, Report, and Retest.</b>  Consolidate and share results data. Analyze the data both individually and as a cross-functional team. Reprioritize the remaining tests and re-execute them as needed. When all of the metric values are within accepted limits, none of the set thresholds have been violated, and all of the desired information has been collected, you have finished testing that particular scenario on that particular configuration.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../554/214554.xml">
Stress testing</link></entry>
<entry level="1" type="bullet">

 <standard wordnetid="107260623" confidence="0.8">
<benchmark wordnetid="107261143" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../870/1980870.xml">
Benchmark (computing)</link></system_of_measurement>
</benchmark>
</standard>
</entry>
<entry level="1" type="bullet">

 <standard wordnetid="107260623" confidence="0.8">
<benchmark wordnetid="107261143" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../331/14244331.xml">
Web server benchmarking</link></system_of_measurement>
</benchmark>
</standard>
</entry>
</list>
</p>

<ss1>
<st>
 Articles and White Papers </st>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.articlesbase.com/programming-articles/performance-testing-for-ajaxbased-applications-414007.html">
Performance Testing for AJAX Application</weblink></entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Newsgroups </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://groups.google.com/groups?group=comp.software.measurement">
comp.software.measurement</weblink></entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 Resources/References </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://msdn2.microsoft.com/en-us/library/bb924375.aspx">
Performance Testing Guidance for Web Applications</weblink> (MSDN)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.amazon.com/dp/0735625700">
Performance Testing Guidance for Web Applications</weblink> (Book)</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.codeplex.com/PerfTestingGuide/Release/ProjectReleases.aspx?ReleaseId=6690">
Performance Testing Guidance for Web Applications</weblink> (PDF)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.codeplex.com/PerfTesting">
Performance Testing Guidance</weblink> (Online KB)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://msdn2.microsoft.com/en-us/library/bb671346.aspx">
Performance Testing Videos</weblink> (MSDN)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.opensourcetesting.org/performance.php">
Open Source Performance Testing tools</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.perftestplus.com/pubs.htm">
"User Experience, not Metrics" and "Beyond Performance Testing"</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
