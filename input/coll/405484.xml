<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:01:12[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<scientist  confidence="0.9511911446218017" wordnetid="110560637">
<research_worker  confidence="0.9511911446218017" wordnetid="110523076">
<header>
<title>Jürgen Schmidhuber</title>
<id>405484</id>
<revision>
<id>231883642</id>
<timestamp>2008-08-14T11:15:48Z</timestamp>
<contributor>
<username>Tbsdy lives</username>
<id>6290372</id>
</contributor>
</revision>
<categories>
<category>Artificial intelligence researchers</category>
<category>1963 births</category>
<category>Living people</category>
<category>Swiss computer scientists</category>
<category>Machine learning researchers</category>
</categories>
</header>
<bdy>

<b>Jürgen Schmidhuber</b> (born <link xlink:type="simple" xlink:href="../648/34648.xml">
1963</link> in <location wordnetid="100027167" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../058/19058.xml">
Munich</link></location>
) is a <link xlink:type="simple" xlink:href="../784/328784.xml">
computer scientist</link> and <link xlink:type="simple" xlink:href="../212/1212.xml">
artist</link> known for his work on <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, universal <link>
Artificial Intelligence</link> (AI), artificial <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link>s, <link xlink:type="simple" xlink:href="../493/405493.xml">
digital physics</link>, and <link xlink:type="simple" xlink:href="../489/405489.xml">
low-complexity art</link>. His contributions also include generalizations of <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov complexity</link> and the <link xlink:type="simple" xlink:href="../703/402703.xml">
Speed Prior</link>. Since <link xlink:type="simple" xlink:href="../658/34658.xml">
1995</link> he has been co-director of the Swiss AI lab <point wordnetid="108620061" confidence="0.8">
<institute wordnetid="108407330" confidence="0.8">
<geographic_point wordnetid="108578706" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<association wordnetid="108049401" confidence="0.8">
<workplace wordnetid="104602044" confidence="0.8">
<lab wordnetid="103629986" confidence="0.8">
<link xlink:type="simple" xlink:href="../607/4103607.xml">
IDSIA</link></lab>
</workplace>
</association>
</location>
</geographic_point>
</institute>
</point>
 in <town wordnetid="108665504" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../861/295861.xml">
Lugano</link></town>
, since <link xlink:type="simple" xlink:href="../524/35524.xml">
2004</link> also professor of Cognitive <link xlink:type="simple" xlink:href="../673/46673.xml">
Robotics</link> at the Tech. University <location wordnetid="100027167" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../058/19058.xml">
Munich</link></location>
, since <link xlink:type="simple" xlink:href="../164/36164.xml">
2006</link> also in the faculty of the University of <town wordnetid="108665504" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../861/295861.xml">
Lugano</link></town>
.
<sec>
<st>
Contributions</st>

<ss1>
<st>
Recurrent Neural Networks</st>
<p>

The dynamic <link xlink:type="simple" xlink:href="../303/1706303.xml">
recurrent neural networks</link> developed in his lab are simplified mathematical models of the <link xlink:type="simple" xlink:href="../672/1726672.xml">
biological neural network</link>s found in <link xlink:type="simple" xlink:href="../620/490620.xml">
human brain</link>s. A particularly successful model of this type is called "Long Short-Term Memory" (Hochreiter &amp; Schmidhuber, 1997). From training sequences it <it><link xlink:type="simple" xlink:href="../403/183403.xml">
learns</link></it> to solve numerous tasks unsolvable by previous such models. Applications range from automatic <link xlink:type="simple" xlink:href="../962/47962.xml">
music composition</link> to <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link> and <link xlink:type="simple" xlink:href="../673/46673.xml">
robotics</link> in partially observable environments.</p>

</ss1>
<ss1>
<st>
Artificial Evolution / Genetic Programming</st>
<p>

As an undergrad at TUM Schmidhuber evolved <link xlink:type="simple" xlink:href="../783/5783.xml">
computer programs</link> through <link xlink:type="simple" xlink:href="../254/40254.xml">
genetic algorithms</link>. The method was published in <link xlink:type="simple" xlink:href="../760/34760.xml">
1987</link> as one of the first papers in the emerging field that later became known as <link xlink:type="simple" xlink:href="../424/12424.xml">
genetic programming</link>. Since then he has co-authored numerous additional papers on artificial <link xlink:type="simple" xlink:href="../236/9236.xml">
evolution</link>. Applications include <link xlink:type="simple" xlink:href="../781/25781.xml">
robot</link> control, soccer learning, <link xlink:type="simple" xlink:href="../292/2137292.xml">
drag</link> minimization, and <link xlink:type="simple" xlink:href="../624/406624.xml">
time series</link> prediction.</p>

</ss1>
<ss1>
<st>
Neural Economy</st>
<p>

In <link xlink:type="simple" xlink:href="../847/34847.xml">
1989</link> he created the first learning <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> for <link>
neural networks</link> based on principles of the <link xlink:type="simple" xlink:href="../852/48852.xml">
market economy</link> (inspired by <link xlink:type="simple" xlink:href="../868/147868.xml">
John Holland</link>'s <link xlink:type="simple" xlink:href="../708/1738708.xml">
bucket brigade</link> algorithm for <link xlink:type="simple" xlink:href="../543/1508543.xml">
classifier</link> systems): adaptive <link xlink:type="simple" xlink:href="../120/21120.xml">
neurons</link> compete for being active in response to certain input patterns; those that are active when there is external <link xlink:type="simple" xlink:href="../573/2352573.xml">
reward</link> get stronger <link xlink:type="simple" xlink:href="../809/27809.xml">
synapses</link>, but active neurons have to pay those that activated them, by transferring parts of their <link xlink:type="simple" xlink:href="../809/27809.xml">
synapse</link> strengths, thus rewarding "hidden" neurons setting the stage for later success.</p>

</ss1>
<ss1>
<st>
Artificial Curiosity</st>
<p>

In <link xlink:type="simple" xlink:href="../635/34635.xml">
1990</link> he published the first in a long series of papers on <link xlink:type="simple" xlink:href="../842/2839842.xml">
artificial</link> <link xlink:type="simple" xlink:href="../353/1077353.xml">
curiosity</link> for an <link xlink:type="simple" xlink:href="../145/191145.xml">
autonomous</link> agent. The agent is equipped with an adaptive <link xlink:type="simple" xlink:href="../069/246069.xml">
predictor</link> trying to predict future events from the history of previous events and actions. A reward-maximizing, <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link>, adaptive <link xlink:type="simple" xlink:href="../161/1714161.xml">
controller</link> is steering the agent and gets <it>curiosity reward</it> for executing action sequences that improve the predictor. This discourages it from executing actions leading to boring outcomes that are either predictable or totally unpredictable. Instead the controller is motivated to learn actions that help the predictor to learn new, previously unknown regularities in its environment, thus improving its model of the world, which in turn can greatly help to solve externally given tasks. This has become an important concept of <link xlink:type="simple" xlink:href="../176/1422176.xml">
developmental robotics</link>.</p>

</ss1>
<ss1>
<st>
Unsupervised Learning / Factorial Codes</st>
<p>

During the early <link xlink:type="simple" xlink:href="../635/34635.xml">
1990</link>s Schmidhuber also invented a <link xlink:type="simple" xlink:href="../944/21944.xml">
neural</link> method for <link xlink:type="simple" xlink:href="../103/146103.xml">
nonlinear</link> <link xlink:type="simple" xlink:href="../031/598031.xml">
independent component analysis</link> (ICA) called <link xlink:type="simple" xlink:href="../070/246070.xml">
predictability</link> <link xlink:type="simple" xlink:href="../929/11244929.xml">
minimization</link>. It is based on <link xlink:type="simple" xlink:href="../835/190835.xml">
co-evolution</link> of adaptive predictors and initially random, adaptive <link xlink:type="simple" xlink:href="../752/187752.xml">
feature</link> <link xlink:type="simple" xlink:href="../757/235757.xml">
detectors</link> processing input patterns from the environment. For each detector there is a predictor trying to predict its current value from the values of neighboring detectors, while each detector is simultaneously trying to become as unpredictable as possible. It can be shown that the best the detectors can do is to create a <link xlink:type="simple" xlink:href="../606/10606.xml">
factorial</link> <link xlink:type="simple" xlink:href="../225/5225.xml">
code</link> of the environment, that is, a code that conveys all the information about the inputs such that the code components are <link xlink:type="simple" xlink:href="../593/27593.xml">
statistically independent</link>, which is desirable for many <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link> applications.</p>

</ss1>
<ss1>
<st>
Kolmogorov Complexity / Computer-Generated Universe</st>
<p>

In <link xlink:type="simple" xlink:href="../601/34601.xml">
1997</link> Schmidhuber published a paper based on <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../810/16810.xml">
Konrad Zuse</link></scientist>
</person>
´s assumption (<link xlink:type="simple" xlink:href="../749/34749.xml">
1967</link>) that the history of the universe is computable. He pointed out that the simplest explanation of the universe would be a very simple <invention wordnetid="105633385" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../403/30403.xml">
Turing machine</link></method>
</know-how>
</invention>
 programmed to systematically execute all possible programs computing all possible histories for all types of computable physical laws. He also pointed out that there is an optimally efficient way of computing all computable universes based on <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../708/402708.xml">
Leonid Levin</link></person>
´s universal search algorithm (<link xlink:type="simple" xlink:href="../751/34751.xml">
1973</link>). In <link xlink:type="simple" xlink:href="../548/34548.xml">
2000</link> he expanded this work by combining <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<theorist wordnetid="110706812" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../673/402673.xml">
Ray Solomonoff</link></causal_agent>
</intellectual>
</theorist>
</person>
</physical_entity>
´s theory of inductive inference with the assumption that quickly computable universes are more likely than others. This work on <link xlink:type="simple" xlink:href="../493/405493.xml">
digital physics</link> also led to limit-computable generalizations of algorithmic <link xlink:type="simple" xlink:href="../062/18985062.xml">
information</link> or <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov Complexity</link> and the concept of <it>Super Omegas</it>, which are limit-computable numbers that are even more <link xlink:type="simple" xlink:href="../523/19196523.xml">
random</link> (in a certain sense) than <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../308/12308.xml">
Gregory Chaitin</link></person>
´s <it>number of wisdom</it> <link xlink:type="simple" xlink:href="../205/6205.xml">
Omega</link>.</p>

</ss1>
<ss1>
<st>
Universal AI</st>
<p>

Important recent research topics of his group include <link xlink:type="simple" xlink:href="../245/32245.xml">
universal</link> learning algorithms and universal <link xlink:type="simple" xlink:href="../268/1268.xml">
AI</link>. Contributions include the first theoretically <link xlink:type="simple" xlink:href="../033/52033.xml">
optimal</link> decision makers living in environments obeying arbitrary unknown but <link xlink:type="simple" xlink:href="../084/3244084.xml">
computable</link> <link xlink:type="simple" xlink:href="../934/22934.xml">
probabilistic</link> laws, and <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematically</link> sound general problem solvers such as the remarkable <link xlink:type="simple" xlink:href="../503/3469503.xml">
asymptotically</link> fastest algorithm for all well-defined problems, by his former postdoc <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../730/4420730.xml">
Marcus Hutter</link></person>
. Based on the theoretical results obtained in the early <link xlink:type="simple" xlink:href="../548/34548.xml">
2000</link>s, Schmidhuber is actively promoting the view that in the new <link xlink:type="simple" xlink:href="../504/204504.xml">
millennium</link> the field of general <link xlink:type="simple" xlink:href="../268/1268.xml">
AI</link> has matured and become a real <link xlink:type="simple" xlink:href="../774/3694774.xml">
formal science</link>.</p>

</ss1>
<ss1>
<st>
Low-Complexity Art / Theory of Beauty</st>
<p>

Schmidhuber's <link xlink:type="simple" xlink:href="../489/405489.xml">
low-complexity art</link>works (since 1997) can be described by very short computer programs containing very few <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link>s of information, and reflect his formal theory of <link xlink:type="simple" xlink:href="../431/4431.xml">
beauty</link> based on the concepts of <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov complexity</link> and <link xlink:type="simple" xlink:href="../325/331325.xml">
minimum description length</link>.</p>
<p>

Schmidhuber writes that since age 15 or so his main scientific ambition has been to build an optimal scientist, then retire. First he wants to build a scientist better than himself (humorously, he quips that his colleagues claim that should be easy) who will then do the remaining work. He claims he "cannot see any more efficient way of using and multiplying the little creativity he's got".</p>

</ss1>
</sec>
<sec>
<st>
Partial bibliography</st>
<p>

His academical production includes:
<list>
<entry level="1" type="bullet">

 J. Schmidhuber. Optimal Ordered Problem Solver. Machine Learning, 54, 211-254, 2004</entry>
<entry level="1" type="bullet">

 J. Schmidhuber. Hierarchies of generalized Kolmogorov complexities and nonenumerable universal measures computable in the limit. International Journal of Foundations of Computer Science 13(4):587-612, 2002</entry>
<entry level="1" type="bullet">

 J. Schmidhuber. The Speed Prior: A New Simplicity Measure Yielding Near-Optimal Computable Predictions. Proceedings of the 15th Annual Conference on Computational Learning Theory (COLT 2002), Sydney, Australia, LNAI, 216-228, Springer, 2002</entry>
<entry level="1" type="bullet">

 J. Schmidhuber. Low-Complexity Art. Leonardo, Journal of the International Society for the Arts, Sciences, and Technology, 30(2):97-103, MIT Press, 1997</entry>
<entry level="1" type="bullet">

 J. Schmidhuber. A computer scientist's view of life, the universe, and everything. Foundations of Computer Science: Potential - Theory - Cognition, Lecture Notes in Computer Science, pages 201-208, Springer, 1997</entry>
<entry level="1" type="bullet">

 S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735-1780, 1997</entry>
<entry level="1" type="bullet">

 J. Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863-879, 1992</entry>
<entry level="1" type="bullet">

 J. Schmidhuber. Curious model-building control systems. In Proc. International Joint Conference on Neural Networks, Singapore, volume 2, pages 1458-1463. IEEE, 1991</entry>
<entry level="1" type="bullet">

 J. Schmidhuber. A local learning algorithm for dynamic feedforward and recurrent networks. Connection Science, 1(4):403-412, 1989</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.idsia.ch/~juergen/">
Home page</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.idsia.ch/~juergen/onlinepub.html">
Publications</weblink></entry>
</list>
</p>



</sec>
</bdy>
</research_worker>
</scientist>
</article>
