<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 16:58:28[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Inductive inference</title>
<id>405562</id>
<revision>
<id>235679524</id>
<timestamp>2008-09-01T21:52:53Z</timestamp>
<contributor>
<username>RussBot</username>
<id>279219</id>
</contributor>
</revision>
<categories>
<category>Logic and statistics</category>
</categories>
</header>
<bdy>

<indent level="1">

<it>This article is about the mathematical concept, for inductive inference in logic, see <link xlink:type="simple" xlink:href="../736/393736.xml">
Inductive reasoning</link>.</it>
</indent>

Around 1960, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<theorist wordnetid="110706812" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../673/402673.xml">
Ray Solomonoff</link></causal_agent>
</intellectual>
</theorist>
</person>
</physical_entity>
 founded the theory of universal <b>inductive inference</b>, the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. Solomonoff's theory attempts to be mathematically rigorous. <p>

Fundamental ingredients of the theory are the concepts of <link xlink:type="simple" xlink:href="../688/402688.xml">
algorithmic probability</link>  and <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov complexity</link>. The universal <link xlink:type="simple" xlink:href="../877/472877.xml">
prior probability</link> of any prefix p of a computable sequence x is the sum of the probabilities of all programs (for a <link xlink:type="simple" xlink:href="../403/30403.xml">
universal computer</link>) that compute something starting with p. Given some p and any computable but unknown probability distribution from which x is sampled, the universal prior and <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
 can be used to predict the yet unseen parts of x in optimal fashion.</p>
<p>

Another direction of inductive inference is based on <link>
E. Mark Gold</link>'s model of <language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../933/1299933.xml">
learning in the limit</link></language>
 from 1967 and has developed since then more and more models of learning. The general scenario is the following: Given a class S of computable functions, is there a learner (that is, recursive functional) which outputs for any input of the form (f(0),f(1),...,f(n)) a hypothesis. A learner M learns a function f if almost all hypotheses are the same index e of f with respect to a previously agreed on acceptable numbering of all computable functions; M learns S if M learns every f in S. Basic results are that all recursively enumerable classes of functions are learnable while the class REC of all computable functions is not learnable. Many related models have been considered and also the learning of classes of recursively enumerable sets from positive data is a topic studied from Gold's pioneering paper in 1967 onwards.
A far reaching extension of the Gold’s approach is developed by Burgin theory of inductive Turing machines, which are kinds of <link xlink:type="simple" xlink:href="../067/15641067.xml">
super-recursive algorithm</link>s. </p>


<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 Angluin, D., and Smith, C. H. (1983) Inductive Inference: Theory and Methods, Comput. Surveys, v. 15, no. 3, pp. 237—269</entry>
<entry level="1" type="bullet">

 Mark Burgin (2005), Super-recursive algorithms, Monographs in computer science, Springer. ISBN 0387955690</entry>
<entry level="1" type="bullet">

 Burgin, M. and Klinger, A. Experience, Generations, and Limits in Machine Learning, Theoretical Computer Science, v. 317, No. 1/3, 2004, pp. 71-91</entry>
<entry level="1" type="bullet">

 Gasarch, W. and Smith, C. H. (1997) A survey of inductive inference with an emphasis on queries. Complexity, logic, and recursion theory, Lecture Notes in Pure and Appl. Math., 187, Dekker, New York, pp. 225-260</entry>
<entry level="1" type="bullet">

 Ming Li and Paul Vitanyi, An Introduction to Kolmogorov Complexity and Its Applications, 2nd Edition, Springer Verlag, 1997.</entry>
<entry level="1" type="bullet">

 Ray Solomonoff "Two Kinds of Probabilistic Induction," The Computer Journal, Vol. 42, No. 4, 1999</entry>
<entry level="1" type="bullet">

 Ray Solomonoff "A Formal Theory of Inductive Inference, Part I" Information and Control, Part I: Vol 7, No. 1, pp. 1-22, March 1964</entry>
<entry level="1" type="bullet">

 Ray Solomonoff "A Formal Theory of Inductive Inference, Part II" Information and Control, Part II: Vol. 7, No. 2, pp. 224-254, June 1964</entry>
</list>
</p>


</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../688/402688.xml">
Algorithmic probability</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../635/1635.xml">
Kolmogorov complexity</link></entry>
<entry level="1" type="bullet">

 <inclination wordnetid="106196584" confidence="0.8">
<bias wordnetid="106201908" confidence="0.8">
<partiality wordnetid="106201136" confidence="0.8">
<link xlink:type="simple" xlink:href="../160/59160.xml">
Confirmation bias</link></partiality>
</bias>
</inclination>
</entry>
<entry level="1" type="bullet">

 <language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../933/1299933.xml">
Language identification in the limit</link></language>
</entry>
</list>
</p>



</sec>
</bdy>
</article>
