<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:40:55[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Convolutional code</title>
<id>40962</id>
<revision>
<id>217265740</id>
<timestamp>2008-06-05T08:37:29Z</timestamp>
<contributor>
<username>Denelson83</username>
<id>23958</id>
</contributor>
</revision>
<categories>
<category>Error detection and correction</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../322/30322.xml">
telecommunication</link>, a <b>convolutional code</b> is a type of <link>
error-correcting code</link> in which (a) each <it>m</it>-<link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link> <link xlink:type="simple" xlink:href="../062/18985062.xml">
information</link> symbol (each <it>m</it>-<link xlink:type="simple" xlink:href="../701/27701.xml">
bit string</link>) to be encoded is transformed into an <it>n</it>-bit symbol, where <it>m</it>/<it>n</it> is the code <it>rate</it> (<it>n</it> &amp;ge; <it>m</it>) and (b) the transformation is a function of the last <it>k</it> information symbols, where <it>k</it> is the constraint length of the code. 
<sec>
<st>
Where convolutional codes are used</st>
<p>

Convolutional codes are often used to improve the performance of digital <link xlink:type="simple" xlink:href="../428/15368428.xml">
radio</link>, mobile phones, <link xlink:type="simple" xlink:href="../683/27683.xml">
satellite</link> links, and <computer wordnetid="103082979" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<standard wordnetid="107260623" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../742/3742.xml">
Bluetooth</link></system_of_measurement>
</machine>
</device>
</standard>
</instrumentality>
</artifact>
</computer>
 implementations.</p>

</sec>
<sec>
<st>
Convolutional encoding</st>
<p>

To convolutionally encode data, start with <it>k</it> <link xlink:type="simple" xlink:href="../432/486432.xml">
memory register</link>s, each holding 1 input bit.  Unless otherwise specified, all memory registers start with a value of 0.  The encoder has <it>n</it> modulo-2 <link xlink:type="simple" xlink:href="../856/426856.xml">
adder</link>s, and <it>n</it>  <link>
generator polynomial</link>s &mdash; one for each adder (see figure below).  An input bit <it>m</it>1 is fed into the leftmost register.  Using the generator polynomials and the existing values in the remaining registers, the encoder outputs <it>n</it> bits.  Now <link>
bit shift</link> all register values to the right (<it>m</it>1 moves to <it>m</it>0, <it>m</it>0 moves to <it>m</it>-1) and wait for the next input bit.  If there are no remaining input bits, the encoder continues output until all registers have returned to the zero state.</p>
<p>

The figure below is a rate 1/3 (<it>m</it>/<it>n</it>) encoder with constraint length (<it>k</it>) of 3.  Generator polynomials are <it>G</it>1 = (1,1,1), <it>G</it>2 = (0,1,1), and <it>G</it>3 = (1,0,1).  Therefore, output bits are calculated (modulo 2) as follows:</p>
<p>

<indent level="1">

<it>n</it>1 = <it>m</it>1 + <it>m</it>0 + <it>m-1</it>
</indent>
:<it>n</it>2 = <it>m</it>0 + <it>m</it>-1
<indent level="1">

<it>n</it>3 = <it>m</it>1 + <it>m</it>-1.
</indent>

<image location="none" width="150px" src="Convolutional_encoder.png" type="frame">
<caption>

Img.1. Rate 1/3 non-recursive, non-systematic convolutional encoder with constraint length 3
</caption>
</image>
</p>

</sec>
<sec>
<st>
Recursive and non-recursive codes</st>

<p>

The encoder on the picture above is a <it>non-recursive</it> encoder. Here's an example of a recursive one:</p>
<p>

<image location="none" width="340px" src="Convolutional_encoder_recursive.svg" type="thumb">
<caption>

Img.2. Rate 1/2 recursive, systematic convolutional encoder with constraint length 4
</caption>
</image>
</p>
<p>

One can see that the input being encoded is included in the output sequence too (look at the output 2). Such codes are referred to as <b>systematic</b>; otherwise the code is called  <b>non-systematic</b>.</p>
<p>

Recursive codes are almost always systematic and, conversely, non-recursive codes are non-systematic. It isn't a strict requirement, but a common practice.</p>

</sec>
<sec>
<st>
Impulse response, transfer function, and constraint length</st>

<p>

A convolutional encoder is called so because it performs a <it>convolution</it> of the input stream with encoder's <it>impulse responses</it>:</p>
<p>

<indent level="1">

<math>y_i^j=\sum_{k=0}^{\infty} h^j_k x_{i-k},</math>
</indent>

where <math>x\,</math> is an input sequence, <math>y^j\,</math> is a sequence from output <math>j\,</math> and <math>h^j\,</math> is an impulse response for output <math>j\,</math>.</p>
<p>

A convolutional encoder is a discrete <link xlink:type="simple" xlink:href="../899/1383899.xml">
linear time-invariant system</link>. Every output of an encoder can be described by its own <link xlink:type="simple" xlink:href="../146/31146.xml">
transfer function</link>, which is closely related to a generator polynomial. An impulse response is connected with a transfer function through <link xlink:type="simple" xlink:href="../589/171589.xml">
Z-transform</link>.</p>
<p>

Transfer functions for the first (non-recursive) encoder are:
<list>
<entry level="1" type="bullet">

 <math>H_1(z)=1+z^{-1}+z^{-2},\,</math></entry>
<entry level="1" type="bullet">

 <math>H_2(z)=z^{-1}+z^{-2},\,</math></entry>
<entry level="1" type="bullet">

 <math>H_3(z)=1+z^{-2}.\,</math></entry>
</list>
</p>
<p>

Transfer functions for the second (recursive) encoder are:
<list>
<entry level="1" type="bullet">

 <math>H_1(z)=\frac{1+z^{-1}+z^{-3}}{1-z^{-2}-z^{-3}},\,</math></entry>
<entry level="1" type="bullet">

 <math>H_2(z)=1.\,</math></entry>
</list>
</p>
<p>

Define <math> m  \,</math> by
<indent level="1">

 <math> m = max_i polydeg (H_i(1/z)) \,</math>
</indent>

where, for any <link xlink:type="simple" xlink:href="../210/361210.xml">
rational function</link> <math>f(z)  = P(z)/Q(z) \,</math>,
<indent level="1">

 <math> polydeg(f) = max (deg(P), deg(Q)) \,</math>.
</indent>

Then <math> m  \,</math> is the maximum of the <link xlink:type="simple" xlink:href="../652/5930652.xml">
polynomial degrees</link> of the 
<math> H_i(1/z) \,</math>, and the <it>constraint length</it> is defined as <math> K = m + 1  \,</math>. For instance, in the first example the constraint length is 3, and in the second the constraint length is 4.</p>

</sec>
<sec>
<st>
Trellis diagram</st>

<p>

A convolutional encoder is a <link xlink:type="simple" xlink:href="../931/10931.xml">
finite state machine</link>. An encoder with <it>n</it> binary cells will have 2<it>n</it> states.</p>
<p>

Imagine that the encoder (shown on Img.1, above) has '1' in the left memory cell (<it>m</it>0), and '0' in the right one (<it>m</it>-1). (<it>m</it>1 is not really a memory cell because it represents a current value). We will designate such a state as "10". According to an input bit the encoder at the next turn can convert either to the "01" state or the "11" state. One can see that not all transitions are possible (e.g., a decoder can't convert from "10" state to "00" or even stay in "10" state).</p>
<p>

All possible transitions can be shown as below:</p>
<p>

<image location="none" width="340px" src="Convolutional_code_trellis_diagram.png" type="thumb">
<caption>

Img.3. A trellis diagram for the encoder on Img.1
</caption>
</image>
</p>
<p>

An actual encoded sequence can be represented as a path on this graph. One valid path is shown in red as an example.</p>
<p>

This diagram gives us an idea about <it>decoding</it>: if a received sequence doesn't fit this graph, then it was received with errors, and we must choose the nearest <it>correct</it> (fitting the graph) sequence. The real decoding algorithms exploit this idea.</p>

</sec>
<sec>
<st>
Free distance and error distribution</st>

<p>

A <b>free distance</b> (<it>d</it>) is a minimal <link xlink:type="simple" xlink:href="../227/41227.xml">
Hamming distance</link> between different encoded sequences. A <it>correcting capability</it> (<it>t</it>) of a convolutional code is a number of errors that can be corrected by the code. It can be calculated as</p>
<p>

<indent level="1">

<math>t=\left \lfloor \frac{d-1}{2} \right \rfloor.</math>
</indent>

Since a convolutional code doesn't use blocks, processing instead a continuous bitstream, the value of <it>t</it> applies to a quantity of errors located relatively near to each other. That is, multiple groups of <it>t</it> errors can usually be fixed when they are relatively far.</p>
<p>

Free distance can be interpreted as a minimal length of an erroneous "burst" at the output of a convolutional decoder. The fact that errors appears as "bursts" should be accounted for when designing a <link xlink:type="simple" xlink:href="../375/11763375.xml">
concatenated code</link> with an inner convolutional code. The popular solution for this problem is to <link xlink:type="simple" xlink:href="../495/177495.xml">
interleave</link> data before convolutional encoding, so that the outer block (usually <link xlink:type="simple" xlink:href="../600/45600.xml">
Reed-Solomon</link>) code can correct most of the errors.</p>

</sec>
<sec>
<st>
Decoding convolutional codes</st>
<p>

Several <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>s exist for decoding convolutional codes. For relatively small values of <it>k</it>, the <link xlink:type="simple" xlink:href="../015/228015.xml">
Viterbi algorithm</link> is universally used as it provides <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood</link> performance and is highly parallelizable. Viterbi decoders are thus easy to implement in <link xlink:type="simple" xlink:href="../823/32823.xml">
VLSI</link> hardware and in software on CPUs with <link xlink:type="simple" xlink:href="../359/55359.xml">
SIMD</link> instruction sets.</p>
<p>

Longer constraint length codes are more practically decoded with any of several <b>sequential</b> decoding algorithms, of which the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<theorist wordnetid="110706812" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<traveler wordnetid="109629752" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<refugee wordnetid="110516016" confidence="0.8">
<exile wordnetid="110071332" confidence="0.8">
<absentee wordnetid="109757653" confidence="0.8">
<link xlink:type="simple" xlink:href="../212/434212.xml">
Fano</link></absentee>
</exile>
</refugee>
</scientist>
</causal_agent>
</traveler>
</intellectual>
</theorist>
</person>
</physical_entity>
 algorithm is the best known. Unlike Viterbi decoding, sequential decoding is not maximum likelihood but its complexity increases only slightly with constraint length, allowing the use of strong, long-constraint-length codes. Such codes were used in the <idea wordnetid="105833840" confidence="0.8">
<plan wordnetid="105898568" confidence="0.8">
<link xlink:type="simple" xlink:href="../040/25040.xml">
Pioneer program</link></plan>
</idea>
 of the early 1970s to Jupiter and Saturn, but gave way to shorter, Viterbi-decoded codes, usually concatenated with large <link xlink:type="simple" xlink:href="../600/45600.xml">
Reed-Solomon error correction</link> codes that steepen the overall bit-error-rate curve and produce extremely low residual undetected error rates.</p>
<p>

Both Viterbi and sequential decoding algorithms return hard-decisions: the bits that form the most likely codeword.  An approximate confidence measure can be added to each bit by use of the <link xlink:type="simple" xlink:href="../500/3772500.xml">
Soft output Viterbi algorithm</link>.  <link xlink:type="simple" xlink:href="../433/1792433.xml">
Maximum a posteriori</link> (MAP) soft-decisions for each bit can be obtained by use of the <link xlink:type="simple" xlink:href="../521/8846521.xml">
BCJR algorithm</link>.</p>

</sec>
<sec>
<st>
Popular convolutional codes</st>
<p>

An especially popular Viterbi-decoded convolutional code, used at least since the <idea wordnetid="105833840" confidence="0.8">
<plan wordnetid="105898568" confidence="0.8">
<link xlink:type="simple" xlink:href="../795/47795.xml">
Voyager program</link></plan>
</idea>
 has a constraint length <it>k</it> of 7 and a rate <it>r</it> of 1/2.</p>
<p>

<list>
<entry level="1" type="bullet">

 Longer constraint lengths produce more powerful codes, but the <link xlink:type="simple" xlink:href="../363/7363.xml">
complexity</link> of the Viterbi algorithm <link xlink:type="simple" xlink:href="../933/191933.xml">
increases exponentially</link> with constraint lengths, limiting these more powerful codes to deep space missions where the extra performance is easily worth the increased decoder complexity.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<probe wordnetid="105800611" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../323/177323.xml">
Mars Pathfinder</link></higher_cognitive_process>
</probe>
</problem_solving>
</thinking>
</inquiry>
</process>
, <mission wordnetid="108403225" confidence="0.8">
<nongovernmental_organization wordnetid="108009834" confidence="0.8">
<link xlink:type="simple" xlink:href="../908/252908.xml">
Mars Exploration Rover</link></nongovernmental_organization>
</mission>
 and the <link xlink:type="simple" xlink:href="../941/67941.xml">
Cassini probe</link> to Saturn use a <it>k</it> of 15 and a rate of 1/6; this code performs about 2 dB better than the simpler <it>k</it>=7 code at a cost of 256x in decoding complexity (compared to Voyager mission codes).</entry>
</list>
</p>

</sec>
<sec>
<st>
Punctured convolutional codes</st>

<p>

<link xlink:type="simple" xlink:href="../060/3547060.xml">
Puncturing</link> is a technique used to make a <it>m</it>/<it>n</it> rate code from a "basic" rate 1/2 code. It is reached by deletion of some bits in the encoder output. Bits are deleted according to <it>puncturing matrix</it>. The following puncturing matrices are the most frequently used:</p>
<p>

<table border="1">
<row>
<col>
code rate</col>
<col>
puncturing matrix</col>
<col>
free distance (for NASA standard K=7 convolutional code)</col>
</row>
<row>
<col>
1/2(No perf.)</col>
<col>
<table border="0">
<row>
<col>
1</col>
</row>
<row>
<col>
1</col>
</row>
</table>
</col>
<col>
10</col>
</row>
<row>
<col>
2/3</col>
<col>
<table border="0">
<row>
<col>
1</col>
<col>
0</col>
</row>
<row>
<col>
1</col>
<col>
1</col>
</row>
</table>
</col>
<col>
6</col>
</row>
<row>
<col>
3/4</col>
<col>
<table border="0">
<row>
<col>
1</col>
<col>
0</col>
<col>
1</col>
</row>
<row>
<col>
1</col>
<col>
1</col>
<col>
0</col>
</row>
</table>
</col>
<col>
5</col>
</row>
<row>
<col>
5/6</col>
<col>
<table border="0">
<row>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
</row>
<row>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0</col>
</row>
</table>
</col>
<col>
4</col>
</row>
<row>
<col>
7/8</col>
<col>
<table border="0">
<row>
<col>
1</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
</row>
<row>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
1</col>
<col>
0</col>
</row>
</table>
</col>
<col>
3</col>
</row>
</table>

|}</p>

<p>

For example, if we want to make a code with rate 2/3 using the appropriate matrix from the above table, we should take a basic encoder output and transmit every second bit from the first branch and every bit from the second one. The specific order of transmission is defined by the respective communication standard.</p>
<p>

Punctured convolutional codes are widely used in the <link xlink:type="simple" xlink:href="../655/47655.xml">
satellite communications</link>, for example, in <link xlink:type="simple" xlink:href="../516/15516.xml">
INTELSAT</link> systems and <standard wordnetid="107260623" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../977/146977.xml">
Digital Video Broadcasting</link></system_of_measurement>
</standard>
.</p>
<p>

Punctured convolutional codes are also called "perforated".</p>

</sec>
<sec>
<st>
Turbo codes: replacing convolutional codes</st>
<p>

Simple Viterbi-decoded convolutional codes are now giving way to <link xlink:type="simple" xlink:href="../535/497535.xml">
turbo code</link>s, a new class of iterated short convolutional codes that closely approach the theoretical limits imposed by <link xlink:type="simple" xlink:href="../289/3474289.xml">
Shannon's theorem</link> with much less decoding complexity than the Viterbi algorithm on the long convolutional codes that would be required for the same performance.
Turbo codes have not yet been concatenated with solid (low complexity) <link xlink:type="simple" xlink:href="../600/45600.xml">
Reed-Solomon error correction</link> codes. However, in the interest of planetary exploration this may someday be done.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../393/516393.xml">
Low-density parity-check code</link></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://complextoreal.com/chapters/convo.pdf">
Tutorial on Convolutional Coding and Decoding</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/">
The on-line textbook: Information Theory, Inference, and Learning Algorithms</weblink>, by <link xlink:type="simple" xlink:href="../315/2679315.xml">
David J.C. MacKay</link>, discusses LDPC codes in Chapter 47.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.eccpage.com/">
The Error Correcting Codes (ECC) Page</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.eembc.org/benchmark/telecom.asp?APPL=TLC">
EEMBC benchmark scores for microprocessors tested for convolutional encoding performance</weblink></entry>
</list>

</p>

</sec>
</bdy>
</article>
