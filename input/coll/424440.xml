<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:01:55[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<statement  confidence="0.8" wordnetid="106722453">
<message  confidence="0.8" wordnetid="106598915">
<idea  confidence="0.8" wordnetid="105833840">
<concept  confidence="0.8" wordnetid="105835747">
<theorem  confidence="0.8" wordnetid="106752293">
<proposition  confidence="0.8" wordnetid="106750804">
<header>
<title>H-theorem</title>
<id>424440</id>
<revision>
<id>241552168</id>
<timestamp>2008-09-28T15:59:06Z</timestamp>
<contributor>
<username>The Anomebot2</username>
<id>1979668</id>
</contributor>
</revision>
<categories>
<category>Statistical theorems</category>
<category>Articles lacking in-text citations</category>
<category>Physics theorems</category>
<category>Non-equilibrium thermodynamics</category>
<category>Philosophy of thermal and statistical physics</category>
<category>Thermodynamic entropy</category>
<category>Fundamental physics concepts</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../952/29952.xml">
thermodynamics</link>, the <b>H-theorem</b>, introduced by <link xlink:type="simple" xlink:href="../255/544255.xml">
Boltzmann</link> in <link xlink:type="simple" xlink:href="../768/34768.xml">
1872</link>, describes the increase in the <link xlink:type="simple" xlink:href="../891/9891.xml">
entropy</link> of an <link xlink:type="simple" xlink:href="../905/65905.xml">
ideal gas</link> in an irreversible process, by considering the <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../522/1026522.xml">
Boltzmann equation</link></concept>
</idea>
.<p>

It appears to predict an irreversible increase in entropy, despite microscopically reversible dynamics.  This has led to much discussion.</p>

<sec>
<st>
 Boltzmann's H-theorem </st>
<p>

The quantity <it>H</it> is defined as the integral over velocity space :
<indent level="1">

| style="width:100%" border="0"
</indent>
|-
| style="width:95%"  |
<math>
   \displaystyle 
   H 
   \ \stackrel{\mathrm{def}}{=}\  
   \int { P ({\ln P}) d^3 v} 
   = \left\langle { \ln P } \right\rangle
</math>
| style= | (1)
|}
where P(v) is the probability.  <it>H</it> is a forerunner of 
Shannon's <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link>.</p>
<p>

The article on 
Shannon's <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link>
contains a 
<link>
good explanation</link>
of the discrete counterpart of the quantity
<math>\displaystyle H</math>, known as the information entropy or 
information uncertainty (with a minus sign).
By 
, 
also called <link xlink:type="simple" xlink:href="../168/3504168.xml">
differential entropy</link>, one obtains
the expression in Eq.(1), and thus a better feel for the meaning of
<math>\displaystyle H</math>.</p>
<p>

Using the Boltzmann equation one can prove that <it>H</it> can only decrease.  </p>
<p>

For a system of <it>N</it> statistically independent particles, <it>H</it> is related to the thermodynamic entropy <it>S</it> through:
<indent level="1">

<math>S \ \stackrel{\mathrm{def}}{=}\  - N k H</math>
</indent>
so, according to the H-theorem, <it>S</it> can only increase.</p>
<p>

However, <link xlink:type="simple" xlink:href="../705/663705.xml">
Loschmidt</link> objected that it should not be possible to deduce an irreversible process from time-symmetric dynamics and a time-symmetric formalism: something must be wrong (<statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<contradiction wordnetid="107206887" confidence="0.8">
<paradox wordnetid="106724559" confidence="0.8">
<falsehood wordnetid="106756407" confidence="0.8">
<link xlink:type="simple" xlink:href="../839/708839.xml">
Loschmidt's paradox</link></falsehood>
</paradox>
</contradiction>
</message>
</statement>
).  The answer is that the theorem is based on Boltzmann's assumption of "<link xlink:type="simple" xlink:href="../796/2960796.xml">
molecular chaos</link>", i.e., that it is acceptable for all the particles to be considered independent and uncorrelated.  This in fact breaks time reversal symmetry and therefore <link xlink:type="simple" xlink:href="../582/43582.xml">
begs the question</link>.</p>

</sec>
<sec>
<st>
 Quantum mechanical H-theorem </st>

<p>

The following quantum-mechanical analogue of Boltzmann's H-theorem is sometimes given (e.g., Waldram (1985), p.39).   </p>
<p>

Starting from the Gibbs definition of thermodynamic entropy,</p>
<p>

<indent level="1">

<math>S = - k \sum_i p_i \ln p_i \,</math>
</indent>

differentiating gives</p>
<p>

<indent level="1">

<math>\frac{dS}{dt} =  - k \sum_i \ln p_i \frac{dp_i}{dt}</math>
</indent>

(using the fact that <it>∑ dpi/dt = 0</it>, since <it>∑ pi = 1</it>).</p>
<p>

Now <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../567/922567.xml">
Fermi's golden rule</link></concept>
</idea>
 gives a <link xlink:type="simple" xlink:href="../106/840106.xml">
master equation</link> for the probability of quantum jumps from state α to β; and from state β to α.  For an isolated system the jumps will make a contribution <it>ναβ(pβ-pα)</it> to <it>dpα/dt</it>, and a contribution <it>ναβ(pα-pβ)</it> to <it>dpβ/dt</it>; the micro-reversibility of the dynamics ensuring that the same transition constant ναβ appears in both expressions.</p>
<p>

Thus</p>
<p>

<indent level="1">

<math>\frac{dS}{dt} =  \frac{1}{2} k \sum_{\alpha\beta} \nu_{\alpha\beta}(\ln p_{\beta}-\ln p_{\alpha})(p_{\beta}- p_{\alpha}).</math>
</indent>

But the two brackets will have the same sign, so each contribution to <it>dS/dt</it> cannot be negative.</p>
<p>

Therefore
<indent level="1">

<math>\Delta S \geq 0</math>
</indent>
for an isolated system.</p>
<p>

The same mathematics is sometimes also presented for classical systems, considering probability flows between <link xlink:type="simple" xlink:href="../684/792684.xml">
coarse-grained</link> cells in the <link xlink:type="simple" xlink:href="../101/191101.xml">
phase space</link> (e.g., <link xlink:type="simple" xlink:href="../973/5236973.xml">
Tolman</link> (1938)).</p>

<ss1>
<st>
 Critique </st>

<p>

Several criticisms can be made of the above "proof", for example by Gull (1989):</p>
<p>

<list>
<entry level="1" type="number">

  It relies on the use of approximate quantum mechanics (Fermi's golden rule), not necessarily valid for large perturbations.</entry>
<entry level="1" type="number">

  Are the probabilities to be considered as representing <it>N</it> independent systems of 1 particle, or as applying to 1 system of <it>N</it> particles?  If it is the former, then it is ignoring the inter-particle correlations between the systems after collisions, explaining the information loss.  The 1-particle entropy also ignores many-body effects in the potential energy, so bears little relation to the entropy of any real gas.</entry>
<entry level="1" type="number">

  On the other hand, treated properly, an N-particle system has N-particle states. An isolated system will presumably sit in one of its N-particle microstates and make no transitions at all.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 Analysis </st>


<p>

At the heart of the H-theorem is the replacement of <it>1-state to 1-state</it> deterministic dynamics by <it>many-state to many-state</it> <link xlink:type="simple" xlink:href="../772/98772.xml">
Markovian</link> mixing, with information lost at each Markovian transition.</p>
<p>

Gull is correct that, with the powers of <work wordnetid="100575741" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../793/344793.xml">
Laplace's demon</link></activity>
</psychological_feature>
</research>
</act>
</investigation>
</experiment>
</event>
</scientific_research>
</work>
, one could in principle map forward exactly the ensemble of the original possible states of the N-particle system exactly, and lose no information.  But this would not be very interesting.  Part of the program of statistical mechanics,  not least the <link xlink:type="simple" xlink:href="../758/3015758.xml">
MaxEnt school</link> of which Gull is an enthusiastic proponent, is to see just how much of the detail information in the system one can ignore, and yet still correctly predict experimentally reproducible results.</p>
<p>

The H-theorem's program of regularly throwing information away, either by systematically ignoring detailed correlations between particles, or between particular sub-systems, or through systematic regular coarse-graining, leads to predictions such as those from the <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../522/1026522.xml">
Boltzmann equation</link></concept>
</idea>
 for dilute ideal gases or from the recent entropy-production <link xlink:type="simple" xlink:href="../170/333170.xml">
fluctuation theorem</link>, which are useful and reproducibly observable.  They also mean that we have <it>learnt</it> something qualitative about the system, and which parts of its information are useful for which purposes, which is additional beyond even the full specification of the microscopic dynamical particle trajectories.</p>
<p>

(It may be interesting that having rounded on the H-theorem for not considering the microscopic detail of the microscopic dynamics, Gull then chooses to demonstrate the power of the extended-time MaxEnt/Gibbsian method by applying it to a Brownian motion example - a not so dissimilar replacement of detailed deterministic dynamical information by a simplified stochastic/probabilistic summary!)</p>
<p>

However, it is an <it>assumption</it> that the H-theorem's coarse-graining is not getting rid of any 'interesting' information.  With such an assumption, one moves firmly into the domain of <it>predictive</it> physics: if the assumption goes wrong, it may produce predictions which are systematically and reproducibly wrong.</p>

</sec>
<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

 <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<contradiction wordnetid="107206887" confidence="0.8">
<paradox wordnetid="106724559" confidence="0.8">
<falsehood wordnetid="106756407" confidence="0.8">
<link xlink:type="simple" xlink:href="../839/708839.xml">
Loschmidt's paradox</link></falsehood>
</paradox>
</contradiction>
</message>
</statement>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../639/296639.xml">
Arrow of time</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../017/133017.xml">
Second Law of Thermodynamics</link></entry>
<entry level="1" type="bullet">

 <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<contradiction wordnetid="107206887" confidence="0.8">
<paradox wordnetid="106724559" confidence="0.8">
<falsehood wordnetid="106756407" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../170/333170.xml">
Fluctuation theorem</link></proposition>
</theorem>
</falsehood>
</paradox>
</contradiction>
</message>
</statement>
</entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Text_document_with_red_question_mark.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 This article or section includes a  or , but its sources remain unclear because it lacks <b>.</b>
You can  this article by introducing more precise citations . <it>(February 2008)''</it></col>
</row>
</table>

</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book">Lifshitz, E. M.; and Pitaevskii, L. P.&#32;(1981). Physical kinetics.&#32;London:&#32;Pergamon. ISBN 0-08-026480-8 ISBN 0-7506-2635-6.</cite>&nbsp; Vol. 10 of the Course of Theoretical Physics (3rd Ed).</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book">Waldram, J. R.&#32;(1985). The theory of thermodynamics.&#32;Cambridge:&#32;University Press. ISBN 0-521-28796-0.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book">Tolman, Richard C.&#32;(1938). The principles of statistical mechanics.&#32;Oxford:&#32;Clarendon Press.</cite>&nbsp;; (1979) New York: Dover ISBN 0-486-63896-0</entry>
<entry level="1" type="bullet">

 Gull, S.F. (1989) <weblink xlink:type="simple" xlink:href="http://www.ucl.ac.uk/~ucesjph/reality/entropy/text.html">
Some misconceptions about entropy</weblink> in:  <cite style="font-style:normal" class="book">&#32;(1991)&#32;in B. Buck, V. A. Macaulay (Eds.): Maximum Entropy in Action.&#32;Oxford University Press. ISBN 0-19-853963-0.</cite>&nbsp;.</entry>
</list>
</p>


</sec>
</bdy>
</proposition>
</theorem>
</concept>
</idea>
</message>
</statement>
</article>
