<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 21:04:36[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Speech segmentation</title>
<id>4273403</id>
<revision>
<id>147133421</id>
<timestamp>2007-07-26T01:53:00Z</timestamp>
<contributor>
<username>Eaefremov</username>
<id>400903</id>
</contributor>
</revision>
<categories>
<category>Natural language processing</category>
</categories>
</header>
<bdy>

<b>Speech segmentation</b> is the process of identifying the boundaries between <link xlink:type="simple" xlink:href="../866/1449866.xml">
word</link>s, <link xlink:type="simple" xlink:href="../911/44911.xml">
syllable</link>s, or <link xlink:type="simple" xlink:href="../980/22980.xml">
phoneme</link>s in spoken <link xlink:type="simple" xlink:href="../173/21173.xml">
natural languages</link>.  The term applies both to the <link xlink:type="simple" xlink:href="../620/490620.xml">
mental</link> processes used by humans, and to artificial processes of <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link>.<p>

Speech segmentation is an important subproblem of <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, and  cannot be adequately solved in isolation. As in most <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link> problems, one must take into account , <link xlink:type="simple" xlink:href="../569/12569.xml">
grammar</link>, and <link xlink:type="simple" xlink:href="../107/29107.xml">
semantics</link>, and even so the result is often a <link xlink:type="simple" xlink:href="../934/22934.xml">
probabilistic</link> division rather than a categorical.</p>

<sec>
<st>
Phonetic segmentation</st>
<p>

The lowest level of speech segmentation is the breakup and classification of the sound signal into a string of phones.  The difficulty of this problem is compounded by the phenomenon of <link xlink:type="simple" xlink:href="../179/2765179.xml">
co-articulation</link> of speech sounds, where one  may be modified in various ways by the adjacent sounds: it may blend smoothly with them, fuse with them, split, or even disappear.  This phenomenon may happen between adjacent words just as easily as within a single word.</p>
<p>

The notion that speech is produced like writing, as a sequence of distinct vowels and consonants, is a relic of our alphabetic heritage.  In fact, the way we produce vowels depends on the surrounding consonants and the way we produce consonants depends on the surrounding vowels.  For example, when we say 'kit', the [k] is farther forward than when we say 'caught'.  But also the vowel in 'kick' is phonetically different from the vowel in 'kit', though we normally do not hear this.  In addition, there are language-specific changes which occur on casual speech which makes it quite different from spelling.  For example, in English, the phrase 'hit you' could often be more appropriately spelled 'hitcha'.   Therefore, even with the best algorithms, the result of phonetic segmentation will usually be very distant from the standard written language. For this reason, the lexical and syntactic parsing of spoken text normally requires specialized algorithms, distinct from those used for parsing written text.</p>
<p>

Statistical models can be used to segment and align recorded speech to words or phones.
Applications include automatic lip-synch timing for cartoon animation, follow-the-bouncing-ball video sub-titling, and linguistic research.  Automatic segmentation and alignment software is commercially available.</p>

</sec>
<sec>
<st>
Lexical segmentation</st>
<p>

In all natural languages, the meaning of a complex spoken sentence (which often has never been heard or uttered before) can be understood only by decomposing it into smaller <it>lexical segments</it> (roughly, the <link xlink:type="simple" xlink:href="../866/1449866.xml">
word</link>s of the language), associating a meaning to each segment, and then combining those meanings according to the grammar rules of the language.  The recognition of each lexical segment in turn requires its decomposition into a sequence of discrete <it>phonetic segments</it> and mapping each segment to one element of a finite set of elementary sounds (roughly, the <link xlink:type="simple" xlink:href="../980/22980.xml">
phoneme</link>s of the language); the meaning then can be found by standard <link>
table lookup algorithms</link>.</p>
<p>

For most spoken languages, the boundaries between lexical units are surprisingly difficult to identify. One might expect that the inter-word spaces used by many written languages, like English or Spanish, would correspond to pauses in their spoken version; but that is true only in very slow speech, when the speaker deliberately inserts those pauses.  In normal speech, one typically finds many consecutive words being said with no pauses between them, and often the final sounds of one word blend smoothly or fuse with the initial sounds of the next word.</p>
<p>

Moreover, an utterance can have different meanings depending on how it is split into words. A popular example, often quoted in the field, is the phrase <it>How to wreck a nice beach</it>, which sounds very similar to <it>How to recognize speech</it>. As this example shows, proper lexical segmentation depends on context and semantics which draws on the whole of human knowledge and experience, and would thus require advanced pattern recognition and artificial intelligence technologies to be implemented on a computer.</p>
<p>

This problem overlaps to some extent with the problem of <link xlink:type="simple" xlink:href="../339/4274339.xml">
text segmentation</link> that occurs in some languages which are traditionally written without inter-word spaces, like <language wordnetid="106282651" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../751/5751.xml">
Chinese</link></language>
 and <language wordnetid="106282651" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../606/15606.xml">
Japanese</link></language>
.  However, even for those languages, text segmentation is often much easier than speech segmentation, because the written language usually has little interference between adjacent words, and often contains additional clues not present in speech (such as the use of <link xlink:type="simple" xlink:href="../604/37604.xml">
Chinese characters</link> for word stems in Japanese).</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <link>
Alexander Faaborg</link>, <link>
Waseem Daher</link>, <link>
Jos√© Espinosa</link>, and <link>
Henry Lieberman</link>. <it><link>
How to wreck a nice beach you sing calm incense</link></it>  International Conference on Intelligent User Interfaces (IUI 2005), San Diego (2005).</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../677/677.xml">
Ambiguity</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../468/29468.xml">
Speech recognition</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../448/28448.xml">
Speech processing</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../347/59347.xml">
Hyphenation</link></entry>
<entry level="1" type="bullet">

 <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<link xlink:type="simple" xlink:href="../038/20038.xml">
Mondegreen</link></device>
</instrumentality>
</artifact>
</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.sprex.com/phonolyze">
"Phonolyze" speech segmentation software</weblink></entry>
</list>
</p>



</sec>
</bdy>
</article>
