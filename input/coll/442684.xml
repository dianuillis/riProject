<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:06:38[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Relevance (information retrieval)</title>
<id>442684</id>
<revision>
<id>243096418</id>
<timestamp>2008-10-05T03:41:38Z</timestamp>
<contributor>
<username>DragonBot</username>
<id>5466012</id>
</contributor>
</revision>
<categories>
<category>Information retrieval</category>
<category>Information science</category>
</categories>
</header>
<bdy>

In the context of <link xlink:type="simple" xlink:href="../354/149354.xml">
information science</link> and <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link>, <b><link xlink:type="simple" xlink:href="../688/442688.xml">
relevance</link></b> denotes how well a retrieved set of documents (or a single document) meets the <link xlink:type="simple" xlink:href="../342/11016342.xml">
information need</link> of the user.  


<sec>
<st>
Topical relevance and other kinds of relevance</st>
<p>

<it>Relevance</it> most commonly refers to <it>topical</it> relevance or <it>aboutness</it>, i.e. to what extent the topic of a result matches the topic of the query or information need. Relevance can also be interpreted more broadly, referring to generally how "good" a retrieved result is with regard to the information need.  The latter definition of relevance, sometimes referred to as <it>user</it> relevance, encompasses <it>topical</it> relevance and possibly other concerns of the user such as timeliness, authority or novelty of the result.</p>


</sec>
<sec>
<st>
 History </st>

<p>

The concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in 17th Century.</p>
<p>

The formal study of relevance began in the 20th Century with the study of what would later be called <link xlink:type="simple" xlink:href="../245/1223245.xml">
bibliometrics</link>. In the 1930s and 1940s, S. C. Bradford used of the term "relevant" to characterize articles relevant to a subject (cf., <link xlink:type="simple" xlink:href="../119/62119.xml">
Bradford's law</link>). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>
<p>

Since 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".</p>

</sec>
<sec>
<st>
 Evaluation and Relevance </st>

<p>

The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the <link>
Cranfield Experiments</link> of the early 1960s and culminating in the <link xlink:type="simple" xlink:href="../745/7107745.xml">
TREC</link> evaluations that continue to this day as the main evaluation framework for information retrieval research.</p>
<p>

In order to evaluate how well an <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link> system retrieved topically relevant results, the relevance of retrieved results must be quantified.  In <link>
Cranfield</link>-style evaluations, this typically involves assigning a <it>relevance level</it> to each retrieved result, a process known as <it>relevance assessment</it>.  Relevance levels can be binary, indicating a result <b>is</b> or <b>is not</b> relevant, or graded, indicating results have a varying degree of match between the topic of the result and the information need.   Once relevance levels have been assigned to the retrieved results, <link>
information retrieval performance measures</link> can be used to assess the quality of a retrieval system's output.</p>
<p>

In contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance. These studies often focus on aspects of <link xlink:type="simple" xlink:href="../516/13516.xml">
human-computer interaction</link> (see also <link xlink:type="simple" xlink:href="../878/14473878.xml">
human-computer information retrieval</link>).</p>

</sec>
<sec>
<st>
 Clustering and Relevance </st>

<p>

The <link>
cluster hypothesis</link>, proposed by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link>
C. J. van Rijsbergen</link></associate>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
 in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>  The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity.  These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request).  Methods in this spirit include,
<list>
<entry level="1" type="bullet">

 cluster-based information retrieval<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></entry>
<entry level="1" type="bullet">

 cluster-based document expansion such as <link>
latent semantic analysis</link> or its language modeling equivalents.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></entry>
</list>

It is important to ensure that clusters-either in isolation or combination-successfully model the set of possible relevant documents.  </p>

<p>

A second interpretation, most notably advanced by Ellen Voorhees,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> focuses on the local relationships between documents.  The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales.  Methods in this spirit include,
<list>
<entry level="1" type="bullet">

 multiple cluster retrieval<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></entry>
<entry level="1" type="bullet">

 spreading activation<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref> and relevance propagation<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> methods</entry>
<entry level="1" type="bullet">

 local document expansion<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref></entry>
<entry level="1" type="bullet">

 score regularization<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></entry>
</list>

Local methods require a accurate and appropriate document similarity measure.</p>

</sec>
<sec>
<st>
Epistemological issues</st>
<p>

Are users best to evaluate the relevance of a given documents, or is it better to use experts?
Most research about relevance in information retrieval in recent years have implicitly assumed that the users' evaluation of the output a given system should be used to increase "relevance" output. An alternative strategy would be to use journal impact factor to rank output and thus base relevance on expert evaluations. Other strategies may be used. The important thing to recognize is, however, that relevance is fundamentally a question of epistemology, not psychology. (Peoples' psychology reflects certain epistemological influences).   </p>


</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
 Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810‐832. </entry>
<entry id="2">
F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.</entry>
<entry id="3">
W. B. Croft, “A model of cluster searching based on classification,” Information Systems, vol. 5, pp. 189–195, 1980.</entry>
<entry id="4">
A. Griffiths, H. C. Luckhurst, and P. Willett, “Using interdocument similarity information in document retrieval systems,” Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3–11, 1986.</entry>
<entry id="5">
X. Liu and W. B. Croft, “Cluster-based retrieval using language models,” in SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186–193, ACM Press, 2004.</entry>
<entry id="6">
E. M. Voorhees, “The cluster hypothesis revisited,” in SIGIR ’85: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188–196, ACM Press, 1985.</entry>
<entry id="7">
S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.</entry>
<entry id="8">
T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, “A study of relevance propagation for web search,” in SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408–415, ACM Press, 2005.</entry>
<entry id="9">
A. Singhal and F. Pereira, “Document expansion for speech retrieval,” in SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34–41, ACM Press, 1999.</entry>
<entry id="10">
F. Diaz, “Regularizing query-based retrieval scores,” Information Retrieval, vol. 10, pp. 531–562, December 2007.</entry>
</reflist>
</p>

</sec>
<sec>
<st>
Additional reading</st>
<p>

<list>
<entry level="1" type="bullet">

Relevance : communication and cognition. by Dan Sperber;  Deirdre Wilson. 2nd ed. Oxford ; Cambridge, MA : Blackwell Publishers, 2001. ISBN: 9780631198789</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. (<weblink xlink:type="simple" xlink:href="http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf">
pdf</weblink>)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. (<weblink xlink:type="simple" xlink:href="http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf">
pdf</weblink>)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. Sept. 19, 2007. (<weblink xlink:type="simple" xlink:href="http://www.sis.utk.edu/lazerow2007">
video</weblink>)</entry>
</list>
</p>


</sec>
</bdy>
</article>
