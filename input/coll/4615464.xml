<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 21:23:08[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Meta learning (computer science)</title>
<id>4615464</id>
<revision>
<id>231297596</id>
<timestamp>2008-08-11T20:10:25Z</timestamp>
<contributor>
<username>WhatamIdoing</username>
<id>1998764</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
</categories>
</header>
<bdy>

This article is about meta learning in computer science.&#32;&#32;For meta learning in social psychology, see <link xlink:type="simple" xlink:href="../280/2432280.xml">
Meta learning</link>.&#32;&#32;<p>

<b>Meta learning</b> is a subfield of <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link> where automatic learning algorithms are applied on <link xlink:type="simple" xlink:href="../632/18933632.xml">
meta-data</link> about machine learning experiments. Although different researchers hold different views as to what the term exactly means (see below), the main goal is to use such meta-data to understand how automatic learning can become flexible in solving different kinds of learning problems, hence to improve the performance of existing <link xlink:type="simple" xlink:href="../488/233488.xml">
learning algorithms</link>. </p>
<p>

Flexibility is very important because each learning algorithm is based on a set of assumptions about the data, its <link xlink:type="simple" xlink:href="../926/173926.xml">
inductive bias</link>. This means that it will only learn well if the bias matches the data in the learning problem. A learning algorithm may perform very well on one learning problem, but very badly on the next. From a non-expert point of view, this poses strong restrictions on the use of <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> or <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link> techniques, since the relationship between the learning problem (often some kind of <link xlink:type="simple" xlink:href="../377/8377.xml">
database</link>) and the effectiveness of different learning algorithms is not yet understood.</p>
<p>

By using different kinds of meta-data, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to select, alter or combine different learning algorithms to effectively solve a given learning problem. </p>

<sec>
<st>
Different views on meta learning</st>

<p>

These are some of the views on (and approaches to) meta learning, please note that there exist many variations on these general approaches:</p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Discovering meta-knowledge</it> works by inducing knowledge (e.g. rules) that expresses how each learning method will perform on different learning problems. The meta-data is formed by characteristics of the data (general, statistical, information-theoretic,... ) in the learning problem, and characteristics of the learning algorithm (type, parameter settings, performance measures,...). Another learning algorithm then learns how the data characteristics relate to the algorithm characteristics. Given a new learning problem, the data characteristics are measured, and the performance of different learning algorithms can be predicted. Hence, one can select the algorithms best suited for the new problem, at least if the induced relationship holds.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Stacked generalisation</it> works by combining a number of (different) learning algorithms. The meta-data is formed by the predictions of those different algorithms. Then another learning algorithm learns from this meta-data to predict which combinations of algorithms give generally good results. Given a new learning problem, the predictions of the selected set of algorithms are combined (e.g. by (weighted) voting) to provide the final prediction. Since each algorithm is deemed to work on a subset of problems, a combination is hoped to be more flexible and still able to make good predictions.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <it><event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../500/90500.xml">
Boosting</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</it> is related to stacked generalisation, but uses the same algorithm multiple times, where the examples in the training data get different weights over each run. This yields different predictions, each focused on rightly predicting a subset of the data, and combining those predictions leads to better (but more expensive) results. </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Dynamic bias selection</it> works by altering the inductive bias of a learning algorithm to match the given problem. This is done by altering key aspects of the learning algorithm, such as the hypothesis representation, heuristic formulae, or parameters. Many different approaches exist.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <it><link xlink:type="simple" xlink:href="../550/3920550.xml">
Inductive transfer</link></it> also called learning to learn, studies how the learning process can be improved over time. Meta-data consists of knowledge about previous learning episodes, and is used to efficiently develop an effective hypothesis for a new task. A related approach is called <link xlink:type="simple" xlink:href="../663/938663.xml">
learning to learn</link>, in which the goal is to use acquired knowledge from one domain to help learning in other domains.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Other approaches using meta-data to improve automatic learning are <link xlink:type="simple" xlink:href="../461/854461.xml">
learning classifier system</link>s, <link xlink:type="simple" xlink:href="../333/170333.xml">
case-based reasoning</link> and <link xlink:type="simple" xlink:href="../966/949966.xml">
constraint satisfaction</link>.</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 Vilalta R. and Drissi Y. (2002). <it>A perspective view and survey of meta-learning</it>, Artificial Intelligence Review, 18(2), 77--95</entry>
<entry level="1" type="bullet">

 Giraud-Carrier, C., &amp; Keller, J. (2002). Dealing with the data flood, J. Meij (ed), chapter Meta-Learning. STT/Beweton, The Hague.</entry>
</list>
</p>


</sec>
</bdy>
</article>
