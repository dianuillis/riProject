<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:07:42[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<measure  confidence="0.9511911446218017" wordnetid="100174412">
<header>
<title>Kullback–Leibler divergence</title>
<id>467527</id>
<revision>
<id>243737323</id>
<timestamp>2008-10-07T21:00:51Z</timestamp>
<contributor>
<username>Djinn112</username>
<id>38898</id>
</contributor>
</revision>
<categories>
<category>Thermodynamics</category>
<category>Entropy and information</category>
<category>Statistical distance measures</category>
<category>Statistical theory</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../542/23542.xml">
probability theory</link> and <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link>, the <b>Kullback–Leibler divergence</b><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> (also <b>information divergence</b>, <b>information gain</b>, or <b>relative entropy</b>) is a <link xlink:type="simple" xlink:href="../390/294390.xml">
non-commutative</link> measure of the difference between two probability distributions P and Q. KL measures the expected difference in the number of bits required to code samples from P when using a code based on P, and when using a code based on Q.  Typically <it>P</it> represents the "true" distribution of data, observations, or a precise calculated theoretical distribution. The measure <it>Q</it> typically represents a theory, model, description, or approximation of <it>P</it>.  <p>

It is a special case of a broader class of divergences called <link xlink:type="simple" xlink:href="../289/15224289.xml">
<it>f</it>-divergences</link>. Although it is often intuited as a <link xlink:type="simple" xlink:href="../467/1561467.xml">
distance metric</link>, the KL divergence is <b>not</b> a true <link xlink:type="simple" xlink:href="../467/1561467.xml">
metric</link> since it is not symmetric (hence 'divergence' rather than 'distance').</p>

<sec>
<st>
Definition</st>
<p>

For probability distributions <it>P</it> and <it>Q</it> of a <link xlink:type="simple" xlink:href="../015/3285015.xml">
discrete random variable</link>
the K–L divergence of <it>Q</it> from <it>P</it> is defined to be</p>
<p>

<indent level="1">

<math>D_{\mathrm{KL}}(P\|Q) = \sum_i P(i) \log \frac{P(i)}{Q(i)} \!</math>
</indent>

For distributions <it>P</it> and <it>Q</it> of a <link xlink:type="simple" xlink:href="../792/5792.xml">
continuous random variable</link> the summations give way to integrals, so that</p>
<p>

<indent level="1">

<math>D_{\mathrm{KL}}(P\|Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} \; dx \!</math>
</indent>

where <it>p</it> and <it>q</it> denote the densities of <it>P</it> and <it>Q</it>.</p>
<p>

More generally, if <math>P</math> and <math>Q</math> are probability
<link xlink:type="simple" xlink:href="../873/19873.xml">
measure</link>s over a set <it>X</it>, and <math>Q</math> 
is <link>
absolutely continuous</link> with respect to <math>P</math>, then
the Kullback-Leibler
divergence from <it>P</it> to <it>Q</it> is defined as</p>

<p>

<indent level="1">

<math> D_{\mathrm{KL}}(P\|Q) = -\int_X \log \frac{d Q}{d P} \; dP, \!</math>
</indent>

where
<math>\frac{dQ}{dP} </math> is the <link xlink:type="simple" xlink:href="../746/338746.xml">
Radon-Nikodym derivative</link> of <it>Q</it>
with respect to <it>P,</it>
and provided the expression on the right-hand side exists. 
Likewise, if <it>P</it> is absolutely continuous with respect to <it>Q,</it> then</p>
<p>

<indent level="1">

<math> D_{\mathrm{KL}}(P\|Q) = \int_X \log \frac{dP}{dQ} \; dP 
                      = \int_X \frac{dP}{dQ} \log\frac{dP}{dQ}\; dQ</math>,
</indent>

which we recognize as the entropy of P relative to Q. 
Continuing in this case, if <math>\mu</math> is any measure on <it>X</it> for which
<math>p = \frac{d P}{d \mu}</math> and <math>q = \frac{d Q}{d \mu}</math> exist, then the Kullback-Leibler divergence from <it>P</it> to <it>Q</it> is given as</p>
<p>

<indent level="1">

<math> D_{\mathrm{KL}}(P\|Q) = \int_X p \log \frac{p}{q} \;d\mu.
\!</math>
</indent>

The logarithms in these formulae are taken to base 2 if information is measured in units of <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link>s, or to base <it>e</it> if information is measured in <link xlink:type="simple" xlink:href="../481/3070481.xml">
nat</link>s. Most formulas involving the KL divergence hold irrespective of log base.</p>

</sec>
<sec>
<st>
Motivation, properties and terminology</st>

<p>

In information theory, the <link xlink:type="simple" xlink:href="../798/1471798.xml">
Kraft-McMillan theorem</link> establishes that any directly-decodable coding scheme for coding a message to identify one value <it>xi</it> out of a set of possibilities <it>X</it> can be seen as representing an implicit probability distribution <it>q(xi)</it> = 2-<it>li</it> over <it>X</it>, where <it>li</it> is the length of the code for <it>xi</it> in bits.  Therefore, KL divergence can be interpreted as the expected extra message-length per datum that must be communicated if a code that is optimal for a given (wrong) distribution <it>Q</it> is used, compared to using a code based on the true distribution <it>P</it>. </p>
<p>

It can be seen from the definition of the Kullback-Leibler divergence that</p>
<p>

<indent level="1">

<math>
\begin{matrix} 
D_{\mathrm{KL}}(P\|Q) &amp; = &amp; -\sum_x p(x) \log q(x)&amp; + &amp; \sum_x p(x) \log p(x) \\
&amp; =  &amp; H(P,Q) &amp; - &amp; H(P)\, \!
\end{matrix}</math>
</indent>

where <it>H</it>(<it>P</it>,<it>Q</it>) is called the <link xlink:type="simple" xlink:href="../250/1735250.xml">
cross entropy</link> of <it>P</it> and <it>Q</it>, and <it>H</it>(<it>P</it>) is the <link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link> of <it>P</it>.</p>
<p>

The Kullback–Leibler divergence is always non-negative,
<indent level="1">

<math>D_{\mathrm{KL}}(P\|Q) \geq 0, \,</math>
</indent>
a result known as <link xlink:type="simple" xlink:href="../678/2035678.xml">
Gibbs' inequality</link>, with <it>D</it>KL(<it>P</it>||<it>Q</it>) zero if and only if <it>P</it>&nbsp;=&nbsp;<it>Q</it>.  The entropy <it>H(P)</it> thus sets a minimum value for the cross-entropy <it>H(P,Q)</it>, the expected number of bits required when using a code based on <it>Q</it> rather than <it>P</it>; and the KL divergence therefore represents the expected number of extra bits that must be transmitted to identify a value <it>x</it> drawn from <it>X</it>, if a code is used corresponding to the probability distribution <it>Q</it>, rather than the "true" distribution <it>P</it>.</p>
<p>

Originally introduced by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<cryptanalyst wordnetid="109981540" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<decoder wordnetid="109995398" confidence="0.8">
<link xlink:type="simple" xlink:href="../420/506420.xml">
Solomon Kullback</link></decoder>
</causal_agent>
</intellectual>
</cryptanalyst>
</person>
</physical_entity>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<cryptanalyst wordnetid="109981540" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<decoder wordnetid="109995398" confidence="0.8">
<link xlink:type="simple" xlink:href="../853/4869853.xml">
Richard Leibler</link></decoder>
</mathematician>
</scientist>
</causal_agent>
</intellectual>
</cryptanalyst>
</person>
</physical_entity>
 in <link xlink:type="simple" xlink:href="../602/34602.xml">
1951</link> as the <b>directed divergence</b> between two distributions, it is not the same as a <link xlink:type="simple" xlink:href="../328/8328.xml">
divergence</link> in <link xlink:type="simple" xlink:href="../640/32640.xml">
calculus</link>.  One might be tempted to call it a "<link xlink:type="simple" xlink:href="../018/20018.xml">
distance metric</link>" on the space of probability distributions, but this would not be correct as the Kullback-Leibler divergence is not <link xlink:type="simple" xlink:href="../741/53741.xml">
symmetric</link>, </p>
<p>

<indent level="1">

<math>D_{\mathrm{KL}}(P\|Q) \neq D_{\mathrm{KL}}(Q\|P).</math>
</indent>

Moreover, <it>D</it>KL(<it>P</it>||<it>Q</it>) does not satisfy the <link xlink:type="simple" xlink:href="../941/53941.xml">
triangle inequality</link>.  </p>
<p>

Following <link>
Renyi</link> (1961), the term is sometimes also called the <b>information gain</b> about <it>X</it> achieved if <it>P</it> can be used instead of <it>Q</it>. It is also called the <b>relative entropy</b>, for using <it>Q</it> instead of <it>P</it>.</p>
<p>

The Kullback–Leibler divergence remains well-defined for continuous distributions, and furthermore is invariant under parameter transformations.  It can therefore be seen as in some ways a more fundamental quantity than some other properties in information theory (such as <link xlink:type="simple" xlink:href="../447/542447.xml">
self-information</link> or <link xlink:type="simple" xlink:href="../445/15445.xml">
Shannon entropy</link>), which can become undefined or negative for non-discrete probabilities.</p>

</sec>
<sec>
<st>
Relation to other quantities of information theory</st>

<p>

Many of the other quantities of information theory can be interpreted as applications of the KL divergence to specific cases.</p>
<p>

The <link xlink:type="simple" xlink:href="../447/542447.xml">
self-information</link>,</p>
<p>

<indent level="1">

<math>I(m) = D_{\mathrm{KL}}(\delta_{im} \| \{ p_i \}), </math>
</indent>

is the KL divergence of the probability distribution <it>P(i)</it> from a <link xlink:type="simple" xlink:href="../890/182890.xml">
Kronecker delta</link> representing certainty that <it>i=m</it> &mdash; i.e. the number of extra bits that must be transmitted to identify <it>i</it> if only the probability distribution <it>P(i)</it> is available to the receiver, not the fact that <it>i=m</it>.</p>
<p>

The <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>,</p>
<p>

<indent level="1">

<math>\begin{align}I(X;Y) &amp; = D_{\mathrm{KL}}(P(X,Y) \| P(X)P(Y) ) \\
&amp; = \mathbb{E}_X \{D_{\mathrm{KL}}(P(Y|X) \| P(Y) ) \} \\
&amp; = \mathbb{E}_Y \{D_{\mathrm{KL}}(P(X|Y) \| P(X) ) \}\end{align} </math>
</indent>

is the KL divergence of the product <it>P(X)P(Y)</it> of the two <link xlink:type="simple" xlink:href="../791/5791.xml">
marginal probability</link> distributions from the joint probability distribution <it>P(X,Y)</it> &mdash; i.e. the expected number of extra bits that must be transmitted to identify <it>X</it> and <it>Y</it> if they are coded using only their marginal distributions instead of the joint distribution.  Equivalently, if the joint probability <it>P(X,Y)</it> <it>is</it> known, it is the expected number of extra bits that must on average be sent to identify <it>Y</it> if the value of <it>X</it> is not already known to the receiver.</p>
<p>

The <link xlink:type="simple" xlink:href="../445/15445.xml">
Shannon entropy</link>,</p>
<p>

<indent level="1">

<math>\begin{align}H(X) &amp; = \mathrm{(i)} \, \mathbb{E}_x \{I(x)\} \\
&amp; = \mathrm{(ii)} \log N - D_{\mathrm{KL}}(P(X) \| P_U(X) )\end{align}</math>
</indent>

is the number of bits which would have to be transmitted to identify <it>X</it> from <it>N</it> equally likely possibilities, <it>less</it> the KL divergence of the uniform distribution <it>P</it>U<it>(X)</it> from the true distribution <it>P(X)</it> &mdash; i.e. <it>less</it> the expected number of bits saved, which would have had to be sent if the value of <it>X</it> were coded according to the uniform distribution <it>P</it>U<it>(X)</it> rather than the true distribution <it>P(X)</it>.</p>
<p>

The <link xlink:type="simple" xlink:href="../548/908548.xml">
conditional entropy</link>,</p>
<p>

<indent level="1">

<math>\begin{align}H(X|Y) &amp; = \log N - D_{\mathrm{KL}}(P(X,Y) \| P_U(X) P(Y) ) \\
&amp; = \mathrm{(i)} \,\, \log N - D_{\mathrm{KL}}(P(X,Y) \| P(X) P(Y) ) - D_{\mathrm{KL}}(P(X) \| P_U(X)) \\
&amp; = H(X) - I(X;Y) \\
&amp; = \mathrm{(ii)} \, \log N - \mathbb{E}_Y \{ D_{\mathrm{KL}}(P(X|Y) \| P_U(X)) \}\end{align}</math>
</indent>

is the number of bits which would have to be transmitted to identify <it>X</it> from <it>N</it> equally likely possibilities, <it>less</it> the KL divergence of the product distribution <it>P</it>U<it>(X) P(Y)</it> from the true joint distribution <it>P(X,Y)</it> &mdash; i.e. <it>less</it> the expected number of bits saved which would have had to be sent if the value of <it>X</it> were coded according to the uniform distribution <it>P</it>U<it>(X)</it> rather than the conditional distribution <it>P(X|Y)</it> of <it>X</it> given <it>Y</it>.</p>
<p>

the <link xlink:type="simple" xlink:href="../250/1735250.xml">
cross entropy</link> between two <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link>s measures the average number of <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link>s needed to identify an event from a set of possibilities, if a coding scheme is used based on a given probability distribution <math>q</math>, rather than the "true" distribution <math>p</math>.
The cross entropy for two distributions <math>p</math> and <math>q</math> over the same <link xlink:type="simple" xlink:href="../325/43325.xml">
probability space</link> is thus defined as follows:</p>
<p>

<indent level="1">

<math>\mathrm{H}(p, q) = \mathrm{E}_p[-\log q] = \mathrm{H}(p) + D_{\mathrm{KL}}(p \| q)\!</math>,
</indent>

</p>
</sec>
<sec>
<st>
KL divergence and Bayesian updating</st>

<p>

In <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian statistics</link> the KL divergence can be used as a measure of the information gain in moving from a <link xlink:type="simple" xlink:href="../877/472877.xml">
prior distribution</link> to a <link xlink:type="simple" xlink:href="../672/357672.xml">
posterior distribution</link>.  If some new fact <it>Y=y</it> is discovered, it can be used to update the probability distribution for <it>X</it> from <it>p</it>(<it>x</it>|I) to a new posterior probability distribution <it>p</it>(<it>x</it>|<it>y</it>) using <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
:
<indent level="1">

<math>p(x|y) = \frac{p(y|x) p(x|I)}{p(y|I)}</math>
</indent>

This distribution has a new entropy,  <it>H(p(x|y))</it> = -&amp;sum; <it>p</it>(<it>x</it>|<it>y</it>) log <it>p</it>(<it>x</it>|<it>y</it>), which may be less than or greater than the original entropy <it>H(p(x|I))</it>.  
However, from the standpoint of the new probability distribution one can estimate that to have used the original code based on <it>p</it>(<it>x</it>|I) instead of a new code based on <it>p</it>(<it>x</it>|<it>y</it>) would have added an expected number of bits</p>
<p>

<indent level="1">

<math>D_{\mathrm{KL}}(p(x|y)\|p(x|I)) = \sum p(x|y) \log \frac{p(x|y)}{p(x|I)}</math>
</indent>

to the message length. This therefore represents the amount of useful information, or information gain, about <it>X</it>, that we can estimate has been learned by discovering <it>Y=y</it>.</p>
<p>

If a further piece of data, <it>Y</it>2=<it>y</it>2, subsequently comes in, the probability distribution for <it>x</it> can be updated further, to give a new best guess <it>p</it>(<it>x</it>|<it>y</it>1,<it>y</it>2).  If one reinvestigates the information gain for using <it>p</it>(<it>x</it>|<it>y</it>1) rather than <it>p</it>(<it>x</it>|I), it turns out that it may be either greater or less than previously estimated:
<indent level="1">

<math>\sum_x p(x|y_1,y_2) \log \frac{p(x|y_1,y_2)}{p(x|I)}</math> may be  = or &amp;gt; than <math>\sum_x p(x|y_1) \log \frac{p(x|y_1)}{p(x|I)}</math>
</indent>
and so the combined information gain does <it>not</it> obey the triangle inequality:
<indent level="1">

<math>D_{\mathrm{KL}}(p(x|y_1,y_2)\|p(x|I))</math> may be  = or &amp;gt; than <math>D_{\mathrm{KL}}(p(x|y_1,y_2)\|p(x|y_1)) + D_{\mathrm{KL}}(p(x|y_1)\|p(x|I))</math>
</indent>
All one can say is that on <it>average</it>, averaging using <it>p</it>(<it>y</it>2|<it>y</it>1,<it>x</it>), the two sides will average out. </p>

<ss1>
<st>
Bayesian experimental design</st>

<p>

A common goal in <link xlink:type="simple" xlink:href="../860/1706860.xml">
Bayesian experimental design</link> is to maximise the expected KL divergence between the prior and the posterior. When posteriors are approximated to be Gaussian distributions, a design maximising the expected KL divergence is called <link xlink:type="simple" xlink:href="../142/1292142.xml#xpointer(//*[./st=%22D-optimality%22])">
Bayes d-optimal</link>.</p>

</ss1>
</sec>
<sec>
<st>
 Discrimination information </st>

<p>

The Kullback–Leibler divergence  <it>D</it>KL( <it>p</it>(<it>x</it>|<it>H</it>1) || <it>p</it>(<it>x</it>|<it>H</it>0) ) can also be interpreted as the expected <b>discrimination information</b> for <it>H</it>1 over <it>H</it>0: the mean information per sample for discriminating in favour of a hypothesis <it>H</it>1 against a hypothesis <it>H</it>0, when hypothesis <it>H</it>1 is true.  Another name for this quantity, given to it by <link xlink:type="simple" xlink:href="../404/404404.xml">
I.J. Good</link>, is the expected <link xlink:type="simple" xlink:href="../552/824552.xml">
weight of evidence</link> for <it>H</it>1 over <it>H</it>0 to be expected from each sample.</p>
<p>

The expected weight of evidence for <it>H</it>1 over <it>H</it>0 is <b>not</b> the same as the information gain expected per sample about the probability distribution <it>p</it>(<it>H</it>) of the hypotheses, </p>
<p>

<indent level="1">

<it>D</it>KL( <it>p</it>(<it>x</it>|<it>H</it>1) || <it>p</it>(<it>x</it>|<it>H</it>0) )  &nbsp;<math>\neq</math>&nbsp; <it>IG</it> = <it>D</it>KL( <it>p</it>(<it>H</it>|x) || <it>p</it>(<it>H</it>|I) ).
</indent>

Either of the two quantities can be used as a <link xlink:type="simple" xlink:href="../479/45479.xml">
utility function</link> in Bayesian experimental design, to choose an optimal next question to investigate: but they will in general lead to rather different experimental strategies.  </p>
<p>

On the entropy scale of <it>information gain</it> there is very little difference between near certainty and absolute certainty -- coding according to a near certainty requires hardly any more bits than coding according to an absolute certainty. On the other hand, on the <link xlink:type="simple" xlink:href="../068/172068.xml">
logit</link> scale implied by weight of evidence, the difference between the two is enormous - infinite perhaps; this might reflect the difference between being almost sure (on a probabilistic level) that, say, the <condition wordnetid="113920835" confidence="0.8">
<state wordnetid="100024720" confidence="0.8">
<problem wordnetid="114410605" confidence="0.8">
<difficulty wordnetid="114408086" confidence="0.8">
<link xlink:type="simple" xlink:href="../125/19344125.xml">
Riemann hypothesis</link></difficulty>
</problem>
</state>
</condition>
 is correct, compared to being certain that it is correct because one has a mathematical proof.  These two different scales of <link xlink:type="simple" xlink:href="../137/442137.xml">
loss function</link> for uncertainty are <it>both</it> useful, according to how well each reflects the particular circumstances of the problem in question.</p>

<ss1>
<st>
Principle of minimum discrimination information</st>

<p>

The idea of Kullback–Leibler divergence as discrimination information led Kullback to propose the Principle of <b>Minimum Discrimination Information</b> (MDI): given new facts, a new distribution <it>f</it> should be chosen which is as hard to discriminate from the original distribution <it>f</it>0 as possible; so that the new data produces as small an information gain <it>D</it>KL( <it>f</it> || <it>f</it>0 ) as possible.</p>
<p>

For example, if one had a prior distribution <it>p</it>(<it>x</it>,<it>a</it>) over <it>x</it> and <it>a</it>, and subsequently learnt the true distribution of <it>a</it> was <it>u</it>(<it>a</it>), the Kullback–Leibler divergence between the new joint distribution for <it>x</it> and <it>a</it>, <it>q</it>(<it>x</it>|<it>a</it>) <it>u</it>(<it>a</it>), and the earlier prior distribution would be:</p>
<p>

<indent level="1">

<math>D_\mathrm{KL}(q(x|a)u(a)||p(x,a)) =  \mathbb{E}_{u(a)}\{D_\mathrm{KL}(q(x|a)||p(x|a))\} + D_\mathrm{KL}(u(a)||p(a)),</math>
</indent>

i.e. the sum of the KL divergence of <it>p</it>(<it>a</it>) the prior distribution for <it>a</it> from the updated distribution <it>u</it>(<it>a</it>), plus the expected value (using the probability distribution <it>u</it>(<it>a</it>)) of the KL divergence of the prior conditional distribution <it>p</it>(<it>x</it>|<it>a</it>) from the new conditional distribution <it>q</it>(<it>x</it>|<it>a</it>).  This is minimised if <it>q</it>(<it>x</it>|<it>a</it>) = <it>p</it>(<it>x</it>|<it>a</it>) over the whole support of <it>u</it>(<it>a</it>); and we note that this result incorporates Bayes' theorem, if the new distribution <it>u</it>(<it>a</it>) is in fact a &amp;delta; function representing certainty that <it>a</it> has one particular value.</p>
<p>

MDI can be seen as an extension of <link xlink:type="simple" xlink:href="../783/344783.xml">
Laplace</link>'s <link xlink:type="simple" xlink:href="../701/285701.xml">
Principle of Insufficient Reason</link>, and the <link xlink:type="simple" xlink:href="../718/201718.xml">
Principle of Maximum Entropy</link> of <link xlink:type="simple" xlink:href="../971/166971.xml">
E.T. Jaynes</link>.  In particular, it is the natural extension of the principle of maximum entropy from discrete to continuous distributions, for which Shannon entropy ceases to be so useful (see <it><link xlink:type="simple" xlink:href="../168/3504168.xml">
differential entropy</link></it>), but the KL divergence continues to be just as relevant.</p>
<p>

In the engineering literature, MDI is sometimes called the <b>Principle of Minimum Cross-Entropy</b> (MCE) or <b>Minxent</b> for short.  This is not entirely helpful.  Minimising the KL divergence of <it>m</it> from <it>p</it> with respect to <it>m</it> is equivalent to minimising the cross-entropy of <it>p</it> and <it>m</it>, since</p>
<p>

<indent level="1">

<math>H(p,m) = H(p) + D_{\mathrm{KL}}(p\|m),</math>
</indent>

which is appropriate if one is trying to choose a least 'brain-damaged' approximation to <it>p</it>. However, this is just as often <it>not</it> the task one is trying to achieve. Instead, just as often it is <it>m</it> that is some fixed prior reference measure, and <it>p</it> that one is attempting to optimise by minimising <it>D</it>KL(<it>p</it>||<it>m</it>) subject to some constraint.  This has led to some ambiguity in the literature, with some authors attempting to resolve the inconsistency by redefining cross-entropy to be <it>D</it>KL(<it>p</it>||<it>m</it>), rather than <it>H</it>(<it>p</it>,<it>m</it>).</p>

</ss1>
</sec>
<sec>
<st>
Relationship to available work</st>

<p>

<image location="right" width="220px" src="ArgonKLdivergence.png" type="thumb">
<caption>

Pressure versus volume plot of available work from a mole of Argon gas relative to ambient, calculated as To times KL divergence.
</caption>
</image>

<link xlink:type="simple" xlink:href="../447/542447.xml">
Surprisal</link>s<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> add where probabilities multiply. The surprisal for an event of probability p is defined as s&amp;equiv;k<it>ln</it>[1/p].  If k is {1,1/<it>ln</it>2,1.38&amp;times;10-23} then surprisal is in {nats, bits, or J/K} so that, for instance, there are N bits of surprisal for landing all "heads" on a toss of N coins. </p>
<p>

Best-guess states (e.g. for atoms in a gas) are inferred by maximizing the <it>average-surprisal</it> S (<link xlink:type="simple" xlink:href="../891/9891.xml">
entropy</link>) for a given set of control parameters (like pressure P or volume V). This constrained <link>
entropy maximization</link>, both classically<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> and quantum mechanically<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>, minimizes <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link>
Gibbs</link></scientist>
</person>
 availability in entropy units<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref> A&amp;equiv;-k<it>ln</it>Z where Z is a constrained multiplicity or <link xlink:type="simple" xlink:href="../849/16846849.xml">
partition function</link>. </p>
<p>

When temperature T is fixed, free-energy (T times A) is also minimized. Thus if T, V and number of molecules N are constant, the <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../447/255447.xml">
Helmholtz free energy</link></concept>
</idea>
 F&amp;equiv;U-TS (where U is energy) is minimized as a system "equilibrates". If T and P are held constant (say during processes in your body), the <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../181/238181.xml">
Gibbs free energy</link></concept>
</idea>
 G&amp;equiv;U+PV-TS is minimized instead. The change in free energy under these conditions is a measure of available <link>
work</link> that might be done in the process.  Thus available work for an ideal gas at constant temperature To and pressure Po is W = ΔG = NkTo<b>Θ['''V/Vo''']</b> where Vo = NkTo/Po and <b>Θ['''x''']</b>&amp;equiv;x-1-<it>ln</it>x≥0 (see also <link xlink:type="simple" xlink:href="../678/2035678.xml">
Gibbs inequality</link>).</p>
<p>

More generally<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> the <link xlink:type="simple" xlink:href="../005/1075005.xml">
work available</link> relative to some ambient is obtained by multiplying ambient temperature To by KL-divergence or <it>net-surprisal</it> ΔI≥0, defined as the average value of k<it>ln</it>[p/po] where po is the probability of a given state under ambient conditions. For instance, the work available in equilibrating a monatomic ideal gas to ambient values of Vo and To is thus W=ToΔI, where KL-divergence ΔI=Nk(<b>Θ['''V/Vo''']</b>+3⁄2<b>Θ['''T/To''']</b>).  The resulting contours of constant KL-divergence, at right for a mole of Argon at standard temperature and pressure, for example put limits on the conversion of hot to cold as in flame-powered air-conditioning or in the unpowered device to convert boiling-water to ice-water discussed here<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>. Thus KL-divergence measures thermodynamic availability in bits.</p>

</sec>
<sec>
<st>
Quantum information theory</st>

<p>

For <link xlink:type="simple" xlink:href="../844/62844.xml">
density matrices</link> <it>P</it> and <it>Q</it> on a Hilbert space
the K–L divergence (or relative entropy as it is often called in this case) from <it>P</it> to <it>Q</it> is defined to be</p>
<p>

<indent level="1">

<math> D_{\mathrm{KL}}(P\|Q) = Tr(P( \log(P) - \log(Q))). \!</math>
</indent>

In <link xlink:type="simple" xlink:href="../094/659094.xml">
quantum information science</link> it can also be used as a measure of <link xlink:type="simple" xlink:href="../336/25336.xml">
entanglement</link> in a state.</p>

</sec>
<sec>
<st>
Relationship between models and reality</st>

<p>

Just as KL-divergence of "ambient from actual" measures thermodynamic availability, KL-divergence of "model from reality" is also useful even if the only clues we have about reality are some experimental measurements.  In the former case KL-divergence describes <it>distance to equilibrium</it> or (when multiplied by ambient temperature) the amount of <it>available work</it>, while in the latter case it tells you about surprises that reality has up its sleeve or, in other words, <it>how much the model has yet to learn</it>.  </p>
<p>

Although this tool for evaluating models against systems that are accessible experimentally may be applied in any field, its application to models in ecology via <link xlink:type="simple" xlink:href="../512/690512.xml">
Akaike Information Criterion</link> are particularly well described in papers<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref> and a book<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref> by Burnham and Anderson.  In a nutshell the KL-divergence of a model from reality may be estimated, to within a constant additive term, by a function (like the squares summed) of the deviations observed between data and the model's predictions.  Estimates of such divergence for models that share the same additive term can in turn be used to choose between models.</p>

</sec>
<sec>
<st>
 Symmetrised divergence </st>

<p>

Kullback and Leibler themselves actually defined the divergence as:</p>
<p>

<indent level="1">

<math> D_{\mathrm{KL}}(P\|Q) + D_{\mathrm{KL}}(Q\|P)\, \!</math>
</indent>

which is symmetric and nonnegative. This quantity has sometimes been used for feature selection in <link xlink:type="simple" xlink:href="../244/1579244.xml">
classification</link> problems, where <it>P</it> and <it>Q</it> are the conditional pdfs of a feature under two different classes. </p>
<p>

An alternative is given via the &amp;lambda; divergence,</p>
<p>

<indent level="1">

<math> D_{\lambda}(P\|Q) = \lambda D_{\mathrm{KL}}(P\|\lambda P + (1-\lambda)Q) + (1-\lambda) D_{\mathrm{KL}}(Q\|\lambda P + (1-\lambda)Q),\, \!</math>
</indent>

which can be interpreted as the expected information gain about <it>X</it> from discovering which probability distribution <it>X</it> is drawn from, <it>P</it> or <it>Q</it>, if they currently have probabilities &amp;lambda; and (1&nbsp;&amp;minus;&nbsp;&amp;lambda;) respectively.</p>
<p>

The value &amp;lambda; = 0.5 gives the <link xlink:type="simple" xlink:href="../573/4019573.xml">
Jensen-Shannon divergence</link>, defined by</p>
<p>

<indent level="1">

<math> D_{\mathrm{JS}} = \tfrac{1}{2} D_{\mathrm{KL}} \left (P  \| M \right ) + \tfrac{1}{2} D_{\mathrm{KL}}\left (Q \| M \right )\, \!</math>
</indent>

where <it>M</it> is the average of the two distributions,
<indent level="1">

<math> M = \tfrac{1}{2}(P+Q). \, </math>
</indent>

<it>D</it>JS can also be interpreted as the capacity of a noisy information channel with two inputs giving the output distributions <it>p</it> and <it>q</it>. The Jensen-Shannon divergence is the square of a metric that is equivalent to the Hellinger metric, and the Jensen-Shannon divergence is also equal to one-half the so-called <it>Jeffreys divergence</it> (Rubner et al., 2000; Jeffreys 1946).</p>

</sec>
<sec>
<st>
Relationship to Hellinger Distance</st>

<p>

If P and Q are two probability measures, then the squared <link xlink:type="simple" xlink:href="../709/13035709.xml">
Hellinger distance</link> is the quantity given by</p>
<p>

<indent level="1">

<math>H^2(P,Q) = \frac{1}{2}\int |\sqrt{dP} - \sqrt{dQ}|^2 </math>.
</indent>

Noting that <math>x-1 \geq \log(x)</math> or <math>\sqrt{x}-1 \geq \frac{1}{2}\log(x)</math>, we see that </p>
<p>

<indent level="1">

<math>\sqrt{\frac{dP}{dQ}}-1 \geq \frac{1}{2}\log(\frac{dP}{dQ})</math>.
</indent>

Taking expectations with respect to Q, we get</p>
<p>

<indent level="1">

<math>H^2(P,Q) \leq E_Q \log \frac{dQ}{dP} = D_{KL}(Q||P).</math>
</indent>

</p>
</sec>
<sec>
<st>
Other probability-distance measures</st>

<p>

Other measures of probability distance are the <it>histogram intersection</it>, <process wordnetid="105701363" confidence="0.8">
<information wordnetid="105816287" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../680/226680.xml">
<it>Chi-square statistic''</it></link></higher_cognitive_process>
</trial>
</datum>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</information>
</process>
, <it>quadratic form distance</it>, <it>match distance</it>, <it><process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../714/16714.xml">
Kolmogorov-Smirnov distance</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
</it>, and <link xlink:type="simple" xlink:href="../675/11915675.xml">
<it>earth mover's distance''</it></link> (Rubner et al. 2000).</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../573/4019573.xml">
Jensen-Shannon divergence</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../248/4491248.xml">
Bregman divergence</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../512/690512.xml">
Akaike information criterion</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../500/3103500.xml">
Deviance information criterion</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../272/2473272.xml">
Bayesian information criterion</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../806/5993806.xml">
Quantum relative entropy</link></entry>
<entry level="1" type="bullet">

<plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../412/2507412.xml">
Information gain in decision trees</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
<entry level="1" type="bullet">

<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<cryptanalyst wordnetid="109981540" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<decoder wordnetid="109995398" confidence="0.8">
<link xlink:type="simple" xlink:href="../420/506420.xml">
Solomon Kullback</link></decoder>
</causal_agent>
</intellectual>
</cryptanalyst>
</person>
</physical_entity>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<cryptanalyst wordnetid="109981540" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<decoder wordnetid="109995398" confidence="0.8">
<link xlink:type="simple" xlink:href="../853/4869853.xml">
Richard Leibler</link></decoder>
</mathematician>
</scientist>
</causal_agent>
</intellectual>
</cryptanalyst>
</person>
</physical_entity>
</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 Fuglede B, and Topsøe F., 2004, Jensen-Shannon Divergence and Hilbert Space Embedding, <it>IEEE Int Sym Information Theory</it>.</entry>
<entry level="1" type="bullet">

 Kullback, S., and Leibler, R. A.,  1951, On information and sufficiency, <it>Annals of Mathematical Statistics</it> <b>22</b>: 79-86.</entry>
<entry level="1" type="bullet">

 Rubner, Y., Tomasi, C., and Guibas, L. J., 2000. The Earth Mover's distance as a metric for image retrieval. <it>International Journal of Computer Vision</it>, <b>40</b>(2): 99-121.</entry>
<entry level="1" type="bullet">

 Kullback, S.  <it>Information Theory and Statistics</it>.  Dover reprint.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=13089&amp;objectType=file">
 Matlab code for calculating KL divergence</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
Footnotes</st>

<p>

<reflist>
<entry id="1">
S. Kullback and R. A. Leibler (1951) On information and sufficiency, <it>Annals of Mathematical Statistics</it> <b>22</b>:79-86.</entry>
<entry id="2">
S. Kullback (1959) <it>Information theory and statistics</it> (John Wiley and Sons, NY).</entry>
<entry id="3">
S. Kullback (1987) The Kullback-Leibler distance, <it>The American Statistician</it> <b>41</b>:340-341.</entry>
<entry id="4">
Myron Tribus (1961) <it>Thermodynamics and thermostatics</it> (D. Van Nostrand, New York)</entry>
<entry id="5">
E. T. Jaynes (1957) <weblink xlink:type="simple" xlink:href="http://bayes.wustl.edu/etj/articles/theory.1.pdf">
Information theory and statistical mechanics</weblink>, <it>Physical Review</it> <b>106</b>:620</entry>
<entry id="6">
E. T. Jaynes (1957) <weblink xlink:type="simple" xlink:href="http://bayes.wustl.edu/etj/articles/theory.2.pdf">
Information theory and statistical mechanics II</weblink>, <it>Physical Review</it> <b>108</b>:171</entry>
<entry id="7">
J.W. Gibbs (1873) A method of geometrical representation of thermodynamic properties of substances by means of surfaces, reprinted in <it>The Collected Works of J. W. Gibbs, Volume I Thermodynamics</it>, ed. W. R. Longley and R. G. Van Name (New York: Longmans, Green, 1931) footnote page 52.</entry>
<entry id="8">
M. Tribus and E. C. McIrvine (1971) Energy and information, <it>Scientific American</it> <b>224</b>:179-186.</entry>
<entry id="9">
P. Fraundorf (2007) <weblink xlink:type="simple" xlink:href="http://www3.interscience.wiley.com/cgi-bin/abstract/117861985/ABSTRACT">
Thermal roots of correlation-based complexity</weblink>, <it>Complexity</it> <b>13</b>:3, 18-26</entry>
<entry id="10">
Kenneth P. Burnham and David R. Anderson (2001) <weblink xlink:type="simple" xlink:href="http://www.publish.csiro.au/paper/WR99107.htm">
Kullback-Leibler information as a basis for strong inference in ecological studies</weblink>, <it>Wildlife Research</it> <b>28</b>:111-119.</entry>
<entry id="11">
Burnham, K. P. and Anderson D. R. (2002) <it>Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition</it> (Springer Science, New York) ISBN 978-0-387-95364-9.</entry>
</reflist>
</p>


</sec>
</bdy>
</measure>
</article>
