<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:10:22[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<idea  confidence="0.8" wordnetid="105833840">
<concept  confidence="0.8" wordnetid="105835747">
<numerical_quantity  confidence="0.8" wordnetid="105856066">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<value  confidence="0.8" wordnetid="105856388">
<quantity  confidence="0.8" wordnetid="105855125">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Expectation-maximization algorithm</title>
<id>470752</id>
<revision>
<id>243903349</id>
<timestamp>2008-10-08T15:08:33Z</timestamp>
<contributor>
<username>Rama</username>
<id>84330</id>
</contributor>
</revision>
<categories>
<category>Estimation theory</category>
<category>Optimization algorithms</category>
<category>Machine learning</category>
<category>Missing values</category>
</categories>
</header>
<bdy>

An <b>expectation-maximization</b> (<b>EM</b>) <b>algorithm</b> is used in <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link> for finding <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood</link> estimates of <link xlink:type="simple" xlink:href="../065/25065.xml">
parameter</link>s in probabilistic models, where the model depends on unobserved <link xlink:type="simple" xlink:href="../330/2649330.xml">
latent variable</link>s. EM alternates between performing an expectation (E) step, which computes an expectation of the likelihood by including the latent variables as if they were observed, and a maximization (M) step, which computes the maximum likelihood estimates of the parameters by maximizing the expected likelihood found on the E step. The parameters found on the M step are then used to begin another E step, and the process is repeated.
<sec>
<st>
History</st>
<p>

The EM algorithm was explained and given its name in a classic 1977 paper by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../761/7040761.xml">
Arthur Dempster</link></associate>
</scholar>
</mathematician>
</scientist>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</statistician>
</person>
</peer>
</physical_entity>
, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../623/18434623.xml">
Nan Laird</link></associate>
</scholar>
</mathematician>
</scientist>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</statistician>
</person>
</peer>
</physical_entity>
, and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../659/7790659.xml">
Donald Rubin</link></scientist>
 in the Journal of the <social_group wordnetid="107950920" confidence="0.8">
<society wordnetid="107966140" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../664/552664.xml">
Royal Statistical Society</link></group>
</society>
</social_group>
 <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>. They pointed out that the method had been "proposed many times in special circumstances" by other authors, but the 1977 paper generalized the method and developed the theory behind it.</p>

</sec>
<sec>
<st>
 Applications </st>

<p>

EM is frequently used for <link xlink:type="simple" xlink:href="../675/669675.xml">
data clustering</link> in <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> and <link xlink:type="simple" xlink:href="../596/6596.xml">
computer vision</link>. In <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link>, two prominent instances of the algorithm are the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../778/822778.xml">
Baum-Welch algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (also known as <it>forward-backward</it>) and the <link xlink:type="simple" xlink:href="../939/4492939.xml">
inside-outside algorithm</link> for unsupervised induction of <link xlink:type="simple" xlink:href="../329/299329.xml">
probabilistic context-free grammar</link>s.</p>
<p>

In <link xlink:type="simple" xlink:href="../982/24982.xml">
psychometrics</link>, EM is almost indispensable for estimating item parameters and latent abilities of <link xlink:type="simple" xlink:href="../159/420159.xml">
item response theory</link> models.</p>
<p>

With the ability to deal with missing data and observe unidentified variables, EM is becoming a useful tool to price and manage risk of a portfolio.</p>
<p>

The EM algorithm (and its faster variant <link xlink:type="simple" xlink:href="../972/650972.xml">
OS-EM</link>) is also widely used in <link xlink:type="simple" xlink:href="../714/234714.xml">
medical image</link> reconstruction, especially in <link xlink:type="simple" xlink:href="../032/24032.xml">
Positron Emission Tomography</link> and <link xlink:type="simple" xlink:href="../499/291499.xml">
Single Photon Emission Computed Tomography</link>. See below for other faster variants of EM.</p>

</sec>
<sec>
<st>
Demonstrations and activities</st>
<p>

Various 1D, 2D and 3D <weblink xlink:type="simple" xlink:href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture">
demonstrations of EM together with Mixture Modeling</weblink> are provided as part of the paired <link xlink:type="simple" xlink:href="../480/3217480.xml">
SOCR</link> activities and applets. These applets and activities show empirically the properties of the EM algorithm for parameter estimation in diverse settings.</p>
<p>

A number of methods have been proposed to accelerate the sometimes slow convergence of the EM algorithm, such as those utilising <link xlink:type="simple" xlink:href="../821/1448821.xml">
conjugate gradient</link> and modified <link>
Newton-Raphson</link> techniques. Additionally EM can be utilised with constrained estimation techniques.</p>



</sec>
<sec>
<st>
 EM in a nutshell </st>

<p>

EM is an iterative technique for estimating the value of some unknown quantity, given the values of some correlated, known quantity.  </p>
<p>

The approach is to first assume that the quantity is represented as a value in some parameterized probability distribution (the popular application is a mixture of Gaussians, hence the example below).  The EM procedure, then, is: </p>
<p>

<list>
<entry level="1" type="number">

Initialize the distribution parameters</entry>
<entry level="1" type="number">

Repeat until <it>convergence:''</it></entry>
<entry level="2" type="number">

E-Step: estimate the [E]xpected value of the unknown variables, given the current parameter estimate</entry>
<entry level="2" type="number">

M-Step: re-estimate the distribution parameters to [M]aximize the likelihood of the data, given the expected estimates of the unknown variables</entry>
</list>
</p>
<p>

Steps 1 and 2 are loaded, and depend on what distribution you choose, how many parameters there are, and how complicated the missing value is. Two other caveats: First, the choice of initial parameters technically does not matter, but in practice a poor choice can lead to a bad estimate.  Second, convergence, though guaranteed, may take a long time to get to.  In practice, if the values of the missing variables and parameters do not change significantly between two iterations, then the algorithm terminates.  </p>
<p>

A simple application is filling missing values in a column of a database.  Assume you know about 50% of the values in a column, but the remaining values are corrupt or missing.  For simplicity, assume that the data is distributed normally with a unit variance.  Then, the only parameter that must be computed is the mean value.  What's more convenient, is that the E-step is simple: the expected value of each missing value is the mean.  But the E-step changes the overall mean of the data, and so the estimate can be improved.  This will continue for a few iterations.  An example: </p>
<p>

Initialization Step:
Data: [4, 10, ?, ?]
Initial mean value: 0 
New Data: [4, 10, 0, 0]</p>
<p>

Step 1:
New Mean: 3.5
New Data:[4, 10, 3.5, 3.5]</p>
<p>

Step 2:
New Mean: 5.25
New Data: [4, 10, 5.25, 5.25]</p>
<p>

Step 3:
New Mean: 6.125
New Data: [4, 10, 6.125, 6.125]</p>
<p>

Step 4:
New Mean:  6.5625
New Data: [4, 10, 6.5625, 6.5625]</p>
<p>

Step 5:
New Mean: 6.7825
New Data: [4, 10, 6.7825, 6.7825]</p>
<p>

Result:
New Mean: 6.890625</p>
<p>

From here you can see the value is slowly converging toward 7. For this simple model (a univariate normal distribution with unit variance), it can be seen that substituting the average of the known values is the best answer.  For more complex models, there is no easy way to find the best answer, and the EM algorithm is a very popular approach for estimating the answer.  </p>
<p>

In the mixture of Gaussians demonstration below, we have a collection of multi-dimensional objects.  We'll assume that each individual data object was generated from one of K Gaussians. If we knew <it>which</it> Gaussian each object came from, then estimating the parameters is easy (using <link xlink:type="simple" xlink:href="../806/140806.xml">
Maximum likelihood</link> Estimation techniques). Instead, we must first compute the Expected value that the object came from each Gaussian (E-step) and then estimate the parameters, given these expected assignments (M-step).</p>

</sec>
<sec>
<st>
 Specification of the EM procedure </st>

<p>

Let <math>\textbf{y}</math> denote incomplete data consisting of values of observable variables, and let <math>\textbf{z}</math> denote the missing data. Together, <math>\textbf{y}</math> and <math>\textbf{z}</math> form the complete data.  <math>\textbf{z}</math> can either be actual missing measurements or a hidden variable that would make the problem easier if its value were known. For instance, in <link xlink:type="simple" xlink:href="../681/871681.xml">
mixture model</link>s, the likelihood formula would be much more convenient if mixture components that "generated" the samples were known (see example below).</p>
<p>

Let <math>p\,</math> be the joint <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link> (continuous case) or <link xlink:type="simple" xlink:href="../725/154725.xml">
probability mass function</link> (discrete case) of the complete data with parameters given by the vector <math>\theta</math>: <math>p( \mathbf y, \mathbf z | \theta)</math>. This function can also be considered as the complete data <link xlink:type="simple" xlink:href="../968/44968.xml">
likelihood</link>, that is, it can be thought of as a function of <math>\theta</math>.
Further, note that the <link xlink:type="simple" xlink:href="../458/504458.xml">
conditional distribution</link> of the missing data given the observed can be expressed as:</p>
<p>

<indent level="1">

<math>p(\mathbf z |\mathbf y, \theta) = \frac{p(\mathbf y, \mathbf z | \theta)}{p(\mathbf y | \theta)} = \frac{p(\mathbf y|\mathbf z, \theta) p(\mathbf z |\theta) }{\int p(\mathbf y|\mathbf \hat{z}, \theta) p(\mathbf \hat{z} |\theta) d\mathbf \hat{z}}</math>
</indent>

by using the <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes rule</link></proposition>
</theorem>
</message>
</statement>
 and the <link xlink:type="simple" xlink:href="../383/312383.xml">
law of total probability</link>. (This formulation only requires knowledge of the observation likelihood given the unobservable data <math>p(\mathbf y|\mathbf z, \theta)</math> and the probability of the unobservable data <math>p(\mathbf z |\theta)</math>.)</p>
<p>

An EM algorithm iteratively improves an initial estimate <math>\theta_0</math> by constructing new estimates <math>\theta_1, \theta_2, </math> and so on.
An individual re-estimation step that derives <math>\theta_{n+1}\,</math> from <math>\theta_n\,</math> takes the following form:</p>
<p>

<indent level="1">

<math>
\theta_{n+1}
=
\arg\max_{\theta}Q(\theta)
</math>
</indent>

where <math>Q(\theta)</math> is the <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> of the log-likelihood. In other words, we do not know the complete data, so we cannot say what is the exact value of the likelihood, but given the data that we do know (the <math>y</math>'s), we can find <it>a posteriori</it> estimates of the probabilities for the various values of the unknown <math>z</math>'s. For each set of <math>z</math>'s there is a likelihood value for <math>\theta</math>, and we can thus calculate an <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> of the likelihood with the given values of <math>y</math>'s (and which depends on the previously assumed value of <math>\theta</math> because this influenced the probabilities of the <it>z</it>'s).</p>
<p>

<it>Q</it> is given by</p>
<p>

<indent level="1">

<math>
Q(\theta)
=
 \sum_z
  p \left(z \,|\, y, \theta_n \right)
  \log p \left(y, z \,|\, \theta \right)
</math>
</indent>

or more generally</p>
<p>

<indent level="1">

<math>
Q(\theta)
=
 E_{\mathbf z} \! \! \left[ \log p \left(\mathbf y, \mathbf z \,|\, \theta \right) \Big| \mathbf y \right]
</math>
</indent>
where it is understood that this denotes the conditional expectation of <math>\log p \left( \mathbf y, \mathbf z \,|\, \theta \right) </math> being taken with the <math> \theta </math> used in the conditional distribution of <math>\textbf{z}</math> fixed at <math> \theta_n </math>. (The log of the likelihood is often used instead of true likelihood because it leads to easier formulas, but still attains its maximum at the same point as the likelihood.)</p>
<p>

In other words, <math>\theta_{n+1}</math> is the value that maximizes (M) the <link xlink:type="simple" xlink:href="../099/435099.xml">
conditional expectation</link> (E) of the complete data log-likelihood given the observed variables under the previous parameter value.
The expectation <math>Q(\theta)</math> in the continuous case would be given by</p>
<p>

<indent level="1">

<math>
Q(\theta)
=
E_{\mathbf z} \! \! \left[ \log p \left(\mathbf y, \mathbf z \,|\, \theta \right) \Big| \mathbf y \right]
=
\int^\infty _{- \infty}
 p \left(\mathbf z \,|\, \mathbf y, \theta_n \right)
 \log p \left(\mathbf y, \mathbf z \,|\, \theta \right) d\mathbf z
</math>
</indent>

Speaking of an expectation (E) step is a bit of a misnomer.
What is calculated in the first step are the fixed, data-dependent parameters of the function <it>Q</it>.
Once the parameters of <it>Q</it> are known, it is fully determined and is maximized in the second (M) step of an EM algorithm.</p>
<p>

The origin of the name comes from the fact that in the paper by Dempster, Laird and Rubin, they first discuss a less general problem in which the probability distribution is of the <link xlink:type="simple" xlink:href="../174/339174.xml">
exponential family</link>, and in that case the so-called E step consists of finding the expected values of certain <link>
sufficient statistic</link>s of the complete data.</p>

<ss1>
<st>
 Properties </st>
<p>

Part of the reason for the popularity of EM algorithms is that,
as can be shown, an EM iteration does not decrease the observed data likelihood function.  However, there is no guarantee that the sequence converges to a <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood estimator</link>. For <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link>
multimodal</link></distribution>
</arrangement>
</structure>
 distributions, this means that an EM algorithm will converge to a <link xlink:type="simple" xlink:href="../420/298420.xml">
local maximum</link> (or <link xlink:type="simple" xlink:href="../249/640249.xml">
saddle point</link>) of the observed data likelihood function, depending on starting values. There are a variety of heuristic approaches for escaping a local maximum such as using several different random initial estimates, <math>\theta_0</math>, or applying <link xlink:type="simple" xlink:href="../244/172244.xml">
simulated annealing</link>.</p>
<p>

EM is particularly useful when <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood estimation</link> of a complete data model is easy.
If <link xlink:type="simple" xlink:href="../327/358327.xml">
closed-form</link> estimators exist, the M step is often trivial.
A classic example is maximum likelihood estimation of a finite mixture of <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussians</link>, where each component of the mixture can be estimated trivially if the mixing distribution is known.</p>
<p>

"Expectation-maximization" is a description of a class of related algorithms, not a specific algorithm; EM is a recipe or meta-algorithm which is used to devise particular algorithms.
The <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../778/822778.xml">
Baum-Welch algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 is an example of an EM algorithm applied to <link xlink:type="simple" xlink:href="../770/98770.xml">
hidden Markov model</link>s.
Another example is the EM algorithm for fitting a <link xlink:type="simple" xlink:href="../681/871681.xml#xpointer(//*[./st=%22Expectation+maximization%22])">
mixture density model</link>.</p>
<p>

An EM algorithm can also find <link xlink:type="simple" xlink:href="../433/1792433.xml">
maximum a posteriori</link> (MAP) estimates, by performing MAP estimation in the M step, rather than maximum likelihood.</p>
<p>

There are other methods for finding maximum likelihood estimates, such as <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link>, <link xlink:type="simple" xlink:href="../821/1448821.xml">
conjugate gradient</link> or variations of the <link xlink:type="simple" xlink:href="../753/1164753.xml">
Gauss-Newton method</link>. Unlike EM, such methods typically require the evaluation of first and/or second derivatives of the likelihood function.</p>

</ss1>
</sec>
<sec>
<st>
 Incremental versions </st>

<p>

The classic EM procedure is to replace both <it>Q</it> and <it>θ</it> with their optimal possible (argmax) values at each iteration.  However it can be shown (see Neal &amp; Hinton, 1999) that simply finding <it>Q</it> and <it>θ</it> to give <it>some</it> improvement over their current value will also ensure successful convergence.  </p>
<p>

For example, to improve <it>Q</it>, we could restrict the space of possible functions to a computationally simple distribution such as a factorial distribution,</p>
<p>

<indent level="1">

<math>Q=\prod_i Q_i. \!</math>
</indent>

Thus at each E step we compute the variational approximation of <it>Q</it>.</p>
<p>

To improve <it>θ</it>, we could use any <link xlink:type="simple" xlink:href="../002/364002.xml">
hill-climbing</link> method, and not worry about finding the optimal <it>θ</it>, just some improvement. This method is also known as <it>Generalized EM</it> (GEM).</p>

</sec>
<sec>
<st>
 Relation to variational Bayes methods </st>

<p>

EM is a partially non-Bayesian, maximum likelihood method.  Its final result gives a <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> over the latent variables (in the Bayesian style) together with a point estimate for <it>θ</it> (either a <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood estimate</link> or a posterior mode).  We may want a fully Bayesian version of this, giving a probability distribution over <it>θ</it> as well as the latent variables.  In fact the Bayesian approach to inference is simply to treat <it>θ</it> as another latent variable.  In this paradigm, the distinction between the E and M steps disappears.  If we use the factorized Q approximation as described above (<link xlink:type="simple" xlink:href="../480/1208480.xml">
variational Bayes</link>), we may iterate over each latent variable (now including <it>θ</it>) and optimize them one at a time.  There are now <it>k</it> steps per iteration, where <it>k</it> is the number of latent variables.  For <link xlink:type="simple" xlink:href="../298/447298.xml">
graphical models</link> this is easy to do as each variable's new <it>Q</it> depends only on its <system wordnetid="108435388" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<network wordnetid="108434259" confidence="0.8">
<link xlink:type="simple" xlink:href="../984/1169984.xml">
Markov blanket</link></network>
</group>
</system>
, so local <link xlink:type="simple" xlink:href="../867/1324867.xml">
message passing</link> can be used for efficient inference.</p>

</sec>
<sec>
<st>
 Example: Gaussian Mixture </st>

<p>

Assume that a sample of <it>m</it> vectors (or scalars) <math>\mathbf y_1, \dots, \textbf{y}_m</math>, where <math>\mathbf y_j \in \mathbb{R}^l</math>, is drawn from one of <it>n</it> <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian distribution</link>s. Let <math>z_j \in \{1,2,\ldots,n\}</math> denote which Gaussian <math>\mathbf y_j</math> is from. The probability that a particular <math>\mathbf y</math> comes from the <math>i^{\mathrm{th}}</math> <math>D</math>-dimensional Gaussian is </p>
<p>

<math>
P(\mathbf y | z=i,\theta) = \mathcal{N}(\mu_i,\sigma_i) = (2\pi)^{-D/2} {\left| \sigma_i \right|}^{-1/2} \exp\left(-\frac{1}{2}(\mathbf y - \mathbf \mu_i)^T \sigma_i^{-1} (\mathbf y - \mathbf \mu_i)\right)
</math></p>
<p>

Our task is to estimate the unknown parameters <math>\theta = \left\{ \mu_1, \dots, \mu_n, \sigma_1, \dots, \sigma_n, P(z=1), \dots, P(z=n) \right\}</math>, that is, the mean and standard deviation of each Gaussian and the probability for each Gaussian being drawn for any given point. (Actually it is not clear that we should allow the standard deviations to take any value because then the maximum likelihood may be unbounded as one centers a particular Gaussian on a particular data point and decreases the standard deviation toward zero!)</p>

<ss1>
<st>
 E-step </st>

<p>

Estimation of the unobserved <it>z</it>'s (which Gaussian is used), conditioned on the observation, using the values from the last maximization step:</p>
<p>

<indent level="1">

<math>
p(z_j=i|\mathbf y_j,\theta_t) 
= \frac{p(z_j=i, \mathbf y_j | \theta_t)}{p(\mathbf y_j|\theta_t)} 
= \frac{p(\mathbf y_j|z_j=i,\theta_t) p(z=i|\theta_t)}{\sum_{k=1}^n p(\mathbf y_j | z_j=k, \theta_t) p(z=k|\theta_t)}
</math>
</indent>

</p>
</ss1>
<ss1>
<st>
 M-step </st>

<p>

We now want to maximize the expected log-likelihood of the joint event:</p>
<p>

<indent level="1">

<math>
\begin{align}
Q(\theta) 
 &amp; = E_{z} \left[ \ln \prod_{j=1}^m p \left(\mathbf y_j, \mathbf z | \theta \right) \Big| \mathbf y_j \right] \\
 &amp; = E_{z} \left[ \sum_{j=1}^m \ln p \left(\mathbf y_j, \mathbf z | \theta \right) \Big| \mathbf y_j \right] \\
 &amp; = \sum_{j=1}^m E_{z} \left[ \ln p \left(\mathbf y_j, \mathbf z | \theta \right) \Big| \mathbf y_j \right] \\
 &amp; = \sum_{j=1}^m \sum_{i=1}^n  p \left(z_j=i | \mathbf y_j, \theta_t \right) \ln p\left(z_j=i, \mathbf y_j | \theta \right) \\
\end{align}
</math>
</indent>

If we expand the probability of the joint event, we get</p>
<p>

<indent level="1">

<math>
Q(\theta) 
= \sum_{j=1}^m \sum_{i=1}^n p(z_j=i | \mathbf y_j, \theta_t) \ln \left( p(\mathbf y_j | z_j=i, \theta) p(z_j=i | \theta) \right)
</math>
</indent>

We have the constraint</p>
<p>

<indent level="1">

<math>
\sum_{i=1}^{n} p(z_j=i|\theta) = 1
</math>
</indent>

If we add a <link>
Lagrange multiplier</link>, and expand the <link xlink:type="simple" xlink:href="../487/43487.xml">
pdf</link>, we get</p>
<p>

<indent level="1">

<math>
\begin{align}
\mathcal{L}(\theta)
= &amp; \left( \sum_{j=1}^m \sum_{i=1}^n p(z_j=i | \mathbf y_j, \theta_t) \left( - \frac{D}{2} \ln (2\pi) - \frac{1}{2} \ln \left| \sigma_i \right| - \frac{1}{2}(\mathbf y_j - \mathbf \mu_i)^T \sigma_i^{-1} (\mathbf y_j - \mathbf \mu_i) + \ln p(z=i | \theta) \right) \right) \\
&amp; - \lambda \left( \sum_{i=1}^{n} p(z=i | \theta) - 1 \right)
\end{align}
</math>
</indent>

To find the new estimate <math>\theta_{t+1}</math>, we find a maximum where <math>\frac{\partial \mathcal{L}(\theta)}{\partial \theta} = 0</math>.</p>
<p>

New estimate for mean (using some differentiation rules from <link xlink:type="simple" xlink:href="../852/1765852.xml">
matrix calculus</link>):</p>
<p>

<indent level="1">

<math>
\begin{align}
\frac{\partial \mathcal{L}(\theta)}{\partial \mu_i} 
 &amp; = \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \left( - \frac{\partial}{\partial \mu_i} \frac{1}{2}(\mathbf y_j - \mathbf \mu_i)^T \sigma_i^{-1} (\mathbf y_j - \mathbf \mu_i) \right) \\
 &amp; = \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \left( - \frac{1}{2}(\sigma_i^{-1} +\sigma_i^{-T})(\mathbf y_j - \mathbf \mu_i)(-1) \right) \\
 &amp; = \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \left( \sigma_i^{-1}(\mathbf y_j - \mathbf \mu_i) \right)  \\
 &amp; = 0 \\
 &amp; \Downarrow \\
\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \sigma_i^{-1} \mathbf \mu_i 
 &amp; = \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \sigma_i^{-1} \mathbf y_j \\
 &amp; \Downarrow \\
\mu_i \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) 
 &amp; = \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \mathbf y_j \\
 &amp; \Downarrow \\
\mu_i 
 &amp; = \frac{\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \mathbf y_j}{\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t)} \\
\end{align}
</math>
</indent>

New estimate for covariance:</p>
<p>

<indent level="1">

<math>
\begin{align}
\frac{\partial \mathcal{L}(\theta)}{\partial \sigma_i}
 &amp; =  \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \left( - \frac{\partial}{\partial \sigma_i} \frac{1}{2} \ln \left| \sigma_i \right| - \frac{\partial}{\partial \sigma_i} \frac{1}{2}(\mathbf y_j - \mathbf \mu_i)^T \sigma_i^{-1} (\mathbf y_j - \mathbf \mu_i) \right) \\
 &amp; = \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \left( - \frac{1}{2} \sigma_i^{-T} + \frac{1}{2} \sigma_i^{-T}(\mathbf y_j - \mathbf \mu_i) (\mathbf y_j - \mathbf \mu_i)^T \sigma_i^{-T} \right) \\
 &amp; = 0 \\
 &amp; \Downarrow \\
\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \sigma_i^{-1} 
 &amp; = \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \sigma_i^{-1} (\mathbf y_j - \mathbf \mu_i) (\mathbf y_j - \mathbf \mu_i)^T \sigma_i^{-1} \\
 &amp; \Downarrow \\
\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) 
 &amp; = \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \sigma_i^{-1} (\mathbf y_j - \mathbf \mu_i) (\mathbf y_j - \mathbf \mu_i)^T \\
 &amp; \Downarrow \\
\sigma_i  
 &amp; = \frac{\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) (\mathbf y_j - \mathbf \mu_i) (\mathbf y_j - \mathbf \mu_i)^T}{\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t)} \\
\end{align}
</math>
</indent>

New estimate for class probability:</p>
<p>

<indent level="1">

<math>
\begin{align}
\frac{\partial \mathcal{L}(\theta)}{\partial p(z=i|\theta)}
 &amp; = \left( \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \frac{\partial \ln p(z=i|\theta)}{\partial p(z=i|\theta)} \right) - \lambda \left( \frac{\partial p(z=i|\theta)}{\partial p(z=i|\theta)} \right) \\
 &amp; = \left( \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \frac{1}{p(z=i|\theta)} \right) - \lambda \\
 &amp; = 0 \\
 &amp; \Downarrow \\
\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \frac{1}{p(z=i|\theta)}
 &amp; = \lambda \\
 &amp; \Downarrow \\
p(z=i|\theta)
 &amp; = \frac{1}{\lambda} \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \\
\end{align}
</math>
</indent>

Inserting into the constraint:</p>
<p>

<indent level="1">

<math>
\begin{align}
\sum_{i=1}^{n} p(z=i|\theta)
 &amp; = \sum_{i=1}^{n} \frac{1}{\lambda} \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \\
 &amp; = 1 \\
 &amp; \Downarrow \\
\lambda 
 &amp; = \sum_{i=1}^{n} \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \\
\end{align}
</math>
</indent>

Inserting <math>\lambda</math> into our estimate:</p>
<p>

<indent level="1">

<math>
\begin{align}
p(z=i|\theta)
 &amp; = \frac{1}{\lambda} \sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t) \\
 &amp; = {\displaystyle\frac{\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t)}{\sum_{k=1}^{n} \sum_{j=1}^m p(z_j=k | \mathbf y_j, \theta_t)}} \\
 &amp; = \frac{1}{m}\sum_{j=1}^m p(z_j=i | \mathbf y_j, \theta_t)
\end{align}
</math>
</indent>

These estimates now become our <math>\theta_{t+1}</math>, to be used in the next estimation step.</p>

</ss1>
</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
<physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../761/7040761.xml">
Arthur Dempster</link></associate>
</scholar>
</mathematician>
</scientist>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</statistician>
</person>
</peer>
</physical_entity>
, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../623/18434623.xml">
Nan Laird</link></associate>
</scholar>
</mathematician>
</scientist>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</statistician>
</person>
</peer>
</physical_entity>
, and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../659/7790659.xml">
Donald Rubin</link></scientist>
. "<weblink xlink:type="simple" xlink:href="http://links.jstor.org/sici?sici=0035-9246%281977%2939%3A1%3C1%3AMLFIDV%3E2.0.CO%3B2-ZMaximum">
likelihood from incomplete data via the EM algorithm</weblink>". <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../143/13327143.xml">
Journal of the Royal Statistical Society</link></periodical>
</it>, Series B, 39(1):1&ndash;38, 1977</entry>
</reflist>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Robert Hogg, Joseph McKean and Allen Craig. <it>Introduction to Mathematical Statistics</it>. pp. 359-364.  Upper Saddle River, NJ: Pearson Prentice Hall, 2005.</entry>
<entry level="1" type="bullet">

 Radford Neal, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../174/507174.xml">
Geoffrey Hinton</link></associate>
</research_worker>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
. "A view of the EM algorithm that justifies incremental, sparse, and other variants". In <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../732/1513732.xml">
Michael I. Jordan</link></associate>
</research_worker>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
 (editor), <it>Learning in Graphical Models</it> pp 355-368. Cambridge, MA: MIT Press, 1999.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/">
The on-line textbook: Information Theory, Inference, and Learning Algorithms</weblink>, by <link xlink:type="simple" xlink:href="../315/2679315.xml">
David J.C. MacKay</link> includes simple examples of the E-M algorithm such as clustering using the soft K-means algorithm, and emphasizes the variational view of the E-M algorithm.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/bilmes98gentle.html">
A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models</weblink>, by <weblink xlink:type="simple" xlink:href="http://ssli.ee.washington.edu/people/bilmes/">
Jeff Bilmes</weblink> includes a simplified derivation of the EM equations for Gaussian Mixtures and Gaussian Mixture Hidden Markov Models.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cse.buffalo.edu/faculty/mbeal/thesis/index.html">
Variational Algorithms for Approximate Bayesian Inference</weblink>, by M. J. Beal includes comparisons of EM to Variational Bayesian EM and derivations of several models including Variational Bayesian HMMs.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cc.gatech.edu/~dellaert/em-paper.pdf">
The Expectation Maximization Algorithm</weblink>, by Frank Dellaert, gives an easier explanation of EM algorithm in terms of lowerbound maximization.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.seanborman.com/publications/EM_algorithm.pdf">
The Expectation Maximization Algorithm: A short tutorial</weblink>, A self contained derivation of the EM Algorithm by Sean Borman.</entry>
<entry level="1" type="bullet">

http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture SOCR demonstrations of EM and Mixture Modeling]</entry>
<entry level="1" type="bullet">

 Jamshidian, Mortaza and Jennrich, Robert I. (1997). "Acceleration of the EM Algorithm by Using Quasi-Newton Methods,"Journal of the Royal Statistical Society, Ser. B , 59, 569--587.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://lcn.epfl.ch/tutorial/english/mixtureModel/index.html">
Java code example</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.neurosci.aist.go.jp/~akaho/MixtureEM.html">
Another Java code example</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../926/1565926.xml">
Estimation theory</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../553/9303553.xml">
Constrained clustering</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../675/669675.xml">
Data clustering</link></entry>
<entry level="1" type="bullet">

<information wordnetid="105816287" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../407/1860407.xml">
K-means algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</datum>
</rule>
</event>
</information>
</entry>
<entry level="1" type="bullet">

<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<numerical_quantity wordnetid="105856066" confidence="0.8">
<value wordnetid="105856388" confidence="0.8">
<quantity wordnetid="105855125" confidence="0.8">
<link xlink:type="simple" xlink:href="../220/1309220.xml">
Imputation (statistics)</link></quantity>
</value>
</numerical_quantity>
</concept>
</idea>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../480/3217480.xml">
SOCR</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../972/650972.xml">
Ordered subset expectation maximization</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../778/822778.xml">
Baum-Welch algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, a particular case</entry>
<entry level="1" type="bullet">

<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../572/4237572.xml">
Latent variable model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../770/98770.xml">
Hidden Markov Model</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../748/2007748.xml">
Structural equation model</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../418/5700418.xml">
Rubin causal model</link> </entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</quantity>
</value>
</rule>
</event>
</numerical_quantity>
</concept>
</idea>
</article>
