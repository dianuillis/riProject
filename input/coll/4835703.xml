<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 21:24:06[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Yao&apos;s principle</title>
<id>4835703</id>
<revision>
<id>197006625</id>
<timestamp>2008-03-09T14:48:52Z</timestamp>
<contributor>
<username>Andreas Kaufmann</username>
<id>72502</id>
</contributor>
</revision>
<categories>
<category>Computational complexity theory</category>
<category>Articles with Alice and Bob explanations</category>
<category>Randomness</category>
</categories>
</header>
<bdy>

<b>Yao&#39;s principle</b> states that the expected cost of any <link xlink:type="simple" xlink:href="../383/495383.xml">
randomized algorithm</link> for solving a given problem, on the <link xlink:type="simple" xlink:href="../956/37956.xml">
worst case</link> input for that algorithm, can be no better than the expected cost, for a worst-case random <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> on the inputs, of the <link xlink:type="simple" xlink:href="../951/665951.xml">
deterministic algorithm</link> that performs best against that distribution. Thus, to establish a lower bound on the performance of randomized algorithms, it suffices to find an appropriate distribution of difficult inputs, and to prove that no deterministic algorithm can perform well against that distribution. This principle is named after <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../560/299560.xml">
Andrew Yao</link></scientist>
</person>
, who first proposed it.<p>

Yao's principle may be interpreted in <link xlink:type="simple" xlink:href="../924/11924.xml">
game theoretic</link> terms, via a two-player <link xlink:type="simple" xlink:href="../417/34417.xml">
zero sum game</link> in which one player, <link xlink:type="simple" xlink:href="../079/679079.xml">
Alice</link>, selects a deterministic algorithm, the other player, Bob, selects an input, and the payoff is the cost of the selected algorithm on the selected input. Any randomized algorithm <it>R</it> may be interpreted as a randomized choice among deterministic algorithms, and thus as a strategy for Alice. By <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../942/15942.xml">
von Neumann's</link></scientist>
</person>
 <link>
minimax theorem</link>, Bob has a randomized strategy that performs at least as well against <it>R</it> as it does against the best pure strategy Alice might choose; that is, Bob's strategy defines a distribution on the inputs such that the expected cost of <it>R</it> on that distribution (and therefore also the worst case expected cost of <it>R</it>) is no better than the expected cost of any single deterministic algorithm against the same distribution. </p>

<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="CITEREFYao1977" style="font-style:normal"><link>
Yao, Andrew</link>&#32;(1977),&#32;"Probabilistic computations: Toward a unified measure of complexity",&#32;<it>Proceedings of the 18th IEEE Symposium on Foundations of Computer Science (FOCS)</it>, pp. 222â€“227</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://weblog.fortnow.com/2006/10/favorite-theorems-yao-principle.html">
Favorite theorems: Yao principle</weblink>, Lance Fortnow, October 16, 2006.</entry>
</list>
</p>

</sec>
</bdy>
</article>
