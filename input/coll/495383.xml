<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:10:42[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Randomized algorithm</title>
<id>495383</id>
<revision>
<id>229337470</id>
<timestamp>2008-08-02T02:03:50Z</timestamp>
<contributor>
<username>SmackBot</username>
<id>433328</id>
</contributor>
</revision>
<categories>
<category>Randomness</category>
<category>Algorithms</category>
</categories>
</header>
<bdy>

A <b>randomized algorithm</b> or <b>probabilistic algorithm</b> is an <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> which employs a degree of randomness as part of its logic. In common practice, this means that the machine implementing the algorithm has access to a <link xlink:type="simple" xlink:href="../524/45524.xml">
pseudorandom number generator</link>.  The algorithm typically uses the <link xlink:type="simple" xlink:href="../523/19196523.xml">
random</link> bits as an auxiliary input to guide its <link xlink:type="simple" xlink:href="../805/4805.xml">
behavior</link>, in the hope of achieving good performance in the "average case". Formally, the algorithm's performance will be a <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> determined by the random bits, with (hopefully) good <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link>; this expected value is called the <it>expected <link xlink:type="simple" xlink:href="../ury/23rd_century.xml">
running time</link></it>. The "<link xlink:type="simple" xlink:href="../956/37956.xml">
worst case</link>" is typically so unlikely to occur that it can be ignored.
<sec>
<st>
 Motivation </st>

<p>

As a motivating example, consider the problem of finding an 'a' in an <link xlink:type="simple" xlink:href="../052/2052.xml">
array</link> of <it>n</it> elements, given that half are 'a's and the other half are 'b's. The obvious approach is to look at each element of the <link xlink:type="simple" xlink:href="../052/2052.xml">
array</link>, but this would take very long (n/2 operations) if the array were ordered as 'b's first followed by 'a's. There is a similar drawback with checking in the reverse order, or checking every second element. In fact, with any <link xlink:type="simple" xlink:href="../607/29607.xml">
strategy</link> at all in which the order in which the elements will be checked is fixed, i.e, a <it>deterministic</it> algorithm, we cannot guarantee that the algorithm will complete quickly <it>for all possible inputs</it>. On the other hand, if we were to check array elements <it>at random</it>, then we will quickly find an 'a' <it>with high probability</it>, <it>whatever be the input</it>.</p>
<p>

Randomized algorithms are particularly useful when faced with a malicious "adversary" or <link xlink:type="simple" xlink:href="../940/487940.xml">
attacker</link> who deliberately tries to feed a bad input to the algorithm (see <link xlink:type="simple" xlink:href="../743/8198743.xml">
competitive analysis (online algorithm)</link>). It is for this reason that <link xlink:type="simple" xlink:href="../523/19196523.xml">
randomness</link> is ubiquitous in <link xlink:type="simple" xlink:href="../432/18934432.xml">
cryptography</link>. In cryptographic applications, pseudo-random numbers cannot be used, since the adversary can predict them, making the algorithm effectively deterministic. Therefore either a source of truly random numbers or a <link xlink:type="simple" xlink:href="../249/182249.xml">
cryptographically secure pseudo-random number generator</link> is required.  Another area in which randomness is inherent is <link xlink:type="simple" xlink:href="../220/25220.xml">
quantum computing</link>.</p>
<p>

In the example above, the randomized algorithm always outputs the correct answer, it is just that there is a small probability of taking long to execute. Sometimes we want the algorithm to always complete quickly, but allow a <it>small probability of error</it>. The former type are called <link xlink:type="simple" xlink:href="../519/537519.xml">
Las Vegas algorithm</link>s, and the latter are <link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo algorithm</link>s (related to the <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo method</link></method>
</know-how>
</technique>
 for simulation). Observe that any Las Vegas algorithm can be converted into a Monte Carlo algorithm, by having it output an arbitrary, possibly incorrect answer if it fails to complete within a specified time.</p>

</sec>
<sec>
<st>
 Computational complexity </st>

<p>

The <link xlink:type="simple" xlink:href="../543/7543.xml">
computational complexity theory</link> models randomized algorithms as <it><link xlink:type="simple" xlink:href="../812/197812.xml">
probabilistic</link> <invention wordnetid="105633385" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../403/30403.xml">
Turing machine</link></method>
</know-how>
</invention>
s</it>. Both  Las Vegas and Monte Carlo algorithms are considered, and several <link xlink:type="simple" xlink:href="../426/502426.xml">
complexity class</link>es are studied. The most basic randomized complexity class is <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../771/54771.xml">
RP</link></group>
</collection>
</class>
, which is the class of <link xlink:type="simple" xlink:href="../336/8336.xml">
decision problem</link>s for which there is an efficient (polynomial time) randomized algorithm (or probabilistic Turing machine) which recognizes NO-instances with absolute certainty and recognizes YES-instances with a probability of at least 1/2. The complement class is Co-RP, and the class of problems for which both YES and NO answers are allowed to be probabilistic is <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../079/4079.xml">
BPP</link></group>
</collection>
</class>
. Problem classes having (possibly nonterminating) algorithms with <link xlink:type="simple" xlink:href="../576/44576.xml">
polynomial time</link> average case running time whose output is always correct are said to be in <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../772/54772.xml">
ZPP</link></group>
</collection>
</class>
. For problems that (are believed to) lie outside these classes, such as <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../681/54681.xml">
NP-hard</link></group>
</collection>
</class>
 problems, even randomized algorithms do not suffice, and it becomes necessary to resort to <link xlink:type="simple" xlink:href="../105/563105.xml">
approximation algorithm</link>s.</p>
<p>

Historically, the first randomized algorithm was a method developed by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../404/298404.xml">
Michael O. Rabin</link></scientist>
</person>
 for the <link xlink:type="simple" xlink:href="../111/9311111.xml">
closest pair problem</link> in <link xlink:type="simple" xlink:href="../927/176927.xml">
computational geometry</link>. The study of randomized algorithms was spurred by the 1977 discovery of a <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../089/1013089.xml">
randomized primality test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
 (i.e., determining the <link xlink:type="simple" xlink:href="../751/183751.xml">
primality</link> of a number) by <physical_entity wordnetid="100001930" confidence="0.8">
<expert wordnetid="109617867" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<theorist wordnetid="110706812" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<logician wordnetid="110269785" confidence="0.8">
<link xlink:type="simple" xlink:href="../110/3160110.xml">
Robert M. Solovay</link></logician>
</mathematician>
</scientist>
</causal_agent>
</intellectual>
</theorist>
</person>
</expert>
</physical_entity>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../960/771960.xml">
Volker Strassen</link></mathematician>
</scientist>
</causal_agent>
</person>
</physical_entity>
. Soon afterwards <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../404/298404.xml">
Michael O. Rabin</link></scientist>
</person>
 demonstrated that the 1976 <region wordnetid="108630985" confidence="0.8">
<process wordnetid="105701363" confidence="0.8">
<field wordnetid="108569998" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../999/188999.xml">
Miller's primality test</link></higher_cognitive_process>
</trial>
</geographical_area>
</experiment>
</problem_solving>
</tract>
</location>
</thinking>
</inquiry>
</field>
</process>
</region>
 can be turned into a randomized algorithm. At that time, no practical <link xlink:type="simple" xlink:href="../951/665951.xml">
deterministic algorithm</link> for primality was known. </p>
<p>

The Miller-Rabin primality test relies on a binary relation between two positive integers <it>k</it> and <it>n</it> that can be expressed by saying that <it>k</it> "is a witness to the compositeness of" <it>n</it>.  It can be shown that
<list>
<entry level="1" type="bullet">

 If there is a witness to the compositeness of <it>n</it>, then <it>n</it> is composite (i.e., <it>n</it> is not <link xlink:type="simple" xlink:href="../666/23666.xml">
prime</link>), and</entry>
<entry level="1" type="bullet">

 If <it>n</it> is composite then at least three-fourths of the natural numbers less than <it>n</it> are witnesses to its compositeness, and</entry>
<entry level="1" type="bullet">

 There is a fast algorithm that, given <it>k</it> and <it>n</it>, ascertains whether <it>k</it> is a witness to the compositeness of <it>n</it>.</entry>
</list>

Observe that this implies that the primality problem is in Co-RP.</p>
<p>

If one <link xlink:type="simple" xlink:href="../523/19196523.xml">
random</link>ly chooses 100 numbers less than a composite number <it>n</it>, then the probability of failing to find such a "witness" is (1/4)100 so that for most practical purposes, this is a good <link xlink:type="simple" xlink:href="../751/183751.xml">
primality test</link>.  If <it>n</it> is big, there may be no other test that is practical. The probability of error can be reduced to an arbitrary degree by performing enough independent tests. </p>
<p>

Therefore, in practice, there is no penalty associated with accepting a small probability of error, since with a little care the probability of error can be made astronomically small.  Indeed, even though a deterministic polynomial-time primality test has since been <process wordnetid="105701363" confidence="0.8">
<institute wordnetid="108407330" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<association wordnetid="108049401" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../847/405847.xml">
found</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</association>
</thinking>
</inquiry>
</institute>
</process>
, it has not replaced the older probabilistic tests in <link xlink:type="simple" xlink:href="../432/18934432.xml">
cryptographic</link> <link xlink:type="simple" xlink:href="../309/5309.xml">
software</link> nor is it expected to do so for the foreseeable future.</p>
<p>

If, using a randomized method, the probability of error is 2&amp;minus;1000, the philosophical question arises: is this a <link xlink:type="simple" xlink:href="../285/82285.xml">
mathematical proof</link>? After all, the probability of the algorithm's error is distinctly smaller than the probability of an error in the computer hardware executing it, or the reader  making an error in reading a proof - what does it mean in real terms to consider this small a probability?</p>

</sec>
<sec>
<st>
 Applications </st>

<p>

<algorithm wordnetid="105847438" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../249/3268249.xml">
Quicksort</link></algorithm>
 is a familiar, commonly-used algorithm in which randomness can be useful. The deterministic version of this algorithm requires <it><link xlink:type="simple" xlink:href="../578/44578.xml">
O</link>(n2)</it> time to sort <it>n</it> numbers for some degenerate inputs, such as an array whose elements are already in sorted order. However, if the algorithm selects pivot elements uniformly at random, it has a very high probability of finishing in <it>O(n log n)</it> time.</p>
<p>

Randomized algorithms can also be used to solve <link xlink:type="simple" xlink:href="../401/12401.xml">
graph theory</link> problems, as demonstrated by this randomized <link xlink:type="simple" xlink:href="../130/78130.xml">
minimum cut</link> algorithm:</p>
<p>

find_min_cut(undirected graph G) {
while there are more than 2 <link xlink:type="simple" xlink:href="../074/998074.xml">
node</link>s in G do {
pick an <link xlink:type="simple" xlink:href="../401/12401.xml">
edge</link> (u,v) at random in G
contract the edge, while preserving multi-edges
remove all <work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<glossary wordnetid="106420781" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<wordbook wordnetid="106418693" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<reference_book wordnetid="106417598" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../802/325802.xml">
loop</link></publication>
</reference_book>
</book>
</wordbook>
</artifact>
</glossary>
</creation>
</product>
</work>
s
}
output the remaining edges
}</p>
<p>

Here, contracting an edge (u,v) means adding a new vertex w, replacing any edge (u,x) or (v,x) with (w,x), and then deleting u and v from G. </p>
<p>

Let  <it>n = |V[G]|</it>. It can be shown that this algorithm outputs a minimum cut with probability at least <it>n-2</it>, thus running it <it>n2log(n)</it> times and taking the smallest output cut, we find a minimum cut with high probability.</p>

</sec>
<sec>
<st>
Derandomization</st>

<p>

It is usually the case that randomized algorithms either are more elegant or require fewer resources than the corresponding <link xlink:type="simple" xlink:href="../951/665951.xml">
deterministic algorithm</link>s. Removing randomization from randomized algorithms to build equivalently powerful deterministic algorithms is an active field of research in computer science. This general method is central in resolving many <link xlink:type="simple" xlink:href="../426/502426.xml">
complexity class</link>es describing randomized computation.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../889/15383889.xml">
Probabilistic analysis of algorithms</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../475/4108475.xml">
Thomas H. Cormen</link></scientist>
, <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../884/1400884.xml">
Charles E. Leiserson</link></scientist>
, <link xlink:type="simple" xlink:href="../057/68057.xml">
Ronald L. Rivest</link>, and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../993/3489993.xml">
Clifford Stein</link></scientist>
. <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../226/3499226.xml">
Introduction to Algorithms</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
</it>, Second Edition. MIT Press and McGraw-Hill, 1990. ISBN 0-262-03293-7. Chapter 5: Probabilistic Analysis and Randomized Algorithms, pp.91&ndash;122.</entry>
<entry level="1" type="bullet">

 Don Fallis. 2000. <weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1093/bjps/51.2.255">
"The Reliability of Randomized Algorithms."</weblink> <it>British Journal for the Philosophy of Science</it> 51:255-71.</entry>
<entry level="1" type="bullet">

 M. Mitzenmacher and E. Upfal. Probability and Computing : Randomized Algorithms and Probabilistic Analysis. Cambridge University Press, New York (NY), 2005.</entry>
<entry level="1" type="bullet">

 R. Motwani and P. Raghavan. Randomized Algorithms. Cambridge University Press, New York (NY), 1995.</entry>
<entry level="1" type="bullet">

 R. Motwani and P. Raghavan. Randomized Algorithms. A survey on Randomized Algorithms. <weblink xlink:type="simple" xlink:href="http://portal.acm.org/citation.cfm?id=234313.234327">
http://portal.acm.org/citation.cfm?id=234313.234327</weblink></entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book"><physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<honoree wordnetid="110183757" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<acquirer wordnetid="109764201" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<recipient wordnetid="109627906" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<laureate wordnetid="110249011" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../428/3509428.xml">
Christos Papadimitriou</link></scholar>
</writer>
</causal_agent>
</academician>
</alumnus>
</laureate>
</communicator>
</associate>
</educator>
</recipient>
</professional>
</adult>
</scientist>
</acquirer>
</colleague>
</intellectual>
</honoree>
</person>
</peer>
</physical_entity>
&#32;(1993). Computational Complexity, 1st edition,&#32;Addison Wesley. ISBN 0-201-53082-1.</cite>&nbsp; Chapter 11: Randomized computation, pp.241&ndash;278.</entry>
<entry level="1" type="bullet">

 M. O. Rabin. (1980), "Probabilistic Algorithm for Testing Primality." <it>Journal of Number Theory</it> 12:128-38.</entry>
</list>
</p>


</sec>
</bdy>
</article>
