<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:44:07[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Bayesian inference</title>
<id>49571</id>
<revision>
<id>240127941</id>
<timestamp>2008-09-22T01:02:01Z</timestamp>
<contributor>
<username>Giftlite</username>
<id>37986</id>
</contributor>
</revision>
<categories>
<category>Statistics articles linked to the portal</category>
<category>Statistical theory</category>
<category>Bayesian statistics</category>
<category>Statistical inference</category>
<category>Statistics articles with navigational template</category>
</categories>
</header>
<bdy>

<b>Bayesian inference</b> is <link xlink:type="simple" xlink:href="../577/27577.xml">
statistical inference</link> in which evidence or observations are used to update or to newly infer the <link xlink:type="simple" xlink:href="../934/22934.xml">
probability</link> that a hypothesis may be true.  The name "Bayesian" comes from the frequent use of <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
 in the inference process.  Bayes' theorem was derived from the work of the Reverend <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../117/149117.xml">
Thomas Bayes</link></scientist>
</person>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>
<sec>
<st>
 Evidence and changing beliefs </st>

<p>

Bayesian inference uses aspects of the <link xlink:type="simple" xlink:href="../833/26833.xml">
scientific method</link>, which involves collecting <link xlink:type="simple" xlink:href="../854/5236854.xml">
evidence</link> that is meant to be consistent or inconsistent with a given <link xlink:type="simple" xlink:href="../281/14281.xml">
hypothesis</link>. As evidence accumulates, the degree of belief in a hypothesis ought to change. With enough evidence, it should become very high or very low. Thus, proponents of Bayesian inference say that it can be used to discriminate between conflicting hypotheses: hypotheses with very high support should be accepted as true and those with very low support should be rejected as false. However, detractors say that this inference method may be biased due to initial beliefs that one holds before any evidence is ever collected. (This is a form of <link xlink:type="simple" xlink:href="../926/173926.xml">
inductive bias</link>).</p>
<p>

Bayesian inference uses a numerical estimate of the degree of belief in a hypothesis before evidence has been observed and calculates a numerical estimate of the degree of belief in the hypothesis after evidence has been observed. (This process is repeated when additional evidence is obtained.) Bayesian inference usually relies on degrees of belief, or subjective probabilities, in the induction process and does not necessarily claim to provide an objective method of induction.  Nonetheless, some Bayesian statisticians believe probabilities can have an objective value and therefore Bayesian inference can provide an objective method of induction.  See <link xlink:type="simple" xlink:href="../833/26833.xml">
scientific method</link>.</p>
<p>

Bayes' theorem adjusts probabilities given new evidence in the following way:</p>
<p>

<indent level="1">

 P(H|E) = \frac{P(E|H)\;P(H)}{P(E)}
</indent>

where</p>
<p>

<list>
<entry level="1" type="bullet">

 H represents a specific hypothesis, which may or may not be some <link xlink:type="simple" xlink:href="../673/226673.xml">
null hypothesis</link>.</entry>
<entry level="1" type="bullet">

 P(H) is called the <it><link xlink:type="simple" xlink:href="../877/472877.xml">
prior probability</link></it> of <math>H</math> that was inferred before new evidence, <math>E</math>, became available.</entry>
<entry level="1" type="bullet">

 P(E|H) is called the <it><link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probability</link></it> of seeing the evidence <math>E</math> if the hypothesis <math>H</math> happens to be true.  It is also called a <it><link xlink:type="simple" xlink:href="../968/44968.xml">
likelihood function</link></it> when it is considered as a function of <math>H</math> for fixed <math>E</math>.</entry>
<entry level="1" type="bullet">

 P(E) is called the <it><link xlink:type="simple" xlink:href="../791/5791.xml">
marginal probability</link></it> of <math>E</math>: the <it>a priori</it> probability of witnessing the new evidence <math>E</math> under all possible hypotheses. It can be calculated as the sum of the product of all probabilities of any complete set of mutually exclusive hypotheses and corresponding conditional probabilities: <math>P(E) = \sum  P(E|H_i)P(H_i)</math>.</entry>
<entry level="1" type="bullet">

 P(H|E) is called the <it><link xlink:type="simple" xlink:href="../672/357672.xml">
posterior probability</link></it> of <math>H</math> given <math>E</math>.</entry>
</list>
</p>
<p>

The factor <math>P(E|H) / P(E)</math> represents the impact that the evidence has on the belief in the hypothesis.  If it is likely that the evidence <math>E</math> would be observed when the hypothesis under consideration is true, but unlikely that <math>E</math> would have been the outcome of the observation, then this factor will be large.  Multiplying the prior probability of the hypothesis by this factor would result in a larger posterior probability of the hypothesis given the evidence.  Conversely, if it is unlikely that the evidence <math>E</math> would be observed if the hypothesis under consideration is true, but <it>a priori</it> likely that <math>E</math> would be observed, then the factor would reduce the posterior probability for <math>H</math>.  Under Bayesian inference, Bayes' theorem therefore measures how much new evidence should alter a belief in a hypothesis.</p>
<p>

Bayesian statisticians argue that even when people have very different prior subjective probabilities, new evidence from repeated observations will tend to bring their posterior subjective probabilities closer together. However, others argue that when people hold widely different prior subjective probabilities their posterior subjective probabilities may never converge even with repeated collection of evidence.  These critics argue that worldviews which are completely different initially can remain completely different over time despite a large accumulation of evidence.</p>
<p>

Multiplying the prior probability <math>P(H)</math> by the factor <math>P(E|H) / P(E)</math> will never yield a probability that is greater than 1, since <math>P(E)</math> is at least as great as <math>P(E \cap H)</math> (where <math>\cap</math> denotes "and"), which equals <math>P(E|H)\,P(H)</math> (see <link xlink:type="simple" xlink:href="../637/879637.xml">
joint probability</link>).</p>
<p>

The probability of <math>E</math> given <math>H</math>, <math>P(E|H)</math>, can be represented as a function of its second argument with its first argument held fixed.  Such a function is called a <link xlink:type="simple" xlink:href="../968/44968.xml">
likelihood function</link>; it is a function of <math>H</math> alone, with <math>E</math> treated as a <link xlink:type="simple" xlink:href="../065/25065.xml">
parameter</link>.  A ratio of two likelihood functions is called a likelihood ratio, <math>\Lambda </math>.  For example,</p>
<p>

<indent level="1">

 \Lambda_E = \frac{L(H|E)}{L(\mathrm{not}\,H|E)} = \frac{P(E|H)}{P(E|\mathrm{not}\,H)} ,
</indent>

where the dependence of <math>\Lambda_E</math> on <math>H</math> is suppressed for simplicity (as <math>E</math> might have been, except we will need to use that parameter below).</p>
<p>

Since <math>H</math> and not-<math>H</math> are mutually exclusive and span all possibilities, the sum previously given for the marginal probability reduces to <math>P(E) = P(E|H)\,P(H)+P(E|\mathrm{not}\,H)\,P(\mathrm{not}\,H) </math>.  As a result, we can rewrite Bayes' theorem as</p>
<p>

<indent level="1">

 P(H|E) = \frac{P(E|H)\,P(H)}{P(E|H)\,P(H)+ P(E|\mathrm{not}\,H)\,P(\mathrm{not}\,H)} = \frac{\Lambda_E P(H)}{\Lambda_E P(H) +P(\mathrm{not}\,H)}.
</indent>

We could then exploit the identity <math>P(\mathrm{not}\,H) = 1 - P(H)</math> to exhibit <math>P(H|E)</math> as a function of just <math>P(H)</math> (and <math>\Lambda_E</math>, which is computed directly from the evidence).</p>
<p>

With two <link xlink:type="simple" xlink:href="../593/27593.xml">
independent</link> pieces of evidence <math>E_1</math> and <math>E_2</math>, Bayesian inference can be applied iteratively.  We could use the first piece of evidence to calculate an initial posterior probability, and then use that posterior probability as a new prior probability to calculate a second posterior probability given the second piece of evidence.  Bayes' theorem applied iteratively yields</p>
<p>

<indent level="1">

 P(H|E_1 \cap E_2) = \frac{P(E_2|H)\;P(E_1|H)\,P(H)}{P(E_2)\;P(E_1)}
</indent>

Using likelihood ratios, we find that</p>
<p>

<indent level="1">

 P(H|E_1 \cap E_2) = \frac{\Lambda_1 \Lambda_2 P(H)}{[\Lambda_1 P(H) + P(\mathrm{not}\,H)]\;[\Lambda_2 P(H) + P(\mathrm{not}\,H)]} ,
</indent>

This iteration of Bayesian inference could be extended with more independent pieces of evidence.</p>
<p>

Bayesian inference is used to calculate probabilities for decision making under uncertainty.  Besides the probabilities, a <link xlink:type="simple" xlink:href="../137/442137.xml">
loss function</link> should be evaluated to take into account the relative impact of the alternatives.</p>

</sec>
<sec>
<st>
 Simple examples of Bayesian inference </st>


<ss1>
<st>
 From which bowl is the cookie? </st>

<p>

To illustrate, suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?</p>
<p>

Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. Let <math>H_1</math> correspond to bowl #1, and <math>H_2</math> to bowl #2.
It is given that the bowls are identical from Fred's point of view, thus <math>P(H_1)=P(H_2)</math>, and the two must add up to 1, so both are equal to 0.5.
The event <math>E</math> is the observation of a plain cookie. From the contents of the bowls, we know that <math>P(E|H_1) = 30/40 = 0.75</math> and <math>P(E|H_2) = 20/40 = 0.5</math>. Bayes' formula then yields</p>
<p>

<indent level="1">

 
</indent>

\begin{matrix} P(H_1|E) &amp;=&amp; \frac{P(E|H_1)\,P(H_1)}{P(E|H_1)\,P(H_1)\;+\;P(E|H_2)\,P(H_2)} \\  \\  \ &amp; =&amp; \frac{0.75 \times 0.5}{0.75 \times 0.5 + 0.5 \times 0.5} \\  \\  \ &amp; =&amp; 0.6 \end{matrix}</p>
<p>

Before we observed the cookie, the probability we assigned for Fred having chosen bowl #1 was the prior probability, <math>P(H_1)</math>, which was 0.5. After observing the cookie, we must revise the probability to <math>P(H_1|E)</math>, which is 0.6.</p>

</ss1>
<ss1>
<st>
 False positives in a medical test </st>

<p>

<link xlink:type="simple" xlink:href="../877/5657877.xml">
False positive</link>s result when a test falsely or incorrectly reports a positive result.  For example, a medical test for a <link xlink:type="simple" xlink:href="../072/8072.xml">
disease</link> may return a positive result indicating that patient has a disease even if the patient does not have the disease.  We can use Bayes' theorem to determine the probability that a positive result is in fact a false positive.  We find that if a disease is rare, then the majority of positive results may be false positives, even if the test is accurate.</p>
<p>

Suppose that a test for a disease generates the following results:</p>
<p>

<list>
<entry level="1" type="bullet">

 If a tested patient has the disease, the test returns a positive result 99% of the time, or with probability 0.99</entry>
<entry level="1" type="bullet">

 If a tested patient does not have the disease, the test returns a positive result 5% of the time, or with probability 0.05.</entry>
</list>
</p>
<p>

Naively, one might think that only 5% of positive test results are false, but that is quite wrong, as we shall see.</p>
<p>

Suppose that only 0.1% of the population has that disease, so that a randomly selected patient has a 0.001 prior probability of having the disease.</p>
<p>

We can use Bayes' theorem to calculate the probability that a positive test result is a false positive.</p>
<p>

Let <it>A</it> represent the condition in which the patient has the disease, and <it>B</it> represent the evidence of a positive test result.  Then, probability that the patient actually has the disease given the positive test result is</p>
<p>

<indent level="1">

 \begin{matrix} P(A | B) &amp;=&amp; \frac{P(B | A) P(A)}{P(B | A)P(A) + P(B |\mathrm{not}\,A)P(\mathrm{not}\,A)} \\ \\
</indent>

&amp;= &amp;\frac{0.99\times 0.001}{0.99 \times 0.001 + 0.05\times 0.999}  \\ ~\\ &amp;\approx &amp;0.019 .\end{matrix}</p>
<p>

and hence the probability that a positive result is a false positive is about <math>1-0.019 = 0.98</math>, or 98%.</p>
<p>

Despite the apparent high accuracy of the test, the incidence of the disease is so low that the vast majority of patients who test positive do not have the disease. Nonetheless, the fraction of patients who test positive who do have the disease (.019) is 19 times the fraction of people who have not yet taken the test who have the disease (.001). Thus the test is not useless, and re-testing may improve the reliability of the result.</p>
<p>

In order to reduce the problem of false positives, a test should be very accurate in reporting a <it>negative</it> result when the patient does not have the disease.  If the test reported a negative result in patients without the disease with probability 0.999, then</p>
<p>

<indent level="1">

 P(A|B) = \frac{0.99\times 0.001}{0.99 \times 0.001 + 0.001\times 0.999} \approx 0.5 ,
</indent>

so that <math>1-0.5 = 0.5</math> now is the probability of a false positive.</p>
<p>

On the other hand, <link xlink:type="simple" xlink:href="../877/5657877.xml">
false negative</link>s result when a test falsely or incorrectly reports a negative result.  For example, a medical test for a <link xlink:type="simple" xlink:href="../072/8072.xml">
disease</link> may return a negative result indicating that patient does not have a disease even though the patient actually has the disease.  We can also use Bayes' theorem to calculate the probability of a false negative.  In the first example above,</p>
<p>

<indent level="1">

 \begin{matrix} P(A |\mathrm{not}\,B) &amp;=&amp; \frac{P(\mathrm{not}\,B | A) P(A)}{P(\mathrm{not}\,B | A)P(A) + P(\mathrm{not}\,B |\mathrm{not}\,A)P(\mathrm{not}\,A)} \\ \\
</indent>

&amp;= &amp;\frac{0.01\times 0.001}{0.01 \times 0.001 + 0.95\times 0.999}\, ,\\ ~\\ &amp;\approx &amp;0.0000105\, .\end{matrix}</p>
<p>

The probability that a negative result is a false negative is about 0.0000105 or 0.00105%.  When a disease is rare, false negatives will not be a major problem with the test.</p>
<p>

But if 60% of the population had the disease, then the probability of a false negative would be greater.  With the above test, the probability of a false negative would be</p>
<p>

<indent level="1">

 \begin{matrix} P(A |\mathrm{not}\,B) &amp;=&amp; \frac{P(\mathrm{not}\,B | A) P(A)}{P(\mathrm{not}\,B | A)P(A) + P(\mathrm{not}\,B |\mathrm{not}\,A)P(\mathrm{not}\,A)} \\ \\
</indent>

&amp;= &amp;\frac{0.01\times 0.6}{0.01 \times 0.6 + 0.95\times 0.4}\, ,\\ ~\\ &amp;\approx &amp;0.0155\, .\end{matrix}</p>
<p>

The probability that a negative result is a false negative rises to 0.0155 or 1.55%.</p>

</ss1>
<ss1>
<st>
 In the courtroom </st>

<p>

Bayesian inference can be used in a court setting by an individual juror to coherently accumulate the evidence for and against the guilt of the defendant, and to see whether, in totality, it meets their personal threshold for 'beyond a reasonable doubt'.</p>
<p>

<list>
<entry level="1" type="bullet">

 Let <math>G</math> denote the event that the defendant is guilty.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Let <math>E</math> denote the event that the defendant's DNA matches DNA found at the crime scene.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Let <math>P(E|G)</math> denote the probability of seeing event <math>E</math> if the defendant actually is guilty. (Usually this would be taken to be near unity.)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Let <math>P(G|E)</math> denote the probability that the defendant is guilty assuming the DNA match (event <math>E</math>).</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Let <math>P(G)</math> denote the juror's personal estimate of the probability that the defendant is guilty, based on the evidence <it>other than</it> the DNA match. This could be based on his responses under questioning, or previously presented evidence.</entry>
</list>
</p>
<p>

Bayesian inference tells us that if we can assign a probability p(G) to the defendant's guilt before we take the DNA evidence into account, then we can revise this probability to the conditional probability <math>P(G | E)</math>, since</p>
<p>

<indent level="1">

 P(G | E) = \frac{P(G) P(E | G)}{P(E)}.
</indent>

Suppose, on the basis of other evidence, a juror decides that there is a 30% chance that the defendant is guilty. Suppose also that the forensic testimony was that the probability that a person chosen at random would have DNA that matched that at the crime scene is 1 in a million, or 10âˆ’6.</p>
<p>

The event  E can occur in two ways. Either the defendant is guilty (with prior probability 0.3) and thus his DNA is present with probability 1, or he is innocent (with prior probability 0.7) and he is unlucky enough to be one of the 1 in a million matching people.</p>
<p>

Thus the juror could coherently revise his opinion to take into account the <link xlink:type="simple" xlink:href="../290/44290.xml">
DNA evidence</link> as follows:</p>
<p>

<indent level="1">

 P(G | E) = (0.3 \times 1.0) /(0.3 \times 1.0 + 0.7 \times 10^{-6}) = 0.99999766667.
</indent>

The benefit of adopting a Bayesian approach is that it gives the juror a formal mechanism for combining the evidence presented. The approach can be applied successively to all the pieces of evidence presented in court, with the posterior from one stage becoming the prior for the next.</p>
<p>

The juror would still have to have a prior estimate for the guilt probability before the first piece of evidence is considered. It has been suggested that this could reasonably be the guilt probability of a random person taken from the qualifying population. Thus, for a crime known to have been committed by an adult male living in a town containing 50,000 adult males, the appropriate initial prior probability might be 1/50,000.</p>
<p>

<image location="right" width="256px" src="Ebits2c.png" type="thumb">
<caption>

Adding up evidence.
</caption>
</image>
</p>
<p>

For the purpose of explaining Bayes' theorem to jurors, it will usually be appropriate to give it in the form of <link xlink:type="simple" xlink:href="../069/172069.xml">
betting odds</link> rather than probabilities, as these are more widely understood. In this form Bayes' theorem states that</p>
<p>

<indent level="1">

 Posterior odds = prior odds x <link xlink:type="simple" xlink:href="../552/824552.xml">
Bayes factor</link>
</indent>

In the example above, the juror who has a prior probability of 0.3 for the defendant being guilty would now express that in the form of odds of 3:7 in favour of the defendant being guilty, the Bayes factor is one million, and the resulting posterior odds are 3 million to 7 or about 429,000 to one in favour of guilt.</p>
<p>

A <link xlink:type="simple" xlink:href="../853/5642853.xml">
logarithmic approach</link> which replaces multiplication with addition and reduces the range of the numbers involved might be easier for a jury to handle.  This approach, developed by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Alan Turing</link></scientist>
</person>
 during <military_action wordnetid="100952963" confidence="0.8">
<group_action wordnetid="101080366" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<conflict wordnetid="100958896" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<war wordnetid="100973077" confidence="0.8">
<link xlink:type="simple" xlink:href="../927/32927.xml">
World War II</link></war>
</psychological_feature>
</act>
</conflict>
</event>
</group_action>
</military_action>
 and later promoted by <person wordnetid="100007846" confidence="0.9508927676800064">
<statistician wordnetid="110653238" confidence="0.9173553029164789">
<link xlink:type="simple" xlink:href="../404/404404.xml">
I. J. Good</link></statistician>
</person>
 and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../971/166971.xml">
E. T. Jaynes</link></scientist>
 among others, amounts to the use of <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link>.</p>
<p>

In the United Kingdom, Bayes' theorem was explained to the jury in the odds form by a statistician <link xlink:type="simple" xlink:href="../115/10115.xml">
expert witness</link> in the rape case of <link xlink:type="simple" xlink:href="../626/1699626.xml">
Regina versus Denis John Adams</link>. A conviction was secured but the case went to Appeal, as no means of accumulating evidence had been provided for those jurors who did not want to use Bayes' theorem. The Court of Appeal upheld the conviction, but also gave their opinion that "To introduce Bayes' Theorem, or any similar method, into a criminal trial plunges the Jury into inappropriate and unnecessary realms of theory and complexity, deflecting them from their proper task." No further appeal was allowed and the issue of Bayesian assessment of forensic DNA data remains controversial.</p>
<p>

Gardner-Medwin argues that the criterion on which a verdict in a criminal trial should be based is <it>not</it> the probability of guilt, but rather the <it>probability of the evidence, given that the defendant is innocent</it> (akin to a <link xlink:type="simple" xlink:href="../869/10869.xml">
frequentist</link> <link xlink:type="simple" xlink:href="../994/554994.xml">
p-value</link>). He argues that if the posterior probability of guilt is to be computed by Bayes' theorem, the prior probability of guilt must be known. This will depend on the incidence of the crime, which is an unusual piece of evidence to consider in a criminal trial. Consider the following three propositions:</p>
<p>

A: The known facts and testimony could have arisen if the defendant is guilty,</p>
<p>

B: The known facts and testimony could have arisen if the defendant is innocent,</p>
<p>

C: The defendant is guilty.</p>
<p>

Gardner-Medwin argues that the jury should believe both A and not-B in order to convict. A and not-B implies the truth of C, but the reverse is not true. It is possible that B and C are both true, but in this case he argues that a jury should acquit, even though they know that they will be letting some guilty people go free. See also <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<contradiction wordnetid="107206887" confidence="0.8">
<paradox wordnetid="106724559" confidence="0.8">
<falsehood wordnetid="106756407" confidence="0.8">
<link xlink:type="simple" xlink:href="../049/5915049.xml">
Lindley's paradox</link></falsehood>
</paradox>
</contradiction>
</message>
</statement>
.</p>
<p>

Other court cases in which probabilistic arguments played some role were the <link xlink:type="simple" xlink:href="../614/902614.xml">
Howland will forgery trial</link>, the <belief wordnetid="105941423" confidence="0.8">
<conviction wordnetid="105942888" confidence="0.8">
<link xlink:type="simple" xlink:href="../132/319132.xml">
Sally Clark</link></conviction>
</belief>
 case, and the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<nurse wordnetid="110366966" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<health_professional wordnetid="110165109" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<link xlink:type="simple" xlink:href="../895/7802895.xml">
Lucia de Berk</link></professional>
</adult>
</health_professional>
</causal_agent>
</nurse>
</person>
</physical_entity>
 case.</p>

</ss1>
<ss1>
<st>
 Search theory </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../587/1510587.xml">
Bayesian search theory</link></it>
</indent>

In May 1968 the US nuclear submarine <career wordnetid="100583246" confidence="0.9508927676800064">
<image wordnetid="105928118" confidence="0.9508927676800064">
<feature wordnetid="105849789" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../216/417216.xml">
<it>Scorpion</it> (SSN-589)</link></feature>
</image>
</career>
 failed to arrive as expected at her home port of <village wordnetid="108672738" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../898/57898.xml">
Norfolk, Virginia</link></village>
. The US Navy was convinced that the vessel had been lost off the Eastern seaboard but an extensive search failed to discover the wreck. The US Navy's deep water expert, <link xlink:type="simple" xlink:href="../210/1940210.xml">
John Craven USN</link>, believed that it was elsewhere and he organised a search south west of the <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../226/3226.xml">
Azores</link></country>
 based on a controversial approximate triangulation by hydrophones. He was allocated only a single ship, the <career wordnetid="100583246" confidence="0.9508927676800064">
<image wordnetid="105928118" confidence="0.9508927676800064">
<feature wordnetid="105849789" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../315/1718315.xml">
<it>Mizar''</it></link></feature>
</image>
</career>
, and he took advice from a firm of consultant mathematicians in order to maximise his resources. A Bayesian search methodology was adopted. Experienced submarine commanders were interviewed to construct hypotheses about what could have caused the loss of the <it>Scorpion</it>.</p>
<p>

The sea area was divided up into grid squares and a probability assigned to each square, under each of the hypotheses, to give a number of probability grids, one for each hypothesis. These were then added together to produce an overall probability grid. The probability attached to each square was then the probability that the wreck was in that square. A second grid was constructed with probabilities that represented the probability of successfully finding the wreck if that square were to be searched and the wreck were to be actually there. This was a known function of water depth. The result of combining this grid with the previous grid is a grid which gives the probability of finding the wreck in each grid square of the sea if it were to be searched.</p>
<p>

This sea grid was systematically searched in a manner which started with the high probability regions first and worked down to the low probability regions last. Each time a grid square was searched and found to be empty its probability was reassessed using <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
. This then forced the probabilities of all the other grid squares to be reassessed (upwards), also by Bayes' theorem. The use of this approach was a major computational challenge for the time but it was eventually successful and the <it>Scorpion</it> was found about 740 kilometers southwest of the <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../226/3226.xml">
Azores</link></country>
 in October of that year. </p>
<p>

Suppose a grid square has a probability <it>p</it> of containing the wreck and that the probability of successfully detecting the wreck if it is there is <it>q</it>. If the square is searched and no wreck is found, then, by Bayes' theorem, the revised probability of the wreck being in the square is given by</p>
<p>

<indent level="1">

   p' = \frac{p(1-q)}{(1-p)+p(1-q)}.
</indent>

</p>
</ss1>
</sec>
<sec>
<st>
 More mathematical examples </st>

<ss1>
<st>
 Naive Bayes classifier </st>

<p>

See <link xlink:type="simple" xlink:href="../339/87339.xml">
naive Bayes classifier</link>.</p>

</ss1>
<ss1>
<st>
 Posterior distribution of the binomial parameter </st>

<p>

In this example we consider the computation of the posterior distribution for the binomial parameter.
This is the same problem considered by Bayes  in Proposition 9 of his essay.</p>
<p>

We are given <it>m</it> observed successes and <it>n</it> observed failures in a binomial experiment.
The experiment may be tossing a coin, drawing a ball from an urn, or asking someone their opinion, among many other possibilities.
What we know about the parameter (let's call it <it>a</it>) is stated as the prior distribution, <it>p</it>(<it>a</it>).</p>
<p>

For a given value of <it>a</it>,
the probability of <it>m</it> successes in <it>m</it>+<it>n</it> trials is</p>
<p>

<indent level="1">

  p(m,n|a) = \begin{pmatrix} n+m \\ m \end{pmatrix} a^m (1-a)^n. 
</indent>

Since <it>m</it> and <it>n</it> are fixed, and <it>a</it> is unknown,
this is a likelihood function for <it>a</it>.
From the continuous form of the <link xlink:type="simple" xlink:href="../383/312383.xml">
law of total probability</link> we have</p>
<p>

<indent level="1">

  p(a|m,n) = \frac{p(m,n|a)\,p(a)}{\int_0^1 p(m,n|a)\,p(a)\,da}
</indent>

= \frac{\begin{pmatrix} n+m \\ m \end{pmatrix} a^m (1-a)^n\,p(a)}
{\int_0^1 \begin{pmatrix} n+m \\ m \end{pmatrix} a^m (1-a)^n\,p(a)\,da}.</p>

<p>

For some special choices of the prior distribution <it>p</it>(<it>a</it>),
the integral can be solved and the posterior takes a convenient form.
In particular,
if <it>p</it>(<it>a</it>) is a <link xlink:type="simple" xlink:href="../074/207074.xml">
beta distribution</link> with parameters <it>m</it>0 and <it>n</it>0,
then the posterior is also a beta distribution with parameters <it>m</it>+<it>m</it>0 and <it>n</it>+<it>n</it>0.</p>
<p>

A <it><link xlink:type="simple" xlink:href="../412/846412.xml">
conjugate prior</link></it> is a prior distribution, such as the beta distribution in the above example, which has the property that the posterior is the same type of distribution.</p>
<p>

What is "Bayesian" about Proposition 9 is that Bayes presented it as a probability for the parameter <it>a</it>. That is, not only can one compute probabilities for experimental outcomes, but also for the parameter which governs them, and the same algebra is used to make inferences of either kind. Interestingly, Bayes actually states his question in a way that might make the idea of assigning a probability distribution to a parameter palatable to a <link xlink:type="simple" xlink:href="../869/10869.xml">
frequentist</link>. He supposes that a billiard ball is thrown at random onto a billiard table, and that the probabilities <it>p</it> and <it>q</it> are the probabilities that subsequent billiard balls will fall above or below the first ball. By making the binomial parameter <it>a</it> depend on a random event, he cleverly escapes a philosophical quagmire that was an issue he most likely was not even aware of.</p>

</ss1>
<ss1>
<st>
 Computer applications </st>

<p>

Bayesian inference has applications in <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> and <link xlink:type="simple" xlink:href="../136/10136.xml">
expert system</link>s.  Bayesian inference techniques have been a fundamental part of computerized <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link> techniques since the late 1950s. There is also an ever growing connection between Bayesian methods and simulation-based <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo</link></method>
</know-how>
</technique>
 techniques since complex models cannot be processed in closed form by a Bayesian analysis, while the <link xlink:type="simple" xlink:href="../298/447298.xml">
graphical model</link> structure inherent to statistical models, may allow for efficient simulation algorithms like the <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../709/509709.xml">
Gibbs sampling</link></method>
</know-how>
 and other <link xlink:type="simple" xlink:href="../107/56107.xml">
Metropolis-Hastings algorithm</link> schemes. Recently Bayesian inference has gained popularity amongst the <link xlink:type="simple" xlink:href="../962/23962.xml">
phylogenetics</link> community for these reasons; applications such as <weblink xlink:type="simple" xlink:href="http://beast.bio.ed.ac.uk/">
BEAST</weblink>,  <weblink xlink:type="simple" xlink:href="http://mrbayes.csit.fsu.edu/">
MrBayes</weblink> and <weblink xlink:type="simple" xlink:href="http://www.bmnh.org/~pf/p4.html">
P4</weblink> allow many demographic and evolutionary parameters to be estimated simultaneously.</p>
<p>

As applied to <link xlink:type="simple" xlink:href="../244/1579244.xml">
statistical classification</link>, Bayesian inference has been used in recent years to develop algorithms for identifying unsolicited bulk <link xlink:type="simple" xlink:href="../847/459847.xml">
e-mail spam</link>. Applications which make use of Bayesian inference for spam filtering include <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../868/3531868.xml">
DSPAM</link></software>
, <link xlink:type="simple" xlink:href="../313/233313.xml">
Bogofilter</link>, <link xlink:type="simple" xlink:href="../137/274137.xml">
SpamAssassin</link>, <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../047/3085047.xml">
SpamBayes</link></software>
, and <link xlink:type="simple" xlink:href="../225/20225.xml">
Mozilla</link>. Spam classification is treated in more detail in the article on the <b><link xlink:type="simple" xlink:href="../339/87339.xml">
naive Bayes classifier</link></b>.</p>
<p>

In some applications <link xlink:type="simple" xlink:href="../180/49180.xml">
fuzzy logic</link> is an alternative to Bayesian inference.  Fuzzy logic and Bayesian inference, however, are mathematically and semantically not compatible: You cannot, in general, understand the <it>degree of truth</it> in fuzzy logic as probability and vice versa.</p>

</ss1>
</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
Douglas Hubbard "How to Measure Anything: Finding the Value of Intangibles in Business" pg. 46, John Wiley &amp; Sons, 2007</entry>
</reflist>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">
On-line textbook: Information Theory, Inference, and Learning Algorithms</weblink>, by David MacKay, has chapters on Bayesian methods, including examples; arguments in favour of Bayesian methods (in the style of <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../971/166971.xml">
Edwin Jaynes</link></scientist>
); modern <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo method</link></method>
</know-how>
</technique>
s, <link xlink:type="simple" xlink:href="../745/12156745.xml">
message-passing method</link>s, and <link xlink:type="simple" xlink:href="../882/171882.xml">
variational methods</link>; and examples illustrating the connections between Bayesian inference and <link xlink:type="simple" xlink:href="../013/8013.xml">
data compression</link>.</entry>
<entry level="1" type="bullet">

 Berger, J.O. (1999) Statistical Decision Theory and Bayesian Analysis. Second Edition. Springer Verlag, New York. ISBN 0-387-96098-8 and also ISBN 3-540-96098-8.</entry>
<entry level="1" type="bullet">

 Bolstad, William M. (2004) Introduction to Bayesian Statistics, John Wiley ISBN 0-471-27020-2</entry>
<entry level="1" type="bullet">

 Bretthorst, G. Larry, 1988, <weblink xlink:type="simple" xlink:href="http://bayes.wustl.edu/glb/book.pdf">
<it>Bayesian Spectrum Analysis and Parameter Estimation''</it></weblink> in Lecture Notes in Statistics, 48, Springer-Verlag, New York, New York</entry>
<entry level="1" type="bullet">

 Carlin, B.P. and Louis, T.A. (2008) Bayesian Methods for Data Analysis, Third Edition. Chapman &amp; Hall/CRC, Boca Raton, Florida. <weblink xlink:type="simple" xlink:href="http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C6978">
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C6978</weblink> ISBN 1-58488-697-8.</entry>
<entry level="1" type="bullet">

 Dawid, A.P. and Mortera, J. (1996) Coherent analysis of forensic identification evidence. <periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../143/13327143.xml">
Journal of the Royal Statistical Society</link></periodical>
, Series B, 58,425-443.</entry>
<entry level="1" type="bullet">

 Foreman, L.A; Smith, A.F.M. and Evett, I.W. (1997). Bayesian analysis of deoxyribonucleic acid profiling data in forensic identification applications (with discussion). Journal of the Royal  Statistical Society, Series A, 160, 429-469.</entry>
<entry level="1" type="bullet">

 Gardner-Medwin, A. <it>What probability should the jury address?</it>. Significance. Volume 2, Issue 1, March 2005</entry>
<entry level="1" type="bullet">

 Gelman, A., Carlin, J., Stern, H., and Rubin, D.B. (2003). Bayesian Data Analysis. Second Edition. Chapman &amp; Hall/CRC, Boca Raton, Florida. <weblink xlink:type="simple" xlink:href="http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C388X">
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C388X</weblink> ISBN 1-58488-388-X.</entry>
<entry level="1" type="bullet">

 Gelman, A. and Meng, X.L. (2004). Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives: an essential journey with Donald Rubin's statistical family. John Wiley &amp; Sons, Chichester, UK. ISBN 0-470-09043-X</entry>
<entry level="1" type="bullet">

 Giffin, A. and Caticha, A. (2007) <weblink xlink:type="simple" xlink:href="http://arxiv.org/abs/0708.1593">
<it>Updating Probabilities with Data and Moments''</it></weblink></entry>
<entry level="1" type="bullet">

 Jaynes, E.T. (1998) <weblink xlink:type="simple" xlink:href="http://www-biba.inrialpes.fr/Jaynes/prob.html">
<it>Probability Theory: The Logic of Science''</it></weblink>.</entry>
<entry level="1" type="bullet">

 Lee, Peter M. Bayesian Statistics: An Introduction. Second Edition. (1997). ISBN 0-340-67785-6.</entry>
<entry level="1" type="bullet">

 Loredo, Thomas J. (1992) "Promise of Bayesian Inference in Astrophysics" in <it>Statistical Challenges in Modern Astronomy</it>, ed. Feigelson &amp; Babu.</entry>
<entry level="1" type="bullet">

 O'Hagan, A. and Forster, J. (2003) Kendall's Advanced Theory of Statistics, Volume 2B: Bayesian Inference. Arnold, New York. ISBN 0-340-52922-9.</entry>
<entry level="1" type="bullet">

 Pearl, J. (1988) <it>Probabilistic Reasoning in Intelligent Systems,</it> San Mateo, CA: Morgan Kaufmann.</entry>
<entry level="1" type="bullet">

 Robert, C.P. (2001) The Bayesian Choice. Springer Verlag, New York.</entry>
<entry level="1" type="bullet">

 Robertson, B. and Vignaux, G.A. (1995) Interpreting Evidence: Evaluating Forensic Science in the Courtroom. John Wiley and Sons. Chichester.</entry>
<entry level="1" type="bullet">

 Winkler, Robert L, <it>Introduction to Bayesian Inference and Decision, 2nd Edition</it> (2003) Probabilistic. ISBN 0-9647938-4-9</entry>
</list>
</p>

</sec>
<sec>
<st>
 See also </st>

<p>

<table style="background:#f9f9f9; font-size:85%; line-height:110%; ">
<row>
<col>
 <image width="75x28px" src="Fisher_iris_versicolor_sepalwidth.svg">
</image>
</col>
<col style="padding:0 0.2em;">
 <b><it>
Statistics&#32;portal</it></b></col>
</row>
</table>
</p>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../329/331329.xml">
Bayesian model comparison</link></entry>
<entry level="1" type="bullet">

 <representation wordnetid="105926676" confidence="0.8">
<interpretation wordnetid="105928513" confidence="0.8">
<link xlink:type="simple" xlink:href="../890/4890.xml">
Bayesian probability</link></interpretation>
</representation>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes theorem</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../815/6978815.xml">
Bayesian estimation</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../957/3595957.xml">
Bayesian filtering</link></entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian network</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../552/824552.xml">
Bayes factor</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../547/3332547.xml">
Hierarchical Bayes model</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../577/27577.xml">
Inferential statistics</link></entry>
<entry level="1" type="bullet">

 <system wordnetid="108435388" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<network wordnetid="108434259" confidence="0.8">
<link xlink:type="simple" xlink:href="../259/1194259.xml">
Influence diagram</link></network>
</group>
</system>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../773/14773.xml">
Information theory</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../797/36797.xml">
Occam's Razor</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../329/1690329.xml">
Cromwell's rule</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../148/50148.xml">
Prosecutor's fallacy</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../210/302210.xml">
Minimum message length</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../325/331325.xml">
Minimum description length</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../026/477026.xml">
Gaussian process regression</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../758/3015758.xml">
Maximum entropy thermodynamics</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../879/708879.xml#xpointer(//*[./st=%22Bayesian+statistics%22])">
Important publications in Bayesian statistics</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../337/1728337.xml">
The Wisdom of Crowds</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../391/37391.xml">
Raven paradox</link></entry>
</list>
</p>


</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.sciam.com/askexpert_question.cfm?articleID=3A86E7BA-E7F2-99DF-3F475245D8E9C780">
Scientific American essay</weblink> on Bayesian inference and the probability of God's existence by <weblink xlink:type="simple" xlink:href="http://www.columbia.edu/~chw2/">
Chris Wiggins</weblink>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dcs.qmw.ac.uk/%7Enorman/BBNs/BBNs.htm">
A nice on-line introductory tutorial to Bayesian probability</weblink>from Queen Mary University of London</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://yudkowsky.net/bayes/bayes.html">
An Intuitive Explanation of Bayesian Reasoning</weblink> Bayes' Theorem for the curious and bewildered; an excruciatingly gentle introduction by <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../858/65858.xml">
Eliezer Yudkowsky</link></scientist>
</person>
</entry>
<entry level="1" type="bullet">

 Paul Graham. <weblink xlink:type="simple" xlink:href="http://www.paulgraham.com/spam.html">
"A Plan for Spam"</weblink> <it>(exposition of a popular approach for spam classification)''</it></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.mcs.vuw.ac.nz/~vignaux/docs/Adams_NLJ.html">
Commentary on Regina versus Adams</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://webuser.bus.umich.edu/plenk/downloads.htm">
Mathematical notes on Bayesian statistics and Markov chain Monte Carlo</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.thebroth.com/blog/118/bayesian-rating">
Bayesian Rating/Ranking</weblink> How to implement Bayes' Theorem for online rating and ranking systems</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://cocosci.berkeley.edu/tom/bayes.html">
Bayesian reading list</weblink>, categorized and annotated. Designed for cognitive science; maintained by <weblink xlink:type="simple" xlink:href="http://psychology.berkeley.edu/faculty/profiles/tgriffiths.html">
Tom Griffiths</weblink>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.eucognition.org/wiki/index.php?title=Bayesian_Multisensory_Perception">
http://www.eucognition.org/wiki/index.php?title=Bayesian_Multisensory_Perception</weblink> A short article on Baysian Multisensory Perception</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.eucognition.org/wiki/index.php?title=Bayesian_Probabilistic_Learning_in_Robots">
http://www.eucognition.org/wiki/index.php?title=Bayesian_Probabilistic_Learning_in_Robots</weblink>  Bayesian probabilistic learning in robots</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://plato.stanford.edu/entries/logic-inductive/">
Stanford Encyclopedia of Philosophy: Inductive Logic</weblink> a comprehensive Bayesian treatment of Inductive Logic and Confirmation Theory</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://faculty-staff.ou.edu/H/James.A.Hawthorne-1/Hawthorne%20--%20Confirmation%20Theory.pdf">
Confirmation Theory</weblink>  An extensive presentation of Bayesian Confirmation Theory</entry>
</list>
</p>

<p>

<table style=";" class="navbox" cellspacing="0">
<row>
<col style="padding:2px;">
<table style="width:100%;background:transparent;color:inherit;;" class="nowraplinks collapsible autocollapse " cellspacing="0">
<row>
<header colspan="2" style=";" class="navbox-title">
<link xlink:type="simple" xlink:href="../685/26685.xml">
Statistics</link></header>
</row>
<row style="height:2px;">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../541/9541.xml">
Design of experiments</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../585/27585.xml">
Population</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../361/160361.xml">
Sampling</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../596/27596.xml">
Stratified sampling</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../262/10306262.xml">
Replication</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../339/1822339.xml">
Blocking</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../839/1776839.xml">
Sample size estimation</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../673/226673.xml">
Null hypothesis</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../892/645892.xml">
Alternative hypothesis</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../877/5657877.xml">
Type I and type II errors</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../695/238695.xml">
Statistical power</link>&nbsp;&amp;bull; <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../276/437276.xml">
Effect size</link></method>
</know-how>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../187/8187.xml">
Descriptive statistics</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<table style="width:100%;;;;" class="nowraplinks  navbox-subgroup" cellspacing="0">
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<structure wordnetid="105726345" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../792/5792.xml">
Continuous  data</link></kind>
</distribution>
</type>
</arrangement>
</category>
</concept>
</idea>
</structure>
</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<table style="width:100%;;;;" class="nowraplinks  navbox-subgroup" cellspacing="0">
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../516/17516.xml">
Location</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../192/19192.xml">
Mean</link> (<link xlink:type="simple" xlink:href="../612/612.xml">
Arithmetic</link>, <link xlink:type="simple" xlink:href="../046/13046.xml">
Geometric</link>, <link xlink:type="simple" xlink:href="../463/14463.xml">
Harmonic</link>)&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../837/18837.xml">
Median</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../127/1432127.xml">
Mode</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../589/27589.xml">
Dispersion</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../588/27588.xml">
Range</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../590/27590.xml">
Standard deviation</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../687/1012687.xml">
Coefficient of variation</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../907/354907.xml">
Percentile</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<link xlink:type="simple" xlink:href="../684/368684.xml">
Moments</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../344/32344.xml">
Variance</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../212/28212.xml">
Skewness</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../848/16848.xml">
Kurtosis</link></col>
</row>
</table>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";padding-left:0em;padding-right:0em;;" class="navbox-group">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../188/8188.xml">
Categorical data</link></kind>
</type>
</category>
</concept>
</idea>
</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../019/4839019.xml">
Frequency</link>&nbsp;&amp;bull; <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../515/935515.xml">
Contingency table</link></datum>
</information>
</col>
</row>
</table>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../577/27577.xml">
Inferential statistics</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian inference</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../284/30284.xml">
Hypothesis testing</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../995/160995.xml">
Significance</link>&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../213/332213.xml">
Z-test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../080/536080.xml">
Student's t-test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<information wordnetid="105816287" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../680/226680.xml">
Chi-square test</link></higher_cognitive_process>
</trial>
</datum>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</information>
</process>
&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../976/318976.xml">
F-test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../994/554994.xml">
P-value</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../381/160381.xml">
Interval estimation</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../806/140806.xml">
Maximum likelihood</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../533/19456533.xml">
Minimum distance</link>&nbsp;&amp;bull; <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../329/62329.xml">
Meta-analysis</link></method>
</know-how>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../911/280911.xml">
Confidence interval</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../259/419259.xml">
Survival analysis</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../178/4649178.xml">
Survival function</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../650/3168650.xml">
Kaplan-Meier</link>&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../026/11871026.xml">
Logrank test</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../960/1336960.xml">
Failure rate</link>&nbsp;&amp;bull; <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../267/5352267.xml">
Proportional hazards models</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../057/157057.xml">
Correlation</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../999/3105999.xml">
Confounding variable</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../708/221708.xml">
Pearson product-moment correlation coefficient</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../627/3316627.xml">
Rank correlation</link> (<process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../623/235623.xml">
Spearman's rank correlation coefficient</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
, <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../830/7287830.xml">
Kendall tau rank correlation coefficient</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
)</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../904/17904.xml">
Linear model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../698/877698.xml">
General linear model</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../122/747122.xml">
Generalized linear model</link>&nbsp;&amp;bull; <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../634/634.xml">
Analysis of variance</link></higher_cognitive_process>
</trial>
</experiment>
</problem_solving>
</thinking>
</inquiry>
</process>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../926/404926.xml">
Analysis of covariance</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../997/826997.xml">
Regression analysis</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../903/17903.xml">
Linear regression</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../012/1045012.xml">
Nonlinear regression</link>&nbsp;&amp;bull; <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../631/226631.xml">
Logistic regression</link></datum>
</information>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../463/15934463.xml">
Statistical graphics</link></visual_communication>
</chart>
</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../311/393311.xml">
Bar chart</link>&nbsp;&amp;bull; <chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../166/14306166.xml">
Biplot</link></visual_communication>
</chart>
&nbsp;&amp;bull; <chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../960/160960.xml">
Box plot</link></visual_communication>
</chart>
&nbsp;&amp;bull; <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<tool wordnetid="104451818" confidence="0.8">
<chart wordnetid="106999802" confidence="0.8">
<implement wordnetid="103563967" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../754/435754.xml">
Control chart</link></visual_communication>
</implement>
</chart>
</tool>
</instrumentality>
</artifact>
&nbsp;&amp;bull; <know-how wordnetid="105616786" confidence="0.8">
<chart wordnetid="106999802" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../359/11394359.xml">
Forest plot</link></visual_communication>
</method>
</chart>
</know-how>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../266/13266.xml">
Histogram</link>&nbsp;&amp;bull; <chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../859/4031859.xml">
Q-Q plot</link></visual_communication>
</chart>
&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../749/6392749.xml">
Run chart</link>&nbsp;&amp;bull; <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<tool wordnetid="104451818" confidence="0.8">
<chart wordnetid="106999802" confidence="0.8">
<implement wordnetid="103563967" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../544/412544.xml">
Scatterplot</link></visual_communication>
</implement>
</chart>
</tool>
</instrumentality>
</artifact>
&nbsp;&amp;bull; <chart wordnetid="106999802" confidence="0.8">
<visual_communication wordnetid="106873252" confidence="0.8">
<link xlink:type="simple" xlink:href="../649/977649.xml">
Stemplot</link></visual_communication>
</chart>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../442/14986442.xml">
History</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../442/14986442.xml">
History of statistics</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../208/19095208.xml">
Founders of statistics</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../692/19373692.xml">
Timeline of probability and statistics</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Publications</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../827/18753827.xml">
Journals in statistics</link>&nbsp;&amp;bull; <link xlink:type="simple" xlink:href="../879/708879.xml">
Important publications</link></col>
</row>
<row style="height:2px;">

</row>
<row>
<col colspan="2" style=";" class="navbox-abovebelow">
<b>
Statistics|Category</b>&nbsp;&amp;bull; <b>
Portal</b>&nbsp;&amp;bull; <b><link xlink:type="simple" xlink:href="../457/191457.xml">
List of topics</link></b></col>
</row>
</table>
</col>
</row>
</table>
</p>


</sec>
</bdy>
</article>
