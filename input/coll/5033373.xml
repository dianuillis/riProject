<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 21:36:59[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Action selection</title>
<id>5033373</id>
<revision>
<id>244478398</id>
<timestamp>2008-10-11T00:21:19Z</timestamp>
<contributor>
<username>Beland</username>
<id>57939</id>
</contributor>
</revision>
<categories>
<category>Artificial intelligence</category>
<category>Cognitive science</category>
</categories>
</header>
<bdy>

<table style="background:#f9f9f9; font-size:85%; line-height:110%; ">
<row>
<col>
 <image width="32x28px" src="Portal.svg">
</image>
</col>
<col style="padding:0 0.2em;">
 <b><it>
Artificial intelligence&#32;portal</it></b></col>
</row>
</table>
<p>

<b>Action selection</b> is a way of characterizing the most basic problem of intelligent systems:  what to do next.  In <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> and computational <link xlink:type="simple" xlink:href="../626/5626.xml">
cognitive science</link>, "the action selection problem" is typically associated with <link xlink:type="simple" xlink:href="../317/2711317.xml">
intelligent agents</link> and <link xlink:type="simple" xlink:href="../757/1660757.xml">
animat</link>s—artificial systems that exhibit complex behaviour in an <link>
agent environment</link>. The term is also sometimes used in <link xlink:type="simple" xlink:href="../425/9425.xml">
ethology</link> or <link xlink:type="simple" xlink:href="../425/9425.xml">
animal behavior</link>.</p>
<p>

One problem for understanding action selection is determining the level of abstraction used for specifying an "act". At the most basic level of abstraction, an <link xlink:type="simple" xlink:href="../194/1194.xml">
atomic</link> act could be anything from <it>contracting a muscle cell</it> to <it>provoking a war</it>. Typically for any one action-selection mechanism, the set of possible actions is predefined and fixed.</p>
<p>

Most researchers working in this field place high demands on their agents: 
<list>
<entry level="1" type="bullet">

 The acting <link xlink:type="simple" xlink:href="../317/2711317.xml">
agent</link> typically must select its action in <link>
dynamic</link> and <link>
unpredictable</link> environments. </entry>
<entry level="1" type="bullet">

 The agents typically act in <link xlink:type="simple" xlink:href="../767/25767.xml">
real time</link>; therefore they must make decisions in a timely fashion.</entry>
<entry level="1" type="bullet">

 The agents are normally created to perform several different tasks.  These tasks may conflict for resource allocation (e.g. can the agent put out a fire and deliver a cup of coffee at the same time?)</entry>
<entry level="1" type="bullet">

 The environment the agents operate in may include <link xlink:type="simple" xlink:href="../482/682482.xml">
humans</link>, who may make things more difficult for the agent (either intentionally or by attempting to assist.) </entry>
<entry level="1" type="bullet">

 The agents themselves are often intended to <link xlink:type="simple" xlink:href="../416/375416.xml">
model</link> animals and/or humans, and animal/human <link xlink:type="simple" xlink:href="../805/4805.xml">
behaviour</link> is quite complicated.</entry>
</list>
</p>
<p>

For these reasons action selection is not trivial and attracts a good deal of research.</p>

<sec>
<st>
 Characteristics of the action selection problem </st>

<p>

The main problem for action selection is <link xlink:type="simple" xlink:href="../363/7363.xml">
complexity</link>.  Since all <link xlink:type="simple" xlink:href="../926/5926.xml">
computation</link> takes both <link xlink:type="simple" xlink:href="../012/30012.xml">
time</link> and <link xlink:type="simple" xlink:href="../667/27667.xml">
space</link> (in memory), agents cannot possibly consider every option available to them at every instant in time.  Consequently, they must be <link xlink:type="simple" xlink:href="../786/40786.xml">
biased</link>, and <link xlink:type="simple" xlink:href="../360/206360.xml">
constrain</link> their <link xlink:type="simple" xlink:href="../149/19386149.xml">
search</link> in some way.  For AI, the question of action selection is <it>what is the best way to constrain this search</it>?  For biology and ethology, the question is <it>how do various types of animals constrain their search?  Do all animals use the same approaches?  Why do they use the ones they do?</it></p>
<p>

One fundamental question about action selection is whether it is really a problem at all for an agent, or whether it is just a description of an <link xlink:type="simple" xlink:href="../518/514518.xml">
emergent</link> property of an intelligent agent's behaviour.  However, if we consider how we are going to build an intelligent agent, then it becomes apparent there must be <it>some</it> mechanism for action selection.  This mechanism may be highly distributed (as in the case of distributed organisms such as <link xlink:type="simple" xlink:href="../192/44192.xml">
social insect</link> colonies or <link xlink:type="simple" xlink:href="../725/26725.xml">
slime mold</link>) or it may be a special-purpose module.</p>
<p>

The action selection mechanism  (ASM) determines not only the agent’s actions in terms of impact on the world, but also directs its perceptual <link xlink:type="simple" xlink:href="../753/68753.xml">
attention</link>, and updates its <link xlink:type="simple" xlink:href="../844/18844.xml">
memory</link>.  These <link xlink:type="simple" xlink:href="../995/1451995.xml">
egocentric</link> sorts of actions may in turn result in modifying the agents basic behavioural capacities, particularly in that updating memory implies some form of  <link xlink:type="simple" xlink:href="../488/233488.xml">
learning</link> is possible.  Ideally, action selection itself should also be able to learn and adapt, but there are many problems of <link xlink:type="simple" xlink:href="../170/5170.xml">
combinatorial complexity</link> and computational <link xlink:type="simple" xlink:href="../755/2995755.xml">
tractability</link> that may require restricting the search space for learning.</p>
<p>

In AI, an ASM is also sometimes either referred to as an <link xlink:type="simple" xlink:href="../677/4510677.xml">
agent architecture</link> or thought of as a substantial part of one.</p>

</sec>
<sec>
<st>
 AI mechanisms of action selection </st>

<p>

Generally, artificial action selection mechanisms can be divided into several categories:  <link xlink:type="simple" xlink:href="../641/1505641.xml">
symbol-based systems</link> sometimes known as classical planning, <link xlink:type="simple" xlink:href="../501/8501.xml">
distributed solutions</link>, and reactive or <link xlink:type="simple" xlink:href="../376/5034376.xml">
dynamic planning</link>.  Some approaches do not fall neatly into any one of these categories.  Others are really more about providing <link xlink:type="simple" xlink:href="../795/3224795.xml">
scientific model</link>s than practical AI control, these last are described further in the next section.</p>

<ss1>
<st>
 Symbolic approaches </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../641/1505641.xml">
Automated planning and scheduling</link></it>
</indent>
Early in  the <link xlink:type="simple" xlink:href="../560/2894560.xml">
history of artificial intelligence</link>, it was assumed that the best way for an agent to choose what to do next would be to compute a <link xlink:type="simple" xlink:href="../310/13018310.xml">
provably optimal</link> plan, and then execute that plan.  This led to the <link xlink:type="simple" xlink:href="../999/2685999.xml">
physical symbol system</link> hypothesis, that a physical agent that can manipulate symbols is <link xlink:type="simple" xlink:href="../319/169319.xml">
necessary and sufficient</link> for intelligence.  Many <link xlink:type="simple" xlink:href="../106/430106.xml">
software agents</link> still use this approach for action selection.  It normally requires describing all sensor readings, the world, all of ones actions and all of one's goals in some form of <link xlink:type="simple" xlink:href="../970/74970.xml">
predicate logic</link>.  Critics of this approach complain that it is too slow real-time planning and that, despite the proofs, it is still unlikely to produce optimal plans because reducing descriptions of reality to logic is a process prone to errors.</p>
<p>

<work wordnetid="100575741" confidence="0.8">
<principle wordnetid="105913538" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<examination wordnetid="100635850" confidence="0.8">
<generalization wordnetid="105913275" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<survey wordnetid="100644503" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../401/70401.xml">
Satisficing</link></activity>
</psychological_feature>
</act>
</investigation>
</survey>
</event>
</generalization>
</examination>
</idea>
</principle>
</work>
 is a decision-making strategy which attempts to meet criteria for adequacy, rather than identify an optimal solution. A satisficing strategy may often, in fact, be (near) optimal if the costs of the decision-making process itself, such as the cost of obtaining complete information, are considered in the outcome calculus.</p>
<p>

<list>
<entry level="1" type="bullet">

 <b>Goal driven architectures</b> - In these <link xlink:type="simple" xlink:href="../576/443576.xml">
symbolic</link> architectures, agent's behaviour is typically described by a set of goals. Each goal can be achieved by a process or an activity, which is described by a prescripted plan. The agent must just decide which process to carry on to accomplish a given goal. The plan can expand to subgoals, which makes the process slightly recursive. Technically, more or less, the plans exploits condition-rules. These architectures are <link xlink:type="simple" xlink:href="../376/5034376.xml">
reactive</link> or hybrid.  Classical examples of goal driven architectures are implementable refinements of <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../235/1974235.xml">
Belief-Desire-Intention</link></instrumentality>
</artifact>
</system>
 architecture like <weblink xlink:type="simple" xlink:href="http://www.marcush.net/IRS/irs_downloads.html">
JAM</weblink> or <weblink xlink:type="simple" xlink:href="http://urtax.ms.mff.cuni.cz/ive/public/about.php">
IVE</weblink>.</entry>
<entry level="1" type="bullet">

 <b><weblink xlink:type="simple" xlink:href="http://www.ai-center.com/projects/excalibur/index.html">
Excalibur</weblink></b> was a research project led by Alexander Nareyek featuring any-time planning agents for computer games. The architecture is based on structural <link xlink:type="simple" xlink:href="../966/949966.xml">
constraint satisfaction</link>, which is an advanced <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> technique.</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Distributed approaches </st>

<p>

In contrast to the symbolic approach, distributed systems of action selection actually have no one "box" in the agent which decides the next action.  At least in their idealized form, distributed systems have many <link xlink:type="simple" xlink:href="../133/939133.xml">
modules</link> running in parallel and determining the best action based on local expertise. In these idealized systems, overall coherence is expected to <link xlink:type="simple" xlink:href="../108/648108.xml">
emerge</link> somehow, possibly through careful design of the interacting components. This approach is often inspired by  <link>
neural networks</link> research.  In practice, there is almost always <it>some</it> centralised system determining which module is "the most active" or has the most salience.  There is evidence real biological brains also have such <link xlink:type="simple" xlink:href="../475/3704475.xml">
executive decision systems</link> which evaluate which of competing systems deserves the most <link xlink:type="simple" xlink:href="../753/68753.xml">
attention</link>, or more properly, has its desired actions <link xlink:type="simple" xlink:href="../018/2105018.xml">
disinhibited</link>.
<list>
<entry level="1" type="bullet">

 <b>Spreading activation</b> including  <link>
Maes Nets (ANA)</link></entry>
<entry level="1" type="bullet">

 <b>Extended Rosenblatt &amp; Payton</b>  is a spreading activation architecture developed by Toby Tyrrell in 1993. The agent's behaviour is stored in the form of a hierarchical <link xlink:type="simple" xlink:href="../636/263636.xml">
connectionism</link> network, which Tyrrell named free-flow hierarchy. Recently exploited for example by <weblink xlink:type="simple" xlink:href="http://vrlab.epfl.ch/Publications/pdf/Sevin_Thalmann_CGI_05.pdf">
de Sevin &amp; Thalmann</weblink> (2005) or <weblink xlink:type="simple" xlink:href="http://cyber.felk.cvut.cz/gerstner/eth/download/dpdk2.pdf">
Kadleček</weblink> (2001).</entry>
<entry level="1" type="bullet">

 <b><link xlink:type="simple" xlink:href="../919/3918919.xml">
Behavior based AI</link></b> including <link xlink:type="simple" xlink:href="../552/83552.xml">
subsumption architecture</link>; <region wordnetid="108630985" confidence="0.8">
<administrative_district wordnetid="108491826" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<municipality wordnetid="108626283" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<town wordnetid="108665504" confidence="0.8">
<district wordnetid="108552138" confidence="0.8">
<urban_area wordnetid="108675967" confidence="0.8">
<link xlink:type="simple" xlink:href="../400/1360400.xml">
Blumberg</link></urban_area>
</district>
</town>
</geographical_area>
</municipality>
</location>
</administrative_district>
</region>
</entry>
<entry level="1" type="bullet">

 <b><event wordnetid="100029378" confidence="0.8">
<social_event wordnetid="107288639" confidence="0.8">
<contest wordnetid="107456188" confidence="0.8">
<game wordnetid="100456199" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/297098.xml">
Creatures</link></psychological_feature>
</game>
</contest>
</social_event>
</event>
</b> are virtual pets from a computer game driven by three-layered <link xlink:type="simple" xlink:href="../523/21523.xml">
neural network</link>, which is adaptive. Their mechanism is reactive since the network in every time step determines the task that has to be performed by the pet. The network is described well in the paper of <weblink xlink:type="simple" xlink:href="http://www.cp.eng.chula.ac.th/~vishnu/gameResearch/AI/creatures.pdf">
Grand et al.</weblink> (1997) and in <weblink xlink:type="simple" xlink:href="http://www.double.co.nz/creatures/">
The Creatures Developer Resources</weblink>. See also <weblink xlink:type="simple" xlink:href="http://creatures.wikia.com/wiki/Creatures_Wiki_Homepage">
Creatures wiki</weblink>.</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Dynamic planning approaches </st>

<p>

Because purely distributed systems are difficult to construct, many researchers have turned to using explicit hard-coded plans to determine the priorities of their system.  </p>
<p>

Dynamic or <link xlink:type="simple" xlink:href="../376/5034376.xml">
reactive planning</link> methods compute just one next action in every instant based on the current context and pre-scripted plans. In contrast to classical planning methods, reactive or dynamic approaches do not suffer <link xlink:type="simple" xlink:href="../738/7835738.xml">
combinatorial explosion</link>.  On the other hand, they are sometimes seen as too rigid to be considered <link xlink:type="simple" xlink:href="../357/586357.xml">
strong AI</link>, since the plans are coded in advance.  At the same time, natural intelligence can be rigid in some contexts although it is fluid and able to adapt in others.  </p>
<p>

Example dynamic planning mechanisms include:
<list>
<entry level="1" type="bullet">

 <b><know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../931/10931.xml">
Finite-state machines</link></method>
</know-how>
</b>  These are <link xlink:type="simple" xlink:href="../376/5034376.xml">
reactive</link> architectures used mostly for computer game agents, in particular for first-person shooters <link xlink:type="simple" xlink:href="../583/1839583.xml">
bots</link>, or for virtual movie actors. Typically, the state-machines are hierarchical. For concrete game examples, see <weblink xlink:type="simple" xlink:href="http://www.gamasutra.com/gdc2005/features/20050311/isla_pfv.htm">
Halo 2 bots paper</weblink> by Damian Isla (2005) or <weblink xlink:type="simple" xlink:href="http://www.kbs.twi.tudelft.nl/Publications/MSc/2001-VanWaveren-MSc.html">
Quake III bots disertation</weblink> of Jean Paul van Waveren (2001). For movie example, see <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../413/951413.xml">
Softimage</link></software>
.</entry>
<entry level="1" type="bullet">

 Other <b>structured reactive plans</b> tend to look a little more like conventional plans, often with ways to represent <link xlink:type="simple" xlink:href="../998/13998.xml">
hierarchical</link> and <link xlink:type="simple" xlink:href="../838/27838.xml">
sequential</link> structure.  Some, such as <weblink xlink:type="simple" xlink:href="http://www.ai.sri.com/~prs/">
PRS</weblink>'s 'acts', have support for <link xlink:type="simple" xlink:href="../013/8152013.xml">
partial plan</link>s.  Many agent architectures from the mid 1990s included such plans as a "middle layer" that provided organization for  low-level <link xlink:type="simple" xlink:href="../919/3918919.xml">
behavior modules</link> while being directed by a higher level real-time planner.  Despite this supposed <link xlink:type="simple" xlink:href="../285/41285.xml">
interoperability</link> with automated planners, most structured reactive plans are hand coded (Bryson 2001, ch. 3).</entry>
</list>

Examples of structured reactive plans include <link>
James Firby</link>'s <weblink xlink:type="simple" xlink:href="http://people.cs.uchicago.edu/~firby/raps/">
RAP</weblink> System and the <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../113/8016113.xml">
Nils Nilsson</link></associate>
</research_worker>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
's <weblink xlink:type="simple" xlink:href="http://ai.stanford.edu/users/nilsson/trweb/tr.html">
Teleo-reactive plans</weblink>.  PRS, RAPs &amp; TRP are no longer developed or supported.  One still-active (as of 2006) descendent of this approach is the Parallel-rooted Ordered Slip-stack Hierarchical (or <weblink xlink:type="simple" xlink:href="http://www.bath.ac.uk/comp-sci/ai/AmonI-sw.html#BOD">
POSH</weblink>) action selection system, which is a part of Joanna Bryson's Behaviour Oriented Design.</p>
<p>

Sometimes to attempt to address the perceived inflexibility of dynamic planning,  hybrid techniques are used. In these, a more conventional AI planning system searches for new plans when the agent has spare time, and updates the dynamic plan library when it finds good solutions.  The important aspect of any such system is that when the agent needs to select an action, some solution exists that can be used immediately (see further <link xlink:type="simple" xlink:href="../771/9025771.xml">
anytime algorithm</link>).</p>

</ss1>
<ss1>
<st>
 Others </st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../751/729751.xml">
Soar</link> is a <link xlink:type="simple" xlink:href="../576/443576.xml">
symbolic</link> <link xlink:type="simple" xlink:href="../176/1700176.xml">
cognitive architecture</link>. It is based on condition-action rules known as <link xlink:type="simple" xlink:href="../457/3157457.xml">
productions</link>. Programmers can use the Soar development toolkit for building both reactive and planning agents, or any compromise between these two extremes.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../071/821071.xml">
ACT-R</link> is similar to Soar. It is less powerful as a programming language, but simpler to get working.  It includes a <link xlink:type="simple" xlink:href="../526/38526.xml">
Bayesian</link> learning system to help prioritize the  productions.</entry>
<entry level="1" type="bullet">

 ABL/Hap</entry>
<entry level="1" type="bullet">

 <b><link xlink:type="simple" xlink:href="../660/48660.xml">
Fuzzy architectures</link></b>  The <link xlink:type="simple" xlink:href="../180/49180.xml">
Fuzzy approach</link> in action selection produces more smooth behaviour than can be produced by architectures exploiting boolean condition-action rules (like Soar or POSH). These architectures are mostly <link xlink:type="simple" xlink:href="../376/5034376.xml">
reactive</link> and <link xlink:type="simple" xlink:href="../576/443576.xml">
symbolic</link>. See the work of <weblink xlink:type="simple" xlink:href="http://aigamedev.com/">
Alex Champandard</weblink>.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 Theories of action selection in nature </st>

<p>

Many dynamic models of artificial action selection were originally inspired by research in <link xlink:type="simple" xlink:href="../425/9425.xml">
ethology</link>.  In particular, <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../077/17077.xml">
Konrad Lorenz</link></scientist>
</person>
 and <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../094/188094.xml">
Nikolaas Tinbergen</link></scientist>
</person>
 provided the idea of an <link xlink:type="simple" xlink:href="../143/2525143.xml">
innate releasing mechanism</link> to explain instinctive behaviors (<link xlink:type="simple" xlink:href="../143/2525143.xml">
fixed action pattern</link>s). Influenced by the ideas of <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../907/1209907.xml">
William McDougall</link></scientist>
, Lorenz developed this into a "<link>
psychohydraulic</link>" model of the <link xlink:type="simple" xlink:href="../495/232495.xml">
motivation</link> of behavior. In ethology, these ideas were influential in the 1960s, but they are now regarded as outdated because of their use of an <link xlink:type="simple" xlink:href="../382/321382.xml">
energy flow</link> metaphor; the <link xlink:type="simple" xlink:href="../944/21944.xml">
nervous system</link> and the control of behavior are now normally treated as involving information transmission rather than energy flow.  Dynamic plans and neural networks are more similar to information transmission, while spreading activation is more similar to the diffuse control of emotional / hormonal systems.</p>
<p>

<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../705/1456705.xml">
Stan Franklin</link></educator>
</research_worker>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
 has proposed that <b>action selection</b> is the right perspective to take in understanding the role and evolution of <link xlink:type="simple" xlink:href="../378/19378.xml">
mind</link>.  See his page on <weblink xlink:type="simple" xlink:href="http://www.msci.memphis.edu/~franklin/paradigm.html">
the action selection paradigm</weblink>.</p>

<ss1>
<st>
 AI models of neural action selection </st>

<p>

Some researchers create elaborate models of neural action selection.  See for example:
<list>
<entry level="1" type="bullet">

 The <weblink xlink:type="simple" xlink:href="http://ccnlab.colorado.edu/mambo/">
Computational Cognitive Neuroscience Lab</weblink> (CU Boulder).</entry>
<entry level="1" type="bullet">

 The <weblink xlink:type="simple" xlink:href="http://www.abrg.group.shef.ac.uk/">
Adaptive Behaviour Research Group</weblink> (Sheffield).</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../164/1164.xml">
Artificial intelligence</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../136/10136.xml">
Expert system</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../769/1654769.xml">
Game artificial intelligence</link></entry>
<entry level="1" type="bullet">

<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../609/418609.xml">
Inference engine</link></instrumentality>
</artifact>
</system>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../317/2711317.xml">
Intelligent agent</link></entry>
<entry level="1" type="bullet">

<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../829/475829.xml">
OPS5</link></language>
</instrumentality>
</artifact>
</system>
</entry>
<entry level="1" type="bullet">

<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../457/3157457.xml">
Production system</link></instrumentality>
</artifact>
</system>
</entry>
<entry level="1" type="bullet">

<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../566/172566.xml">
Rete algorithm</link></instrumentality>
</artifact>
</system>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../910/2934910.xml">
Robot intelligence</link></entry>
</list>
</p>

<ss1>
<st>
 External links </st>

<p>

<list>
<entry level="1" type="bullet">

The University of Memphis: <weblink xlink:type="simple" xlink:href="http://www.msci.memphis.edu/~classweb/comp7990/fall2002/action.htm">
Agents by action selection</weblink></entry>
<entry level="1" type="bullet">

Michael Wooldridge: <weblink xlink:type="simple" xlink:href="http://www.csc.liv.ac.uk/~mjw/pubs/mas99.pdf">
Introduction to agents and their action selection mechanisms</weblink> </entry>
<entry level="1" type="bullet">

Cyril Brom: <weblink xlink:type="simple" xlink:href="http://ksvi.mff.cuni.cz/~brom/teaching.html#umelebytosti">
Slides on a course on action selection of artificial beings</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://sitemaker.umich.edu/soar">
Soar project</weblink>. University of Michigan.</entry>
<entry level="1" type="bullet">

  <weblink xlink:type="simple" xlink:href="http://publishing.royalsoc.ac.uk/natural-action">
Modelling natural action selection</weblink>, a special issue published by  <link xlink:type="simple" xlink:href="../064/496064.xml">
The Royal Society</link> - <work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<periodical wordnetid="106593296" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../316/1835316.xml">
Philosophical Transactions of the Royal Society</link></publication>
</periodical>
</artifact>
</creation>
</product>
</work>
 B: Biological Sciences: </entry>
</list>
</p>
<p>

This theme issue focusses on a particular strategy for finding scientific explanations - computer modelling.  The contributions employ state-of-the-art modelling techniques ranging from large networks of simulated brain cells, through to models of individuals (people or animals) viewed as agents operating in simulated worlds.</p>


</ss1>
</sec>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

 Bratman, M.: Intention, plans, and practical reason. Cambridge, Mass: Harvard University Press (1987)</entry>
<entry level="1" type="bullet">

 Brom, C., Lukavský, J., Šerý, O., Poch, T., Šafrata, P.: <weblink xlink:type="simple" xlink:href="http://urtax.ms.mff.cuni.cz/ive">
Affordances and level-of-detail AI for virtual humans</weblink>. In: Proceedings of Game Set and Match 2, Delft (2006)  </entry>
<entry level="1" type="bullet">

 Bryson, J.: Intelligence by Design: Principles of Modularity and Coordination for Engineering Complex Adaptive Agents. PhD thesis, Massachusetts Institute of Technology (2001)</entry>
<entry level="1" type="bullet">

 Champandard, A. J.: AI Game Development: Synthetic Creatures with learning and Reactive Behaviors. New Riders, USA (2003)  </entry>
<entry level="1" type="bullet">

 Grand, S., Cliff, D., Malhotra, A.: Creatures: Artificial life autonomous software-agents for home entertainment. In: Johnson, W. L. (eds.): Proceedings of the First International Conference on Autonomous Agents. ACM press (1997) 22-29</entry>
<entry level="1" type="bullet">

 Huber, M. J.: <weblink xlink:type="simple" xlink:href="http://www.marcush.net/IRS/irs_downloads.html">
JAM: A BDI-theoretic mobile agent architecture</weblink>. In: Proceedings of the Third International Conference on Autonomous Agents (Agents'99). Seatle (1999) 236-243</entry>
<entry level="1" type="bullet">

 Isla, D.: <weblink xlink:type="simple" xlink:href="http://www.gamasutra.com/gdc2005/features/20050311/isla_pfv.htm">
 Handling complexity in Halo 2</weblink>. In: Gamastura online, 03/11 (2005)</entry>
<entry level="1" type="bullet">

 Maes, P.: The agent network architecture (ANA). In: SIGART Bulletin, 2 (4), pages 115–120 (1991)</entry>
<entry level="1" type="bullet">

 Nareyek, A. <weblink xlink:type="simple" xlink:href="http://www.ai-center.com/projects/excalibur/index.html">
Excalibur project</weblink></entry>
<entry level="1" type="bullet">

 Reynolds, C. W. Flocks, Herds, and Schools: A Distributed Behavioral Model. In: Computer Graphics, 21(4) (SIGGRAPH '87 Conference Proceedings) (1987) 25-34.</entry>
<entry level="1" type="bullet">

 de Sevin, E. Thalmann, D.:A motivational Model of Action Selection for Virtual Humans. In: Computer Graphics International (CGI), IEEE Computer SocietyPress, New York (2005) </entry>
<entry level="1" type="bullet">

 Tyrrell, T.: Computational Mechanisms for Action Selection. Ph.D. Dissertation. Centre for Cognitive Science, University of Edinburgh (1993)</entry>
<entry level="1" type="bullet">

 van Waveren, J. M. P.: The Quake III Arena Bot. Master thesis. Faculty ITS, University of Technology Delft (2001)</entry>
<entry level="1" type="bullet">

 Wooldridge, M. An Introduction to MultiAgent Systems. John Wiley &amp; Sons (2002)</entry>
</list>
</p>

</sec>
</bdy>
</article>
