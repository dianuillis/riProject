<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:12:47[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<know-how  confidence="0.8" wordnetid="105616786">
<method  confidence="0.8" wordnetid="105660268">
<header>
<title>Gibbs sampling</title>
<id>509709</id>
<revision>
<id>241586473</id>
<timestamp>2008-09-28T18:52:36Z</timestamp>
<contributor>
<username>Orderinchaos</username>
<id>1123985</id>
</contributor>
</revision>
<categories>
<category>Monte Carlo methods</category>
<category>Bayesian statistics</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link> and <link xlink:type="simple" xlink:href="../939/22939.xml">
physics</link>, <b>Gibbs sampling</b> is an <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> to generate a sequence of samples from the <link xlink:type="simple" xlink:href="../637/879637.xml">
joint probability distribution</link> of two or more <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>s. The purpose of such a sequence is to approximate the joint distribution, or to compute an <link xlink:type="simple" xlink:href="../532/15532.xml">
integral</link> (such as an <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link>).  Gibbs sampling is a special case of the <link xlink:type="simple" xlink:href="../107/56107.xml">
Metropolis-Hastings algorithm</link>,
and thus an example of a <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../801/236801.xml">
Markov chain Monte Carlo</link></method>
</know-how>
 algorithm.  The algorithm is named after the physicist <link xlink:type="simple" xlink:href="../332/37332.xml">
J. W. Gibbs</link>, in reference to an analogy between the <link xlink:type="simple" xlink:href="../361/160361.xml">
sampling</link> algorithm and <link xlink:type="simple" xlink:href="../518/29518.xml">
statistical physics</link>. The algorithm was devised by S. Geman and Donald Geman, some eight decades after the passing of Gibbs, and is also called the <it>Gibbs sampler</it>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><p>

Gibbs sampling is applicable when the joint distribution is not known explicitly, 
but the <link xlink:type="simple" xlink:href="../458/504458.xml">
conditional distribution</link> of each variable is known.
The Gibbs sampling algorithm generates an instance from the distribution of each variable in turn, 
conditional on the current values of the other variables.
It can be shown (see, for example, Gelman et al. 1995) that the sequence of samples comprises a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
,
and the stationary distribution of that Markov chain is just the sought-after joint distribution.</p>
<p>

Gibbs sampling is particularly well-adapted to sampling the <link xlink:type="simple" xlink:href="../672/357672.xml">
posterior distribution</link> of a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian network</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
, since Bayesian networks are typically specified as a collection of conditional distributions.
BUGS (link below) is a program for carrying out Gibbs sampling on Bayesian networks.</p>

<sec>
<st>
 Background </st>

<p>

Gibbs sampling is a special case of <link xlink:type="simple" xlink:href="../107/56107.xml">
Metropolis-Hastings</link> sampling but the value is always accepted (<math>\left.\alpha = 1\right.</math>).  The point of Gibbs sampling is that given a <link xlink:type="simple" xlink:href="../637/879637.xml">
multivariate distribution</link> it is simpler to sample from a conditional distribution than to integrate over a joint distribution.  Suppose we want to sample <math>\left.k\right.</math> values of <math>\left.x\right.</math> from a joint distribution <math>\left.p(x, y)\right.</math>.  We begin with a value of <math>\left.y_0\right.</math> and sample <math>\left.x\right.</math> by <math>x_i \sim p\left(x | y = y_{i-1}\right)</math>.  Once that value of <math>\left.x\right.</math> is calculated, repeat by sampling for the next <math>\left.y\right.</math>: <math>y_i \sim p\left(y | x = x_i\right)</math>.</p>

</sec>
<sec>
<st>
 Implementation </st>

<p>

Suppose that a sample <math>\left.X\right.</math> is taken from a distribution depending on a parameter vector <math>\theta \in \Theta \,\!</math> of length <math>\left.d\right.</math>, with prior distribution <math>g(\theta_1, \ldots , \theta_d)</math>.  It may be that <math>\left.d\right.</math> is very large and that numerical integration to find the marginal densities of the <math>\left.\theta_i\right.</math> would be computationally expensive. Then an alternative method of calculating the marginal densities is to create a Markov chain on the space <math>\left.\Theta\right.</math> by repeating these two steps:</p>
<p>

<list>
<entry level="1" type="number">

 Pick a random index <math>1 \leq j \leq d</math></entry>
<entry level="1" type="number">

 Pick a new value for <math>\left.\theta_j\right.</math> according to <math>g(\theta_1, \ldots , \theta_{j-1} , \, \cdot \, , \theta_{j+1} , \ldots , \theta_d )</math></entry>
</list>
</p>
<p>

These steps define a <link xlink:type="simple" xlink:href="../867/2212867.xml">
reversible Markov chain</link> with the desired invariant distribution <math>\left.g\right.</math>. This
can be proved as follows. Define <math>x \sim_j y</math> if <math>\left.x_i = y_i\right.</math> for all <math>i \neq j</math> and let <math>\left.p_{xy}\right.</math> denote the probability of a jump from <math>x \in \Theta</math> to <math>y \in \Theta</math>. Then, for <math>x \sim_j y</math> the transition probabilities are</p>
<p>

<indent level="1">

<math>p_{xy} = \frac{1}{d}\frac{g(y)}{\sum_{z \in \Theta: z \sim_j x} g(z) } </math>
</indent>

and <math>\left.p_{xy} = 0\right.</math> otherwise. So </p>
<p>

<indent level="1">

<math>
g(x) p_{xy} = \frac{1}{d}\frac{ g(x) g(y)}{\sum_{z \in \Theta: z \sim_j x} g(z) }
= \frac{1}{d}\frac{ g(y) g(x)}{\sum_{z \in \Theta: z \sim_j y} g(z) }
= g(y) p_{yx}
</math>
</indent>

since <math>x \sim_j y</math> is an equivalence relation. Thus the <link xlink:type="simple" xlink:href="../867/2212867.xml">
detailed balance equations</link> are satisfied, implying the chain is reversible and it has invariant distribution <math>\left.g\right.</math>.</p>
<p>

In practice, the suffix <math>\left.j\right.</math> is not chosen at random, and the chain cycles through the suffixes in order. In general this gives a non-reversible chain, but it will still have the desired invariant distribution (as long as the chain can access all states under the fixed ordering).</p>

</sec>
<sec>
<st>
 Failure Modes </st>

<p>

There are two ways that Gibbs sampling can fail.  The first is when there are islands of high-probability states, with no paths between them.  For example, consider a probability distribution  over 2-bit vectors, where the vectors (0,0) and (1,1) each have probability 1/2, but the other two vectors (0,1) and (1,0) have probability zero.  Gibbs sampling will become trapped in one of the two high-probability vectors, and will never reach the other one.  More generally, for any distribution over high-dimensional, real-valued vectors, if 2 particular elements of the vector are perfectly correlated (or perfectly anti-correleated), those 2 elements will become stuck, and Gibbs sampling will never be able to change them.</p>
<p>

The second problem can happen even when all states have nonzero probability and there is only a single island of high-probability states.  For example, consider a probability distribution over 100-bit vectors, where the all-zeros vector occurs with probability 1/2, and all other vectors are equally probable, and so have a probability of <math>\frac{1}{2(2^{100}-1)}</math> each.  If you want to know the probability of the zero vector, it would be sufficient to take 100 or 1000 samples from the true distribution.  That would very likely give an answer very close to 1/2.  But you would probably have to take more than <math>2^{100}</math> samples from Gibbs sampling to get the same result.  No computer could do this in a lifetime.</p>
<p>

This problem occurs no matter how long the burn in period is.  This is because in the true distribution, the zero vector occurs half the time, and those occurrences are randomly mixed in with the nonzero vectors.  Even a small sample will see both zero and nonzero vectors.  But Gibbs sampling will alternate between returning only the zero vector for long periods (about <math>2^{99}</math> in a row), then only nonzero vectors for long periods (about <math>2^{99}</math> in a row).  Thus convergence to the true distribution is extremely slow, requiring much more than <math>2^{99}</math> steps; taking this many steps is not computationally feasible in a reasonable time period. The slow convergence here can be seen as a consequence of the <link xlink:type="simple" xlink:href="../776/787776.xml">
curse of dimensionality</link>.</p>

</sec>
<sec>
<st>
 See also </st>

<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../084/10794084.xml">
WinBUGS</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../854/3578854.xml">
OpenBUGS</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">S. Geman and <link xlink:type="simple" xlink:href="../884/19582884.xml">
Donald Geman</link>&#32;(1984).&#32;"Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images". <it><link xlink:type="simple" xlink:href="../325/12953325.xml">
IEEE Transactions on Pattern Analysis and Machine Intelligence</link></it>&#32;<b>6</b>: 721&ndash;741.</cite>&nbsp;</entry>
</reflist>
</p>

<ss1>
<st>
 Others </st>

<p>

<list>
<entry level="1" type="bullet">

 George Casella and Edward I. George. "Explaining the Gibbs sampler". <it>The American Statistician</it>, 46:167-174, 1992. <it>(Basic summary and many references.)''</it></entry>
<entry level="1" type="bullet">

 A.E. Gelfand and A.F.M. Smith. "Sampling-Based Approaches to Calculating Marginal Densities". <it>J. American Statistical Association</it>, 85:398-409, 1990.</entry>
<entry level="1" type="bullet">

 Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. <it>Bayesian Data Analysis</it>. London: Chapman and Hall. First edition, 1995. <it>(See Chapter 11.)''</it></entry>
<entry level="1" type="bullet">

 C.P. Robert and G. Casella. "Monte Carlo Statistical Methods" (second edition). New York: Springer-Verlag, 2004.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.mrc-bsu.cam.ac.uk/bugs">
The BUGS Project - Bayesian inference Using Gibbs Sampling</weblink></entry>
<entry level="1" type="bullet">

 A simple explanation of Gibbs sampling can be found on pp. 370-371 of Prof. MacKay's book "Information Theory, Inference, and Learning Algorithms", available for free browsing <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">
here</weblink> or <weblink xlink:type="simple" xlink:href="http://www.cs.utoronto.ca/~mackay/itprnn/ps/">
here</weblink>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://bayesweb.wadsworth.org/gibbs/gibbs.html">
A practical application of Gibbs sampling in genomics.</weblink></entry>
</list>
</p>


</sec>
</bdy>
</method>
</know-how>
</article>
