<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 21:36:56[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<statement  confidence="0.8" wordnetid="106722453">
<message  confidence="0.8" wordnetid="106598915">
<theorem  confidence="0.8" wordnetid="106752293">
<proposition  confidence="0.8" wordnetid="106750804">
<header>
<title>Craps principle</title>
<id>5213404</id>
<revision>
<id>216626486</id>
<timestamp>2008-06-02T14:14:53Z</timestamp>
<contributor>
<username>Melcombe</username>
<id>4682566</id>
</contributor>
</revision>
<categories>
<category>Statistical theorems</category>
<category>Probability theory</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../542/23542.xml">
probability theory</link>, the <b>craps principle</b> is a theorem about <link xlink:type="simple" xlink:href="../961/22961.xml">
event</link> <link xlink:type="simple" xlink:href="../934/22934.xml">
probabilities</link> under repeated <link xlink:type="simple" xlink:href="../067/453067.xml">
iid</link> trials. Let <math>E_1</math> and <math>E_2</math> denote two <link xlink:type="simple" xlink:href="../648/312648.xml">
mutually exclusive</link> events which might occur on a given trial. Then for each trial, the <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probability</link> that <math>E_1</math> occurs given that <math>E_1</math> or <math>E_2</math> occur is<p>

<indent level="1">

<math>\operatorname{P}\left[E_1\mid E_1\cup E_2\right]=\frac{\operatorname{P}[E_1]}{\operatorname{P}[E_1]+\operatorname{P}[E_2]}</math>
</indent>

The events <math>E_1</math> and <math>E_2</math> need not be <link xlink:type="simple" xlink:href="../880/1519880.xml">
collectively exhaustive</link>.
</p>
<sec>
<st>
Proof</st>
<p>

Since <math>E_1</math> and <math>E_2</math> are mutually exclusive,</p>
<p>

<indent level="1">

<math> \operatorname{P}[E_1\cup E_2]=\operatorname{P}[E_1]+\operatorname{P}[E_2]</math>
</indent>

Also due to mutual exclusion,</p>
<p>

<indent level="1">

<math> E_1\cap(E_1\cup E_2)=E_1</math>
</indent>

By <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probability</link>,</p>
<p>

<indent level="1">

<math> \operatorname{P}[E_1\cap(E_1\cup E_2)]=\operatorname{P}\left[E_1\mid E_1\cup E_2\right]\operatorname{P}\left[E_1\cup E_2\right]</math>
</indent>

Combining these three yields the desired result.</p>

</sec>
<sec>
<st>
Application</st>

<p>

If the trials are repetitions of a game between two players, and the events are</p>
<p>

<indent level="1">

<math>E_1:\mathrm{ player\ 1\ wins}</math>
</indent>
:<math>E_2:\mathrm{ player\ 2\ wins}</math></p>
<p>

Then the craps principle gives the respective conditional probabilities of each player winning a certain repetition, given that someone wins (i.e., given that a <link xlink:type="simple" xlink:href="../115/803115.xml">
draw</link> does not occur). In fact, the result is only affected by the relative marginal probabilities of winning <math>\operatorname{P}[E_1]</math> and <math>\operatorname{P}[E_2]</math> ; in particular, the probability of a draw is irrelevant.</p>

<ss1>
<st>
Stopping</st>
<p>

If the game is played repeatedly until someone wins, then the conditional probability above turns out to be the probability that the player wins the game.</p>

</ss1>
</sec>
<sec>
<st>
Etymology</st>
<p>

If the game being played is <link xlink:type="simple" xlink:href="../062/6062.xml">
craps</link>, then this principle can greatly simplify the computation of the probability of winning in a certain scenario. Specifically, if the first roll is a 4, 5, 6, 8, 9, or 10, then the dice are repeatedly re-rolled until one of two events occurs:
<indent level="1">

<math>E_1:\textrm{ the\ original\ roll\ (called\ 'the\ point')\ is\ rolled\ (a\ win) }</math>
</indent>
:<math>E_2:\textrm{ a\ 7\ is\ rolled\ (a\ loss) }</math></p>
<p>

Since <math>E_1</math> and <math>E_2</math> are mutually exclusive, the craps principle applies. For example, if the original roll was a 4, then the probability of winning is</p>
<p>

<indent level="1">

<math>\frac{3/36}{3/36 + 6/36}=\frac{1}{3}</math>
</indent>

This avoids having to sum the <link xlink:type="simple" xlink:href="../287/15287.xml">
infinite series</link> corresponding to all the possible outcomes:</p>
<p>

<indent level="1">

<math>\sum_{i=0}^{\infty}\operatorname{P}[\textrm{first\ }i\textrm{\ rolls\ are\ ties,\ }(i+1)^\textrm{th}\textrm{\ roll\ is\ 'the\ point'}]</math>
</indent>

Mathematically, we can express the probability of rolling <math>i</math> ties followed by rolling the point:</p>
<p>

<indent level="1">

<math>\operatorname{P}[\textrm{first\ }i\textrm{\ rolls\ are\ ties,\ }(i+1)^\textrm{th}\textrm{\ roll\ is\ 'the\ point'}]
 = (1-\operatorname{P}[E_1]-\operatorname{P}[E_2])^i\operatorname{P}[E_1]
</math>
</indent>

The summation becomes an infinite <link xlink:type="simple" xlink:href="../630/12630.xml">
geometric series</link>:</p>
<p>

<indent level="1">

<math>\sum_{i=0}^{\infty} (1-\operatorname{P}[E_1]-\operatorname{P}[E_2])^i\operatorname{P}[E_1]
= \operatorname{P}[E_1] \sum_{i=0}^{\infty} (1-\operatorname{P}[E_1]-\operatorname{P}[E_2])^i
</math>
</indent>

<indent level="2">

<math> = \frac{\operatorname{P}[E_1]}{1-(1-\operatorname{P}[E_1]-\operatorname{P}[E_2])}
= \frac{\operatorname{P}[E_1]}{\operatorname{P}[E_1]+\operatorname{P}[E_2]}
</math>
</indent>

which agrees with the earlier result.</p>

</sec>
<sec>
<st>
References</st>
<p>

<cite style="font-style:normal" class="book">Pitman, Jim&#32;(1993). Probability.&#32;Berlin:&#32;Springer-Verlag. ISBN 0-387-97974-3.</cite>&nbsp;</p>

</sec>
</bdy>
</proposition>
</theorem>
</message>
</statement>
</article>
