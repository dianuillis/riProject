<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 21:44:42[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Growing self-organizing map</title>
<id>5254059</id>
<revision>
<id>211917507</id>
<timestamp>2008-05-12T18:03:30Z</timestamp>
<contributor>
<username>Michael Hardy</username>
<id>4626</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

A <b>growing self-organizing map (GSOM)</b> is a growing variant of the popular <link xlink:type="simple" xlink:href="../996/76996.xml">
self-organizing map</link> (SOM). The GSOM was developed to address the issue of identifying a suitable map size in the <link xlink:type="simple" xlink:href="../996/76996.xml">
SOM</link>. It starts with a minimal number of nodes (usually 4) and grows new nodes on the boundary based on a heuristic. By using the value called Spread Factor (SF), the data analyst has the ability to control the growth of the GSOM.<p>

All the starting nodes of the GSOM are boundary nodes, i.e. each node has the freedom to grow in its own direction at the beginning. (Fig. 1) New Nodes are grown from the boundary nodes. Once a node is selected for growing all its free neighboring positions will be grown new nodes. The figure shows the three possible node growth options for a rectangular GSOM.</p>
<p>

<image width="150px" src="GSOM-new-node-growth.gif" type="thumb">
<caption>

Node growth options in GSOM: (a) one new node, (b) two new nodes and (c) three new nodes.
</caption>
</image>
</p>

<sec>
<st>
The algorithm</st>
<p>

The GSOM process is as follows:
<list>
<entry level="1" type="number">

Initialization phase:</entry>
<entry level="2" type="number">

Initialize the weight vectors of the starting nodes (usually four) with random numbers between 0 and 1.</entry>
<entry level="2" type="number">

Calculate the growth threshold (<math>GT</math>) for the given data set of dimension <math>D</math> according to the spread factor (<math>SF</math>) using the formula <math>GT = - D \times \ln ( SF )</math></entry>
<entry level="1" type="number">

Growing Phase:</entry>
<entry level="2" type="number">

Present input to the network.</entry>
<entry level="2" type="number">

Determine the weight vector that is closest to the input vector mapped to the current feature map (winner), using Euclidean distance (similar to the <link xlink:type="simple" xlink:href="../996/76996.xml">
SOM</link>). This step can be summarized as: find <math>q'</math> such that <math>\left | v - w_{q'} \right \vert \le \left | v - w_q \right \vert \forall q \in \mathbb{N}</math> where <math>v</math>, <math>w</math> are the input and weight vectors respectively, <math>q</math> is the position vector for nodes and <math>\mathbb{N}</math> is the set of natural numbers.</entry>
<entry level="2" type="number">

The weight vector adaptation is applied only to the neighborhood of the winner and the winner itself. The neighborhood is a set of neurons around the winner, but in the GSOM the starting neighborhood selected for weight adaptation is smaller compared to the SOM (localized weight adaptation). The amount of adaptation (learning rate) is also reduced exponentially over the iterations. Even within the neighborhood, weights that are closer to the winner are adapted more than those further away. The weight adaptation can be described by <math>w_j ( k + 1 ) = \begin{cases} w_j ( k ) &amp; \mbox{if}j \notin \Nu_{k+1} \\ w_j ( k ) + LR ( k ) \times (x_k - w_j ( k ) ) &amp; \mbox{if}j \in \Nu_{k+1} \end{cases}</math> where the Learning Rate <math>LR ( k )</math>, <math>k \in \mathbb{N}</math> is a sequence of positive parameters converging to zero as <math>k \to \infty</math>. <math>w_j ( k )</math>, <math>w_j ( k + 1 )</math> are the weight vectors of the node <math>j</math> before and after the adaptation and <math>\Nu_{k+1}</math> is the neighbourhood of the winning neuron at the <math>( k + 1 )</math>th iteration. The decreasing value of <math>LR ( k )</math> in the GSOM depends on the number of nodes existing in the map at time <math>k</math>.</entry>
<entry level="2" type="number">

Increase the error value of the winner (error value is the difference between the input vector and the weight vectors).</entry>
<entry level="2" type="number">

When <math>TE_i &amp;gt; GT</math>(where <math>TE_i</math>  is the total error of node <math>i</math> and <math>GT</math> is the growth threshold). Grow nodes if i is a boundary node. Distribute weights to neighbors if <math>i</math> is a non-boundary node.</entry>
<entry level="2" type="number">

Initialize the new node weight vectors to match the neighboring node weights.</entry>
<entry level="2" type="number">

Initialize the learning rate (<math>LR</math>) to its starting value.</entry>
<entry level="2" type="number">

Repeat steps 2 â€“ 7 until all inputs have been presented and node growth is reduced to a minimum level.</entry>
<entry level="1" type="number">

Smoothing phase.</entry>
<entry level="2" type="number">

Reduce learning rate and fix a small starting neighborhood.</entry>
<entry level="2" type="number">

Find winner and adapt the weights of the winner and neighbors in the same way as in growing phase.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Bibliography </st>
<p>

<list>
<entry level="1" type="number">

 Hsu, A., Tang, S. and Halgamuge, S. K. (2003) An unsupervised hierarchical dynamic self-organizing approach to cancer class discovery and marker gene identification in microarray data. Bioinformatics 19(16): pp 2131-2140</entry>
<entry level="1" type="number">

 Alahakoon, D., Halgamuge, S. K. and Sirinivasan, B. (2000) Dynamic Self Organizing Maps With Controlled Growth for Knowledge Discovery, IEEE Transactions on Neural Networks, Special Issue on Knowledge Discovery and Data Mining, 11, pp 601-614.</entry>
<entry level="1" type="number">

 Alahakoon, D., Halgamuge, S. K. and Sirinivasan, B. (1998) A Structure Adapting Feature Map for Optimal Cluster Representation  in Proceedings of The 5th International Conference on Neural Information Processing (ICONIP 98), Kitakyushu, Japan, pp 809-812</entry>
<entry level="1" type="number">

 Alahakoon, D., Halgamuge, S. K. and Sirinivasan, B. (1998) A Self Growing Cluster Development Approach to Data Mining  in Proceedings of IEEE International Conference on Systems, Man and Cybernetics, San Diego, USA, pp 2901-2906</entry>
<entry level="1" type="number">

 Alahakoon, D. and Halgamuge, S. K. (1998) Knowledge Discovery with Supervised and Unsupervised Self Evolving Neural Networks  in Proceedings of 5th International Conference on Soft Computing and Information/Intelligent Systems, Fukuoka, Japan, pp 907-910</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../996/76996.xml">
Self-organizing map</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../164/1164.xml">
Artificial intelligence</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../253/42253.xml">
Data mining</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../261/309261.xml">
Nonlinear dimensionality reduction</link></entry>
</list>
</p>

</sec>
</bdy>
</article>
