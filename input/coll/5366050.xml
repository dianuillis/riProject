<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 21:48:56[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Speech perception</title>
<id>5366050</id>
<revision>
<id>236470742</id>
<timestamp>2008-09-05T16:06:44Z</timestamp>
<contributor>
<username>LMBM2012</username>
<id>7791052</id>
</contributor>
</revision>
<categories>
<category>Phonetics</category>
<category>Hearing</category>
<category>Developmental psychology</category>
<category>Cognition</category>
<category>Auditory perception</category>
</categories>
</header>
<bdy>

<b>Speech perception</b> refers to the processes by which humans are able to interpret and understand the sounds used in language. The study of speech perception is closely linked to the fields of <link xlink:type="simple" xlink:href="../194/23194.xml">
phonetics</link> and <link xlink:type="simple" xlink:href="../247/23247.xml">
phonology</link> in <link xlink:type="simple" xlink:href="../526/17526.xml">
linguistics</link> and <link xlink:type="simple" xlink:href="../961/5961.xml">
cognitive psychology</link> and <link xlink:type="simple" xlink:href="../140/25140.xml">
perception</link> in <link xlink:type="simple" xlink:href="../921/22921.xml">
psychology</link>. Research in speech perception seeks to understand how human listeners recognize speech sounds and use this information to understand spoken language. Speech research has applications in building computer systems that can recognize speech, as well as improving speech recognition for hearing- and language-impaired listeners.
<sec>
<st>
Basics of speech perception</st>

<p>

The process of perceiving speech begins at the level of the sound signal and the process of audition. (For a complete description of the process of audition see <link xlink:type="simple" xlink:href="../264/331264.xml">
Hearing</link>.) After processing the initial auditory signal, speech sounds are further processed to extract acoustic cues and phonetic information. This speech information can then be used for higher-level language processes, such as word recognition.</p>

<ss1>
<st>
Acoustic cues</st>

<p>

<image location="right" width="250px" src="Spectrograms_of_syllables_dee_dah_doo.png" type="thumb">
<caption>

Figure 1: Spectrograms of syllables "dee" (top), "dah" (middle), and "doo" (bottom) showing how the onset <link>
formant transition</link>s that define perceptually the consonant [d] differ depending on the identity of the following vowel. (<link xlink:type="simple" xlink:href="../024/11024.xml">
Formant</link>s are highlighted by red dotted lines; transitions are the bending beginnings of the formant trajectories.)
</caption>
</image>
</p>
<p>

The speech sound signal contains a number of <link>
acoustic cues</link> that are used in speech perception. The cues differentiate speech sounds belonging to different <link>
phonetic categories</link>. For example, one of the most studied cues in speech is <link xlink:type="simple" xlink:href="../295/1624295.xml">
voice onset time</link> or VOT. VOT is a primary cue signaling the difference between voiced and voiceless stop consonants, such as "b" and "p". Other cues differentiate sounds that are produced at different <link xlink:type="simple" xlink:href="../021/24021.xml">
places of articulation</link> or <link xlink:type="simple" xlink:href="../942/19942.xml">
manners of articulation</link>. The speech system must also combine these cues to determine the category of a specific speech sound. This is often thought of in terms of abstract representations of <link xlink:type="simple" xlink:href="../980/22980.xml">
phonemes</link>. These representations can then be combined for use in word recognition and other language processes.</p>
<p>

It is not easy to identify what acoustic cues listeners are sensitive to when perceiving a particular speech sound:</p>
<p>

<indent level="1">

<it>At first glance, the solution to the problem of how we perceive speech seems deceptively simple. If one could identify stretches of the acoustic waveform that correspond to units of perception, then the path from sound to meaning would be clear. However, this correspondence or mapping has proven extremely difficult to find, even after some forty-five years of research on the problem.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>
</indent>

If a specific aspect of the acoustic waveform indicated one linguistic unit, a series of tests using speech synthesizers would be sufficient to determine such a cue or cues. However, there are two significant obstacles:</p>
<p>

<list>
<entry level="1" type="number">

 One acoustic aspect of the speech signal may cue different linguistically relevant dimensions. For example, the duration of a vowel in English can indicate whether or not the vowel is stressed, or whether it is in a syllable closed by a voiced or a voiceless consonant, and in some cases (like American English /ɛ/ and /æ/) it can distinguish the identity of vowels.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> Some experts even argue that duration can help in distinguishing of what is traditionally called short and long vowels in English.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></entry>
<entry level="1" type="number">

 One linguistic unit can be cued by several acoustic properties. For example in a classic experiment, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<linguist wordnetid="110264437" confidence="0.8">
<link xlink:type="simple" xlink:href="../465/8146465.xml">
Alvin Liberman</link></linguist>
</psychologist>
</scholar>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</person>
</physical_entity>
 (1957) showed that the onset <link>
formant transitions</link> of /d/ differ depending on the following vowel (see Figure 1) but they are all interpreted as the phoneme /d/ by listeners.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></entry>
</list>
</p>


</ss1>
<ss1>
<st>
Linearity and the segmentation problem</st>

<p>

<image location="right" width="300px" src="Spectrogram_of_I_owe_you.png" type="thumb">
<caption>

Figure 2: A spectrogram of the phrase "I owe you". There are no clearly distinguishable boundaries between speech sounds.
</caption>
</image>
</p>
<p>

Although listeners perceive speech as a stream of discrete units (<link xlink:type="simple" xlink:href="../980/22980.xml">
phonemes</link>, <link xlink:type="simple" xlink:href="../911/44911.xml">
syllables</link>, and <link xlink:type="simple" xlink:href="../866/1449866.xml">
words</link>), this linearity is difficult to be seen in the physical speech signal (see Figure 2 for an example). Speech sounds do not strictly follow one another, rather, they overlap.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> A speech sound is influenced by the ones that precede and the ones that follow. This influence can even be exerted at a distance of two or more segments (and across syllable- and word-boundaries)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.</p>
<p>

Having disputed the linearity of the speech signal, the problem of segmentation arises: one encounters serious difficulties trying to delimit a stretch of speech signal as belonging to a single perceptual unit. This can be again illustrated by the fact that the acoustic properties of the phoneme /d/ will depend on the identity of the following vowel (because of <link xlink:type="simple" xlink:href="../179/2765179.xml">
coarticulation</link>).</p>

</ss1>
<ss1>
<st>
Lack of Invariance</st>

<p>

The research and application of speech perception has to deal with several problems which result from what has been termed the lack of invariance. As was suggested above, reliable constant relations between a phoneme of a language and its acoustic manifestation in speech are difficult to find. There are several reasons for this:</p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Context-induced variation.</it> Phonetic environment affects the acoustic properties of speech sounds. For example, /u/ in English is fronted when surrounded by <link xlink:type="simple" xlink:href="../012/39012.xml">
coronal consonant</link>s.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> Or, the VOT values marking the boundary between voiced and voiceless stops are different for labial, alveolar and velar stops and they shift under stress or depending on the position within a syllable.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Variation due to differing speech conditions.</it> One important factor that causes variation is differing speech rate. Many phonemic contrasts are constituted by temporal characteristics (short vs. long vowels or consonants, affricates vs. fricatives, stops vs. glides, voiced vs. voiceless stops, etc.) and they are certainly affected by changes in speaking tempo.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> Another major source of variation is articulatory carefulness versus sloppiness which is typical for connected speech (articulatory ‘undershoot’ is obviously reflected in the acoustic properties of the sounds produced).</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Variation due to different speaker identity.</it> The resulting acoustic structure of concrete speech productions depends on the physical and psychological properties of individual speakers. Men, women, and children generally produce voices having different pitch. Because speakers have vocal tracts of different sizes (due to sex and age especially) the resonant frequencies (<link xlink:type="simple" xlink:href="../024/11024.xml">
formants</link>), which are important for recognition of speech sounds, will vary in their absolute values across individuals<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref> (see Figure 3 for an illustration of this). Dialect and foreign accent cause variation as well.</entry>
</list>
</p>

</ss1>
<ss1>
<st>
Perceptual constancy and normalization</st>

<p>

<image location="right" width="300px" src="Standard_and_normalized_vowel_space2.png" type="thumb">
<caption>

Figure 3: The left panel shows the 3 peripheral American English vowels /i/, /ɑ/, and /u/ in a standard F1 by F2 plot (in Hz). The mismatch between male, female, and child values is apparent. In the right panel formant distances (in <link xlink:type="simple" xlink:href="../815/300815.xml">
Bark</link>) rather than absolute values are plotted using the normalization procedure proposed by Syrdal and Gopal in 1986.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>. Formant values are taken from Hillenbrand et al. (1995)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>
</caption>
</image>
</p>
<p>

Given the lack of invariance, it is remarkable that listeners perceive vowels and consonants produced under different conditions and by different speakers as constant categories. It has been proposed that this is achieved by means of the perceptual normalization process in which listeners filter out the noise (i.e. variation) to arrive at the underlying category. Vocal-tract-size differences result in formant-frequency variation across speakers; therefore a listener has to adjust his/her perceptual system to the acoustic characteristics of a particular speaker. This may be accomplished by considering the ratios of formants rather than their absolute values.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref> This process has been called vocal tract normalization (see Figure 3 for an example). Similarly, listeners are believed to adjust the perception of duration to the current tempo of the speech they are listening to – this has been referred to as speech rate normalization.</p>
<p>

Whether or not normalization actually takes place and what is its exact nature is a matter of theoretical controversy (see <link xlink:type="simple" xlink:href="../050/5366050.xml#xpointer(//*[./st=%22Theories%22])">
theories</link> below). <link xlink:type="simple" xlink:href="../787/2799787.xml">
Perceptual constancy</link> is a phenomenon not specific to speech perception only; it exists in other types of perception too.</p>

</ss1>
<ss1>
<st>
Categorical perception</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../899/4643899.xml">
Categorical perception</link></it>
</indent>

<image location="right" width="300px" src="Categorization-and-discrimination-curves.png" type="thumb">
<caption>

Figure 4: Example identification (red) and discrimination (blue) functions
</caption>
</image>
</p>
<p>

Categorical perception is involved in processes of perceptual differentiation. We perceive speech sounds categorically, that is to say, we are more likely to notice the differences <it>between</it> categories (phonemes) than <it>within</it> categories. The perceptual space between categories is therefore warped, the centers of categories (or 'prototypes') working like a sieve<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref> or like magnets<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref> for in-coming speech sounds.</p>
<p>

Let us consider an artificial continuum between a voiceless and a voiced <link xlink:type="simple" xlink:href="../351/811351.xml">
bilabial stop</link> where each new step differs from the preceding one in the amount of <link xlink:type="simple" xlink:href="../295/1624295.xml">
VOT</link>. The first sound is a <link xlink:type="simple" xlink:href="../070/2609070.xml">
pre-voiced</link> [b], i.e. it has a negative VOT. Then, increasing the VOT, we get to a point where it is zero, i.e. the stop is a plain <link xlink:type="simple" xlink:href="../134/3134.xml">
unaspirated</link> voiceless [p]. Gradually, adding the same amount of VOT at a time, we reach the point where the stop is a strongly aspirated voiceless bilabial [pʰ]. (Such a continuum was used in an experiment by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<linguist wordnetid="110264437" confidence="0.8">
<link xlink:type="simple" xlink:href="../518/8299518.xml">
Lisker</link></linguist>
</scientist>
</causal_agent>
</person>
</physical_entity>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<linguist wordnetid="110264437" confidence="0.8">
<link xlink:type="simple" xlink:href="../052/8387052.xml">
Abramson</link></linguist>
</scholar>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</person>
</physical_entity>
 in 1970. The sounds they used are <weblink xlink:type="simple" xlink:href="http://www.haskins.yale.edu/featured/demo-liskabram/index.html">
available online</weblink>.) In this continuum of, for example, seven sounds, native English listeners will identify the first three sounds as /b/ and the last three sounds as /p/ with a clear boundary between the two categories.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref> A two-alternative identification (or categorization) test will yield a discontinuous categorization function (see red curve in Figure 4).</p>
<p>

If we test the ability to discriminate between two sounds with varying VOT values but having a constant VOT distance from each other (20 ms for instance), listeners are likely to  perform at chance level if both sounds fall within the same category and at nearly-100% level if each sound falls in a different category (see the blue discrimination curve in Figure 4).</p>
<p>

The conclusion to make from both the identification and the discrimination test is that listeners will have different sensitivity to the same relative increase in VOT depending on whether or not the boundary between categories was crossed. Similar perceptual adjustment is attested for other acoustic cues as well.</p>

</ss1>
<ss1>
<st>
Top-down influences on speech perception</st>

<p>

The process of speech perception is not necessarily uni-directional. That is, higher-level language processes connected with <link xlink:type="simple" xlink:href="../646/20646.xml">
morphology</link>, <link xlink:type="simple" xlink:href="../860/26860.xml">
syntax</link>, or <link xlink:type="simple" xlink:href="../107/29107.xml">
semantics</link> may interact with basic speech perception processes to aid in recognition of speech sounds. It may be the case that it is not necessary and maybe even not possible for listener to recognize phonemes before recognizing higher units, like words for example. After obtaining at least a fundamental piece of information about phonemic structure of the perceived entity from the acoustic signal, listeners are able to compensate for missing or noise-masked phonemes using their knowledge of the spoken language.</p>
<p>

In a classic experiment, Richard M. Warren (1970) replaced one phoneme of a word with a cough-like sound. His subjects restored the missing speech sound perceptually without any difficulty and what is more, they were not able to identify accurately which phoneme had been disturbed.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref> This is known as the phonemic restoration effect. Another basic experiment compares recognition of naturally spoken words presented in a sentence (or at least a phrase) and the same words presented in isolation. Perception accuracy usually drops in the latter condition. Garnes and Bond (1976) also used carrier sentences when researching the influence of semantic knowledge on perception. They created series of words differing in one phoneme (bay / day / gay, for example). The quality of the first phoneme changed along a continuum. All these stimuli were put into different sentences each of which made sense with one of the words only. Listeners had a tendency to judge the ambiguous words (when the first segment was at the boundary between categories) according to the meaning of the whole sentence.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref></p>

</ss1>
</sec>
<sec>
<st>
Research topics</st>

<ss1>
<st>
Infant speech perception</st>

<p>

Infants begin the process of language acquisition by being able to detect very small differences between speech sounds. They are able to discriminate all possible speech contrasts (phonemes). Gradually, as they are exposed to their native language, their perception becomes language-specific, i.e. they learn how to ignore the differences within phonemic categories of the language (differences that may well be contrastive in other languages - for example, English distinguishes two voicing categories of <link xlink:type="simple" xlink:href="../480/29480.xml">
stop consonants</link>, whereas <language wordnetid="106282651" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../647/48647.xml#xpointer(//*[./st=%22Consonants%22])">
Thai has three categories</link></language>
; infants must learn which differences are distinctive in their native language uses, and which are not). As infants learn how to sort incoming speech sounds into categories, ignoring irrelevant differences and reinforcing the contrastive ones, their perception becomes <link xlink:type="simple" xlink:href="../050/5366050.xml#xpointer(//*[./st=%22Categorical+perception%22])">
categorical</link>. Infants learn to contrast different vowel phonemes of their native language by approximately 6 months of age. The native consonantal contrasts are acquired by 11 or 12 months of age.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref> Some researchers have proposed that infants may be able to learn the sound categories of their native language through passive listening, using a process called <link xlink:type="simple" xlink:href="../843/305843.xml">
statistical learning</link>. Others even claim that certain sound categories are innate, that is, they are genetically-specified (see discussion about <link xlink:type="simple" xlink:href="../899/4643899.xml#xpointer(//*[./st=%22Acquired+distinctiveness%22])">
innate vs. acquired categorical distinctiveness</link>).</p>
<p>

If day-old babies are presented with their mother’s voice speaking normally, abnormally (in monotone), and a stranger’s voice, they react only to their mother’s voice speaking normally. When a human and a non-human sound is played, babies turn their head only to the source of human sound. It has been suggested that auditory learning begins already in the pre-natal period.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2218%22])">18</ref></p>
<p>

How do researchers know if infants can distinguish between speech sounds? One of the techniques used to examine how infants perceive speech, besides the head-turn procedure mentioned above, is measuring their sucking rate. In such an experiment, a baby is sucking a special nipple while presented with sounds. First, the baby’s normal sucking rate is established. Then a stimulus is played repeatedly. When the baby hears the stimulus for the first time the sucking rate increases but as the baby becomes <link xlink:type="simple" xlink:href="../837/599837.xml">
habituated</link> to the stimulation the sucking rate decreases and levels off. Then, a new stimulus is played to the baby. If the baby perceives the newly introduced stimulus as different from the background stimulus the sucking rate will show an increase.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2218%22])">18</ref> The sucking-rate and the head-turn method are some of the more traditional, behavioral methods for studying speech perception. Among the new methods (see <link xlink:type="simple" xlink:href="../050/5366050.xml#xpointer(//*[./st=%22Research+methods%22])">
research methods</link> below) that help us to study speech perception, <link xlink:type="simple" xlink:href="../432/4807432.xml">
NIRS</link> is widely used in infants.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref></p>

</ss1>
<ss1>
<st>
Cross-language and second-language speech perception</st>

<p>

A large amount of research studies focus on how users of a language perceive <link xlink:type="simple" xlink:href="../802/1094802.xml">
foreign</link> speech (referred to as cross-language speech perception) or <link xlink:type="simple" xlink:href="../986/162986.xml">
second-language</link> speech (second-language speech perception). The latter falls within the domain of <link xlink:type="simple" xlink:href="../964/1199964.xml">
second language acquisition</link>.</p>
<p>

Languages differ in their phonemic inventories. Naturally, this creates difficulties when a foreign language is encountered. For example, if two foreign-language sounds are assimilated to a single mother-tongue category the difference between them will be very difficult to discern. A classic example of this situation is the observation that Japanese learners of English will have problems with identifying or distinguishing English liquids /l/ and /r/.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2219%22])">19</ref></p>
<p>

Best (1995) proposed a Perceptual Assimilation Model which describes possible cross-language category assimilation patterns and predicts their consequences.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2220%22])">20</ref>
Flege (1995) formulated a Speech Learning Model which combines several hypotheses about second-language (L2) speech acquisition and which predicts, in simple words, that an L2 sound that is not too similar to a native-language (L1) sound will be easier to acquire than an L2 sound that is relatively similar to an L1 sound (because it will be perceived as more obviously ‘different’ by the learner).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2221%22])">21</ref></p>

</ss1>
<ss1>
<st>
Speech perception in language or hearing impairment</st>

<p>

Research in how people with language or hearing impairment perceive speech is not only intended to discover possible treatments. It can provide insight into what principles underlie non-impaired speech perception. Two areas of research can serve as an example: </p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Listeners with aphasia.</it> <link xlink:type="simple" xlink:href="../80s/2080s.xml">
Aphasia</link> affects both the expression and reception of language. Both two most common types, <link xlink:type="simple" xlink:href="../841/9841.xml">
Broca's</link> and <link xlink:type="simple" xlink:href="../011/26011.xml">
Wernike's aphasia</link>, affect speech perception to some extent. Broca’s aphasia causes moderate difficulties for language understanding. The effect of Wernike’s aphasia on understanding is much more severe. It is agreed upon, that aphasics suffer from perceptual deficits. They are usually unable to fully distinguish place of articulation and voicing.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2222%22])">22</ref> As for other features, the difficulties vary. It has not yet been proven whether low-level speech-perception skills are affected in aphasia sufferers or whether their difficulties are caused by higher-level impairment alone.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2222%22])">22</ref></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <it>Listeners with cochlear implants.</it> <link xlink:type="simple" xlink:href="../649/241649.xml">
Cochlear implant</link>ation allows partial restoration of hearing in deaf people. The acoustic information conveyed by an implant is usually sufficient for implant users to properly recognize speech of people they know even without visual clues. For cochlear implant users, it is more difficult to understand unknown speakers and sounds. The perceptual abilities of children that received an implant after the age of two are significantly better than of those who were implanted in adulthood. A number of factors have been shown to influence perceptual performance. These are especially duration of deafness prior to implantation, age of onset of deafness, age at implantation (such age affects may be related to the <link xlink:type="simple" xlink:href="../918/8329918.xml">
Critical period hypothesis</link>) and the duration of using an implant. There are differences between children with congenital and acquired deafness. Postlingually deaf children have better results than the prelingually deaf and adapt to a cochlear implant faster.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2223%22])">23</ref></entry>
</list>
</p>

</ss1>
<ss1>
<st>
Noise</st>

<p>

One of the basic problems in the study of speech is how to deal with the noise in the speech signal. This is shown by the difficulty that computer <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link> systems have with recognizing human speech. These programs can do well at recognizing speech when they have been trained on a specific speaker's voice, and under quiet conditions. However, these systems often do poorly in more realistic listening situations where humans are able to understand speech without difficulty.</p>

</ss1>
<ss1>
<st>
Research methods</st>

<p>

The methods used in speech perception research can be roughly divided into three groups: behavioral, computational, and, more recently, neurophysiological methods. Behavioral experiments are based on an active role of a participant, i.e. subjects are presented with stimuli and asked to make conscious decisions about them. This can take the form of an identification test, a <link xlink:type="simple" xlink:href="../334/6185334.xml">
discrimination test</link>, similarity rating, etc. These types of experiments help to provide a basic description of how listeners perceive and categorize speech sounds.</p>
<p>

Computational modeling has also been used to simulate how speech may be processed by the brain to produce behaviors that are observed. Computer models have been used to address several questions in speech perception, including how the sound signal itself is processed to extract the acoustic cues used in speech, as well as how speech information is used for higher-level processes, such as word recognition.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2224%22])">24</ref></p>
<p>

Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref> Methods used to measure neural responses to speech include <link xlink:type="simple" xlink:href="../230/614230.xml">
event-related potential</link>s, <link xlink:type="simple" xlink:href="../211/172211.xml">
magnetoencephalography</link>, and <link xlink:type="simple" xlink:href="../564/979564.xml">
near infrared spectroscopy</link>. One important response used with <link xlink:type="simple" xlink:href="../230/614230.xml">
event-related potential</link>s is the <link xlink:type="simple" xlink:href="../439/6240439.xml">
mismatch negativity</link>, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.</p>
<p>

Neurophysiological methods were introduced into speech perception research for several reasons:
<indent level="1">

<it>Behavioral responses may reflect late, conscious processes and be affected by other systems such as orthography, and thus they may mask speaker’s ability to recognize sounds based on lower-level acoustic distributions.</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2225%22])">25</ref>
</indent>
Without the necessity of taking an active part in the test, even infants can be tested; this feature is crucial in research into acquisition processes. The possibility to observe low-level auditory processes independently from the higher-level ones makes it possible to address long-standing theoretical issues such as whether or not humans possess a specialized module for perceiving speech<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2226%22])">26</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2227%22])">27</ref> or whether or not some complex acoustic invariance (see <link xlink:type="simple" xlink:href="../050/5366050.xml#xpointer(//*[./st=%22Lack+of+Invariance%22])">
lack of invariance</link> above) underlies the recognition of a speech sound<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2228%22])">28</ref>.</p>

</ss1>
</sec>
<sec>
<st>
Theories</st>

<p>

Research into speech perception (SP) has by no means explained every aspect of the processes involved. A lot of what has been said about SP is a matter of theory. Several theories have been devised to develop some of the above mentioned and other unclear issues. Not all of them give satisfactory explanations of all problems, however the research they inspired has yielded a lot of useful data.</p>

<ss1>
<st>
Motor theory of SP</st>

<p>

Some of the earliest work in the study of how humans perceive speech sounds was conducted by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<linguist wordnetid="110264437" confidence="0.8">
<link xlink:type="simple" xlink:href="../465/8146465.xml">
Alvin Liberman</link></linguist>
</psychologist>
</scholar>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</person>
</physical_entity>
 and his colleagues at <institute wordnetid="108407330" confidence="0.8">
<association wordnetid="108049401" confidence="0.8">
<link xlink:type="simple" xlink:href="../324/6202324.xml">
Haskins Laboratories</link></association>
</institute>
.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2229%22])">29</ref> Using a speech synthesizer, they constructed speech sounds that varied in <link xlink:type="simple" xlink:href="../021/24021.xml">
place of articulation</link> along a continuum from /bɑ/ to /dɑ/ to /gɑ/. Listeners were asked to identify which sound they heard and to discriminate between two different sounds. The results of the experiment showed that listeners grouped sounds into discrete categories, even though the sounds they were hearing were varying continuously. Based on these results, they proposed the notion of <link xlink:type="simple" xlink:href="../899/4643899.xml">
categorical perception</link> as a mechanism by which humans are able to identify speech sounds.</p>
<p>

More recent research using different tasks and methodologies suggests that listeners are highly sensitive to acoustic differences within a single phonetic category, contrary to a strict categorical account of speech perception.</p>
<p>

In order to provide a theoretical account of the <link xlink:type="simple" xlink:href="../899/4643899.xml">
categorical perception</link> data, Liberman and colleagues<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2230%22])">30</ref>  worked out the motor theory of speech perception, where “the complicated articulatory encoding was assumed to be decoded in the perception of speech by the same processes that are involved in production”<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> (this is referred to as analysis-by-synthesis).  For instance, the English consonant /d/ may vary in its acoustic details across different phonetic contexts (see <link xlink:type="simple" xlink:href="../050/5366050.xml#xpointer(//*[./st=%22Acoustic+cues%22])">
above</link>), yet all /d/’s as perceived by a listener fall within one category (voiced alveolar stop) and that is because "lingustic representantations are abstract, canonical, phonetic segments or the gestures that underlie these segments."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>  When describing units of perception, Liberman later abandoned articulatory movements and proceeded to the neural commands to the articulators<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2231%22])">31</ref> and even later to intended articulatory gestures<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2232%22])">32</ref>, thus "the neural representation of the utterance that determines the speaker’s production is the <link xlink:type="simple" xlink:href="../689/5424689.xml">
distal object</link> the lister perceives"<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2232%22])">32</ref>. The theory is closely related to the <link xlink:type="simple" xlink:href="../606/446606.xml">
modularity</link> hypothesis, which proposes the existence of a special-purpose module, which is supposed to be innate and probably human-specific.</p>
<p>

The theory has been criticized in terms of not being able to "provide an account of just how acoustic signals are translated into intended gestures" by listeners. Furthermore, it is unclear how indexical information (eg. talker-identity) is encoded/decoded along with liguistically-relevant information.</p>

</ss1>
<ss1>
<st>
Direct realist theory of SP</st>

<p>

The direct realist theory of speech perception (mostly associated with <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../162/8224162.xml">
Carol Fowler</link></psychologist>
</scholar>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</person>
</physical_entity>
) is a part of the more general theory of <link xlink:type="simple" xlink:href="../889/7889.xml">
direct realism</link>, which postulates that perception allows us to have direct awareness of the world because it involves direct recovery of the <link xlink:type="simple" xlink:href="../689/5424689.xml">
distal source</link> of the event that is perceived. For speech perception, the theory asserts that the <link xlink:type="simple" xlink:href="../689/5424689.xml">
objects of perception</link> are actual vocal tract movements, or gestures, and not abstract phonemes or (as in the Motor Theory) events that are causally antecedent to these movements, i.e. intended gestures. Listeners perceive gestures not by means of a specialized decoder (as in the Motor Theory) but because information in the acoustic signal specifies the gestures that form it.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2233%22])">33</ref>
By claiming that the actual articulatory gestures that produce different speech sounds are themselves the units of speech perception, the theory bypasses the problem of <link xlink:type="simple" xlink:href="../050/5366050.xml#xpointer(//*[./st=%22Lack+of+Invariance%22])">
lack of invariance</link>.</p>

</ss1>
<ss1>
<st>
Fuzzy-logical model of SP</st>

<p>

The fuzzy logical theory of speech perception developed by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../234/18439234.xml">
Dominic Massaro</link></associate>
</psychologist>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2234%22])">34</ref> proposes that people remember speech sounds in a probabilistic, or graded, way. It suggests that people remember descriptions of the perceptual units of language, called prototypes. Within each prototype various features may combine. However, features are not just binary (true or false), there is a <link xlink:type="simple" xlink:href="../180/49180.xml">
fuzzy</link> value corresponding to how likely it is that a sound belongs to a particular speech category. Thus, when perceiving a speech signal our decision about what we actually hear is based on the relative goodness of the match between the stimulus information and values of particular prototypes. The final decision is based on multiple features or sources of information, even visual information (this explains the <appearance wordnetid="105939432" confidence="0.8">
<representation wordnetid="105926676" confidence="0.8">
<illusion wordnetid="105939636" confidence="0.8">
<link xlink:type="simple" xlink:href="../584/433584.xml">
McGurk effect</link></illusion>
</representation>
</appearance>
).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2235%22])">35</ref> Computer models of the fuzzy logical theory have been used to demonstrate that the theory's predictions of how speech sounds are categorized correspond to the behavior of human listeners.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2236%22])">36</ref></p>

</ss1>
<ss1>
<st>
Acoustic landmarks and distinctive features</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../570/11285570.xml">
Acoustic landmarks and distinctive features</link></it>
</indent>

In addition to the proposals of Motor Theory and Direct Realism about the relation between phonological features and articulatory gestures, <link xlink:type="simple" xlink:href="../189/4836189.xml">
Kenneth N. Stevens</link> proposed another kind of relation: between phonological features and auditory properties. According to this view, listeners are inspecting the incoming signal for the so-called acoustic landmarks which are particular events in the spectrum carrying information about gestures which produced them. Since these gestures are limited by the capacities of humans’ articulators and listeners are sensitive to their auditory correlates, the <link xlink:type="simple" xlink:href="../050/5366050.xml#xpointer(//*[./st=%22Lack+of+invariance%22])">
lack of invariance</link> simply does not exist in this model. The acoustic properties of the landmarks constitute the basis for establishing the distinctive features. Bundles of them uniquely specify phonetic segments (phonemes, syllables, words).<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2237%22])">37</ref></p>

</ss1>
<ss1>
<st>
Exemplar theory</st>

<p>

Exemplar models of speech perception differ from the four theories mentioned above which suppose that there is no connection between word- and talker-recognition and that the variation across talkers is ‘noise’ to be filtered out. </p>
<p>

The exemplar-based approaches claim listeners store information for word- as well as talker-recognition. According to this theory, particular instances of speech sounds are stored in the memory of a listener. In the process of speech perception, the remembered instances of e.g. a syllable stored in the listener’s memory are compared with the incoming stimulus so that the stimulus can be categorized. Similarly, when recognizing a talker, all the memory traces of utterances produced by that talker are activated and the talker’s identity is determined. Supporting this theory are several experiments reported by Johnson<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref> that suggest that our signal identification is more accurate when we are familiar with the talker or when we have visual representation of the talker’s gender. When the talker is unpredictable or the sex misidentified, the error rate in word-identification is much higher. </p>
<p>

The exemplar models have to face several objections, two of which are (1) insufficient memory capacity to store every utterance ever heard and, concerning the ability to produce what was heard, (2) whether also the talker’s own articulatory gestures are stored or computed when producing utterances that would sound as the auditory memories.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2235%22])">35</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref></p>

</ss1>
</sec>
<sec>
<st>
References</st>


<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal">Nygaard, L.C., Pisoni, D.B.&#32;(1995).&#32;"Speech Perception: New Directions in Research and Theory".&#32;<it>Handbook of Perception and Cognition: Speech, Language, and Communication</it>.&#32;&#32;Ed. J.L. Miller, P.D. Eimas.&#32;San Diego:&#32;Academic Press.</cite>&nbsp;</entry>
<entry id="2">
 <cite style="font-style:normal">Klatt, D.H.&#32;(1976).&#32;"Linguistic uses of segmental duration in English: Acoustic and perceptual evidence". <it>Journal of the Acoustical Society of America</it>&#32;<b>59(5)</b>: 1208–1221. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1121%2F1.380986">
10.1121/1.380986</weblink>.</cite>&nbsp;</entry>
<entry id="3">
 <cite style="font-style:normal">Halle, M., Mohanan, K.P.&#32;(1985).&#32;"Segmental phonology of modern English". <it>Linguistic Inquiry</it>&#32;<b>16(1)</b>: 57–116.</cite>&nbsp;</entry>
<entry id="4">
 <cite style="font-style:normal">Liberman, A.M.&#32;(1957).&#32;"<weblink xlink:type="simple" xlink:href="http://www.haskins.yale.edu/Reprints/HL0016.pdf">
Some results of research on speech perception</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>Journal of the Acoustical Society of America</it>&#32;<b>29(1)</b>: 117–123. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1121%2F1.1908635">
10.1121/1.1908635</weblink>. Retrieved on <link>
2007-05-17</link>.</cite>&nbsp;</entry>
<entry id="5">
 <cite style="font-style:normal">Fowler, C. A.&#32;(1995).&#32;"Speech production".&#32;<it>Handbook of Perception and Cognition: Speech, Language, and Communication</it>.&#32;&#32;Ed. J.L. Miller, P.D. Eimas.&#32;San Diego:&#32;Academic Press.</cite>&nbsp;</entry>
<entry id="6">
 <cite style="font-style:normal">Hillenbrand, J.M., Clark, M.J., Nearey, T.M.&#32;(2001).&#32;"Effects of consonant environment on vowel formant patterns". <it>Journal of the Acoustical Society of America</it>&#32;<b>109(2)</b>: 748–763. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1121%2F1.1337959">
10.1121/1.1337959</weblink>.</cite>&nbsp;</entry>
<entry id="7">
 <cite style="font-style:normal">Lisker, L., Abramson, A.S.&#32;(1967).&#32;"<weblink xlink:type="simple" xlink:href="http://www.haskins.yale.edu/Reprints/HL0067.pdf">
Some effects of context on voice onset time in English stops</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>Language and Speech</it>&#32;<b>10</b>: 1–28. Retrieved on <link>
2007-05-17</link>.</cite>&nbsp;</entry>
<entry id="8">
 <cite style="font-style:normal">Hillenbrand, J., Getty, L.A., Clark, M.J., Wheeler, K.&#32;(1995).&#32;"Acoustic characteristics of American English vowels". <it>Journal of the Acoustical Society of America</it>&#32;<b>97</b>: 3099–3111. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1121%2F1.411872">
10.1121/1.411872</weblink>.</cite>&nbsp;</entry>
<entry id="9">
 <cite style="font-style:normal">Syrdal, A.K., Gopal, H.S.&#32;(1986).&#32;"A perceptual model of vowel recognition based on the auditory representation of American English vowels". <it>Journal of the Acoustical Society of America</it>&#32;<b>79</b>: 1086–1100. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1121%2F1.393381">
10.1121/1.393381</weblink>.</cite>&nbsp;</entry>
<entry id="10">
 <cite style="font-style:normal">Strange, W.&#32;(1999).&#32;"Perception of vowels: Dynamic constancy".&#32;<it>The Acoustics of Speech Communication: Fundamentals, Speech Perception Theory, and Technology</it>.&#32;&#32;Ed. J.M. Pickett.&#32;Needham Heights (MA):&#32;Allyn &amp; Bacon.</cite>&nbsp;</entry>
<entry id="11">
 <cite style="font-style:normal">Johnson, K.&#32;(2005).&#32;"<weblink xlink:type="simple" xlink:href="http://corpus.linguistics.berkeley.edu/~kjohnson/papers/revised_chapter.pdf">
Speaker Normalization in speech perception</weblink>".&#32;<it>The Handbook of Speech Perception</it>.&#32;&#32;Ed. Pisoni, D.B., Remez, R..&#32;Oxford:&#32;Blackwell Publishers.&#32;Retrieved on <link>
2007-05-17</link>.</cite>&nbsp;</entry>
<entry id="12">
 <cite style="font-style:normal" class="book"><physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<linguist wordnetid="110264437" confidence="0.8">
<link xlink:type="simple" xlink:href="../134/31134.xml">
Trubetzkoy, Nikolay S.</link></linguist>
</scholar>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</person>
</physical_entity>
&#32;(<link xlink:type="simple" xlink:href="../610/34610.xml">
1969</link>). Principles of phonology.&#32;Berkeley and Los Angeles:&#32;University of California Press.</cite>&nbsp;</entry>
<entry id="13">
 <cite style="font-style:normal">Iverson, P., Kuhl, P.K.&#32;(1995).&#32;"Mapping the perceptual magnet effect for speech using signal detection theory and multidimensional scaling". <it>Journal of the Acoustical Society of America</it>&#32;<b>97(1)</b>: 553–562. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1121%2F1.412280">
10.1121/1.412280</weblink>.</cite>&nbsp;</entry>
<entry id="14">
 <cite style="font-style:normal">Lisker, L., Abramson, A.S.&#32;(1970). "<weblink xlink:type="simple" xlink:href="http://www.haskins.yale.edu/Reprints/HL0087.pdf">
The voicing dimension: Some experiments in comparative phonetics</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>).&#32;<it>Proc. 6th International Congress of Phonetic Sciences</it>: 563-567, Prague:&#32;Academia. Retrieved on <link>
2007-05-17</link>.</cite>&nbsp;</entry>
<entry id="15">
 <cite style="font-style:normal">Warren, R.M.&#32;(1970).&#32;"Restoration of missing speech sounds". <it>Science</it>&#32;<b>167</b>: 392–393. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1126%2Fscience.167.3917.392">
10.1126/science.167.3917.392</weblink>. PMID 5409744.</cite>&nbsp;</entry>
<entry id="17">
 <cite style="font-style:normal">Minagawa-Kawai, Y., Mori, K., Naoi, N., Kojima, S.&#32;(2006).&#32;"Neural Attunement Processes in Ifants during the Acquisition of a Language-Specific Phonemic Contrast". <it>The Journal of Neuroscience</it>&#32;<b>27(2)</b>: 315–321.</cite>&nbsp;</entry>
<entry id="16">
 <cite style="font-style:normal">Garnes, S., Bond, Z.S.&#32;(1976). "The relationship between acoustic information and semantic expectation".&#32;<it>Phonologica 1976</it>: 285-293.</cite>&nbsp;</entry>
<entry id="19">
 <cite style="font-style:normal">Iverson, P., Kuhl, P.K., Akahane-Yamada, R., Diesh, E., Thokura,  Y., Kettermann, A., Siebert, C.,&#32;(2003).&#32;"A perceptual interference account of acquisition difficulties for non-native phonemes". <it>Cognition</it>&#32;<b>89</b>: B47–B57. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2FS0010-0277%2802%2900198-1">
10.1016/S0010-0277(02)00198-1</weblink>.</cite>&nbsp;</entry>
<entry id="18">
 <cite style="font-style:normal" class="book">Crystal, David&#32;(2005). The Cambridge Encyclopedia of Language.&#32;Cambridge:&#32;CUP.</cite>&nbsp;</entry>
<entry id="21">
 <cite style="font-style:normal">Flege, J.,&#32;(1995).&#32;"Second language speech learning: Theory, findings and problems".&#32;<it>Speech perception and linguistic experience: Theoretical and methodological issues</it>.&#32;&#32;Ed. Winifred Strange.&#32;Baltimore:&#32;York Press.&#32;233–277.</cite>&nbsp;</entry>
<entry id="20">
 <cite style="font-style:normal">Best, K.,&#32;(1995).&#32;"A direct realist view of cross-language speech perception: New Directions in Research and Theory".&#32;<it>Speech perception and linguistic experience: Theoretical
and methodological issues</it>.&#32;&#32;Ed. Winifred Strange.&#32;Baltimore:&#32;York Press.&#32;171–204.</cite>&nbsp;</entry>
<entry id="23">
 <cite style="font-style:normal">Loizou, P.&#32;(1998).&#32;"Introduction to cochlear implants". <it>IEEE Signal Processing Magazine</it>&#32;<b>39(11)</b>: 101–130.</cite>&nbsp;</entry>
<entry id="22">
 <cite style="font-style:normal">Csépe, V., Osman-Sagi, J., Molnar M., Gosy M.&#32;(2001).&#32;"Impaired speech perception in aphasic patients: event-related potential and neuropsychological assessment". <it>Neuropsychologia</it>&#32;<b>39(11)</b>: 1194–1208. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2FS0028-3932%2801%2900052-5">
10.1016/S0028-3932(01)00052-5</weblink>.</cite>&nbsp;</entry>
<entry id="25">
 <cite style="font-style:normal">Kazanina, N., Phillips, C., Idsardi, W.&#32;(2006). "<weblink xlink:type="simple" xlink:href="http://aix1.uottawa.ca/~nkazanin/Papers/kazanina-phillips-idsardi_PNAS_2006_reprint.pdf">
The influence of meaning on the perception of speech sounds</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>).&#32;<it>PNAS</it>&#32;<b>30</b>: 11381-11386. Retrieved on <link>
2007-05-19</link>.</cite>&nbsp;</entry>
<entry id="24">
 <cite style="font-style:normal">McClelland, J. L. and Elman, J. L.&#32;(1986).&#32;"<weblink xlink:type="simple" xlink:href="http://www.cnbc.cmu.edu/~jlm/papers/McClellandElman86.pdf">
The TRACE model of speech perception</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>Cognitive Psychology</it>&#32;<b>18</b>: 1–86. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2F0010-0285%2886%2990015-0">
10.1016/0010-0285(86)90015-0</weblink>. Retrieved on <link>
2007-05-19</link>.</cite>&nbsp;</entry>
<entry id="27">
 <cite style="font-style:normal">Dehaene-Lambertz, G., Pallier, C., Serniclaes, W., Sprenger-Charolles, L., Jobert, A., &amp; Dehaene, S.&#32;(2005).&#32;"<weblink xlink:type="simple" xlink:href="http://www.pallier.org/papers/Dehaene-LambertzPallier_Sinewaves_Neuroimgage_2004.pdf">
Neural correlates of switching from auditory to speech perception</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>NeuroImage</it>&#32;<b>24</b>: 21–33. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2Fj.neuroimage.2004.09.039">
10.1016/j.neuroimage.2004.09.039</weblink>. Retrieved on <link>
2007-07-04</link>.</cite>&nbsp;</entry>
<entry id="26">
 <cite style="font-style:normal">Gocken, J. M. &amp; Fox R. A.&#32;(2001).&#32;"Neurological Evidence in Support of a Specialized Phonetic Processing Module". <it>Brain and Language</it>&#32;<b>78</b>: 241–253. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1006%2Fbrln.2001.2467">
10.1006/brln.2001.2467</weblink>.</cite>&nbsp;</entry>
<entry id="29">
 <cite style="font-style:normal">Liberman, A.M., Harris, K.S., Hoffman, H.S., Griffith, B.C.&#32;(1957).&#32;"<weblink xlink:type="simple" xlink:href="http://www.haskins.yale.edu/Reprints/HL0022.pdf">
The discrimination of speech sounds within and across phoneme boundaries</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>Journal of Experimental Psychology</it>&#32;<b>54</b>: 358–368. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1037%2Fh0044417">
10.1037/h0044417</weblink>. Retrieved on <link>
2007-05-18</link>.</cite>&nbsp;</entry>
<entry id="28">
 <cite style="font-style:normal">Näätänen, R.&#32;(2001).&#32;"The perception of speech sounds by the human brain as reflected by the mismatch negativity (MMN) and its magnetic equivalent (MMNm)". <it>Psychophysiology</it>&#32;<b>38</b>: 1–21.</cite>&nbsp;</entry>
<entry id="31">
 <cite style="font-style:normal">Liberman, A. M.&#32;(1970).&#32;"<weblink xlink:type="simple" xlink:href="http://www.haskins.yale.edu/Reprints/HL0099.pdf">
The grammars of speech and language</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>Cognitive Psychology</it>&#32;<b>1</b>: 301–323. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2F0010-0285%2870%2990018-6">
10.1016/0010-0285(70)90018-6</weblink>. Retrieved on <link>
2007-07-19</link>.</cite>&nbsp;</entry>
<entry id="30">
 <cite style="font-style:normal">Liberman, A.M., Cooper, F.S., Shankweiler, D.P., &amp; Studdert-Kennedy, M.&#32;(1967).&#32;"<weblink xlink:type="simple" xlink:href="http://www.haskins.yale.edu/Reprints/HL0069.pdf">
Perception of the speech code.</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>Psychological Review</it>&#32;<b>74</b>: 431–461. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1037%2Fh0020279">
10.1037/h0020279</weblink>. Retrieved on <link>
2007-05-19</link>.</cite>&nbsp;</entry>
<entry id="34">
 <cite style="font-style:normal">Massaro, D.W.&#32;(1989).&#32;"Testing between the TRACE Model and the Fuzzy Logical Model of Speech perception". <it>Cognitive Psychology</it>&#32;<b>21</b>: 398–421. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2F0010-0285%2889%2990014-5">
10.1016/0010-0285(89)90014-5</weblink>.</cite>&nbsp;</entry>
<entry id="35">
 <cite id="Reference-Hayward-2000" style="font-style:normal" class="book">Hayward, Katrina&#32;(2000). Experimental Phonetics: An Introduction.&#32;Harlow:&#32;Longman.</cite>&nbsp;</entry>
<entry id="32">
 <cite style="font-style:normal">Liberman, A. M. &amp; Mattingly, I. G.&#32;(1985).&#32;"<weblink xlink:type="simple" xlink:href="http://www.haskins.yale.edu/Reprints/HL0519.pdf">
The motor theory of speech perception revised</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>Cognition</it>&#32;<b>21</b>: 1–36. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2F0010-0277%2885%2990021-6">
10.1016/0010-0277(85)90021-6</weblink>. Retrieved on <link>
2007-07-19</link>.</cite>&nbsp;</entry>
<entry id="33">
 <cite style="font-style:normal">Randy L.  Diehl; Andrew J.  Lotto; Lori L.  Holt&#32;(2004).&#32;"Speech perception". <it>Annual Revue of Psychology</it>&#32;<b>55</b>: 149–179. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1146%2Fannurev.psych.55.090902.142028">
10.1146/annurev.psych.55.090902.142028</weblink>.</cite>&nbsp;</entry>
<entry id="36">
 <cite style="font-style:normal">Oden, G. C., Massaro, D. W.&#32;(1978).&#32;"Integration of featural information in speech perception". <it>Psychological Review</it>&#32;<b>85</b>: 172–191.. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1037%2F0033-295X.85.3.172">
10.1037/0033-295X.85.3.172</weblink>.</cite>&nbsp;</entry>
<entry id="37">
 <cite style="font-style:normal">Stevens,  K.N.&#32;(2002).&#32;"<weblink xlink:type="simple" xlink:href="http://linguistics.berkeley.edu/~kjohnson/ling210/stevens2002.pdf">
Toward a model of lexical access based on acoustic landmarks and distinctive features</weblink>"&#32;(<link xlink:type="simple" xlink:href="../077/24077.xml">
PDF</link>). <it>Journal of the Acoustical Society of America</it>&#32;<b>111(4)</b>: 1872–1891. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1121%2F1.1458026">
10.1121/1.1458026</weblink>. Retrieved on <link>
2007-05-17</link>.</cite>&nbsp;</entry>
</reflist>
</p>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://publishing.royalsociety.org/perception-speech">
Dedicated issue of <it>Philosophical Transactions B</it> on the Perception of Speech.  Some articles are freely available.</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
