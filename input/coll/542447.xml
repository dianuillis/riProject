<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:15:37[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Self-information</title>
<id>542447</id>
<revision>
<id>242018717</id>
<timestamp>2008-09-30T15:24:28Z</timestamp>
<contributor>
<username>YOMHER</username>
<id>38178</id>
</contributor>
</revision>
<categories>
<category>Entropy and information</category>
<category>Information theory</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link> (elaborated by <link xlink:type="simple" xlink:href="../693/5693.xml">
Claude E. Shannon</link>, <link xlink:type="simple" xlink:href="../612/34612.xml">
1948</link>), <b>self-information</b> is a measure of the information content associated with the <it>outcome</it> of a <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>. It is expressed in a <unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link xlink:type="simple" xlink:href="../364/586364.xml">
unit</link></definite_quantity>
</unit_of_measurement>
 of <link xlink:type="simple" xlink:href="../062/18985062.xml">
information</link>, for example <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link>s, 
<unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link>
nat</link></definite_quantity>
</unit_of_measurement>
s, 
or 
<unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link xlink:type="simple" xlink:href="../780/3070780.xml">
hartley</link></definite_quantity>
</unit_of_measurement>
s 
(also known as digits, dits, bans), depending on the base of the logarithm used in its definition.<p>

By definition, the amount of self-information contained in a probabilistic <link xlink:type="simple" xlink:href="../961/22961.xml">
event</link> depends only on the <link xlink:type="simple" xlink:href="../934/22934.xml">
probability</link> of that event: the smaller its probability, the larger the self-information associated with receiving the information that the event indeed occurred. </p>
<p>

Further, by definition, the <link xlink:type="simple" xlink:href="../022/19022.xml">
measure</link> of self-information has the following property. If an event <it>C</it> is composed of two mutually <link xlink:type="simple" xlink:href="../593/27593.xml">
independent</link> events <it>A</it> and <it>B</it>, then the amount of information at the proclamation that <it>C</it> has happened, equals the <b>sum</b> of the amounts of information at proclamations of event <it>A</it> and event <it>B</it> respectively. </p>
<p>

Taking into account these properties, the self-information <math>I(\omega_n)</math> (measured in <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link>s) associated with outcome  <math>\omega_n</math> is:</p>
<p>

<indent level="1">

<math>I(\omega_n) = \log_2 \left(\frac{1}{\Pr(\omega_n)} \right) = - \log_2(\Pr(\omega_n)) </math> 
</indent>

This definition, using the <link xlink:type="simple" xlink:href="../992/249992.xml">
binary logarithm</link> function, complies with the above conditions.
In the above definition, the logarithm of base 2 was used, and thus the unit of 
<math>\displaystyle I(\omega_n)</math> is in <link xlink:type="simple" xlink:href="../364/3364.xml">
bit</link>.  
When using the logarithm of base <math>\displaystyle e</math>, the unit will be in 
<unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link>
nat</link></definite_quantity>
</unit_of_measurement>
.  
For the log of base 10, the unit will be in <unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link xlink:type="simple" xlink:href="../780/3070780.xml">
hartley</link></definite_quantity>
</unit_of_measurement>
.</p>
<p>

This measure has also been called <b>surprisal</b>, as it represents the "<link xlink:type="simple" xlink:href="../210/4656210.xml">
surprise</link>" of seeing the outcome (a highly probable outcome is not surprising). This term was coined by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../560/2671560.xml">
Myron Tribus</link></educator>
</professional>
</adult>
</academician>
</causal_agent>
</engineer>
</person>
</physical_entity>
 in his <link xlink:type="simple" xlink:href="../659/34659.xml">
1961</link> book <it>Thermostatics and Thermodynamics</it>.</p>
<p>

The <link xlink:type="simple" xlink:href="../445/15445.xml">
information entropy</link> of a random event is the <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> of its self-information.</p>
<p>

<b>Self-information</b> is an example of a <link>
proper scoring rule</link>.</p>

<sec>
<st>
Examples</st>
<p>
 
<list>
<entry level="1" type="bullet">

On <link xlink:type="simple" xlink:href="../410/494410.xml">
tossing a coin</link>, the chance of 'tail' is 0.5.  When it is proclaimed that indeed 'tail' occurred, this amounts to </entry>
<entry level="1" type="indent">

 <it>I</it>('tail') = log2 (1/0.5) = log2 2 = 1 bits of information.  </entry>
<entry level="1" type="bullet">

When throwing a fair <link xlink:type="simple" xlink:href="../244/8244.xml">
die</link>, the probability of 'four' is 1/6.  When it is proclaimed that 'four' has been thrown, the amount of self-information is </entry>
<entry level="1" type="indent">

<it>I</it>('four') = log2 (1/(1/6)) = log2 (6) = 2.585 bits. </entry>
<entry level="1" type="bullet">

When, independently, two dice are thrown, the amount of information associated with {throw 1 = 'two' &amp; throw 2 = 'four'} equals </entry>
<entry level="1" type="indent">

<it>I</it>('throw 1 is two &amp; throw 2 is four') = log2 (1/P(throw 1 = 'two' &amp; throw 2 = 'four')) = log2 (1/(1/36)) = log2 (36) = 5.170 bits. This outcome equals the sum of the individual amounts of self-information associated with {throw 1 = 'two'} and {throw 2 = 'four'}; namely 2.585 + 2.585 = 5.170 bits.</entry>
<entry level="1" type="bullet">

Suppose that the average probability of finding survivors in a large evolving population is <it>P</it>, then - when a survivor has been found, the amount of self-information will be -loge(<it>P</it>) nats (-log2(<it>P</it>) bits).</entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

C.E. Shannon, A Mathematical Theory of Communication, Bell Syst. Techn. J., Vol. 27, pp 379-423, (Part I), 1948.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.umsl.edu/~fraundor/egsurpri.html">
Examples of surprisal measures</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.hum.uva.nl/mmm/abstracts/honing-2005f.html">
Towards a measure of surprise</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cmh.edu/stats/definitions/entropy.htm">
Entropy and surprisal</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.lecb.ncifcrf.gov/~toms/glossary.html#surprisal">
"Surprisal" entry in a glossary of molecular information theory</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://ilab.usc.edu/surprise/">
Bayesian Theory of Surprise</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
