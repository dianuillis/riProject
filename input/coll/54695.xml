<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:45:47[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>RAID</title>
<id>54695</id>
<revision>
<id>244397939</id>
<timestamp>2008-10-10T16:35:34Z</timestamp>
<contributor>
<username>Salix alba</username>
<id>212526</id>
</contributor>
</revision>
<categories>
<category>RAID</category>
</categories>
</header>
<bdy>

This article is about the computer-related system.&#32;&#32;For other uses, see <link xlink:type="simple" xlink:href="../589/340589.xml">
Raid</link>.&#32;&#32;<p>

<b>RAID</b> — which stands for <b>Redundant Array of Inexpensive Disks</b> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>,
or alternatively <b>Redundant Array of Independent Disks</b> (a less specific name, and thus now the generally accepted one<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>) — is a technology that employs the simultaneous use of two or more <link xlink:type="simple" xlink:href="../777/13777.xml">
hard disk drives</link> to achieve greater levels of performance, reliability, and/or larger data volume sizes.</p>
<p>

The phrase "RAID" is an <link xlink:type="simple" xlink:href="../388/1313388.xml">
umbrella term</link> for <link xlink:type="simple" xlink:href="../300/5300.xml">
computer data storage</link> schemes that can divide and replicate <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../333/2234333.xml">
data</link></datum>
</information>
 among multiple hard disk drives.  RAID's various designs all involve two key design goals: increased <link xlink:type="simple" xlink:href="../838/5380838.xml">
data reliability</link> and increased <link xlink:type="simple" xlink:href="../558/14558.xml">
input/output</link> performance.  When several physical disks are set up to use RAID technology, they are said to be <it>in</it> a <it>RAID</it> array.  This array distributes data across several disks, but the array is seen by the computer user and <link xlink:type="simple" xlink:href="../194/22194.xml">
operating system</link> as one single disk.  RAID can be set up to serve several different purposes.</p>

<sec>
<st>
 Purpose and basics </st>

<p>

Some arrays are "<link xlink:type="simple" xlink:href="../581/1953581.xml">
redundant</link>" in a way that writes extra data derived from the original data across the array organized so that the failure of one (sometimes more) disks in the array will not result in loss of data; the bad disk is replaced by a new one, and the data on it reconstructed from the remaining data and the extra data. A redundant array allows less data to be stored.  For instance, a 2-disk RAID 1 array loses half of its capacity, and a RAID 5 array with several disks loses the capacity of one disk.</p>
<p>

Other RAID arrays are arranged so that they are faster to write to and read from than a single disk.</p>
<p>

There are various combinations of these approaches giving different trade offs of protection against data loss, capacity, and speed. RAID levels 0, 1, and 5 are the most commonly found, and cover most requirements. </p>
<p>

<list>
<entry level="1" type="bullet">

 RAID 0 (striped disks) distributes data across several disks in a way that gives improved speed and full capacity, but all data on all disks will be lost if any one disk fails. </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 RAID 1 (mirrored disks) could be described as a backup solution, using two (possibly more) disks that each store the same data so that data is not lost as long as one disk survives.  Total capacity of the array is just the capacity of a single disk. The failure of one drive, in the event of a hardware or software malfunction, does not increase the chance of a failure nor decrease the reliability of the remaining drives (second, third, etc).</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 RAID 5 (striped disks with parity) combines three or more disks in a way that protects data against loss of any one disk; the storage capacity of the array is reduced by one disk.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 RAID 6 (less common) can recover from the loss of two disks.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 RAID 10 (or 1+0) uses both striping and mirroring. </entry>
</list>
</p>
<p>

RAID involves significant computation when reading and writing information. With true RAID hardware the controller does all of this computation work. In other cases the operating system or simpler and less expensive controllers require the host computer's processor to do the computing, which reduces the computer's performance on processor-intensive tasks (see "Software RAID" and "Fake RAID" below). Simpler <link xlink:type="simple" xlink:href="../505/475505.xml">
RAID controller</link>s may provide only levels 0 and 1, which require less processing.</p>
<p>

RAID systems with redundancy continue working without interruption when one, or sometimes more, disks of the array fail, although they are vulnerable to further failures. When the bad disk is replaced by a new one the array is rebuilt while the system continues to operate normally. Some systems have to be shut down when removing or adding a drive; others support <link xlink:type="simple" xlink:href="../587/287587.xml">
hot swapping</link>, allowing drives to be replaced without powering down. RAID with hot-swap drives is often used in <link xlink:type="simple" xlink:href="../661/3824661.xml">
high availability</link> systems, where it is important that the system keeps running as much of the time as possible.</p>
<p>

RAID is not a good alternative to <link xlink:type="simple" xlink:href="../867/533867.xml">
backing up</link> data. Data may become damaged or destroyed without harm to the drive(s) on which it is stored. For example, part of the data may be overwritten by a system malfunction; a file may be damaged or deleted by user error or malice and not noticed for days or weeks; and of course the entire array is at risk of catastrophes such as theft, flood, and fire.</p>

</sec>
<sec>
<st>
 Principles </st>

<p>

RAID combines two or more physical hard disks into a single logical unit by using either special hardware or software.  Hardware solutions often are designed to present themselves to the attached system as a single hard drive, and the <link xlink:type="simple" xlink:href="../194/22194.xml">
operating system</link> is unaware of the technical workings.  Software solutions are typically implemented in the operating system, and again would present the RAID drive as a single drive to applications.</p>
<p>

There are three key concepts in RAID: <link xlink:type="simple" xlink:href="../823/3118823.xml">
mirroring</link>, the copying of data to more than one disk; <link xlink:type="simple" xlink:href="../567/474567.xml">
striping</link>, the splitting of data across more than one disk; and <link xlink:type="simple" xlink:href="../375/10375.xml">
error correction</link>, where redundant data is stored to allow problems to be detected and possibly fixed (known as <link xlink:type="simple" xlink:href="../720/2573720.xml">
fault tolerance</link>).  Different RAID levels use one or more of these techniques, depending on the system requirements.  The main aims of using RAID are to improve reliability, important for protecting information that is critical to a business, for example a database of customer orders; or to improve speed, for example a system that delivers <link xlink:type="simple" xlink:href="../143/147143.xml">
video on demand</link> TV programs to many viewers.</p>
<p>

The configuration affects reliability and performance in different ways.  The problem with using more disks is that it is more likely that one will go wrong, but by using error checking the total system can be made more reliable by being able to survive and repair the failure.  Basic mirroring can speed up reading data as a system can read different data from both the disks, but it may be slow for writing if the configuration requires that both disks must confirm that the data is correctly written.  Striping is often used for performance, where it allows sequences of data to be read from multiple disks at the same time.  Error checking typically will slow the system down as data needs to be read from several places and compared.  The design of RAID systems is therefore a compromise and understanding the requirements of a system is important.  Modern <link xlink:type="simple" xlink:href="../485/475485.xml">
disk array</link>s typically provide the facility to select the appropriate RAID configuration. PC Format Magazine claims that "in all our real-world tests, the difference between the single drive performance and the dual-drive RAID 0 striped setup was virtually non-existent. And in fact, the single drive was ever-so-slightly faster than the other setups, including the RAID 5 system that we'd hoped would offer the perfect combination of performance and data redundancy"<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.</p>

</sec>
<sec>
<st>
 Standard levels </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../316/8506316.xml">
Standard RAID levels</link></it>
</indent>

A number of standard schemes have evolved which are referred to as <it>levels</it>.  There were five RAID levels originally conceived, but many more variations have evolved, notably several <link xlink:type="simple" xlink:href="../329/8506329.xml">
nested levels</link> and many <link xlink:type="simple" xlink:href="../375/8506375.xml">
non-standard levels</link> (mostly <link xlink:type="simple" xlink:href="../886/18934886.xml">
proprietary</link>).</p>
<p>

Following is a brief summary of the most commonly used RAID levels.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></p>
<p>

<table style="text-align: center;" class="wikitable">
<row>
<header>
Level</header>
<header>
Description</header>
<header>
Minimum # of disks</header>
<header>
Image</header>
</row>
<row>
<col>
 <link>
RAID&nbsp;0</link></col>
<col align="left">
 <b><link xlink:type="simple" xlink:href="../567/474567.xml">
Striped</link> set without <link xlink:type="simple" xlink:href="../467/194467.xml">
parity</link>/[Non-Redundant Array].</b>  Provides improved performance and additional storage but no fault tolerance.  Any disk failure destroys the array, which becomes more likely with more disks in the array.  A single disk failure destroys the entire array because when data is written to a RAID 0 drive, the data is broken into fragments.  The number of fragments is dictated by the number of disks in the array.  The fragments are written to their respective disks simultaneously on the same sector.  This allows smaller sections of the entire chunk of data to be read off the drive in parallel, giving this type of arrangement huge bandwidth.  RAID 0 does not implement error checking so any error is unrecoverable.  More disks in the array means higher bandwidth, but greater risk of data loss.</col>
<col>
2</col>
<col>
 <image width="100px" src="RAID_0.svg">
<caption>

RAID Level 0
</caption>
</image>
</col>
</row>
<row>
<col>
 <link>
RAID&nbsp;1</link></col>
<col align="left">
 <b><link xlink:type="simple" xlink:href="../823/3118823.xml">
Mirrored</link> set without parity.</b>  Provides fault tolerance from disk errors and failure of all but one of the drives.  Increased read performance occurs when using a <link xlink:type="simple" xlink:href="../303/45303.xml">
multi-threaded</link>operating system that supports split seeks, very small performance reduction when writing.  Array continues to operate so long as at least one drive is functioning.  Using RAID 1 with a separate controller for each disk is sometimes called <it>duplexing</it>.</col>
<col>
2</col>
<col>
 <image width="100px" src="RAID_1.svg">
<caption>

RAID Level 1
</caption>
</image>
</col>
</row>
<row>
<col>
 <link>
RAID&nbsp;2</link></col>
<col align="left">
 <b>Redundancy through <link xlink:type="simple" xlink:href="../226/41226.xml">
Hamming code</link>. </b>Disks are synchronised and striped in very small stripes, often in single bytes/words. Hamming codes <link xlink:type="simple" xlink:href="../375/10375.xml">
error correction</link>is calculated across corresponding bits on disks, and is stored on multiple parity disks.</col>
<col>
3</col>

</row>
<row >
<col>
 <link>
RAID&nbsp;3</link></col>
<col align="left">
<b>Striped set with dedicated parity/Bit interleaved parity.</b>  This mechanism provides an improved performance and fault tolerance similar to RAID 5, but with a dedicated parity disk rather than rotated parity stripes.  The single parity disk is a bottle-neck for writing since every write requires updating the parity data.  One minor benefit is the dedicated parity disk allows the parity drive to fail and operation will continue without parity or performance penalty.</col>
<col>
3</col>
<col>
 <image width="150px" src="RAID_3.svg">
<caption>

RAID Level 3
</caption>
</image>
</col>
</row>
<row>
<col>
 <link>
RAID&nbsp;4</link></col>
<col align="left">
<b>Block level parity.</b> Identical to RAID 3, but does block-level striping instead of byte-level striping. In this setup,  files can be distributed between multiple disks. Each disk operates independently which allows I/O requests to be performed in parallel, though data transfer speeds can suffer due to the type of parity. The error detection is achieved through dedicated parity and is stored in a separate, single disk unit.</col>
<col>
3</col>
<col>
 <image width="150px" src="RAID_4.svg">
<caption>

RAID Level 4
</caption>
</image>
</col>
</row>
<row>
<col>
 <link>
RAID&nbsp;5</link></col>
<col align="left">
<b>Striped set with distributed parity.</b>  Distributed parity requires all drives but one to be present to operate; drive failure requires replacement, but the array is not destroyed by a single drive failure.  Upon drive failure, any subsequent reads can be calculated from the distributed parity such that the drive failure is masked from the end user.  The array will have data loss in the event of a second drive failure and is vulnerable until the data that was on the failed drive is rebuilt onto a replacement drive.</col>
<col>
3</col>
<col>
 <image width="150px" src="RAID_5.svg">
<caption>

RAID Level 5
</caption>
</image>
</col>
</row>
<row>
<col>
 <link>
RAID&nbsp;6</link></col>
<col align="left">
<b>Striped set with dual distributed Parity.</b>  Provides fault tolerance from two drive failures; array continues to operate with up to two failed drives.  This makes larger RAID groups more practical, especially for high availability systems.  This becomes increasingly important because large-capacity drives lengthen the time needed to recover from the failure of a single drive.  Single parity RAID levels are vulnerable to data loss until the failed drive is rebuilt: the larger the drive, the longer the rebuild will take.  Dual parity gives time to rebuild the array without the data being at risk if one drive, but no more, fails before the rebuild is complete.</col>
<col>
4</col>
<col>
 <image width="175px" src="RAID_6.svg">
<caption>

RAID Level 6
</caption>
</image>
</col>
</row>
</table>
</p>

</sec>
<sec>
<st>
 Nested levels </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../329/8506329.xml">
Nested RAID levels</link></it>
</indent>
Many storage controllers allow RAID levels to be nested: the elements of a <it>RAID</it> may be either individual disks or RAIDs themselves. Nesting more than two deep is unusual. </p>
<p>

As there is no basic RAID level numbered larger than 9, nested RAIDs are usually unambiguously described by concatenating the numbers indicating the RAID levels, sometimes with a "+" in between.  For example, RAID 10 (or RAID 1+0) consists of several level 1 arrays of physical drives, each of which is one of the "drives" of a level 0 array striped over the level 1 arrays.  It is not called RAID 01, to avoid confusion with RAID 1, or indeed, <link>
RAID 01</link>.  When the top array is a RAID 0 (such as in RAID 10 and RAID 50) most vendors omit the "+", though RAID 5+0 is clearer. </p>
<p>

<list>
<entry level="1" type="bullet">

RAID 0+1: striped sets in a mirrored set (minimum four disks; even number of disks) provides fault tolerance and improved performance but increases complexity.  The key difference from RAID 1+0 is that RAID 0+1 creates a second striped set to mirror a primary striped set.  The array continues to operate with one or more drives failed in the same mirror set, but if drives fail on both sides of the mirror the data on the RAID system is lost.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

RAID 1+0: mirrored sets in a striped set (minimum four disks; even number of disks) provides fault tolerance and improved performance but increases complexity.  The key difference from RAID 0+1 is that RAID 1+0 creates a striped set from a series of mirrored drives.  In a failed disk situation, RAID 1+0 performs better because all the remaining disks continue to be used.  The array can sustain multiple drive losses so long as no mirror loses all its drives.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

RAID 5+0: stripe across distributed parity RAID systems.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

RAID 5+1: mirror striped set with distributed parity (some manufacturers label this as RAID 53).</entry>
</list>
</p>

</sec>
<sec>
<st>
 Non-standard levels </st>


<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../375/8506375.xml">
Non-standard RAID levels</link></it>
</indent>
Many configurations other than the basic numbered RAID levels are possible, and many companies, organizations, and groups have created their own non-standard configurations, in many cases designed to meet the specialised needs of a small niche group.  Most of these non-standard RAID levels are <link xlink:type="simple" xlink:href="../148/166148.xml">
proprietary</link>.</p>
<p>

Some of the more prominent modifications are:
<list>
<entry level="1" type="bullet">

 <link>
Storage Computer Corporation</link> uses <it>RAID 7</it>, which adds <link xlink:type="simple" xlink:href="../829/6829.xml">
caching</link> to RAID 3 and RAID 4 to improve I/O performance.</entry>
<entry level="1" type="bullet">

 <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../707/515707.xml">
EMC Corporation</link></company>
 offered <it>RAID S</it> as an alternative to RAID 5 on their <link xlink:type="simple" xlink:href="../764/730764.xml">
Symmetrix</link> systems (which is no longer supported on the latest releases of Enginuity, the Symmetrix's operating system).</entry>
<entry level="1" type="bullet">

 The <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../693/1205693.xml">
ZFS</link></instrumentality>
</artifact>
</system>
 filesystem, available in <message wordnetid="106598915" confidence="0.8">
<information wordnetid="106634376" confidence="0.8">
<electronic_database wordnetid="106588511" confidence="0.8">
<lexical_database wordnetid="106638868" confidence="0.8">
<wordnet wordnetid="106639428" confidence="0.8">
<database wordnetid="106637824" confidence="0.8">
<link xlink:type="simple" xlink:href="../145/46145.xml">
Solaris</link></database>
</wordnet>
</lexical_database>
</electronic_database>
</information>
</message>
, <link xlink:type="simple" xlink:href="../658/1178658.xml">
OpenSolaris</link>, <platform wordnetid="103961939" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<surface wordnetid="104362025" confidence="0.8">
<horizontal_surface wordnetid="103536348" confidence="0.8">
<link xlink:type="simple" xlink:href="../554/7580554.xml">
FreeBSD</link></horizontal_surface>
</surface>
</artifact>
</platform>
 and <link xlink:type="simple" xlink:href="../640/20640.xml">
Mac OS X</link>, offers <it><link xlink:type="simple" xlink:href="../375/8506375.xml#xpointer(//*[./st=%22RAID-Z%22])">
RAID-Z</link></it>, which solves RAID 5's <link xlink:type="simple" xlink:href="../316/8506316.xml#xpointer(//*[./st=%22RAID+5+performance%22])">
write hole</link> problem.</entry>
<entry level="1" type="bullet">

 <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../623/855623.xml">
NetApp</link></company>
's Data ONTAP uses RAID-DP (also referred to as "double", "dual" or "diagonal" parity), which is a form of RAID 6, but unlike many RAID 6 implementations, does not use distributed parity as in RAID 5.  Instead, two unique parity disks with separate parity calculations are used.  This is a modification of RAID 4 with an extra parity disk.</entry>
<entry level="1" type="bullet">

 <link>
Accusys</link> <it>Triple Parity</it> (RAID TP) implements three independent parities by extending RAID 6 algorithms on its FC-SATA and SCSI-SATA RAID controllers to tolerate three-disk failure.</entry>
<entry level="1" type="bullet">

 <O wordnetid="106832680" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../297/6097297.xml">
Linux</link></O>
 <link>
MD RAID10</link> (RAID10) implements a general RAID driver that defaults to a standard RAID 1+0 with 4 drives, but can have any number of drives, including an odd number.  MD RAID10 can run striped and mirrored with only 2 drives with the f2 layout (mirroring with striped reads, normal Linux software RAID 1 does not stripe reads, but can read in parallel) <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.</entry>
<entry level="1" type="bullet">

 <link>
Infrant</link> (Now part of <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../294/1197294.xml">
Netgear</link></company>
) <it>X-RAID</it> offers dynamic expansion of a RAID5 volume without having to backup/restore the existing content.  Just add larger drives one at a time, let it resync, then add the next drive until all drives are installed.  The resulting volume capacity is increased without user downtime.</entry>
<entry level="1" type="bullet">

 <link>
BeyondRAID</link> created by <link>
Data Robotics</link> and used in the Drobo series of products, implements both mirroring and striping simultaneously or individually dependent on disk and data context. BeyondRAID is more automated and easier to use than many standard RAID levels. It also offers instant expandability without reconfiguration, the ability to mix and match drive sizes and the ability to reorder disks. It is a block-level system and thus file system agnostic although today support is limited to NTFS, HFS+, FAT32, and EXT3. It also utilizes thin provisioning to allow for single volumes up to 16TB depending on the host operating system support.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Implementations </st>
<p>

The distribution of data across multiple drives can be managed either by dedicated <link xlink:type="simple" xlink:href="../615/13615.xml">
hardware</link> or by <link xlink:type="simple" xlink:href="../309/5309.xml">
software</link>.  When done in software the software may be part of the operating system or it may be part of the firmware and drivers supplied with the card.</p>

<ss1>
<st>
Operating system based ("software RAID")</st>
<p>

Software implementations are now provided by many <link xlink:type="simple" xlink:href="../194/22194.xml">
operating systems</link>.  A software layer sits above the (generally <link>
block</link>-based) disk <link xlink:type="simple" xlink:href="../101/9101.xml">
device driver</link>s and provides an abstraction layer between the <link xlink:type="simple" xlink:href="../810/9688810.xml">
logical drives</link> (RAIDs) and <link xlink:type="simple" xlink:href="../472/Species_8472.xml">
physical drives</link>.  Most common levels are RAID 0 (striping across multiple drives for increased space and performance) and RAID 1 (mirroring two drives), followed by RAID 1+0, RAID 0+1, and RAID 5 (data striping with parity) are supported.  </p>
<p>

Microsoft's server operating systems support 3 RAID levels; RAID 0, RAID 1, and RAID 5.  Some of the Microsoft desktop operating systems support RAID such as Windows XP Professional which supports RAID level 0 in addition to spanning multiple disks but only if using dynamic disks and volumes.   </p>
<p>

Apple's <link xlink:type="simple" xlink:href="../086/47086.xml">
Mac OS X Server</link> supports RAID 0, RAID 1, and RAID 1+0.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></p>
<p>

FreeBSD supports RAID 0, RAID 1, RAID 3, and RAID 5.</p>
<p>

<link xlink:type="simple" xlink:href="../495/21495.xml">
NetBSD</link> supports RAID 0, RAID 1, RAID 4 and RAID 5 (and any nested combination of those like 1+0) via its software implementation, named raidframe.</p>
<p>

<link xlink:type="simple" xlink:href="../658/1178658.xml">
OpenSolaris</link> and <link xlink:type="simple" xlink:href="../145/46145.xml">
Solaris 10</link> supports RAID 0, RAID 1, RAID 5, and RAID 6 (and any nested combination of those like 1+0) via ZFS with limited support on the system hard drive ( RAID 1 only ). Through SVM, Solaris 10 and earlier versions support RAID 0, RAID 1, and RAID 5 on both system and data drives</p>
<p>

The software must run on a host server attached to storage, and server's processor must dedicate processing time to run the RAID software.  This is negligible for RAID 0 and RAID 1, but may be significant for more complex parity-based schemes.  Furthermore all the busses between the processor and the disk controller must carry the extra data required by RAID which may cause congestion.  </p>
<p>

Another concern with operating system-based RAID is the boot process, it can be difficult or impossible to set up the boot process such that it can failover to another drive if the usual boot drive fails and therefore such systems can require manual intervention to make the machine bootable again after a failure.  Finally operating system-based RAID usually uses formats specific to the operating system in question so it cannot generally be used for partitions that are shared between operating systems as part of a multi-boot setup.</p>
<p>

Most operating system-based implementations allow RAIDs to be created from <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../380/55380.xml">
partition</link></instrumentality>
</artifact>
</system>
s rather than entire physical drives.  For instance, an administrator could divide an odd number of disks into two partitions per disk, mirror partitions across disks and stripe a volume across the mirrored partitions to emulate a RAID 1E configuration.  Using partitions in this way also allows mixing reliability levels on the same set of disks.  For example, one could have a very robust RAID-1 partition for important files, and a less robust RAID-5 or RAID-0 partition for less important data.  (Some controllers offer similar features, e.g.  <artifact wordnetid="100021939" confidence="0.8">
<merchandise wordnetid="103748886" confidence="0.8">
<commodity wordnetid="103076708" confidence="0.8">
<link xlink:type="simple" xlink:href="../641/15610641.xml">
Intel Matrix RAID</link></commodity>
</merchandise>
</artifact>
.) Using two partitions on the same drive in the same RAID is, however, dangerous.  If, for example, a RAID 5 array is composed of four drives 250 + 250 + 250 + 500 GB, with the 500-GB drive split into two 250 GB partitions, a failure of this drive will remove two partitions from the array, causing all of the data held on it to be lost.</p>

</ss1>
<ss1>
<st>
Hardware-based</st>
<p>

Hardware RAID controllers use different, proprietary disk layouts, so it is not usually possible to span controllers from different manufacturers.  They do not require processor resources, the BIOS can boot from them, and tighter integration with the device driver may offer better error handling.</p>
<p>

A hardware implementation of RAID requires at least a special-purpose <link xlink:type="simple" xlink:href="../505/475505.xml">
RAID controller</link>.  On a desktop system this may be a <link xlink:type="simple" xlink:href="../075/24075.xml">
PCI</link> <link xlink:type="simple" xlink:href="../022/75022.xml">
expansion card</link>, <bus wordnetid="102924116" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../320/143320.xml">
PCI-e</link></bus>
 <link xlink:type="simple" xlink:href="../022/75022.xml">
expansion card</link> or built into the <link xlink:type="simple" xlink:href="../945/19945.xml">
motherboard</link>.  Controllers supporting most types of drive may be used - <connection wordnetid="103091374" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../778/2778.xml">
IDE/ATA</link></connection>
, <bus wordnetid="102924116" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../151/174151.xml">
SATA</link></bus>
, <link xlink:type="simple" xlink:href="../313/28313.xml">
SCSI</link>, <conveyance wordnetid="103100490" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<bus wordnetid="102924116" confidence="0.8">
<public_transport wordnetid="104019101" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<link xlink:type="simple" xlink:href="../129/479129.xml">
SSA</link></device>
</public_transport>
</bus>
</instrumentality>
</artifact>
</conveyance>
, <link xlink:type="simple" xlink:href="../327/143327.xml">
Fibre Channel</link>, sometimes even a combination.  The controller and disks may be in a stand-alone <link xlink:type="simple" xlink:href="../379/891379.xml">
disk enclosure</link>, rather than inside a computer.  The enclosure may be <link xlink:type="simple" xlink:href="../875/2312875.xml">
directly attached</link> to a computer, or connected via <link xlink:type="simple" xlink:href="../593/266593.xml">
SAN</link>.  The controller hardware handles the management of the drives, and performs any parity calculations required by the chosen RAID level.  </p>
<p>

Most hardware implementations provide a read/write <link xlink:type="simple" xlink:href="../829/6829.xml">
cache</link>, which, depending on the I/O workload, will improve performance.  In most systems the write cache is non-volatile (i.e. battery-protected), so pending writes are not lost on a power failure.</p>
<p>

Hardware implementations provide guaranteed performance, add no overhead to the local CPU complex and can support many operating systems, as the controller simply presents a <link xlink:type="simple" xlink:href="../810/9688810.xml">
logical disk</link> to the operating system.  </p>
<p>

Hardware implementations also typically support hot swapping, allowing failed drives to be replaced while the system is running.</p>

</ss1>
<ss1>
<st>
 Firmware/driver based RAID </st>

<p>

Operating system-based RAID cannot easily be used to protect the boot process and is generally impractical on desktop versions of Windows (as described above).  Hardware RAID controllers are expensive.  To fill this gap, cheap "RAID controllers" were introduced that do not contain a RAID controller chip, but simply a standard disk controller chip with special firmware and drivers.  During early stage bootup the RAID is implemented by the firmware; when a protected-mode <link xlink:type="simple" xlink:href="../394/50394.xml">
operating system kernel</link> such as <O wordnetid="106832680" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../297/6097297.xml">
Linux</link></O>
 or a modern version of <link xlink:type="simple" xlink:href="../890/18890.xml">
Microsoft Windows</link> is loaded the drivers take over.</p>
<p>

These controllers are described by their manufacturers as RAID controllers, and it is rarely made clear to purchasers that the burden of RAID processing is borne by the host computer's central processing unit, not the RAID controller itself, thus introducing the aforementioned CPU overhead. Before their introduction, a "RAID controller" implied that the controller did the processing, and the new type has become known in technically knowledgeable circles as "fake RAID" even though the RAID itself is implemented correctly.</p>

</ss1>
<ss1>
<st>
Network-attached storage</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../995/451995.xml">
Network-attached storage</link></it>
</indent>

While not directly associated with RAID, <link xlink:type="simple" xlink:href="../995/451995.xml">
Network-attached storage</link> (NAS) is an enclosure containing disk drives and the equipment necessary to make them available over a <link xlink:type="simple" xlink:href="../592/4122592.xml">
computer network</link>, usually <link xlink:type="simple" xlink:href="../499/9499.xml">
Ethernet</link>.  The enclosure is basically a dedicated computer in its own right, designed to operate over the network without screen or keyboard.  It contains one or more disk drives; multiple drives may be configured as a RAID.</p>

</ss1>
<ss1>
<st>
 Hot spares </st>
<p>

Both hardware and software RAIDs with redundancy may support the use of <it><link xlink:type="simple" xlink:href="../818/3218818.xml">
hot spare</link></it> drives, a drive physically installed in the array which is inactive until an active drive fails, when the system automatically replaces the failed drive with the spare, rebuilding the array with the spare drive included.  This reduces the <link xlink:type="simple" xlink:href="../988/430988.xml">
mean time to recovery</link> (MTTR), though it doesn't eliminate it completely.  A second drive failure in the same RAID redundancy group before the array is fully rebuilt will result in loss of the data; rebuilding can take several hours, especially on busy systems.  </p>
<p>

Rapid replacement of failed drives is important as the drives of an array will all have had the same amount of use, and may tend to fail at about the same time rather than randomly. RAID 6 without a spare uses the same number of drives as RAID 5 with a hot spare and protects data against simultaneous failure of up to two drives, but requires a more advanced RAID controller.</p>

</ss1>
</sec>
<sec>
<st>
 Reliability terms</st>
<p>

<list>
<entry level="1" type="definition">

 <link xlink:type="simple" xlink:href="../960/1336960.xml">
Failure rate</link>: The mean time to failure (MTTF) or the mean time between failure (MTBF) of a given RAID is the same as those of its constituent hard drives, regardless of what type of RAID is employed.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

 Mean time to data loss (MTTDL): In this context, the average time before a loss of data in a given array.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>.  Mean time to data loss of a given RAID may be higher or lower than that of its constituent hard drives, depending upon what type of RAID is employed.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

 <link xlink:type="simple" xlink:href="../988/430988.xml">
Mean time to recovery</link> (MTTR): In arrays that include redundancy for reliability, this is the time following a failure to restore an array to its normal failure-tolerant mode of operation.  This includes time to replace a failed disk mechanism as well as time to re-build the array (i.e.  to replicate data for redundancy).</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

 Unrecoverable bit error rate (UBE): This is the rate at which a disk drive will be unable to recover data after application of cyclic redundancy check (CRC) codes and multiple retries.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

 Write cache reliability: Some RAID systems use <link xlink:type="simple" xlink:href="../847/25847.xml">
RAM</link> write cache to increase performance.  A power failure can result in data loss unless this sort of <link xlink:type="simple" xlink:href="../308/8641308.xml">
disk buffer</link> is supplemented with a battery to ensure that the buffer has enough time to write from <link xlink:type="simple" xlink:href="../847/25847.xml">
RAM</link> back to disk.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="definition">

 <link xlink:type="simple" xlink:href="../991/373991.xml">
Atomic write</link> failure: Also known by various terms such as torn writes, torn pages, incomplete writes, interrupted writes, non-transactional, etc.</entry>
</list>
</p>

</sec>
<sec>
<st>
Problems with RAID</st>

<ss1>
<st>
Correlated failures</st>
<p>

The theory behind the error correction in RAID assumes that failures of drives are independent.  Given these assumptions it is possible to calculate how often they can fail and to arrange the array to make data loss arbitrarily improbable.</p>
<p>

In practice, the drives are often the same ages, with similar wear.  Since many drive failures are due to mechanical issues which are more likely on older drives, this violates those assumptions and failures are in fact statistically correlated.  In practice then, the chances of a second failure before the first has been recovered is not nearly as unlikely as might be supposed, and data loss can in practice occur at significant rates.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref></p>

</ss1>
<ss1>
<st>
Atomicity</st>
<p>

This is a little understood and rarely mentioned failure mode for redundant storage systems that do not utilize transactional features.  Database researcher <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../367/308367.xml">
Jim Gray</link></scientist>
</person>
 wrote "Update in Place is a Poison Apple" <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>during the early days of relational database commercialization. However, this warning largely went unheeded and fell by the wayside upon the advent of RAID, which many software engineers mistook as solving all data storage integrity and reliability problems.  Many software programs update a storage object "in-place"; that is, they write a new version of the object on to the same disk addresses as the old version of the object.  While the software may also log some delta information elsewhere, it expects the storage to present "atomic write semantics," meaning that the write of the data either occurred in its entirety or did not occur at all.</p>
<p>

However, very few storage systems provide support for atomic writes, and even fewer specify their rate of failure in providing this semantic.  Note that during the act of writing an object, a RAID storage device will usually be writing all redundant copies of the object in parallel, although overlapped or staggered writes are more common when a single RAID processor is responsible for multiple drives.  Hence an error that occurs during the process of writing may leave the redundant copies in different states, and furthermore may leave the copies in neither the old nor the new state.  The little known failure mode is that delta logging relies on the original data being either in the old or the new state so as to enable backing out the logical change, yet few storage systems provide an atomic write semantic on a RAID disk.</p>
<p>

While the battery-backed write cache may partially solve the problem, it is applicable only to a power failure scenario.</p>
<p>

Since transactional support is not universally present in hardware RAID, many operating systems include transactional support to protect against data loss during an interrupted write.  Novell Netware, starting with version 3.x, included a transaction tracking system.  Microsoft introduced transaction tracking via the journalling feature in <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../184/39184.xml">
NTFS</link></instrumentality>
</artifact>
</system>
.  NetApp <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../858/1247858.xml">
WAFL</link></instrumentality>
</artifact>
</system>
 file system solves it by never updating the data in place, as does <system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../693/1205693.xml#xpointer(//*[./st=%22Copy-on-write+transactional+model%22])">
ZFS</link></instrumentality>
</artifact>
</system>
.</p>

</ss1>
<ss1>
<st>
Unrecoverable data</st>
<p>

This can present as a sector read failure.  Some RAID implementations protect against this failure mode by remapping the <link xlink:type="simple" xlink:href="../411/7024411.xml">
bad sector</link>, using the redundant data to retrieve a good copy of the data, and rewriting that good data to the newly mapped replacement sector.  The UBE (Unrecoverable Bit Error) rate is typically specified at 1 bit in 1015 for enterprise class disk drives (<link xlink:type="simple" xlink:href="../313/28313.xml">
SCSI</link>, <link xlink:type="simple" xlink:href="../327/143327.xml">
FC</link>, <link xlink:type="simple" xlink:href="../131/1143131.xml">
SAS</link>) , and 1 bit in 1014 for desktop class disk drives (IDE/ATA/PATA, SATA).  Increasing disk capacities and large RAID 5 redundancy groups have led to an increasing inability to successfully rebuild a RAID group after a disk failure because an unrecoverable sector is found on the remaining drives.  Double protection schemes such as RAID 6 are attempting to address this issue, but suffer from a very high write penalty.</p>

</ss1>
<ss1>
<st>
Write cache reliability</st>
<p>

The disk system can acknowledge the write operation as soon as the data is in the cache, not waiting for the data to be physically written.  However, any power outage can then mean a significant data loss of any data queued in such cache.  </p>
<p>

Often a battery is protecting the write cache, mostly solving the problem.  If a write fails because of power failure, the controller may complete the pending writes as soon as restarted.  This solution still has potential failure cases: the battery may have worn out, the power may be off for too long, the disks could be moved to another controller, the controller itself could fail.  Some disk systems provide the capability of testing the battery periodically, however this leaves the system without a fully charged battery for several hours.</p>
<p>

An additional concern about write cache reliability exists, and that is that a lot of them are write-back cache; a caching system which reports the data as written as soon as it is written to cache, as opposed to the non-volatile medium <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref>. The safer cache technique is write-through, which reports transactions as written when they are written to the non-volatile medium.</p>

</ss1>
<ss1>
<st>
Equipment compatibility</st>
<p>

The disk formats on different RAID controllers are not necessarily compatible, so that it may not be possible to read a RAID on different hardware.  Consequently a non-disk hardware failure may require using identical hardware, or a data backup, to recover the data. Software RAID however, such as implemented in the Linux kernel, alleviates this concern, as the setup is not hardware dependent, but runs on ordinary disk controllers. Additionally, Software RAID1 disks can be read like normal disks, so no RAID system is required to retrieve the data.</p>

</ss1>
</sec>
<sec>
<st>
 History </st>
<p>

Norman Ken Ouchi at <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link></company>
 was awarded a 1978 <link xlink:type="simple" xlink:href="../918/2287918.xml">
U.S. patent</link> 4,092,732<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref> titled "System for recovering data stored in failed memory unit." The <link xlink:type="simple" xlink:href="../163/848163.xml">
claim</link>s for this patent describe what would later be termed RAID 5 with full <link xlink:type="simple" xlink:href="../567/474567.xml">
stripe</link> writes.  This 1978 patent also mentions that disk mirroring or duplexing (what would later be termed RAID 1) and protection with dedicated parity (that would later be termed RAID 4) were <link xlink:type="simple" xlink:href="../906/572906.xml">
prior art</link> at that time.</p>
<p>

The term RAID was first defined by <link xlink:type="simple" xlink:href="../239/816239.xml">
David A.  Patterson</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../226/816226.xml">
Garth A. Gibson</link></research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
 and <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../660/8610660.xml">
Randy Katz</link></associate>
</scholar>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</person>
</peer>
</physical_entity>
 at the <university wordnetid="108286163" confidence="0.9508927676800064">
<ranking wordnetid="114429484" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../922/31922.xml">
University of California, Berkeley</link></ranking>
</university>
 in 1987.  They studied the possibility of using two or more drives to appear as a single device to the host system and published a paper: <it>"A Case for Redundant Arrays of Inexpensive Disks (RAID)"</it> in June 1988 at the <link xlink:type="simple" xlink:href="../519/5897519.xml">
SIGMOD</link> conference.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref></p>
<p>

This specification suggested a number of prototype <it>RAID levels</it>, or combinations of drives.  Each had theoretical advantages and disadvantages.  Over the years, different implementations of the RAID concept have appeared.  Most differ substantially from the original idealized RAID levels, but the numbered names have remained.  This can be confusing, since one implementation of RAID 5, for example, can differ substantially from another.  RAID 3 and RAID 4 are often confused and even used interchangeably.</p>
<p>

Their paper formally defined RAID levels 1 through 5 in sections 7 to 11:
<list>
<entry level="1" type="bullet">

 First level RAID: mirrored drives</entry>
<entry level="1" type="bullet">

 Second level RAID: <link xlink:type="simple" xlink:href="../226/41226.xml">
Hamming code</link> for <link xlink:type="simple" xlink:href="../375/10375.xml">
error correction</link></entry>
<entry level="1" type="bullet">

 Third level RAID: single check disk per group</entry>
<entry level="1" type="bullet">

 Fourth level RAID: independent reads and writes</entry>
<entry level="1" type="bullet">

 Fifth level RAID: spread data/parity over all drives (no single check disk)</entry>
</list>
</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../276/3610276.xml">
Disk Data Format</link> (DDF)</entry>
<entry level="1" type="bullet">

 <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<link xlink:type="simple" xlink:href="../505/475505.xml">
Disk array controller</link></device>
</instrumentality>
</artifact>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../469/5279469.xml">
Redundant Array of Inexpensive Nodes</link></entry>
<entry level="1" type="bullet">

 <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<medium wordnetid="106254669" confidence="0.8">
<link xlink:type="simple" xlink:href="../947/6165947.xml">
Vinum volume manager</link></medium>
</instrumentality>
</artifact>
</entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>


<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal"><link xlink:type="simple" xlink:href="../239/816239.xml">
Patterson, David</link>;&#32;<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../226/816226.xml">
Garth A. Gibson</link></research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../660/8610660.xml">
Randy Katz</link></associate>
</scholar>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</person>
</peer>
</physical_entity>
&#32;(1988). "<weblink xlink:type="simple" xlink:href="http://www.eecs.berkeley.edu/Pubs/TechRpts/1987/CSD-87-391.pdf">
A Case for Redundant Arrays of Inexpensive Disks (RAID)</weblink>".&#32;<it><link xlink:type="simple" xlink:href="../519/5897519.xml">
SIGMOD</link> Conference</it>: pp 109–116.</cite>&nbsp; retrieved <link>
2006-12-31</link></entry>
<entry id="2">
"Because the term inexpensive is relative and can be misleading, the proper meaning of the acronym is now generally accepted as Redunant Array of Independent Disks" - Chapter 7, page 302, The Essentials of Computer Organization and Architecture; Linda Null and Julia Lobur; ISBN 0-76370444-X</entry>
<entry id="3">
 <cite style="font-style:normal">Pickstock, Simon&#32;(June 2008).&#32;"The truth about RAID"&#32;(in English)&#32;(Magazine). <it>PC Format Magazine</it>&#32;(214): 35.&#32;United Kingdom:&#32;Future Publishing.&#32;“In all our real-world tests, the difference between the single drive performance and the dual-drive RAID 0 striped setup was virtually non-existent. And in fact, the single drive was ever-so-slightly faster than the other setups, including the RAID 5 system that we'd hoped would offer the perfect combination of performance and data redundancy.”</cite>&nbsp;</entry>
<entry id="4">
<weblink xlink:type="simple" xlink:href="http://www.snia.org/education/dictionary">
SNIA Dictionary</weblink></entry>
<entry id="5">
<weblink xlink:type="simple" xlink:href="http://linux-raid.osdl.org/">
Main Page - Linux-raid</weblink></entry>
<entry id="6">
"<weblink xlink:type="simple" xlink:href="http://www.apple.com/server/macosx/technology/filesystem.html">
Apple Mac OS X Server File Systems</weblink>".&#32;Retrieved on <link>
2008-04-23</link>.</entry>
<entry id="7">
Jim Gray and Catharine van Ingen, <weblink xlink:type="simple" xlink:href="http://research.microsoft.com/research/pubs/view.aspx?msr_tr_id=MSR-TR-2005-166">
"Empirical Measurements of Disk Failure Rates and Error Rates"</weblink>, MSTR-2005-166, December 2005</entry>
<entry id="8">
<weblink xlink:type="simple" xlink:href="http://www.usenix.org/events/fast07/tech/schroeder.html">
Disk Failures in the Real World: What Does an MTTF of 1,000,000 Hours Mean to You?</weblink> Bianca Schroeder and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../226/816226.xml">
Garth A. Gibson</link></research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
</entry>
<entry id="9">
<weblink xlink:type="simple" xlink:href="http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Gray81.html">
Jim Gray: The Transaction Concept: Virtues and Limitations</weblink> (Invited Paper) <weblink xlink:type="simple" xlink:href="http://www.informatik.uni-trier.de/~ley/db/conf/vldb/vldb81.html#Gray81">
VLDB 1981</weblink>: 144-154</entry>
<entry id="10">
"<weblink xlink:type="simple" xlink:href="http://www.snia.org/education/dictionary/w/">
Definition of write-back cache at SNIA dictionary</weblink>".</entry>
<entry id="11">
<weblink xlink:type="simple" xlink:href="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4092732">
U.S. Patent 4,092,732</weblink><weblink xlink:type="simple" xlink:href="http://www.pat2pdf.org/pat2pdf/foo.pl?number=4092732">
&nbsp;</weblink></entry>
<entry id="12">
 <cite style="font-style:normal"><link xlink:type="simple" xlink:href="../239/816239.xml">
Patterson, David</link>;&#32;<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<link xlink:type="simple" xlink:href="../226/816226.xml">
Garth A. Gibson</link></research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../660/8610660.xml">
Randy Katz</link></associate>
</scholar>
</causal_agent>
</alumnus>
</colleague>
</intellectual>
</person>
</peer>
</physical_entity>
&#32;(1988). "<weblink xlink:type="simple" xlink:href="http://www.eecs.berkeley.edu/Pubs/TechRpts/1987/CSD-87-391.pdf">
A Case for Redundant Arrays of Inexpensive Disks (RAID)</weblink>".&#32;<it><link xlink:type="simple" xlink:href="../519/5897519.xml">
SIGMOD</link> Conference</it>: pp 109–116.</cite>&nbsp; retrieved <link>
2006-12-31</link></entry>
</reflist>
</p>

</sec>
<sec>
<st>
 Further reading </st>
<p>

<list>
<entry level="1" type="bullet">

 Charles M. Kozierok&#32;(<link>
2001-04-17</link>).&#32;"<weblink xlink:type="simple" xlink:href="http://pcguide.com./ref/hdd/perf/raid/index.htm">
Redundant Arrays of Inexpensive Disks</weblink>".&#32;<it>The PC Guide</it>.&#32;  Pair Networks.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.howtofriends.com/raid/">
Different Types of RAID</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.brainshark.com/winchestersystemsinc1/vu?pi=707421860">
"RAID 6 Essentials" tutorial</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dmoz.org/Computers/Hardware/Storage/Subsystems/RAID/">
RAID</weblink> at the <work wordnetid="100575741" confidence="0.8">
<possession wordnetid="100032613" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<company wordnetid="108058098" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<property wordnetid="113244109" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<subsidiary_company wordnetid="108003935" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../501/18949501.xml">
Open Directory Project</link></institution>
</subsidiary_company>
</activity>
</psychological_feature>
</act>
</property>
</undertaking>
</company>
</event>
</possession>
</work>
</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ibizdir.com/business-research/web-hosting/61-introducing-raid.html">
Introduction to RAID</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.lascon.co.uk/d008005.htm">
Working RAID illustrations</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.eukhost.com/forums/f15/raid-explained-4063/">
RAID system explained in detail</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.acnc.com/raid.html">
RAID Levels &mdash; Tutorial and Diagrams</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.shub-internet.org/brad/FreeBSD/vinum.html">
Logical Volume Manager Performance Measurement</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dellcommunity.com/supportforums/board/message?board.id=pes_hardrive&amp;thread.id=25335">
Animations and Descriptions from Dell to help Learn about RAID Levels 0, 1, 5, 10, and 50</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.utk.edu/~plank/plank/papers/CS-96-332.pdf">
 Tutorial on Reed-Solomon Coding for Fault-Tolerance in RAID-like Systems</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.pdl.cmu.edu/PDL-FTP/Declustering/ASPLOS.pdf">
Parity Declustering for Continuous Operation in Redundant Disk Arrays</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.berkeley.edu/~culler/cs252-s02/papers/p245-blaum.pdf">
An Optimal Scheme for Tolerating Double Disk Failures in RAID Architectures</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.raid-controller.info">
Informations about Raid Systems - German</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
