<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:17:11[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<algorithm  confidence="0.9511911446218017" wordnetid="105847438">
<header>
<title>Selection algorithm</title>
<id>552786</id>
<revision>
<id>241054468</id>
<timestamp>2008-09-26T03:42:17Z</timestamp>
<contributor>
<username>Angela</username>
<id>8551</id>
</contributor>
</revision>
<categories>
<category>Selection algorithms</category>
<category>Wikipedia articles incorporating text from public domain works of the United States Government</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>, a <b>selection algorithm</b> is an <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> for finding the <it>k</it>th smallest number in a list, called <it><link xlink:type="simple" xlink:href="../986/160986.xml">
order statistic</link>s</it>. This includes the cases of finding the minimum, maximum, and median elements. There are worst-case linear time selection algorithms. Selection is a subproblem of more complex problems like the <link xlink:type="simple" xlink:href="../022/7309022.xml">
nearest neighbor problem</link> and <link xlink:type="simple" xlink:href="../985/41985.xml">
shortest path</link> problems.<p>

The term "selection" is used in other contexts in computer science, including the stage of a genetic algorithm in which genomes are chosen from a population for later breeding; see <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../886/1105886.xml">
Selection (genetic algorithm)</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
.  This article addresses only the problem of determining order statistics.</p>

<sec>
<st>
 Selection by sorting </st>
<p>

Selection can be <link xlink:type="simple" xlink:href="../067/848067.xml">
reduced</link> to <link xlink:type="simple" xlink:href="../442/28442.xml">
sorting</link> by sorting the list and then extracting the desired element.  This method is efficient when many selections need to be made from a list, in which case only one initial, expensive sort is needed, followed by many cheap extraction operations. In general, this method requires <math>\mathcal{O}(n~\log~n)</math> time, where <it>n</it> is the length of the list.</p>

</sec>
<sec>
<st>
Linear minimum/maximum algorithms</st>
<p>

Linear time algorithms to find minimums or maximums work by iterating over the list and keeping track of the minimum or maximum element so far.</p>

</sec>
<sec>
<st>
 Nonlinear general selection algorithm </st>
<p>

Using the same ideas used in minimum/maximum algorithms, we can construct a simple, but inefficient general algorithm for finding the <it>k</it>th smallest or <it>k</it>th largest item in a list, requiring O(<it>kn</it>) time, which is effective when <it>k</it> is small. To accomplish this, we simply find the most extreme value and move it to the beginning until we reach our desired index. This can be seen as an incomplete <link xlink:type="simple" xlink:href="../352/29352.xml">
selection sort</link>. Here is the minimum-based algorithm:</p>
<p>

<b>function</b> select(list[1..n], k)
<b>for</b> i <b>from</b> 1 <b>to</b> k
minIndex = i
minValue = list[i]
<b>for</b> j <b>from</b> i+1 <b>to</b> n
<b>if</b> list[j]  minValue
minIndex = j
minValue = list[j]
swap list[i] and list[minIndex]
<b>return</b> list[k]</p>
<p>

Other advantages of this method are:
<list>
<entry level="1" type="bullet">

 After locating the <it>j</it>th smallest element, it requires only O(<it>j</it> + (<it>k</it>-<it>j</it>)2) time to find the <it>k</it>th smallest element, or only O(<it>k</it>) for <it>k</it> ≤ <it>j</it>.</entry>
<entry level="1" type="bullet">

 It can be done with <link xlink:type="simple" xlink:href="../167/18167.xml">
linked list</link> data structures, whereas the one based on partition requires <link xlink:type="simple" xlink:href="../612/25612.xml">
random access</link>.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Partition-based general selection algorithm </st>

<p>

A worst-case linear algorithm for the general case of selecting the <it>k</it>th largest element was published by Blum, Floyd, Pratt, Rivest, and Tarjan in their 1973 paper <it>Time bounds for selection</it>, sometimes called <b>BFPRT</b> after the last names of the authors. The algorithm that it is based on was conceived by the inventor of <link xlink:type="simple" xlink:href="../249/3268249.xml">
quicksort</link>, <link xlink:type="simple" xlink:href="../434/39434.xml">
C.A.R. Hoare</link>, and is known as <b>Hoare's selection algorithm</b> or <b>quickselect</b>.</p>
<p>

In quicksort, there is a subprocedure called partition that can, in linear time, group a list (ranging from indices left to right) into two parts, those less than a certain element, and those greater than or equal to the element. Here is pseudocode that performs a partition about the element list[pivotIndex]:</p>
<p>

<b>function</b> partition(list, left, right, pivotIndex)
pivotValue := list[pivotIndex]
swap list[pivotIndex] and list[right]  <it>// Move pivot to end</it>
storeIndex := left
<b>for</b> i <b>from</b> left <b>to</b> right-1
<b>if</b> list[i]  pivotValue
swap list[storeIndex] and list[i]
storeIndex := storeIndex + 1
swap list[right] and list[storeIndex]  <it>// Move pivot to its final place</it>
<b>return</b> storeIndex</p>
<p>

In quicksort, we recursively sort both branches, leading to best-case <link xlink:type="simple" xlink:href="../578/44578.xml">
Ω</link>(<it>n</it> log <it>n</it>) time. However, when doing selection, we already know which partition our desired element lies in, since the pivot is in its final sorted position, with all those preceding it in sorted order preceding it and all those following it in sorted order following it. Thus a single recursive call locates the desired element in the correct partition:</p>
<p>

<b>function</b> select(list, k, left, right)
select a pivot value list[pivotIndex]
pivotNewIndex := partition(list, left, right, pivotIndex)
<b>if</b> k = pivotNewIndex
<b>return</b> list[k]
<b>else if</b> k  pivotNewIndex
<b>return</b> select(list, k, left, pivotNewIndex-1)
<b>else</b>
<b>return</b> select(list, k, pivotNewIndex+1, right)</p>
<p>

Note the resemblance to quicksort; indeed, just as the minimum-based selection algorithm is a partial selection sort, this is a partial quicksort, generating and partitioning only O(log <it>n</it>) of its O(<it>n</it>) partitions. This simple procedure has expected linear performance, and, like quicksort, has quite good performance in practice. It is also an <link xlink:type="simple" xlink:href="../861/219861.xml">
in-place algorithm</link>, requiring only constant memory overhead, since the <link xlink:type="simple" xlink:href="../742/30742.xml">
tail recursion</link> can be eliminated with a loop like this:</p>
<p>

<b>function</b> select(list, k, left, right)
<b>loop</b>
select a pivot value list[pivotIndex]
pivotNewIndex := partition(list, left, right, pivotIndex)
<b>if</b> k = pivotNewIndex
<b>return</b> list[k]
<b>else if</b> k  pivotNewIndex
right := pivotNewIndex-1
<b>else</b>
left := pivotNewIndex+1</p>
<p>

Like quicksort, the performance of the algorithm is sensitive to the pivot that is chosen. If bad pivots are consistently chosen, this degrades to the minimum-based selection described previously, and so can require as much as Ω(<it>n</it>2) time. David Musser describes a "median-of-3 killer" sequence that can force the well-known median-of-three pivot selection algorithm to fail with worst-case behavior (see <it><link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Introselect%22])">
Introselect</link></it> section below).</p>

</sec>
<sec>
<st>
 Linear general selection algorithm - "Median of Medians algorithm" </st>

<p>

The key to making the previous algorithm worst-case linear and the primary contribution of "Time bounds for selection" is the "median of medians algorithm", which consistently finds good pivots.</p>
<p>

This algorithm divides the list into groups of five elements. Left over elements are ignored for now. Then, for each group of five, the median is calculated: an operation that can potentially be made very fast if the five values can be loaded into registers and compared. These medians are moved into one contiguous block in the list, <it>select</it> is called recursively on this sublist of <it>n</it>/5 elements to find new median values. Finally, the "median of medians" is the pivot.</p>
<p>

The chosen pivot is both less than and greater than half of the elements in the list of medians, which is around <it>n</it>/10 elements for each half. Each of these elements is a median of 5, making it less than 2 other elements and greater than 2 other elements outside the block. Hence, the pivot is less than <math>2(n/10)</math> elements outside the block, and greater than another <math>2(n/10)</math> elements outside the block. Thus the chosen median splits the elements somewhere between 30%/70% and 70%/30%, which assures worst-case linear behavior of the algorithm.</p>
<p>

The median-calculating recursive call does not exceed worst-case linear behavior because the list of medians is 20% of the size of the list, while the other recursive call recurs on at most 70% of the list, making the running time</p>
<p>

T(<it>n</it>) ≤ T(<it>n</it>/5) + T(7<it>n</it>/10) + O(<it>n</it>)</p>
<p>

The O(<it>n</it>) is for the partitioning work. An induction argument can then show that T(<it>n</it>) is indeed O(<it>n</it>).</p>
<p>

Although this approach optimizes quite well, it is typically outperformed in practice by the expected linear algorithm with random pivot choices.</p>
<p>

The worst-case algorithm can construct a worst-case <math>O( n log n )</math> quicksort algorithm, by using it to find the median at every step.</p>

</sec>
<sec>
<st>
 Introselect </st>

<p>

David Musser's well-known <link xlink:type="simple" xlink:href="../477/363477.xml">
introsort</link> achieves practical performance comparable to quicksort while preserving O(<it>n</it> log <it>n</it>) worst-case behavior by creating a hybrid of quicksort and <link xlink:type="simple" xlink:href="../995/13995.xml">
heapsort</link>. In the same paper, Musser introduced an "introspective selection" algorithm, popularly called <b>introselect</b>, which combines Hoare's algorithm with the worst-case linear algorithm described above to achieve worst-case linear selection with performance similar to Hoare's algorithm.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> It works by optimistically starting out with Hoare's algorithm and only switching to the worst-time linear algorithm if it recurs too many times without making sufficient progress. Simply limiting the recursion to constant depth is not good enough, since this would make the algorithm switch on all sufficiently large lists. Musser discusses a couple of simple approaches:
<list>
<entry level="1" type="bullet">

 Keep track of the list of sizes of the subpartitions processed so far. If at any point <it>k</it> recursive calls have been made without halving the list size, for some small positive <it>k</it>, switch to the worst-case linear algorithm.</entry>
<entry level="1" type="bullet">

 Sum the size of all partitions generated so far. If this exceeds the list size times some small positive constant <it>k</it>, switch to the worst-case linear algorithm. This sum is easy to track in a single scalar variable.</entry>
</list>
</p>
<p>

Both approaches limit the recursion depth to O(<it>k</it>log <it>n</it>), which is O(log <it>n</it>) since <it>k</it> is a predetermined constant. The paper suggested that more research on introselect was forthcoming, but as of 2007 it has not appeared.</p>

</sec>
<sec>
<st>
 Selection as incremental sorting </st>
<p>

One of the advantages of the sort-and-index approach, as mentioned, is its ability to <link xlink:type="simple" xlink:href="../683/236683.xml">
amortize</link> the sorting cost over many subsequent selections. However, sometimes the number of selections that will be done is not known in advance, and may be either small or large. In these cases, we can adapt the algorithms given above to simultaneously select an element while partially sorting the list, thus accelerating future selections. </p>
<p>

Both the selection procedure based on minimum-finding and the one based on partitioning can be seen as a form of partial sort. The minimum-based algorithm sorts the list up to the given index, and so clearly speeds up future selections, especially of smaller indexes. The partition-based algorithm does not achieve the same behaviour automatically, but can be adapted to remember its previous pivot choices and reuse them wherever possible, avoiding costly partition operations, particularly the top-level one. The list becomes gradually more sorted as more partition operations are done incrementally; no pivots are ever "lost." If desired, this same pivot list could be passed on to quicksort to reuse, again avoiding many costly partition operations.</p>

</sec>
<sec>
<st>
 Using data structures to select in sublinear time </st>
<p>

Given an unorganized list of data, linear time (Ω(<it>n</it>)) is required to find the minimum element, because we have to examine every element (otherwise, we might miss it). If we organize the list, for example by keeping it sorted at all times, then selecting the <it>k</it>th largest element is trivial, but then insertion requires linear time, as do other operations such as combining two lists.</p>
<p>

The strategy to find an order statistic in <link xlink:type="simple" xlink:href="../541/1750541.xml">
sublinear time</link> is to store the data in an organized fashion using suitable data structures that facilitate the selection. Two such data structures are tree based structures and frequency tables.</p>
<p>

When only the minimum (or maximum) is needed, a good approach is to use a <link xlink:type="simple" xlink:href="../485/24485.xml">
priority queue</link>, which is able to find the minimum (or maximum) element in constant time, while all other operations, including insertion, are O(log <it>n</it>) or better. More generally, a <link xlink:type="simple" xlink:href="../310/378310.xml">
self-balancing binary search tree</link> can easily be augmented to make it possible to both insert an element and find the <it>k</it>th largest element in O(log <it>n</it>) time. We simply store in each node a count of how many descendants it has, and use this to determine which path to follow. The information can be updated efficiently since adding a node only affects the counts of its O(log <it>n</it>) ancestors, and tree rotations only affect the counts of the nodes involved in the rotation.</p>
<p>

Another simple strategy is based on some of the same concepts as the <link xlink:type="simple" xlink:href="../833/13833.xml">
hash table</link>. When we know the range of values beforehand, we can divide that range into <it>h</it> subintervals and assign these to <it>h</it> buckets. When we insert an element, we add it to the bucket corresponding to the interval it falls in. To find the minimum or maximum element, we scan from the beginning or end for the first nonempty bucket and find the minimum or maximum element in that bucket. In general, to find the <it>k</it>th element, we maintain a count of the number elements in each bucket, then scan the buckets from left to right adding up counts until we find the bucket containing the desired element, then use the expected linear-time algorithm to find the correct element in that bucket.</p>
<p>

If we choose <it>h</it> of size roughly sqrt(<it>n</it>), and the input is close to uniformly distributed, this scheme can perform selections in expected O(sqrt(<it>n</it>)) time. Unfortunately, this strategy is also sensitive to clustering of elements in a narrow interval, which may result in buckets with large numbers of elements (clustering can be eliminated through a good hash function, but finding the element with the <it>k</it>th largest hash value isn't very useful). Additionally, like hash tables this structure requires table resizings to maintain efficiency as elements are added and <it>n</it> becomes much larger than <it>h</it>2. A useful case of this is finding an order statistic or extremum in a finite range of data,  like marks limited 1 to 100 numbers for much higher number of students. Using above table with bucket interval 1 and maintaining counts in each bucket is much superior to other methods. Such hash tables are like <link xlink:type="simple" xlink:href="../943/226943.xml">
frequency tables</link> used to classify the data in <link xlink:type="simple" xlink:href="../187/8187.xml">
descriptive statistics</link>.</p>

</sec>
<sec>
<st>
 Selecting k smallest or largest elements </st>
<p>

Another fundamental selection problem is that of selecting the <it>k</it> smallest or <it>k</it> largest elements, which is particularly useful where we want to present just the "top <it>k</it>" of an unsorted list, such as the top 100 corporations by gross sales. </p>

<ss1>
<st>
 Repeated application of simple selection algorithms </st>

<p>

A number of simple but inefficient solutions are possible. We can use the linear-time solutions discussed above to select each element one at a time, resulting in time O(<it>kn</it>) &mdash; we can achieve the same time just by running the first <it>k</it> iterations of <link xlink:type="simple" xlink:href="../352/29352.xml">
selection sort</link>. If log <it>n</it> is much less than <it>k</it>, a better simple strategy is to sort the list and then take the first or last <it>k</it> elements.</p>

</ss1>
<ss1>
<st>
 Direct application of the quick sort based selection algorithm </st>

<p>

The quick sort based selection algorithm can be used to find k smallest or k largest elements. To find k smallest elements find the kth smallest element using the median of medians quick sort based algorithm. After the partition that finds the kth smallest element, all the elements smaller than the kth smaller element will be present left to the kth element and all element larger will be present right to the kth smallest element. Thus all elements from 1st to kth element inclusive constitute the k smallest elements. The time complexity is linear in n, the total number of elements.</p>

</ss1>
<ss1>
<st>
 Data structure based solutions </st>

<p>

Another simple method is to add each element of the list into an ordered set data structure, such as a <link xlink:type="simple" xlink:href="../996/13996.xml">
heap</link> or <link xlink:type="simple" xlink:href="../310/378310.xml">
self-balancing binary search tree</link>, with at most <it>k</it> elements. Whenever the data structure has more than <it>k</it> elements, we remove the largest element, which can be done in O(log <it>k</it>) time. Each insertion operation also takes O(log <it>k</it>) time, resulting in O(<it>n</it>log <it>k</it>) time overall.</p>
<p>

It is possible to transform the list into a <link xlink:type="simple" xlink:href="../996/13996.xml">
heap</link> at Θ(<it>n</it>) time, and then traverse the heap using a modified <algorithm wordnetid="105847438" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../026/97026.xml">
Breadth-first search</link></algorithm>
 algorithm that places the elements in a <link xlink:type="simple" xlink:href="../485/24485.xml">
Priority Queue</link> (instead of the ordinary queue that is normally used in a BFS), and terminate the scan after traversing exactly k elements. As the queue size remains O(<it>k</it>) throughout the traversal, it would require O(<it>k</it>log <it>k</it>) time to complete, leading to a time bound of O(<it>n</it> + <it>k</it>log <it>k</it>) on this algorithm.</p>

</ss1>
<ss1>
<st>
 Optimised sorting algorithms </st>
<p>

More efficient than any of these are specialized partial sorting algorithms based on <link xlink:type="simple" xlink:href="../039/20039.xml">
mergesort</link> and <link xlink:type="simple" xlink:href="../249/3268249.xml">
quicksort</link>. The simplest is the quicksort variation: there is no need to recursively sort partitions which only contain elements that would fall after the <it>k</it>th place in the end. Thus, if the pivot falls in position <it>k</it> or later, we recur only on the left partition:</p>
<p>

<b>function</b> quicksortFirstK(list, left, right, k)
<b>if</b> right &amp;gt; left
select a pivot value list[pivotIndex]
pivotNewIndex := partition(list, left, right, pivotIndex)
quicksortFirstK(list, left, pivotNewIndex-1, k)
<b>if</b> pivotNewIndex  k
quicksortFirstK(list, pivotNewIndex+1, right, k)</p>
<p>

The resulting algorithm requires an expected time of only O(<it>n</it> + <it>k</it>log<it>k</it>), and is quite efficient in practice, especially if we substitute selection sort when <it>k</it> becomes small relative to <it>n</it>.</p>
<p>

Even better is if we don't require those <it>k</it> items to be themselves sorted.  Losing that requirement means we can ignore all partitions that fall entirely before <b>or</b> after the <it>k</it>th place.  We recur only into the partition that actually contains the <it>k</it>th element itself.</p>
<p>

<b>function</b> findFirstK(list, left, right, k)
<b>if</b> right &amp;gt; left
select a pivot value list[pivotIndex]
pivotNewIndex := partition(list, left, right, pivotIndex)
<b>if</b> pivotNewIndex &amp;gt; k  <it>// new condition</it>
findFirstK(list, left, pivotNewIndex-1, k)
<b>if</b> pivotNewIndex  k
findFirstK(list, pivotNewIndex+1, right, k)</p>
<p>

The resulting algorithm requires an expected time of only O(<it>n</it>), which is just about the best any algorithm can hope for.</p>

</ss1>
<ss1>
<st>
Tournament Algorithm</st>
<p>

Another method is tournament algorithm. The idea is to conduct a knockout minimal round tournament to decide the ranks. It first organises the games(comparisons) between adjacent pairs and moves the winners to next round until championship(the first best) is decided. It also constructs the tournament tree along the way.  Now the second best element must be among the direct losers to winner and these losers can be found out by walking in the binary tree in O(log <it>n</it>) time. It organises another tournament to decide the second best among these potential elements. The third best must be one among the losers of the second best in either of the two tournament trees. The approach continues until we find k elements. This algorithm takes O(n + k log <it>n</it>) complexity, which for any fixed <it>k</it> independent of <it>n</it> is O(<it>n</it>).</p>

</ss1>
</sec>
<sec>
<st>
 Lower bounds </st>
<p>

In his seminal <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../358/31358.xml">
The Art of Computer Programming</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
</it>, Donald E. Knuth discussed a number of lower bounds for the number of comparisons required to locate the <it>k</it> smallest entries of an unorganized list of <it>n</it> items (using only comparisons). There's a trivial lower bound of <it>n</it> &amp;minus; 1 for the minimum or maximum entry. To see this, consider a tournament where each game represents one comparison. Since every player except the winner of the tournament must lose a game before we know the winner, we have a lower bound of <it>n</it> &amp;minus; 1 comparisons.</p>
<p>

The story becomes more complex for other indexes. To find the <it>k</it> smallest values requires at least this many comparisons:</p>
<p>

<indent level="1">

<math>n - k + \sum_{n+1-k &amp;lt; j \leq n} \lceil{\operatorname{lg}\, j}\rceil</math>
</indent>

This bound is achievable for <it>k</it>=2 but better, more complex bounds exist for larger <it>k</it>.</p>

</sec>
<sec>
<st>
 Language support </st>
<p>

Very few languages have built-in support for general selection, although many provide facilities for finding the smallest or largest element of a list. A notable exception is <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../038/72038.xml">
C++</link></programming_language>
, which provides a templated nth_element method with a guarantee of expected linear time. It is implied but not required that it is based on Hoare's algorithm by its requirement of expected linear time. (Ref section 25.3.2 of ISO/IEC 14882:2003(E) and 14882:1998(E), see also <weblink xlink:type="simple" xlink:href="http://www.sgi.com/tech/stl/nth_element.html">
SGI STL description of nth_element</weblink>)</p>
<p>

C++ also provides the <weblink xlink:type="simple" xlink:href="http://www.sgi.com/tech/stl/partial_sort.html">
partial_sort</weblink> algorithm, which solves the problem of selecting the smallest <it>k</it> elements (sorted), with a time complexity of O(<it>n</it>log <it>k</it>). No algorithm is provided for selecting the greatest <it>k</it> elements, but this can easily be achieved by inverting the ordering <link>
predicate</link>.</p>
<p>

For <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../939/23939.xml">
Perl</link></programming_language>
, the module <weblink xlink:type="simple" xlink:href="http://search.cpan.org/~salva/Sort-Key-Top">
Sort::Key::Top</weblink>, available from <link xlink:type="simple" xlink:href="../h$at/Internet_R$elay_C$hat.xml">
CPAN</link>, provides a set of functions to select the top n elements from a list using several orderings and custom key extraction procedures.</p>
<p>

Because <link xlink:type="simple" xlink:href="../442/28442.xml#xpointer(//*[./st=%22Language+support%22])">
language support for sorting</link> is more ubiquitous, the simplistic approach of sorting followed by indexing is preferred in many environments despite its disadvantage in speed. Indeed for <change_of_state wordnetid="100199130" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<change wordnetid="100191142" confidence="0.8">
<improvement wordnetid="100248977" confidence="0.8">
<action wordnetid="100037396" confidence="0.8">
<optimization wordnetid="100260051" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<link xlink:type="simple" xlink:href="../155/18155.xml">
lazy languages</link></psychological_feature>
</act>
</optimization>
</action>
</improvement>
</change>
</event>
</change_of_state>
, this simplistic approach can even get you the best complexity possible for the k smallest/greatest sorted (with maximum/minimum as a special case) if your sort is lazy enough.</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../357/308357.xml">
M. Blum</link></scientist>
</person>
, <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../900/301900.xml">
R.W. Floyd</link></scientist>
</person>
, <link xlink:type="simple" xlink:href="../977/1597977.xml">
V. Pratt</link>, <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../057/68057.xml">
R. Rivest</link></scientist>
</person>
 and <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../127/46127.xml">
R. Tarjan</link></scientist>
</person>
, "Time bounds for selection," <it>J. Comput. System Sci</it>. 7 (1973) 448-461.</entry>
<entry level="1" type="bullet">

 <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Donald Knuth</link></scientist>
</person>
. <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../358/31358.xml">
The Art of Computer Programming</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
</it>, Volume 3: <it>Sorting and Searching</it>, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Section 5.3.3: Minimum-Comparison Selection, pp.207&ndash;219.</entry>
<entry level="1" type="bullet">

 <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../475/4108475.xml">
Thomas H. Cormen</link></scientist>
, <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../884/1400884.xml">
Charles E. Leiserson</link></scientist>
, <link xlink:type="simple" xlink:href="../057/68057.xml">
Ronald L. Rivest</link>, and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../993/3489993.xml">
Clifford Stein</link></scientist>
. <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../226/3499226.xml">
Introduction to Algorithms</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
</it>, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapter 9: Medians and Order Statistics, pp.183&ndash;196. Section 14.1: Dynamic order statistics, pp.302&ndash;308.</entry>
<entry level="1" type="bullet">

 Paul E. Black, <weblink xlink:type="simple" xlink:href="http://www.nist.gov/dads/HTML/select.html">
Select</weblink> at the <link xlink:type="simple" xlink:href="../888/21888.xml">
NIST</link> <link xlink:type="simple" xlink:href="../551/1661551.xml">
Dictionary of Algorithms and Data Structures</link>.</entry>
</list>

<reflist>
<entry id="1">
David R. Musser. Introspective Sorting and Selection Algorithms. <it>Software: Practice and Experience</it>, vol. 27, no. 8, pp.983&ndash;993. 1997. Section: Introspective Selection Algorithms.</entry>
</reflist>
</p>

</sec>
<sec>
<st>
External links</st>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ics.uci.edu/~eppstein/161/960130.html">
Design and Analysis of Algorithms</weblink>, for a detailed explanation of the recurrence relation for the median-of-medians</entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</article>
