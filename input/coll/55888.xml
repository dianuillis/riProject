<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:45:21[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<system  confidence="0.8" wordnetid="104377057">
<artifact  confidence="0.8" wordnetid="100021939">
<instrumentality  confidence="0.8" wordnetid="103575240">
<header>
<title>Trusted system</title>
<id>55888</id>
<revision>
<id>235136191</id>
<timestamp>2008-08-30T06:14:21Z</timestamp>
<contributor>
<username>Future Perfect at Sunrise</username>
<id>1224855</id>
</contributor>
</revision>
<categories>
<category>Security</category>
<category>Conceptual systems</category>
</categories>
</header>
<bdy>

In the <link xlink:type="simple" xlink:href="../730/28730.xml">
security engineering</link> subspecialty of <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>, a <b>trusted system</b> is a system that is relied upon to a specified extent to enforce a specified security policy. As such, a trusted system is one whose failure may break a specified security policy.<p>

A different usage of the term can be found in Time Management, specifically, GTD (<book wordnetid="106410904" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../928/959928.xml">
Getting Things Done</link></book>
).</p>

<sec>
<st>
Trusted systems in <link xlink:type="simple" xlink:href="../857/252857.xml">
classified information</link></st>

<p>

Trusted systems used for the processing, storage and retrieval of sensitive or <link xlink:type="simple" xlink:href="../857/252857.xml">
classified information</link>.</p>
<p>

Central to the concept of <agency wordnetid="108337324" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../897/7279897.xml">
U.S. Department of Defense</link></agency>
-style "trusted systems" is the notion of a "<link xlink:type="simple" xlink:href="../037/1928037.xml">
reference monitor</link>", which is an entity that occupies the logical heart of the system and is responsible for all access control decisions. Ideally, the reference monitor is (a) tamperproof, (b) always invoked, and (c) small enough to be subject to independent testing, the completeness of which can be assured. Per the U.S. <agency wordnetid="108337324" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../939/21939.xml">
National Security Agency</link></agency>
's 1983 <event wordnetid="100029378" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../300/6687300.xml">
Trusted Computer System Evaluation Criteria</link></activity>
</procedure>
</psychological_feature>
</act>
</event>
 (TCSEC), or <link xlink:type="simple" xlink:href="../389/60389.xml">
Orange Book</link>, a set of "evaluation classes" were defined that described the features and assurances that the user could expect from a trusted system.</p>
<p>

The highest levels of assurance were guaranteed by significant system engineering directed toward minimization of the size of the <link xlink:type="simple" xlink:href="../826/41826.xml">
trusted computing base</link>, or <link xlink:type="simple" xlink:href="../035/1549035.xml">
TCB</link>, defined as that combination of hardware, software, and firmware that is responsible for enforcing the system's security policy. </p>
<p>

Because failure of the TCB breaks the trusted system, higher assurance is provided by the minimization of the TCB. An inherent engineering conflict arises in higher-assurance systems in that, the smaller the TCB, the larger the set of hardware, software, and firmware that lies outside the TCB. This may lead to some philosophical arguments about the nature of trust, based on the notion that a "trustworthy" implementation may not necessarily be a "correct" implementation from the perspective of users' expectations.</p>
<p>

In stark contrast to the TCSEC's precisely defined hierarchy of six evaluation classes, the more recently introduced <standard wordnetid="107260623" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../625/57625.xml">
Common Criteria</link></activity>
</procedure>
</system_of_measurement>
</psychological_feature>
</act>
</event>
</standard>
 (CC)&mdash;which derive from an uneasy meld of more or less technically mature standards from various NATO countries&mdash;provide a more tenuous spectrum of seven "evaluation classes" that intermix features and assurances in an arguably non-hierarchical manner and lack the philosophic precision and mathematical stricture of the TCSEC. In particular, the CC tolerate very loose identification of the "target of evaluation" (TOE) and support&mdash;even encourage&mdash;a flippant intermixture of security requirements culled from a variety of predefined "protection profiles." While a very strong case can be made that even the more seemingly arbitrary components of the TCSEC contribute to a "chain of evidence" that a fielded system properly enforces its advertised security policy, not even the highest (E7) level of the CC can truly provide analogous consistency and stricture of evidentiary reasoning.</p>
<p>

The mathematical notions of trusted systems for the protection of <link xlink:type="simple" xlink:href="../857/252857.xml">
classified information</link> derive from two independent but interrelated corpora of work. In 1974, David Bell and Leonard LaPadula of MITRE, working under the close technical guidance and economic sponsorship of Maj. Roger Schell, Ph.D., of the U.S. Army Electronic Systems Command (Ft. Hanscom, MA), devised what is known as the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../866/423866.xml">
Bell-LaPadula model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
, in which a more or less trustworthy computer system is modeled in terms of <b>objects</b> (passive repositories or destinations for data, such as files, disks, printers) and <b>subjects</b> (active entities&mdash;perhaps users, or system processes or threads operating on behalf of those users&mdash;that cause information to flow among objects). The entire operation of a computer system can indeed be regarded a "history" (in the serializability-theoretic sense) of pieces of information flowing from object to object in response to subjects' requests for such flows.</p>
<p>

At the same time, Dorothy Denning at Purdue University was publishing her Ph.D. dissertation, which dealt with "lattice-based information flows" in computer systems. (A mathematical "lattice" is a partially ordered set, characterizable as a directed acyclic graph, in which the relationship between any two vertices is either "dominates," "is dominated by," or neither.) She defined a generalized notion of "labels"&mdash;corresponding more or less to the full security markings one encounters on classified military documents, <it>e.g.</it>, TOP SECRET WNINTEL TK DUMBO&mdash;that are attached to entities.  Bell and LaPadula integrated Denning's concept into their landmark MITRE technical report&mdash;entitled, <it>Secure Computer System: Unified Exposition and Multics Interpretation</it>&mdash;whereby labels attached to objects represented the sensitivity of data contained within the object (though there can be, and often is, a subtle semantic difference between the sensitivity of the data within the object and the sensitivity of the object itself)), while labels attached to subjects represented the trustworthiness of the user executing the subject. The concepts are unified with two properties, the "simple security property" (a subject can only read from an object that it <it>dominates</it> [''is greater than'' is a close enough&mdash;albeit mathematically imprecise&mdash;interpretation]) and the "confinement property," or "*-property" (a subject can only write to an object that dominates it). (These properties are loosely referred to as "no-read-up" and "no-write-down," respectively.) Jointly enforced, these properties ensure that information cannot flow "downhill" to a repository whence insufficiently trustworthy recipients may discover it. By extension, assuming that the labels assigned to subjects are truly representative of their trustworthiness, then the  no-read-up and no-write-down rules rigidly enforced by the reference monitor are provably sufficient to constrain <link xlink:type="simple" xlink:href="../955/59955.xml">
Trojan horses</link>, one of the most general classes of attack (<it>sciz.</it>, the popularly reported <link xlink:type="simple" xlink:href="../010/6010.xml">
worms</link> and <link xlink:type="simple" xlink:href="../679/19167679.xml">
viruses</link> are specializations of the Trojan horse concept).</p>
<p>

The Bell-LaPadula model technically enforces only "confidentiality," or "secrecy," controls, <it>i.e.</it>, they address the problem of the sensitivity of objects and attendant trustworthiness of subjects not inappropriately to disclose it. The dual problem of "integrity," i.e., the problem of accuracy (even provenance) of objects  and attendant trustworthiness of subjects not inappropriately to modify or destroy it, is addressed by mathematically affine models, the most important of which is named for its creator, <link>
K. J. Biba</link>. Other integrity models include the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../274/2035274.xml">
Clark-Wilson model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 and Shockley and Schell's <link>
program integrity</link> model.</p>
<p>

An important feature o the class of security controls described <it>supra</it>, termed <link xlink:type="simple" xlink:href="../900/879900.xml">
mandatory access control</link>s, or MAC, is that they are entirely beyond the control of any user: the TCB automatically attaches labels to any subjects executed on behalf of users; files created, deleted, read, or written by users; and so forth. In contrast, an additional class of controls, termed <link xlink:type="simple" xlink:href="../518/881518.xml">
discretionary access control</link>s, <b>are</b> under the direct control of the system users. Familiar protection mechanisms such as <link>
permission bits</link> (supported by UNIX since the late 1960s and&mdash;in a more flexible and powerful form&mdash;by Multics since earlier still) and <link xlink:type="simple" xlink:href="../589/61589.xml">
access control lists (ACLs)</link> are familiar examples of discretionary access controls.</p>
<p>

The behavior of a trusted system is often characterized in terms of a mathematical model&mdash;which may be more or less rigorous, depending upon applicable operational and administrative constraints&mdash;that takes the form of a <link xlink:type="simple" xlink:href="../931/10931.xml">
finite state machine</link> (FSM) with state criteria; state transition constraints; a set of "operations" that correspond to state transitions (usually, but not necessarily, one); and a <link>
descriptive top-level specification</link>, or <link xlink:type="simple" xlink:href="../599/1500599.xml">
DTLS</link>, entailing a user-perceptible <link xlink:type="simple" xlink:href="../275/41275.xml">
interface</link> (<it>e.g.</it>, an <link xlink:type="simple" xlink:href="../ury/24th_century.xml">
API</link>, a set of <link xlink:type="simple" xlink:href="../908/102908.xml">
system call</link>s [in [[UNIX]] parlance] or <link>
system exits</link> [in [[mainframe]] parlance]), each element of which engenders one or more model operations.</p>

</sec>
<sec>
<st>
Trusted systems in <link xlink:type="simple" xlink:href="../608/58608.xml">
trusted computing</link></st>

<p>

Trust is used by the <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../301/2792301.xml">
Trusted Computing Group</link></company>
 mainly in the sense of authorization ("a trusted user is a user authorized  to do X").</p>

</sec>
<sec>
<st>
Trusted systems in <link xlink:type="simple" xlink:href="../389/639389.xml">
policy analysis</link></st>

<p>

Trusted systems in the context of <link xlink:type="simple" xlink:href="../468/240468.xml">
national</link> or <link xlink:type="simple" xlink:href="../105/42105.xml">
homeland security</link>, <link xlink:type="simple" xlink:href="../037/3029037.xml">
law enforcement</link>, or <link xlink:type="simple" xlink:href="../682/81682.xml">
social control</link> policy are systems in which some conditional <link xlink:type="simple" xlink:href="../066/246066.xml">
prediction</link> about the behavior of people or objects within the system has been determined prior to authorizing access to system resources. [ [[#References|1]] ]</p>
<p>

For example, trusted systems include the use of "security envelopes" in national security and counterterrorism applications, "<link xlink:type="simple" xlink:href="../608/58608.xml">
trusted computing</link>" initiatives in technical systems security, and the use of <link xlink:type="simple" xlink:href="../655/3842655.xml">
credit or identity scoring</link> systems in financial and anti-fraud applications; in general, they include any system (i) in which probabilistic threat or <link xlink:type="simple" xlink:href="../077/2258077.xml">
risk analysis</link> is used to assess "trust" for decision-making before authorizing access or for allocating resources against likely threats (including their use in the design of systems <link xlink:type="simple" xlink:href="../360/206360.xml">
constraints</link> to control behavior within the system), or (ii) in which <link xlink:type="simple" xlink:href="../897/3149897.xml">
deviation analysis</link> or systems <link xlink:type="simple" xlink:href="../231/87231.xml">
surveillance</link> is used to insure that behavior within systems complies with expected or authorized parameters.</p>
<p>

The widespread adoption of these authorization-based security strategies (where the default state is DEFAULT=DENY) for counterterrorism, anti-fraud, and other purposes is helping accelerate the ongoing transformation of modern societies from a notional Beccarian model of <link xlink:type="simple" xlink:href="../219/168219.xml">
criminal justice</link> based on accountability for deviant actions after they occur, see <link xlink:type="simple" xlink:href="../198/196198.xml">
Cesare Beccaria</link>, On Crimes and Punishment (1764), to a Foucauldian model based on authorization, preemption, and general social compliance through ubiquitous preventative <link xlink:type="simple" xlink:href="../231/87231.xml">
surveillance</link> and control through system <link xlink:type="simple" xlink:href="../360/206360.xml">
constraints</link>, see <person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../643/47643.xml">
Michel Foucault</link></philosopher>
</person>
, <link xlink:type="simple" xlink:href="../617/1110617.xml">
Discipline and Punish</link> (1975, Alan Sheridan, tr., 1977, 1995).</p>
<p>

In this emergent model, "security" is geared not towards <link xlink:type="simple" xlink:href="../627/23627.xml">
policing</link> but to <link xlink:type="simple" xlink:href="../404/26404.xml">
risk management</link> through <link xlink:type="simple" xlink:href="../231/87231.xml">
surveillance</link>, exchange of information, <link xlink:type="simple" xlink:href="../700/224700.xml">
auditing</link>, communication, and <link xlink:type="simple" xlink:href="../717/72717.xml">
classification</link>.  These developments have led to general concerns about individual <link xlink:type="simple" xlink:href="../009/25009.xml">
privacy</link> and <link xlink:type="simple" xlink:href="../476/37476.xml">
civil liberty</link> and to a broader <link xlink:type="simple" xlink:href="../155/13692155.xml">
philosophical</link> debate about the appropriate forms of <link xlink:type="simple" xlink:href="../682/81682.xml">
social governance</link> methodologies.</p>

</sec>
<sec>
<st>
Trusted systems in <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link></st>

<p>

Trusted systems in the context of <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link> is based on the definition of trust as <b>'Trust is that which is essential to a communication channel but cannot be transferred from a source to a destination using that channel</b>' by Ed Gerck [ [[#References|2]] ].</p>
<p>

In <link xlink:type="simple" xlink:href="../773/14773.xml">
Information Theory</link>, information has nothing to do with knowledge or meaning. In the context of Information Theory, information is simply that which is transferred from a source to a destination, using a communication channel. If, before transmission, the information is available at the destination then the transfer is zero. Information received by a party is that what the party does not expect -- as measured by the uncertainty of the party as to what the message will be. </p>
<p>

Likewise, trust as defined by Gerck has nothing to do with friendship, acquaintances, employee-employer relationships, loyalty, betrayal and other overly-variable concepts.  Trust is not taken  in the purely subjective sense either,  nor as a feeling or something purely personal or psychological -- trust is understood as something potentially communicable. Further, this definition of trust is abstract, allowing different instances and observers in a trusted system  to communicate based on a common idea of trust (otherwise communication would be isolated in domains), where all necessarily different subjective and intersubjective realizations of trust in each subsystem (man and machines) may coexist. [ [[#References|3]] ]</p>
<p>

Taken together in the model of Information Theory, <b>information is what you do not expect</b> and <b>trust is what you know</b>. Linking both concepts, trust is seen as  <b>qualified reliance on received information</b>. In terms of trusted systems, an assertion of trust cannot be based on the record itself, but on information from other information channels. [ [[#References|4]] ]</p>
<p>

An introduction to the calculus of trust (Example: 'If I connect two trusted systems, are they more or less trusted when taken together?') is given in [ [[#References|3]] ].</p>
<p>

The <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link></company>
 Federal Software Group [ [[#References|5]] ] has suggested that [ [[#References|2]] ] provides the most useful definition of trust for application in an information technology environment, because it is related to other information theory concepts and provides a basis for measuring trust. In a network centric enterprise services environment, such notion of trust is considered [ [[#References|5]] ] to be requisite for achieving the desired collaborative, service-oriented architecture vision.  </p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="number">

 The concept of trusted systems described here is discussed in K. A. Taipale, "<weblink xlink:type="simple" xlink:href="http://doi.ieeecomputersociety.org/10.1109/MIS.2005.89">
The Trusted Systems Problem: Security Envelopes, Statistical Threat Analysis, and the Presumption of Innocence</weblink>," Homeland Security - Trends and Controversies, IEEE Intelligent Systems, Vol. 20 No. 5, pp. 80-83 (Sept./Oct. 2005).</entry>
<entry level="1" type="number">

 <it>Trust Points</it>, in Digital Certificates: Applied Internet Security by J. Feghhi, J. Feghhi and P. Williams, Addison-Wesley, ISBN 0-20-130980-7, 1998; <weblink xlink:type="simple" xlink:href="http://mcwg.org/mcg-mirror/trustdef.htm">
Toward Real-World Models of Trust: Reliance on Received Information</weblink></entry>
<entry level="1" type="number">

 "<weblink xlink:type="simple" xlink:href="http://nma.com/papers/it-trust-part1.pdf">
Trust as Qualified Reliance on Information, Part I</weblink>," The COOK Report on Internet, Volume X, No. 10, January 2002, ISSN 1071 - 6327.</entry>
<entry level="1" type="number">

 <weblink xlink:type="simple" xlink:href="http://pages.ca.inter.net/~euclid1/call.html">
John D. Gregory</weblink>, Electronic Legal Records: Pretty Good Authentication?</entry>
<entry level="1" type="number">

 <weblink xlink:type="simple" xlink:href="http://issaa.org/documents/NCEStrustframework.doc">
Christopher Daly</weblink>, A Trust Framework for the DoD Network-Centric Enterprise Services (NCES) Environment, IBM Corp., 2004.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>

<p>

See also, <weblink xlink:type="simple" xlink:href="http://trusted-systems.info/">
The Trusted Systems Project</weblink>, a part of the Global Information Society Project (<weblink xlink:type="simple" xlink:href="http://global-info-society.org/">
GISP</weblink>), a joint research project of the World Policy Institute (<weblink xlink:type="simple" xlink:href="http://worldpolicy.org/">
WPI</weblink>) and the Center for Advanced Studies in Sci. &amp; Tech. Policy (<weblink xlink:type="simple" xlink:href="http://advancedstudies.org/">
CAS</weblink>).</p>
<p>

<it>See also:</it>
<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../398/7398.xml">
computer security</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../398/7398.xml">
secure computing</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../608/58608.xml">
trusted computing</link></entry>
</list>
</p>


</sec>
</bdy>
</instrumentality>
</artifact>
</system>
</article>
