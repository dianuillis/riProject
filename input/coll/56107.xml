<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:46:19[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<method  confidence="0.9511911446218017" wordnetid="105660268">
<header>
<title>Metropolis–Hastings algorithm</title>
<id>56107</id>
<revision>
<id>240917798</id>
<timestamp>2008-09-25T15:33:00Z</timestamp>
<contributor>
<username>Autotomic</username>
<id>6442230</id>
</contributor>
</revision>
<categories>
<category>Monte Carlo methods</category>
</categories>
</header>
<bdy>

<image width="350px" src="Metropolis_hastings_algorithm.png" type="thumb">
<caption>

The Proposal <link>
distribution</link> <it>Q</it> proposes the next point that the <link xlink:type="simple" xlink:href="../451/235451.xml">
random walk</link> might move to.
</caption>
</image>
<p>

In <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link> and <link xlink:type="simple" xlink:href="../939/22939.xml">
physics</link>, the <b>Metropolis-Hastings algorithm</b> is a method for creating a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 that can be used to generate a sequence of <link>
 samples</link> from a <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> that is difficult to <link xlink:type="simple" xlink:href="../361/160361.xml">
sample</link> from directly.  This sequence can be used in <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../801/236801.xml">
Markov chain Monte Carlo</link></method>
</know-how>
 simulation to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). The algorithm was named in reference to <physical_entity wordnetid="100001930" confidence="0.8">
<physicist wordnetid="110428004" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../326/504326.xml">
Nicholas Metropolis</link></educator>
</mathematician>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physicist>
</physical_entity>
, who published it in 1953 for the specific case of the <link xlink:type="simple" xlink:href="../107/4107.xml">
Boltzmann distribution</link>, and W.K. Hastings,<weblink xlink:type="simple" xlink:href="http://probability.ca/hastings/">
http://probability.ca/hastings/</weblink> who generalized it in 1970. The <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../709/509709.xml">
Gibbs sampling</link></method>
</know-how>
 algorithm is a special case of the Metropolis-Hastings algorithm which is usually faster and easier to use but is less generally applicable.</p>
<p>

The Metropolis-Hastings algorithm can draw samples from any <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> <math>P(x)</math>, requiring only that a function proportional to the density can be calculated at <math>x</math>.  In Bayesian applications, the normalization factor is often extremely difficult to compute, so the ability to generate a sample without knowing this constant of proportionality is a major virtue of the algorithm.   The algorithm generates a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 in which each state 
<math>x^{t+1}</math> depends only on the previous state <math>x^t</math>.  The algorithm uses a <it>proposal density</it> 
<math>Q(x'; x^t )</math>, which depends on the current state <math>x^t</math>, to generate a new proposed sample <math>x'</math>.  
This proposal is 'accepted' as the next value (<math>x^{t+1}</math>=<math>x'</math> ) if <math>\alpha</math> drawn from 
<math>U(0,1)</math> satisfies </p>
<p>

<indent level="1">

<math>
\alpha &amp;lt; \frac{P(x')Q(x^t|x')}{P(x^t)Q(x'|x^t)} \,\!.
</math>
</indent>

If the proposal is not accepted, then the current value of <math>x</math> is retained: <math>x^{t+1}=x^t</math>.</p>
<p>

For example, the proposal density could be a <link xlink:type="simple" xlink:href="../552/245552.xml">
Gaussian function</link> centred on the current state <math>x^t</math>:</p>
<p>

<indent level="1">

<math>
Q( x'; x^t ) \sim N( x^t, \sigma^2 I) \,\!
</math>
</indent>

reading <math>Q(x'; x^t)</math> as the probability density function for <math>x'</math> given the previous value <math>x^t</math>.   This proposal density would generate samples centred around the current state with variance <math>\sigma^2 I</math>. The original Metropolis algorithm calls for the proposal density to be symmetric ( <math>Q(x; y) = Q(y; x)</math> ); the generalization by Hastings lifts this restriction. It is also permissible for <math>Q(x', x^t)</math> not to depend on <math>x'</math> at all, in which case the algorithm is called "Independence Chain Metropolis-Hastings" ( as opposed to "Random Walk Metropolis-Hastings" ). The Independence Chain M-H algorithm with a suitable proposal density function can offer higher accuracy than the random walk version, but it requires some <it>a priori</it> knowledge of the distribution.</p>

<sec>
<st>
Step-by-step instructions</st>

<p>

Suppose the most recent value sampled is <math>x^t\,</math>.
To follow the Metropolis-Hastings algorithm, we next draw a new proposal state 
<math>x'\,</math> with probability <math>Q(x'; x^t)\,</math>, and calculate a value</p>
<p>

<indent level="1">

<math>
a = a_1 a_2\,
</math>
</indent>

where</p>
<p>

<indent level="1">

<math>
a_1 = \frac{P(x')}{P(x^t)} \,\!
</math>
</indent>

is the likelihood ratio between the proposed sample <math>x'\,</math> and the previous sample <math>x^t\,</math>, and</p>
<p>

<indent level="1">

<math>
a_2 = \frac{Q( x^t; x' )}{Q(x';x^t)}
</math>
</indent>

is the ratio of the proposal density in two directions (from <math>x^t\,</math> to <math>x'\,</math> and <it>vice versa</it>).
This is equal to 1 if the proposal density is symmetric.
Then the new state <math>x^{t+1}\,</math> is chosen according to the following rules.</p>
<p>

<indent level="1">

<math>
\begin{matrix}
\mbox{If } a \geq 1: &amp;  \\
&amp; x^{t+1} = x',
\end{matrix}
</math>
</indent>
:<math>
\begin{matrix}
\mbox{and if } a &amp;lt; 1: &amp; \\
&amp; x^{t+1} = \left\{
                   \begin{matrix}
                       x'\mbox{ with probability }a \\
                       x^t\mbox{ with probability }1-a.
                   \end{matrix}
            \right.
\end{matrix}
</math></p>
<p>

The Markov chain is started from a random initial value <math>x^0</math> and the algorithm is run for many iterations until this initial state is "forgotten".  
These samples, which are discarded, are known as <it>burn-in</it>. The remaining set of accepted values of <math>x</math> represent a <link>
 sample</link> from the distribution <math>P(x)</math>.</p>
<p>

<image width="350px" src="3dRosenbrock.png" type="thumb">
<caption>

The result of three <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
 Markov chains</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 running on the 3d <link>
 Rosenbrock function </link> using the Metropolis-Hastings algorithm. It is obvious how the algorithm samples from regions where the <link>
 posterior probability</link> is high and how the chains begin to mix in these regions. The approximate position of the maximum has been illuminated.
</caption>
</image>
</p>
<p>

The algorithm works best if the proposal density matches the shape of the target distribution <math>P(x)</math>, that is <math>Q(x'; x^t) \approx P(x') \,\!</math>, but in most cases this is unknown.  
If a Gaussian proposal density <math>Q</math> is used the variance parameter <math>\sigma^2</math> has to be tuned during the burn-in period.
This is usually done by calculating the <it>acceptance rate</it>, which is the fraction of proposed samples that is accepted in a window of the last <math>N</math> samples.
The desired acceptance rate depends on the target distribution, however it has been shown theoretically that the ideal acceptance rate for a one dimensional Gaussian distribution is approx 50%, decreasing to approx 23% for an N-dimensional Gaussian target distribution. 
If <math>\sigma^2</math> is too small the chain will <it>mix slowly</it> (i.e., the acceptance rate will be too high, so the sampling will move around the space slowly and converge slowly to <math>P(x)</math>).
If <math>\sigma^2</math> is too large the acceptance rate will be very low because the proposals are likely to land in regions of much lower probability density so <math>a_1</math> will be very small.</p>

</sec>
<sec>
<st>
 See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../244/172244.xml">
Simulated annealing</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../867/2212867.xml">
Detailed balance</link></entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../630/17238630.xml">
Multiple-try Metropolis</link></method>
</know-how>
</entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

 Bernd A. Berg. <it>Markov Chain Monte Carlo Simulations and Their Statistical Analysis</it>. Singapore, World Scientific 2004.</entry>
<entry level="1" type="bullet">

 Siddhartha Chib and Edward Greenberg: "Understanding the Metropolis&ndash;Hastings Algorithm". <it>American Statistician</it>, 49(4), 327&ndash;335, 1995</entry>
<entry level="1" type="bullet">

 W.K. Hastings. "Monte Carlo Sampling Methods Using Markov Chains and Their Applications", <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../178/1734178.xml">
Biometrika</link></periodical>
</it>, 57(1):97-109, 1970. <weblink xlink:type="simple" xlink:href="http://links.jstor.org/sici?sici=0006-3444%28197004%2957%3A1%3C97%3AMCSMUM%3E2.0.CO%3B2-C">
JSTOR</weblink> <weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.2307/2334940">
doi:10.2307/2334940</weblink></entry>
<entry level="1" type="bullet">

 N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller, and E. Teller. "<link xlink:type="simple" xlink:href="../194/18381194.xml">
Equations of State Calculations by Fast Computing Machines</link>". <it>Journal of Chemical Physics</it>, 21(6):1087-1092, 1953. <weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1063/1.1699114">
http://dx.doi.org/10.1063/1.1699114</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://mathworld.wolfram.com/SimulatedAnnealing.html">
MathWorld article on Simulated Annealing</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://xbeta.org/wiki/show/Metropolis-Hastings+algorithm">
Metropolis-Hastings algorithm on xβ</weblink></entry>
</list>
</p>


</sec>
</bdy>
</method>
</article>
