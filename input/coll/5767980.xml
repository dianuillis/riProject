<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 21:57:22[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<know-how  confidence="0.8" wordnetid="105616786">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<method  confidence="0.8" wordnetid="105660268">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Cross-entropy method</title>
<id>5767980</id>
<revision>
<id>243733191</id>
<timestamp>2008-10-07T20:39:46Z</timestamp>
<contributor>
<username>Twri</username>
<id>7976492</id>
</contributor>
</revision>
<categories>
<category>Heuristics</category>
<category>Monte Carlo methods</category>
<category>Optimization algorithms</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

The <b>cross-entropy (CE) method</b> attributed to Reuven Rubinstein is a general <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link>
Monte Carlo</link></method>
</know-how>
</technique>
 approach to
<link xlink:type="simple" xlink:href="../555/420555.xml">
combinatorial</link> and <link xlink:type="simple" xlink:href="../577/420577.xml">
continuous</link> multi-extremal <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> and <link xlink:type="simple" xlink:href="../671/867671.xml">
importance sampling</link>. 
The method originated from the field of <it>rare event simulation</it>, where
very small probabilities need to be accurately estimated, for example in  network reliability analysis, queueing models, or performance analysis of telecommunication systems.
The CE method can be applied to static and noisy combinatorial optimization problems such as the <link xlink:type="simple" xlink:href="../248/31248.xml">
traveling salesman problem</link>, the <link xlink:type="simple" xlink:href="../520/1636520.xml">
quadratic assignment problem</link>, <link>
DNA sequence alignment</link>, the <link xlink:type="simple" xlink:href="../494/2180494.xml">
max-cut</link> problem and the buffer allocation problem, as well as continuous <link>
global optimization</link> problems with many local <link xlink:type="simple" xlink:href="../420/298420.xml">
extrema</link>.<p>

In a nutshell the CE method consists of two phases:</p>
<p>

<list>
<entry level="1" type="number">

Generate a random data sample (trajectories, vectors, etc.) according to a specified mechanism.</entry>
<entry level="1" type="number">

Update the parameters of the random mechanism based on the data to produce a "better" sample in the next iteration. This step involves minimizing the <link xlink:type="simple" xlink:href="../250/1735250.xml">
<it>cross-entropy''</it></link> or  <link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler divergence</link>.</entry>
</list>
</p>

<sec>
<st>
Estimation via importance sampling</st>
<p>

Consider the general problem of estimating the quantity <math>\ell = \mathbb{E}_{\mathbf{u}}[H(\mathbf{X})] = \int H(\mathbf{x})\, f(\mathbf{x}; \mathbf{u})\, \textrm{d}\mathbf{x}</math>, where <math>H</math> is some <it>performance function</it> and <math>f(\mathbf{x};\mathbf{u})</math> is a member of some <link xlink:type="simple" xlink:href="../337/19653337.xml">
parametric family</link> of distributions. Using <link xlink:type="simple" xlink:href="../671/867671.xml">
importance sampling</link> this quantity can be estimated as <math>\hat{\ell} = \frac{1}{N} \sum_{i=1}^N H(\mathbf{X}_i) \frac{f(\mathbf{X}_i; \mathbf{u})}{g(\mathbf{X}_i)}</math>, where <math>\mathbf{X}_1,\dots,\mathbf{X}_N</math> is a random sample from <math>g\,</math>. For positive <math>H</math>, the theoretically <it>optimal</it> importance sampling <link xlink:type="simple" xlink:href="../487/43487.xml">
density</link> (pdf)is given by 
<math> g^*(\mathbf{x}) = H(\mathbf{x}) f(\mathbf{x};\mathbf{u})/\ell</math>. This, however, depends on the unknown <math>\ell</math>. The CE method aims to approximate the optimal pdf by adaptively selecting members of the parametric family that are closest (in the <link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler</link> sense) to the optimal pdf <math>g^*</math>.</p>

</sec>
<sec>
<st>
Generic CE algorithm</st>
<p>

<list>
<entry level="1" type="number">

 Choose initial parameter vector <math>\mathbf{v}^{(0)}</math>; set t = 1.</entry>
<entry level="1" type="number">

 Generate a random sample <math>\mathbf{X}_1,\dots,\mathbf{X}_N</math> from <math>f(\cdot;\mathbf{v}^{(t-1)})</math></entry>
<entry level="1" type="number">

 Solve for <math>\mathbf{v}^{(t)}</math>, where<math>\mathbf{v}^{(t)} = \mathop{\textrm{argmax}}_{\mathbf{v}} \frac{1}{N} \sum_{i=1}^N H(\mathbf{X}_i)\frac{f(\mathbf{X}_i;\mathbf{u})}{f(\mathbf{X}_i;\mathbf{v}^{(t-1)})} \log f(\mathbf{X}_i;\mathbf{v})</math></entry>
<entry level="1" type="number">

 If convergence is reached then <b>stop</b>; otherwise, increase t by 1 and reiterate from step 2.</entry>
</list>
</p>
<p>

In several cases, the solution to step 3 can be found <it>analytically</it>.  Situations in which this occurs are
<list>
<entry level="1" type="bullet">

 When <math>f\,</math> belongs to the <structure wordnetid="105726345" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link>
natural exponential family</link></kind>
</distribution>
</type>
</arrangement>
</category>
</concept>
</idea>
</structure>
</entry>
<entry level="1" type="bullet">

 When <math>f\,</math> is <link xlink:type="simple" xlink:href="../061/56061.xml">
discrete</link> with finite <link xlink:type="simple" xlink:href="../013/381013.xml">
support</link></entry>
<entry level="1" type="bullet">

 When <math>H(\mathbf{X}) = \mathrm{I}_{\{\mathbf{x}\in A\}}</math> and <math>f(\mathbf{X}_i;\mathbf{u}) = f(\mathbf{X}_i;\mathbf{v}^{(t-1)})</math>, then <math>\mathbf{v}^{(t)}</math> corresponds to the <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood estimator</link> based on those <math>\mathbf{X}_k \in A</math>.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Continuous optimization&mdash;example</st>
<p>

The same CE algorithm can be used for optimization, rather than estimation. 
Suppose the problem is to maximize some function <math>S(x)</math>, for example, 
<math>S(x) = \textrm{e}^{-(x-2)^2} + 0.8\,\textrm{e}^{-(x+2)^2}</math>. 
To apply CE, one considers first the <it>associated stochastic problem</it> of estimating
<math>\mathbb{P}_{\boldsymbol{\theta}}(S(X)\geq\gamma)</math>
for a given <it>level</it> <math>\gamma\,</math>, and parametric family <math>\left\{f(\cdot;\boldsymbol{\theta})\right\}</math>, for example the 1-dimensional 
<link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian distribution</link>,
parameterized by its mean <math>\mu_t\,</math> and variance <math>\sigma_t^2</math> (so <math>\boldsymbol{\theta} = (\mu,\sigma^2)</math> here).
Hence, for a given <math>\gamma\,</math>, the goal is to find <math>\boldsymbol{\theta}</math> so that
<math>D_{\mathrm{KL}}(\textrm{I}_{\{S(x)\geq\gamma\}}\|f_{\boldsymbol{\theta}})</math>
is minimized. This is done by solving the sample version (stochastic counterpart) of the KL divergence minimization problem, as in step 3 above.
It turns out that parameters that minimize the stochastic counterpart for this choice of target distribution and
parametric family are the sample mean and sample variance corresponding to the <it>elite samples</it>, which are those samples that have objective function value <math>\geq\gamma</math>.
The worst of the elite samples is then used as the level parameter for the next iteration.
This yields the following randomized algorithm for this problem.</p>

<ss1>
<st>
Pseudo-code</st>
<p>

1. mu:=-6; sigma2:=100; t:=0; maxits=100;    // Initialize parameters
2. N:=100; Ne:=10;                           //
3. while t  maxits and sigma2 &amp;gt; epsilon     // While not converged and maxits not exceeded
4.  X = SampleGaussian(mu,sigma2,N);         // Obtain N samples from current sampling distribution
5.  S = exp(-(X-2)^2) + 0.8 exp(-(X+2)^2);   // Evaluate objective function at sampled points
6.  X = sort(X,S);                           // Sort X by objective function values (in descending order)
7.  mu = mean(X(1:Ne)); sigma2=var(X(1:Ne)); // Update parameters of sampling distribution
8.  t = t+1;                                 // Increment iteration counter
9. return mu                                 // Return mean of final sampling distribution as solution</p>

</ss1>
</sec>
<sec>
<st>
Related methods</st>
<p>

<list>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../244/172244.xml">
Simulated annealing</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../254/40254.xml">
Genetic algorithms</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../637/3062637.xml">
Estimation of distribution algorithm</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../937/381937.xml">
Tabu search</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../250/1735250.xml">
Cross entropy</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler divergence</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../383/495383.xml">
Randomized algorithm</link></entry>
<entry level="1" type="bullet">

<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../671/867671.xml">
Importance sampling</link></method>
</know-how>
</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

De Boer, P-T., Kroese, D.P, Mannor, S. and Rubinstein, R.Y. (2005). A Tutorial on the Cross-Entropy Method. <it>Annals of Operations Research</it>, <b>134</b> (1), 19--67.</entry>
<entry level="1" type="bullet">

Rubinstein, R.Y. (1997). Optimization of Computer simulation Models with Rare Events, <it>European Journal of Operations Research</it>, <b>99</b>, 89-112.</entry>
<entry level="1" type="bullet">

Rubinstein, R.Y., Kroese, D.P. (2004). <it>The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning</it>, Springer-Verlag, New York.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cemethod.org/">
Homepage for the CE method</weblink></entry>
</list>
</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</method>
</rule>
</event>
</know-how>
</article>
