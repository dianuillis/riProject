<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:19:20[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<plant  confidence="0.8" wordnetid="100017222">
<tree  confidence="0.8" wordnetid="113104059">
<vascular_plant  confidence="0.8" wordnetid="113083586">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<woody_plant  confidence="0.8" wordnetid="113103136">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Decision tree learning</title>
<id>577003</id>
<revision>
<id>239755881</id>
<timestamp>2008-09-20T09:10:17Z</timestamp>
<contributor>
<username>Salix alba</username>
<id>212526</id>
</contributor>
</revision>
<categories>
<category>Decision trees</category>
<category>Data mining</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

<b>Decision tree learning</b>, used in <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link> and <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, uses a <link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link> as a <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../775/2538775.xml">
predictive model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 which maps observations about an item to conclusions about the item's target value. More descriptive names for such tree models are <b>classification trees</b> or <b>regression trees</b>. In these tree structures, <link xlink:type="simple" xlink:href="../228/18228.xml">
leaves</link> represent classifications and branches represent <link xlink:type="simple" xlink:href="../959/74959.xml">
conjunction</link>s of features that lead to those classifications. <p>

In <link xlink:type="simple" xlink:href="../216/446216.xml">
decision theory</link> and <link xlink:type="simple" xlink:href="../842/1190842.xml">
decision analysis</link>, a decision tree is a <link xlink:type="simple" xlink:href="../669/598669.xml">
graph</link> or model of <link xlink:type="simple" xlink:href="../827/1196827.xml">
decisions</link> and their possible consequences, including <link xlink:type="simple" xlink:href="../912/47912.xml">
chance</link> event outcomes, resource costs, and <link xlink:type="simple" xlink:href="../479/45479.xml">
utility</link>. It can be used to create a <link xlink:type="simple" xlink:href="../824/179824.xml">
plan</link> to reach a <link xlink:type="simple" xlink:href="../094/14850094.xml">
goal</link>.  Decision trees are constructed in order to help with making decisions. A decision tree is a special form of <link xlink:type="simple" xlink:href="../821/41821.xml">
tree structure</link>. Another use of trees is as a descriptive means for calculating <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional probabilities</link>.</p>
<p>

In decision analysis, a decision tree can be used to visually and explicitly represent decisions and <link xlink:type="simple" xlink:href="../752/265752.xml">
decision making</link>. In <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link>, a decision tree describes data but not decisions; rather the resulting classification tree can be an input for <link xlink:type="simple" xlink:href="../752/265752.xml">
decision making</link>. This page deals with trees in <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link>.</p>

<sec>
<st>
General</st>
<p>

Decision tree learning is a common method used in data mining. Each <link xlink:type="simple" xlink:href="../806/30806.xml">
interior node</link> corresponds to a variable; an arc to a child represents a possible value of that variable.  A leaf represents a possible value of target variable given the values of the variables represented by the path from the root. </p>
<p>

A tree can be "learned" by splitting the source <link xlink:type="simple" xlink:href="../691/26691.xml">
set</link> into subsets based on an attribute value test.  This process is repeated on each derived subset in a recursive manner.  The <link xlink:type="simple" xlink:href="../407/25407.xml">
recursion</link> is completed when splitting is either non-feasible, or a singular classification can be applied to each element of the derived subset. A <link xlink:type="simple" xlink:href="../880/1363880.xml">
random forest</link> classifier uses a number of decision trees, in order to improve the classification rate.</p>
<p>

In <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link>, trees can be described also as the combination of mathematical and computing techniques to aid the description, categorisation and generalisation of a given set of data.</p>
<p>

Data comes in records of the form:    </p>
<p>

<b>(x, y) = (x1, x2, x3..., xk, y)</b></p>
<p>

The dependent variable, Y, is the variable that we are trying to understand, classify or generalise. The other variables, x1, x2, x3 etc., are the variables that will help with that task.</p>

</sec>
<sec>
<st>
Types</st>
<p>

In <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link>, trees have three more descriptive categories/names:</p>
<p>

<list>
<entry level="1" type="bullet">

<it>Classification tree</it> analysis is when the predicted outcome is the class to which the data belongs. </entry>
<entry level="1" type="bullet">

<it>Regression tree</it> analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient’s length of stay in a hospital).</entry>
<entry level="1" type="bullet">

<it>Classification And Regression Tree</it> (CART) analysis is used to refer to both of the above procedures, first introduced by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../283/4909283.xml">
Breiman</link></associate>
</mathematician>
</scientist>
</causal_agent>
</colleague>
</statistician>
</person>
</peer>
</physical_entity>
 et al. (BFOS84).</entry>
</list>
</p>

</sec>
<sec>
<st>
Practical example</st>
<p>

Our friend David is the manager of a famous golf club. Sadly, he is having some trouble with his customer attendance. There are days when everyone wants to play golf and the staff are overworked. On other days, for no apparent reason, no one plays golf and staff have too much slack time. David’s objective is to optimise staff availability by trying to predict when people will play golf. To accomplish that he needs to understand the reason people decide to play and if there is any explanation for that. He assumes that weather must be an important underlying factor, so he decides to use the weather forecast for the upcoming week. So during two weeks he has been recording:</p>
<p>

<list>
<entry level="1" type="bullet">

 The outlook, whether it was sunny, overcast or raining. </entry>
<entry level="1" type="bullet">

 The temperature (in degrees Fahrenheit). </entry>
<entry level="1" type="bullet">

 The relative humidity in percent. </entry>
<entry level="1" type="bullet">

 Whether it was windy or not.</entry>
<entry level="1" type="bullet">

 Whether people attended the golf club on that day. </entry>
</list>
</p>
<p>

David compiled this dataset into a table containing 14 rows and 5 columns as shown below.</p>
<p>

<image width="150px" src="golf_dataset.png">
<caption>

golf dataset.png
</caption>
</image>
</p>
<p>

He then applied a decision tree model to solve his problem.</p>
<p>

<image width="150px" src="decision_tree_model.png">
<caption>

decision tree model.png
</caption>
</image>
</p>
<p>

A decision tree is a model of the data that encodes the distribution of the class label (again the Y) in terms of the predictor attributes. It is a <link xlink:type="simple" xlink:href="../002/204002.xml">
directed acyclic graph</link> in form of a tree. The top node represents all the data. The classification tree algorithm concludes that the best way to explain the dependent variable, play, is by using the variable "outlook". Using the categories of the variable outlook, three different groups were found:</p>
<p>

<list>
<entry level="1" type="bullet">

One that plays golf when the weather is sunny, </entry>
<entry level="1" type="bullet">

One that plays when the weather is cloudy, and </entry>
<entry level="1" type="bullet">

One that plays when it's raining.</entry>
</list>
</p>
<p>

David's first conclusion: if the outlook is overcast people always play golf, and there are some fanatics who play golf even in the rain. Then he divided the sunny group in two. He realised that people don't like to play golf if the humidity is higher than seventy percent.</p>
<p>

Finally, he divided the rain category in two and found that people will also not play golf if it is windy.</p>
<p>

And lastly, here is the short solution of the problem given by the classification tree: David dismisses most of the staff on days that are sunny and humid or on rainy days that are windy, because almost no one is going to play golf on those days. On days when a lot of people will play golf, he hires extra staff. The conclusion is that the decision tree helped David turn a complex data representation into a much easier structure (parsimonious).</p>

</sec>
<sec>
<st>
Formulae</st>

<ss1>
<st>
Gini impurity</st>
<p>
                                                 
Used by the <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../563/4141563.xml#xpointer(//*[./st=%22Classification+and+regression+trees%22])">
CART algorithm</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
, Gini impurity is based on squared probabilities of membership for each target category in the node. It reaches its minimum (zero) when all cases in the node fall into a single target category.                      </p>
<p>

Suppose y takes on values in {1, 2, ..., m}, and let f(i, j) = probability of getting value j in node i. That is, f(i, j) is the proportion of records assigned to node i for which y = j.                       </p>
<p>

<math>I_{G}(i) = 1 - \sum^{m}_{j=1} f (i,j)^{2} = \sum^{}_{j \ne k} f (i,j) f (i,k)</math></p>

</ss1>
<ss1>
<st>
Information gain</st>

<p>

Used by the <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../797/1966797.xml">
ID3</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
, <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../814/1966814.xml">
C4.5</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
 and C5.0 tree generation algorithms. <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../412/2507412.xml">
Information gain</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
 is based on the concept of <link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link> used in <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link> .</p>
<p>

<math>I_{E}(i) = - \sum^{m}_{j=1} f (i,j) \log^{}_2 f (i, j)</math></p>

</ss1>
</sec>
<sec>
<st>
Decision tree advantages</st>
<p>
                                               
Amongst other data mining methods, decision trees have several advantages:
<list>
<entry level="1" type="bullet">

 Simple to understand and interpret. People are able to understand decision tree models after a brief explanation.</entry>
<entry level="1" type="bullet">

 Requires little data preparation. Other techniques often require data normalisation, <link xlink:type="simple" xlink:href="../156/285156.xml">
dummy variable</link>s need to be created and blank values to be removed.</entry>
<entry level="1" type="bullet">

 Able to handle both numerical and <link xlink:type="simple" xlink:href="../148/821148.xml">
categorical</link> data. Other techniques are usually specialised in analysing datasets that have only one type of variable. Ex: relation rules can be only used with nominal variables while neural networks can be used only with numerical variables.</entry>
<entry level="1" type="bullet">

 Use a <link xlink:type="simple" xlink:href="../308/2288308.xml">
white box</link> model. If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic. An example of a black box model is an <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link> since the explanation for the results is excessively complex to be comprehended.  </entry>
<entry level="1" type="bullet">

 Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.</entry>
<entry level="1" type="bullet">

 Robust, perform well with large data in a short time. Large amounts of data can be analysed using personal computers in a time short enough to enable stakeholders to take decisions based on its analysis.</entry>
</list>
</p>

</sec>
<sec>
<st>
Extending decision trees with decision graphs</st>
<p>

In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or <it>AND</it>.
In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together.  This was probably first developed in a series of <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/oliver93decision.html">
three papers</weblink> in 1991-1993 with Jon Oliver  as first author using <link xlink:type="simple" xlink:href="../210/302210.xml">
Minimum Message Length</link> (<link xlink:type="simple" xlink:href="../210/302210.xml">
MML</link>).  Decision graphs have been further extended in <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/Publications/2003/Tan+Dowe2003_MMLDecisionGraphs.pdf">
Tan &amp; Dowe (2003)</weblink>, again using <link xlink:type="simple" xlink:href="../210/302210.xml">
MML</link>, to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.  The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring.  In general, decision graphs infer models with fewer leaves than decision trees.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../075/5462075.xml">
Pruning (algorithm)</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<link xlink:type="simple" xlink:href="../855/576855.xml">
Binary decision diagram</link></artifact>
</structure>
</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link>
CART</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../797/1966797.xml">
ID3 algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../814/1966814.xml">
C4.5 algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../880/1363880.xml">
Random forest</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<link xlink:type="simple" xlink:href="../179/9903179.xml">
Decision stump</link></woody_plant>
</vascular_plant>
</tree>
</plant>
</entry>
</list>
</p>

</sec>
<sec>
<st>
External sources</st>
<p>

<list>
<entry level="1" type="bullet">

 V.Berikov, A.Litvinenko, "Methods for statistical data analysis with decision trees". Novosibirsk, Sobolev Institute of Mathematics, 2003. <weblink xlink:type="simple" xlink:href="http://www.math.nsc.ru/AP/datamine/eng/decisiontree.htm">
Methods for statistical data analysis with decision trees</weblink></entry>
<entry level="1" type="bullet">

[BFOS84] L. Breiman, J. Friedman, R. A. Olshen and C. J. Stone, "Classification and regression trees". Wadsworth, 1984.</entry>
<entry level="1" type="bullet">

[1] T. Menzies, Y. Hu, <it>Data Mining For Very Busy People</it>.  IEEE Computer, October 2003, pgs. 18-25.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mindtools.com/pages/article/newTED_04.htm">
Decision Tree Analysis</weblink> mindtools.com</entry>
<entry level="1" type="bullet">

 J.W. Comley and <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld">
D.L. Dowe</weblink>, "<weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2005">
Minimum Message Length, MDL and Generalised Bayesian Networks with Asymmetric Languages</weblink>", <weblink xlink:type="simple" xlink:href="http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=10478&amp;mode=toc">
chapter 11</weblink> (pp<weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p265.jpg">
265</weblink>-<weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p294.jpg">
294</weblink>) in P. Grunwald, M.A. Pitt and I.J. Myung (eds)., <weblink xlink:type="simple" xlink:href="http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&amp;ttype=2&amp;tid=10478">
Advances in Minimum Description Length: Theory and Applications</weblink>, M.I.T. Press, April 2005, ISBN 0-262-07262-9.  (This paper puts decision trees in internal nodes of <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian network</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s using <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/MML.html">
Minimum Message Length</weblink> (<link xlink:type="simple" xlink:href="../210/302210.xml">
MML</link>). An earlier version is <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2003">
Comley and Dowe (2003)</weblink>, <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/Publications/2003/Comley+Dowe03_HICS2003_GeneralBayesianNetworksAsymmetricLanguages.pdf">
.pdf</weblink>.)</entry>
<entry level="1" type="bullet">

P.J. Tan and <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld">
D.L. Dowe</weblink> (2003), <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#TanDowe2003">
MML Inference of Decision Graphs with Multi-Way Joins and Dynamic Attributes</weblink>, Proc. 16th Australian Joint Conference on Artificial Intelligence (AI'03), Perth, Australia, 3-5 Dec. 2003, Published in Lecture Notes in Artificial Intelligence (LNAI) 2903, Springer-Verlag, <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/Publications/2003/Tan+Dowe2003_MMLDecisionGraphs.pdf">
pp269-281</weblink>.</entry>
<entry level="1" type="bullet">

P.J. Tan and <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld">
D.L. Dowe</weblink> (2004), <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#TanDowe2004">
MML Inference of Oblique Decision Trees</weblink>, Lecture Notes in Artificial Intelligence (LNAI) 3339, Springer-Verlag, <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/Publications/2004/Tan+DoweAI2004.pdf">
pp1082-1088</weblink>.  (This paper uses <link xlink:type="simple" xlink:href="../210/302210.xml">
Minimum Message Length</link> and actually incorporates probabilistic <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link>s in the leaves of the decision trees.)</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://decisiontrees.net">
decisiontrees.net Interactive Tutorial</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html">
Building Decision Trees in Python</weblink> From O'Reilly.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.aaai.org/aitopics/html/trees.html">
Decision Trees page at aaai.org</weblink>, a page with commented links.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.dezide.com">
Bayesian Networks Applied in Real World Troubleshooting Scenario</weblink> Dezide</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://blogs.isixsigma.com/archive/what_makes_a_satisfied_customer.html">
Practical Application of Decision Trees</weblink> by Robin Barnwell</entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</article>
