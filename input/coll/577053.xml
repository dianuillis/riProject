<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:19:36[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Association rule learning</title>
<id>577053</id>
<revision>
<id>244088139</id>
<timestamp>2008-10-09T08:24:00Z</timestamp>
<contributor>
<username>Timwi</username>
<id>13051</id>
</contributor>
</revision>
<categories>
<category>Data mining</category>
<category>Data management</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link>, <b>association rule learning</b> is a popular and
well researched method for discovering interesting relations between variables
in large databases.  Piatetsky-Shapiro 
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> 
describes analyzing and presenting
strong rules discovered in databases using different measures of
interestingness. Based on the concept of strong rules, Agrawal et al.  
R. Agrawal; T. Imielinski; A.  Swami: <it>Mining Association Rules
Between Sets of Items in Large Databases", SIGMOD Conference 1993:
207-216 
introduced association rules for discovering regularities between
products in large scale transaction data recorded by <link xlink:type="simple" xlink:href="../633/220633.xml">
point-of-sale</link> (POS)
systems in supermarkets. For example, the rule
<math>\{onions, potatoes\} \Rightarrow \{beef\}</math>
found in the sales data of a supermarket would indicate that if a customer buys
onions and potatoes together, he or she is likely to also buy beef.  Such
information can be used as the basis for decisions about marketing activities
such as, e.g., promotional <link xlink:type="simple" xlink:href="../887/239887.xml">
pricing</link> or <link>
product placements</link>.  In addition to the
above example from <link xlink:type="simple" xlink:href="../086/15270086.xml">
market basket analysis</link> association rules are employed
today in many application areas including <link xlink:type="simple" xlink:href="../887/797887.xml">
Web usage mining</link>, <link xlink:type="simple" xlink:href="../184/2010184.xml">
intrusion detection</link> and <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>.</it>
<sec>
<st>
 Definition </st>
<p>

Following the original definition by Agrawal et al
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> the problem of association rule mining is
defined as: 
Let <math>I=\{i_1, i_2,\ldots,i_n\}</math> be a set of
<math>n</math> binary attributes called <it>items</it>.  Let <math>D = \{t_1, t_2,
\ldots, t_m\}</math> be a set of transactions called the <it>database</it>.  Each
transaction in <math>D</math> has a unique transaction ID and contains a
subset of the items in <math>I</math>.  A <it>rule</it> is defined as an implication
of the form <math>X \Rightarrow Y</math> where <math>X, Y \subseteq I</math>
and <math>X \cap Y = \emptyset</math>.  The sets of items (for short
<it>itemsets</it>) <math>X</math> and <math>Y</math> are called <it>antecedent</it>
(left-hand-side or LHS) and <it>consequent</it> (right-hand-side or RHS) of the
rule.</p>
<p>

<table style="float: right;" class="wikitable">
<caption>
Example data base with 4 items and 5 transactions</caption>
<row>
<header>
transaction ID</header>
<header>
milk</header>
<header>
bread</header>
<header>
butter</header>
<header>
beer</header>
</row>
<row>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
</row>
<row>
<col>
2</col>
<col>
0</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
</row>
<row>
<col>
3</col>
<col>
0</col>
<col>
0</col>
<col>
0</col>
<col>
1</col>
</row>
<row>
<col>
4</col>
<col>
1</col>
<col>
1</col>
<col>
1</col>
<col>
0</col>
</row>
<row>
<col>
5</col>
<col>
0</col>
<col>
1</col>
<col>
0</col>
<col>
0</col>
</row>
</table>
</p>
<p>

To illustrate the concepts, we use a small example from the supermarket
domain. The set of items is <math>I= \{\mathrm{milk, bread, butter, beer}\}</math>
and a small database containing the items (1 codes presence and 0 absence
of an item in a transaction) is shown in
the table to the right.  An example rule for the supermarket
could be <math>\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}</math>
meaning that if milk and bread is bought, customers also buy butter.</p>
<p>

Note: this example is extremely small.  In practical applications, a rule needs a support of several hundred itemsets before it can be considered statistically significant, and datasets often contain thousands or millions of itemsets.</p>
<p>

To select interesting rules from the set of all possible rules,
constraints on various measures of significance and interest can be
used.  The best-known constraints are minimum thresholds on support and
confidence.  The <it>support</it> <math>\mathrm{supp}(X)</math> of an itemset 
<math>X</math> is
defined as the proportion of transactions in the data set which contain
the itemset.  In the example database,
the itemset <math>\{\mathrm{milk, bread}\}</math> has a support of 
<math>2/5=0.4</math> since
it occurs in 40% of all transactions (2 out of 5 transactions).</p>
<p>

The <it>confidence</it> of a rule is defined <math>\mathrm{conf}(X\Rightarrow
Y) = \mathrm{supp}(X \cup Y) / \mathrm{supp}(X)</math>.  For example, the rule
<math>\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}</math> has a
confidence of <math>0.2/0.4=0.5</math> in the database, which means that for 
50% of the
transactions containing milk and bread the rule is correct.
Confidence can be interpreted as an estimate of the probability
<math>P(Y|X)</math>, the probability of finding the RHS of the rule in transactions
under the condition that these transactions also contain the LHS
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.</p>
<p>

The <it>lift</it> of a rule is defined as <math> \mathrm{lift}(X\Rightarrow
Y) = \frac{ \mathrm{supp}(X \cup Y)}{ \mathrm{supp}(Y) * \mathrm{supp}(X) } </math> or the ratio of the observed confidence to that expected by chance. The rule
<math>\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}</math> has a
lift of <math>\frac{0.2}{0.4*0.4} = 1.25 </math>.</p>
<p>

The <it>conviction</it> of a rule is defined as <math> \mathrm{conv}(X\Rightarrow Y) =\frac{ 1 - \mathrm{supp}(Y) }{ 1 - \mathrm{conf}(X\Rightarrow Y)}</math>. The rule
<math>\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}</math> has a
conviction of <math>\frac{1 - 0.4}{1 - .5} = 1.2 </math>, and be interpreted as the ratio of the expected frequency that X occurs without Y if they were independent to the observed frequency.</p>
<p>

Association rules are required to satisfy a user-specified minimum support
and a user-specified minimum confidence at the same time. 
To achieve this, association rule
generation is a two-step process. First, minimum support is applied
to find all <it>frequent itemsets</it> in a database. 
In a second step, these frequent
itemsets and the minimum confidence constraint are used to form rules.
While the second step is straight forward, the first step needs more attention.</p>

<p>

Finding all frequent itemsets in a database is difficult since it involves
searching all possible itemsets (item combinations).  The set of possible
itemsets is the <link xlink:type="simple" xlink:href="../799/23799.xml">
power set</link> over <math>I</math> and has size
<math>2^n-1</math> (excluding the empty set which is not a valid itemset). 
Although the size of the powerset grows exponentially in the
number of items <math>n</math> in <math>I</math>, efficient search is possible
using the <it>downward-closure property</it> of support <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>(also called <it>anti-monotonicity</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>) which guarantees
that for a frequent itemset also all its subsets are frequent and thus for an
infrequent itemset, all its supersets must be infrequent.  Exploiting this
property, efficient algorithms (e.g., Apriori <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>
and Eclat <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>) can find all
frequent itemsets.</p>

</sec>
<sec>
<st>
 Alternative Measures of Interestingness </st>

<p>

Next to confidence also other measures of interestingness for
rules were proposed. Some popular measures are:</p>
<p>

<list>
<entry level="1" type="bullet">

  All-confidence <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Collective strength <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  Conviction <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  Leverage <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  Lift (originally called interest) <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref></entry>
</list>
</p>
<p>

A definition of these measures can be found <weblink xlink:type="simple" xlink:href="http://michael.hahsler.net/research/association_rules/measures.html">
here</weblink>. Several more measures are presented and compared by Tan et al.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref></p>

</sec>
<sec>
<st>
 Algorithms </st>

<p>

Many algorithms for generating association rules were presented over time. Some well known algorithms are Apriori, Eclat and FP-Growth.</p>

<ss1>
<st>
 Apriori algorithm </st>

<p>

<indent level="1">

<it>Main article: <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../194/608194.xml">
Apriori algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</it>
</indent>

Apriori<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> is the best-known algorithm to mine association rules. It uses a breadth-first search strategy to counting the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.</p>

</ss1>
<ss1>
<st>
 Eclat algorithm </st>

<p>

Eclat<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref> is a depth-first search algorithm using set intersection.</p>

</ss1>
<ss1>
<st>
 FP-Growth algorithm </st>

<p>

FP-growth (frequent pattern growth)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref> uses an extended prefix-tree (FP-tree) structure to store the database in a compressed form. FP-growth adopts a divide-and-conquer approach to decompose both the mining tasks and the databases. It uses a pattern fragment growth method to avoid the costly process of candidate generation and testing used by Apriori.</p>

</ss1>
<ss1>
<st>
 One-attribute-rule </st>

<p>

The <b>one-attribute-rule</b>, or OneR, is an <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> for finding <link xlink:type="simple" xlink:href="../053/577053.xml">
association rules</link>. According to Ross, very simple association rules, involving just one attribute in the condition part, often work well in practice with real-world data.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref>. The idea of the OneR (one-attribute-rule) algorithm is to find the one attribute to use to classify a novel datapoint that makes fewest prediction errors.</p>
<p>

For example, to classify a <link xlink:type="simple" xlink:href="../345/13673345.xml">
car</link> you haven't seen before, you might apply the following rule: <it>If Fast Then Sportscar</it>, as opposed to a rule with multiple attributes in the condition: <it>If Fast And Softtop And Red Then Sportscar</it>.</p>
<p>

The algorithm is as follows:</p>

<p>

For each attribute A:
For each value V of that attribute, create a rule:
1. count how often each class appears
2. find the most frequent class, c
3. make a rule "if A=V then C=c"
Calculate the error rate of this rule
Pick the attribute whose rules produce the lowest error rate</p>


</ss1>
</sec>
<sec>
<st>
 Lore </st>

<p>

A famous story about association rule mining is the "beer and diaper" story.  A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer.  This anecdote became popular as an example of how unexpected association rules might be found from everyday data.</p>


</sec>
<sec>
<st>
 Other types of Association Mining </st>

<p>

<b>Contrast set learning</b> is a form of associative learning.  <b>Contrast set learners</b> use rules that differ meaningfully in their distribution across subsets<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref>.</p>
<p>

<b>Weighted class learning</b> is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.</p>
<p>

<b><link xlink:type="simple" xlink:href="../355/5149355.xml">
K-optimal pattern discovery</link></b> provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.</p>
<p>

<b>Mining frequent sequences</b> uses support to find sequences in temporal data<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref>.</p>

</sec>
<sec>
<st>
External links</st>

<ss1>
<st>
Bibliographies</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://michael.hahsler.net/research/bib/association_rules/">
Annotated Bibliography on Association Rules</weblink> by M. Hahsler</entry>
</list>
</p>

</ss1>
<ss1>
<st>
Implementations</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://cran.r-project.org/package=arules">
arules</weblink>, a package for mining association rules and frequent itemsets with <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../707/376707.xml">
R</link></programming_language>
.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.borgelt.net/fpm.html">
C. Boergelt's implementation of Apriori and Eclat</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://fimi.cs.helsinki.fi/">
Frequent Itemset Mining Implementations Repository (FIMI)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://adrem.ua.ac.be/~goethals/software/">
Frequent pattern mining implementations from Bart Goethals</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.waikato.ac.nz/ml/weka/">
Weka</weblink>, a collection of machine learning algorithms for data mining tasks written in <message wordnetid="106598915" confidence="0.8">
<request wordnetid="106513366" confidence="0.8">
<link xlink:type="simple" xlink:href="../881/15881.xml">
Java</link></request>
</message>
. </entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.rpi.edu/~zaki/software/">
 Data Mining Software by Mohammed J. Zaki</weblink></entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../457/3157457.xml">
production system</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
Piatetsky-Shapiro,
G. (1991), Discovery, analysis, and presentation of strong rules, in G.
Piatetsky-Shapiro &amp; W. J. Frawley, eds, ‘Knowledge Discovery in Databases’,
AAAI/MIT Press, Cambridge, MA.  </entry>
<entry id="2">
R. Agrawal; T. Imielinski; A. Swami: <it>Mining Association Rules Between Sets of Items in Large Databases", SIGMOD Conference 1993: 207-216</it></entry>
<entry id="3">
Jochen Hipp, Ulrich Güntzer, and Gholamreza Nakhaeizadeh. Algorithms for association rule mining - A general survey and comparison. SIGKDD Explorations, 2(2):1-58, 2000.</entry>
<entry id="4">
Jian Pei, Jiawei Han, and Laks V.S. Lakshmanan. Mining frequent itemsets with convertible constraints. In Proceedings of the 17th International Conference on Data Engineering, April 2-6, 2001, Heidelberg, Germany, pages 433-442, 2001.</entry>
<entry id="5">
Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules in large databases. Proceedings of the 20th International Conference on Very Large Data Bases, VLDB, pages 487-499, Santiago, Chile, September 1994.</entry>
<entry id="6">
Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering 12(3):372-390, May/June 2000.</entry>
<entry id="7">
Edward R. Omiecinski. Alternative interest measures for mining associations in databases. IEEE Transactions on Knowledge and Data Engineering, 15(1):57-69, Jan/Feb 2003.</entry>
<entry id="8">
 C. C. Aggarwal and P. S. Yu. A new framework for itemset generation. In PODS 98, Symposium on Principles of Database Systems, pages 18-24, Seattle, WA, USA, 1998.</entry>
<entry id="9">
 Sergey Brin, Rajeev Motwani, Jeffrey D. Ullman, and Shalom Tsur. Dynamic itemset counting and implication rules for market basket data. In SIGMOD 1997, Proceedings ACM SIGMOD International Conference on Management of Data, pages 255-264, Tucson, Arizona, USA, May 1997.</entry>
<entry id="10">
Piatetsky-Shapiro, G., Discovery, analysis, and presentation of strong rules. Knowledge Discovery in Databases, 1991: p. 229-248.</entry>
<entry id="11">
 S. Brin, R. Motwani, J. D. Ullman, and S. Tsur. Dynamic itemset counting and implication rules for market basket data. In Proc. of the ACM SIGMOD Int'l Conf. on Management of Data (ACM SIGMOD '97), pages 265-276, 1997.</entry>
<entry id="12">
Pang-Ning Tan, Vipin Kumar, and Jaideep Srivastava. Selecting the right objective measure for association analysis. Information Systems, 29(4):293-313, 2004.</entry>
<entry id="13">
Jiawei Han, Jian Pei, Yiwen Yin, and Runying Mao. Mining frequent patterns without candidate generation. Data Mining and Knowledge Discovery 8:53-87, 2004.</entry>
<entry id="14">
Ross, Peter.&#32;"<weblink xlink:type="simple" xlink:href="http://www.dcs.napier.ac.uk/~peter/vldb/dm/node8.html">
OneR: the simplest method</weblink>".</entry>
<entry id="15">
T. Menzies, Y. Hu, "Data Mining For Very Busy People."  <it>IEEE Computer</it>, October 2003, pgs. 18-25.</entry>
<entry id="16">
M. J. Zaki. (2001). SPADE: An Efficient Algorithm for Mining Frequent Sequences. Machine Learning Journal, 42, 31–60.</entry>
</reflist>
</p>


</sec>
</bdy>
</article>
