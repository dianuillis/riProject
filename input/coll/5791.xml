<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:23:23[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Conditional probability</title>
<id>5791</id>
<revision>
<id>243575419</id>
<timestamp>2008-10-07T02:43:29Z</timestamp>
<contributor>
<username>Ciphers</username>
<id>7643502</id>
</contributor>
</revision>
<categories>
<category>All articles with unsourced statements</category>
<category>Probability theory</category>
<category>Logical fallacies</category>
<category>Articles with unsourced statements since December 2007</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-content" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_content.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>This article or section is missing  or needs .</b>
Using helps guard against copyright violations and factual inaccuracies. <it>(December 2007)''</it></col>
</row>
</table>

<p>

<b>Conditional probability</b> is the <link xlink:type="simple" xlink:href="../934/22934.xml">
probability</link> of some <link xlink:type="simple" xlink:href="../961/22961.xml">
event</link> <it>A</it>, given the occurrence of some other event <it>B</it>. Conditional probability is written <it>P</it>(<it>A</it>|<it>B</it>), and is read "the probability of <it>A</it>, given <it>B</it>".</p>
<p>

<link xlink:type="simple" xlink:href="../637/879637.xml">
Joint probability</link> is the probability of two events in conjunction. That is, it is the probability of both events together. The joint probability of  <it>A</it> and <it>B</it> is written <math>\scriptstyle P(A \cap B)</math> or <math>\scriptstyle P(A, B).</math></p>
<p>

<b>Marginal probability</b> is then the unconditional probability <it>P</it>(<it>A</it>) of the event <it>A</it>; that is, the probability of <it>A</it>, regardless of whether event <it>B</it> did or did not occur. If <it>B</it> can be thought of as the event of a <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> <it>X</it> having a given outcome, the marginal probability of <it>A</it> can be obtained by <link xlink:type="simple" xlink:href="../160/246160.xml">
summing</link> (or <link xlink:type="simple" xlink:href="../532/15532.xml">
integrating</link>, more generally) the joint probabilities over all outcomes for <it>X</it>. For example, if there are two possible outcomes for <it>X</it> with corresponding events <it>B</it> and <it>B'</it>, this means that <math>\scriptstyle P(A) = P(A \cap B) + P(A \cap B^')</math>. This is called <b>marginalization</b>.</p>
<p>

In these definitions, note that there need not be a <link xlink:type="simple" xlink:href="../196/37196.xml">
causal</link> or <link xlink:type="simple" xlink:href="../844/253844.xml">
temporal</link> relation between <it>A</it> and <it>B</it>. <it>A</it> may precede <it>B</it> or vice versa or they may happen at the same time. <it>A</it> may <link xlink:type="simple" xlink:href="../129/274129.xml">
cause</link> <it>B</it> or vice versa or they may have no causal relation at all. Notice, however, that causal and temporal relations are informal notions, not belonging to the probabilistic framework. They may apply in some examples, depending on the interpretation given to events.</p>
<p>

<b>Conditioning</b> of probabilities, i.e. updating them to take account of (possibly new) information, may be achieved through <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
.
In such conditioning, the probability of A given only initial information I, <it>P</it>(<it>A</it>|<it>I</it>), is known as the <link xlink:type="simple" xlink:href="../877/472877.xml">
prior probability</link>.  The updated conditional probability of A, given I and the outcome of the event B, is known as the <link xlink:type="simple" xlink:href="../672/357672.xml">
posterior probability</link>, <it>P</it>(<it>A</it>|<it>B</it>,<it>I</it>).</p>

<sec>
<st>
Introduction</st>

<p>

Consider the simple scenario of rolling two fair six-sided <link xlink:type="simple" xlink:href="../244/8244.xml">
dice</link>, labelled die 1 and die 2. Define the following three <link xlink:type="simple" xlink:href="../961/22961.xml">
events</link>:</p>
<p>

<indent level="1">

<b>A</b>: Die 1 lands on 3.
</indent>
:<b>B</b>: Die 2 lands on 1.
<indent level="1">

<b>C</b>: The dice sum to 8.
</indent>

The <link xlink:type="simple" xlink:href="../877/472877.xml">
prior probability</link> of each event describes how likely the outcome is before the dice are rolled, without any knowledge of the roll's outcome. For example, die 1 is equally likely to fall on each of its 6 sides, so <math>P(A) = 1/6</math>. Similarly <math>P(B) = 1/6</math>. Likewise, of the 6 &amp;times; 6 = 36 possible ways that a pair of dice can land, just 5 result in a sum of 8 (namely 2 and 6, 3 and 5, 4 and 4, 5 and 3, and 6 and 2), so <math>P(C) = 5/36</math>.</p>
<p>

Some of these events can both occur at the same time; for example events A and C can happen at the same time, in the case where die 1 lands on 3 and die 2 lands on 5. This is the only one of the 36 outcomes where both A and C occur, so its probability is 1/36. The probability of both A and C occurring is called the <it><link xlink:type="simple" xlink:href="../637/879637.xml">
joint probability</link></it> of A and C and is written <math>P(A \cap C)</math>, so <math>P(A \cap C) = 1/36</math>. On the other hand, if die 2 lands on 1, the dice cannot sum to 8, so <math>P(B \cap C) = 0</math>.</p>
<p>

Now suppose we roll the dice and cover up die 2, so we can only see die 1, and observe that die 1 landed on 3. Given this partial information, the probability that the dice sum to 8 is no longer 5/36; instead it is 1/6, since die 2 must land on 5 to achieve this result. This is called the <it>conditional</it> probability, because it's the probability of C under the condition that is A is observed, and is written <math>P(C \mid A)</math>, which is read "the probability of C given A." Similarly, <math>P(C \mid B) = 0</math>, since if we observe die 2 landed on 1, we already know the dice can't sum to 8, regardless of what the other die landed on.</p>
<p>

On the other hand, if we roll the dice and cover up die 2, and observe die 1, this has no impact on the probability of event B, which only depends on die 2. We say events A and B are <it><link xlink:type="simple" xlink:href="../593/27593.xml">
statistically independent</link></it> or just <it>independent</it> and in this case
<indent level="1">

<math>P(B \mid A) = P(B) \, .</math>
</indent>
In other words, the probability of B occurring after observing that die 1 landed on 3 is the same as before we observed die 1.</p>
<p>

Intersection events and conditional events are related by the formula:</p>
<p>

<indent level="1">

<math>P(C \mid A) = \frac{P(A \cap C)}{P(A)} .</math>
</indent>

In this example, we have:</p>
<p>

<indent level="1">

<math>1/6 = \frac{1/36}{1/6}</math>
</indent>

As noted above, <math>P(B \mid A) = P(B)</math>, so by this formula:</p>
<p>

<indent level="1">

<math>P(B) = P(B \mid A) = \frac{P(A \cap B)}{P(A)} .</math>
</indent>

On multiplying across by P(A),</p>
<p>

<indent level="1">

<math>P(A)P(B) = P(A \cap B) .</math>
</indent>

In other words, if two events are independent, their joint probability is the product of the prior probabilities of each event occurring by itself.</p>

</sec>
<sec>
<st>
Definition</st>
<p>

Given a <link xlink:type="simple" xlink:href="../325/43325.xml">
probability space</link> <math>\scriptstyle (\Omega, F, P)</math> and two <link xlink:type="simple" xlink:href="../961/22961.xml">
events</link> <math>\scriptstyle A, B\,\in\, F</math> with <it>P</it>(<it>B</it>)&nbsp;&amp;gt;&nbsp;0, the conditional probability of <it>A</it> given <it>B</it> is defined by</p>
<p>

<indent level="1">

<math>P(A \mid B) = \frac{P(A \cap B)}{P(B)}.\,</math>
</indent>

If <math>P(B)=0</math> then <math>P(A \mid B)</math> is <link xlink:type="simple" xlink:href="../555/225555.xml">
undefined</link>, or at any rate irrelevant; but see <it><link xlink:type="simple" xlink:href="../326/18963326.xml">
Regular conditional probability</link></it>.</p>

</sec>
<sec>
<st>
Statistical independence</st>
<p>

Two random <link xlink:type="simple" xlink:href="../961/22961.xml">
 events</link> <it>A</it> and <it>B</it> are <link xlink:type="simple" xlink:href="../593/27593.xml">
statistically independent</link> if and only if</p>
<p>

<indent level="1">

<math>P(A \cap B) \ = \ P(A)  P(B)</math>
</indent>

Thus, if <it>A</it> and <it>B</it> are independent, then their joint probability can be expressed as a simple product of their individual probabilities.</p>
<p>

Equivalently, for two independent events <it>A</it> and <it>B</it> with non-zero probabilities,</p>
<p>

<indent level="1">

<math>P(A|B) \ = \ P(A)</math>
</indent>

and</p>
<p>

<indent level="1">

<math>P(B|A) \ = \ P(B).</math>
</indent>

In other words, if <it>A</it> and <it>B</it> are independent, then the conditional probability of <it>A</it>, given <it>B</it> is simply the individual probability of <it>A</it> alone;  likewise, the probability of <it>B</it> given <it>A</it> is simply the probability of <it>B</it> alone.</p>

</sec>
<sec>
<st>
Mutual exclusivity</st>
<p>

Two events <it>A</it> and <it>B</it> are <link xlink:type="simple" xlink:href="../648/312648.xml">
mutually exclusive</link> if and only if
<math>\scriptstyle A \cap B \,=\, \varnothing</math>. Then <math>\scriptstyle P(A \cap B)\, =\, 0</math>.</p>
<p>

Therefore, if <it>P</it>(<it>B</it>) &amp;gt; 0 then <math>\scriptstyle P(A\mid B)</math> is defined and equal to 0.</p>

</sec>
<sec>
<st>
Other considerations</st>
<p>

<list>
<entry level="1" type="bullet">

 If <it>B</it> is an event and <it>P</it>(<it>B</it>) &amp;gt; 0, then the function <it>Q</it> defined by <it>Q</it>(<it>A</it>) = <it>P</it>(<it>A</it>|<it>B</it>) for all events <it>A</it> is a <link>
probability measure</link>.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Many models in <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link> can calculate conditional probabilities, including <link xlink:type="simple" xlink:href="../602/232602.xml">
decision trees</link> and <link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian networks</link>.</entry>
</list>
</p>

</sec>
<sec>
<st>
The conditional probability fallacy</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../964/17118964.xml">
Confusion of the inverse</link></it>
</indent>

The conditional probability <link xlink:type="simple" xlink:href="../986/53986.xml">
fallacy</link> is the assumption that <it>P</it>(<it>A</it>|<it>B</it>) is approximately equal to <it>P</it>(<it>B</it>|<it>A</it>). The mathematician <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../099/595099.xml">
John Allen Paulos</link></scientist>
 discusses this in his book <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../663/4760663.xml">
Innumeracy</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
</it> (p. 63 et seq.), where he points out that it is a mistake often made even by doctors, lawyers, and other highly educated non-<link xlink:type="simple" xlink:href="../661/48661.xml">
statistician</link>s. It can be overcome by describing the data in actual numbers rather than probabilities.</p>
<p>

The relation between  <it>P</it>(<it>A</it>|<it>B</it>) and <it>P</it>(<it>B</it>|<it>A</it>) is given by <link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes Theorem</link>:</p>
<p>

<indent level="1">

<math>P(B {\mid} A)= P(A {\mid} B) \, \frac{P(B)}{P(A)}.</math>
</indent>

In other words, one can only assume that <it>P</it>(<it>A</it>|<it>B</it>) is approximately equal to <it>P</it>(<it>B</it>|<it>A</it>) if the prior probabilities <it>P</it>(<it>A</it>) and <it>P</it>(<it>B</it>) are also approximately equal.</p>

<ss1>
<st>
An example</st>
<p>

In the following constructed but realistic situation, the difference between <it>P</it>(<it>A</it>|<it>B</it>) and <it>P</it>(<it>B</it>|<it>A</it>) may be surprising, but is at the same time obvious.</p>
<p>

In order to identify individuals having a serious disease in an early curable form, one may consider screening a large group of people. While the benefits are obvious, an argument against such screenings is the disturbance caused by false positive screening results: If a person not having the disease is incorrectly found to have it by the initial test, they will most likely be quite distressed until a more careful test shows that they do not have the disease. Even after being told they are well, their lives may be affected negatively.</p>
<p>

The magnitude of this problem is best understood in terms of conditional probabilities.</p>
<p>

Suppose 1% of the group suffer from the disease, and the rest are well. Choosing an individual at random,</p>
<p>

<indent level="1">

<math>P(\text{disease})=1%=0.01\text{ and }P(\text{well})=99%=0.99.</math>
</indent>

Suppose that when the screening test is applied to a person not having the disease, there is a 1% chance of getting a false positive result, i.e.</p>
<p>

<indent level="1">

<math>P(\text{positive}|\text{well})=1%,\text{ and } (\text{negative}|\text{well})=99%.</math>
</indent>

Finally, suppose that when the test is applied to a person having the disease, there is a 1% chance of a false negative result, i.e.</p>
<p>

<indent level="1">

<math>P(\text{negative}|\text{disease})=1%\text{ and }P(\text{positive}|\text{disease})=99%.</math>
</indent>

Now, one may calculate the following:</p>
<p>

The fraction of individuals in the whole group who are well and test negative:</p>
<p>

<indent level="1">

<math>P(\text{well}\cap\text{negative})=P(\text{well})\times P(\text{negative}|\text{well})=99%\times99%=98.01%.</math>
</indent>

The fraction of individuals in the whole group who are ill and test positive:</p>
<p>

<indent level="1">

<math>P(\text{disease}\cap\text{positive})=P(\text{disease})\times P(\text{positive}|\text{disease})=1%\times99%=0.99%.</math>
</indent>

The fraction of individuals in the whole group who have false positive results:</p>
<p>

<indent level="1">

<math>P(\text{well}\cap\text{positive})=P(\text{well})\times P(\text{positive}|\text{well})=99%\times1%=0.99%.</math>
</indent>

The fraction of individuals in the whole group who have false negative results:</p>
<p>

<indent level="1">

<math>P(\text{disease}\cap\text{negative})=P(\text{disease})\times P(\text{negative}|\text{disease})=1%\times1%=0.01%.</math>
</indent>

Furthermore, the fraction of individuals in the whole group who test positive:</p>
<p>

<indent level="1">

<math>
\begin{align}
P(\text{positive}) &amp; {} =P(\text{well }\cap\text{ positive}) + P(\text{disease }\cap\text{ positive}) \\
&amp; {} = 0.99%+0.99%=1.98%.
\end{align}
</math>
</indent>

Finally, the probability that an individual actually has the disease, given that the test result is positive:</p>
<p>

<indent level="1">

<math>P(\text{disease}|\text{positive})=\frac{P(\text{disease }\cap\text{ positive})} {P(\text{positive})} = \frac{0.99%}{1.98%}= 50%.</math>
</indent>

In this example, it should be easy to relate to the difference between the conditional probabilities P(positive|disease) (which is 99%) and P(disease|positive) (which is 50%): the first is the probability that an individual who has the disease tests positive; the second is the probability that an individual who tests positive actually has the disease. With the numbers chosen here, the last result is likely to be deemed unacceptable: half the people testing positive are actually false positives.</p>

</ss1>
</sec>
<sec>
<st>
 Conditioning on a random variable </st>
<p>

There is also a concept of the conditional probability of an event given a discrete <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>.  Such a conditional probability is a random variable in its own right.</p>
<p>

Suppose <it>X</it> is a random variable that can be equal either to 0 or to 1.  As above, one may speak of the conditional probability of any event <it>A</it> given the event <it>X</it>&nbsp;=&nbsp;0, and also of the conditional probability of <it>A</it> given the event <it>X</it>&nbsp;=&nbsp;1.  The former is denoted <it>P</it>(<it>A</it>|<it>X</it>&nbsp;=&nbsp;0) and the latter <it>P</it>(<it>A</it>|<it>X</it>&nbsp;=&nbsp;1).  Now define a new random variable <it>Y</it>, whose value is <it>P</it>(<it>A</it>|<it>X</it>&nbsp;=&nbsp;0) if <it>X</it>&nbsp;=&nbsp;0 and <it>P</it>(<it>A</it>|<it>X</it>&nbsp;=&nbsp;1) if <it>X</it>&nbsp;=&nbsp;1.   That is</p>
<p>

<indent level="1">

<math>
Y = \begin{cases}
P(A\mid X=0) &amp; \text{if }X=0; \\
P(A\mid X=1) &amp; \text{if }X=1.
\end{cases}
</math>
</indent>

This new random variable <it>Y</it> is said to be the conditional probability of the event <it>A</it> given the discrete random variable <it>X</it>:</p>
<p>

<indent level="1">

<math> Y = P(A\mid X) \,</math>
</indent>

According to the "<link xlink:type="simple" xlink:href="../383/312383.xml">
law of total probability</link>", the <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> of <it>Y</it> is just the marginal (or "unconditional") probability of <it>A</it>.</p>
<p>

More generally still, it is possible to speak of the conditional probability of an event given a sigma-algebra.  See <link xlink:type="simple" xlink:href="../099/435099.xml">
conditional expectation</link>.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../968/44968.xml">
Likelihood function</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../672/357672.xml">
Posterior probability</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../542/23542.xml">
Probability theory</link></entry>
<entry level="1" type="bullet">

<statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<contradiction wordnetid="107206887" confidence="0.8">
<paradox wordnetid="106724559" confidence="0.8">
<falsehood wordnetid="106756407" confidence="0.8">
<link xlink:type="simple" xlink:href="../198/6026198.xml">
Monty Hall problem</link></falsehood>
</paradox>
</contradiction>
</message>
</statement>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../148/50148.xml">
Prosecutor's fallacy</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../099/435099.xml">
Conditional expectation</link></entry>
<entry level="1" type="bullet">

<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<category wordnetid="105838765" confidence="0.8">
<type wordnetid="105840188" confidence="0.8">
<kind wordnetid="105839024" confidence="0.8">
<link xlink:type="simple" xlink:href="../458/504458.xml">
Conditional probability distribution</link></kind>
</type>
</category>
</concept>
</idea>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' Theorem</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>



<list>
<entry level="1" type="bullet">

  <cite id="Reference-Mathworld-Conditional Probability"><physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<encyclopedist wordnetid="110055566" confidence="0.8">
<compiler wordnetid="109946957" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../189/836189.xml">
Eric W. Weisstein</link></scholar>
</mathematician>
</writer>
</scientist>
</causal_agent>
</alumnus>
</compiler>
</encyclopedist>
</intellectual>
</person>
</communicator>
</physical_entity>
, <it><weblink xlink:type="simple" xlink:href="http://mathworld.wolfram.com/ConditionalProbability.html">
Conditional Probability</weblink></it> at <computer wordnetid="103082979" confidence="0.8">
<work wordnetid="104599396" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<reference_book wordnetid="106417598" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<encyclopedia wordnetid="106427387" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<web_site wordnetid="106359193" confidence="0.8">
<link xlink:type="simple" xlink:href="../235/374235.xml">
MathWorld</link></web_site>
</device>
</book>
</instrumentality>
</artifact>
</product>
</encyclopedia>
</publication>
</reference_book>
</machine>
</creation>
</work>
</computer>
.</cite></entry>
</list>
</p>



</sec>
</bdy>
</article>
