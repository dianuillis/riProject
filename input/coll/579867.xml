<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:21:45[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Dimension reduction</title>
<id>579867</id>
<revision>
<id>226938294</id>
<timestamp>2008-07-21T05:26:05Z</timestamp>
<contributor>
<username>Movado73</username>
<id>7285945</id>
</contributor>
</revision>
<categories>
<category>Multivariate statistics</category>
<category>Dimension</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, <b>dimension reduction</b> is the process of reducing the number of random variables under consideration, and can be divided into <link xlink:type="simple" xlink:href="../950/1179950.xml">
feature selection</link> and <link xlink:type="simple" xlink:href="../190/242190.xml">
feature extraction</link>. In physics, dimension reduction is a widely discussed phenomenon, whereby a physical system exists in three dimensions, but its properties behave like those of a lower-dimensional system.  It has been experimentally realised at the quantum critical point in an insulating magnet made of the pigment <link xlink:type="simple" xlink:href="../165/5399165.xml">
Han Purple</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.
<sec>
<st>
Feature selection</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../950/1179950.xml">
Feature selection</link></it>
</indent>

<link xlink:type="simple" xlink:href="../950/1179950.xml">
Feature selection</link> approaches try to find a subset of the original variables (also called features or attributes). Two strategies are <it>filter</it> (e.g. <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../412/2507412.xml">
information gain</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
) and <it>wrapper</it> (e.g. <link xlink:type="simple" xlink:href="../254/40254.xml">
genetic algorithm</link>) approaches. See also <link xlink:type="simple" xlink:href="../555/420555.xml">
combinatorial optimization</link> problems.</p>
<p>

It is sometimes the case that <link xlink:type="simple" xlink:href="../954/2720954.xml">
data analysis</link> such as <link xlink:type="simple" xlink:href="../568/26568.xml">
regression</link> or <link xlink:type="simple" xlink:href="../244/1579244.xml">
classification</link> can be done in the reduced space more accurately than in the original space.</p>

</sec>
<sec>
<st>
Feature extraction</st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../190/242190.xml">
Feature extraction</link></it>
</indent>

<link xlink:type="simple" xlink:href="../190/242190.xml">
Feature extraction</link> is applying a <link xlink:type="simple" xlink:href="../937/18937.xml">
mapping</link> of the multidimensional <link xlink:type="simple" xlink:href="../894/5308894.xml">
space</link> into a space of fewer <link xlink:type="simple" xlink:href="../398/8398.xml">
dimension</link>s. This means that the original feature space is transformed by applying e.g. a linear transformation via a <link xlink:type="simple" xlink:href="../340/76340.xml">
principal components analysis</link>.</p>
<p>

For example, consider a string of beads, first 100 black and then 100 white. If the string is wadded up, a classification boundary between black and white beads will be very complicated in three dimensions. However, there is a mapping from three dimensions to one dimension, namely distance along the string, which makes the classification trivial. Unfortunately, a simplification as dramatic as that is rarely possible in practice.</p>
<p>

The main linear technique for dimensionality reduction, <link xlink:type="simple" xlink:href="../340/76340.xml">
principal components analysis</link> (PCA), performs a linear mapping of the data to a lower dimensional space in such a way, that the variance of the data in the low-dimensional representation is maximized. In practice, the <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation</link> matrix of the data is constructed and the <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link>s on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behaviour of the system. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.</p>
<p>

Principal component analysis can be employed in a nonlinear way by means of the <link xlink:type="simple" xlink:href="../912/303912.xml">
kernel trick</link>. The resulting techniques is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled <link xlink:type="simple" xlink:href="../424/5978424.xml">
Kernel PCA</link>. Other nonlinear techniques include techniques for locally linear embedding (such as <link xlink:type="simple" xlink:href="../261/309261.xml">
locally linear embedding</link> (LLE), <link>
Hessian LLE</link>, <link>
Laplacian eigenmap</link>s, and <link xlink:type="simple" xlink:href="../383/18224383.xml">
LTSA</link>). These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data (actually these techniques can be viewed upon as defining a graph-based kernel for Kernel PCA). In this way, the techniques are capable of unfolding datasets such as the <substance wordnetid="100020090" confidence="0.8">
<solid wordnetid="115046900" confidence="0.8">
<food wordnetid="107555863" confidence="0.8">
<food wordnetid="100021265" confidence="0.8">
<bread wordnetid="107679356" confidence="0.8">
<foodstuff wordnetid="107566340" confidence="0.8">
<starches wordnetid="107566863" confidence="0.8">
<baked_goods wordnetid="107622061" confidence="0.8">
<link xlink:type="simple" xlink:href="../683/1935683.xml">
Swiss roll</link></baked_goods>
</starches>
</foodstuff>
</bread>
</food>
</food>
</solid>
</substance>
. Techniques that employ neighborhood graphs in order to retain global properties of the data include <link xlink:type="simple" xlink:href="../623/14628623.xml">
Isomap</link> and <link xlink:type="simple" xlink:href="../410/1187410.xml">
maximum variance unfolding</link>.The neighbourhood preservation can also be achieved through the minimisation of the weighted difference between distances in the input and output spaces (i.e. <link>
curvilinear component analysis</link> (CCA) and <link>
data-driven high-dimensional scaling</link> (DD-HDS)).</p>
<p>

<it>Principal <link xlink:type="simple" xlink:href="../246/89246.xml">
curve</link>s and <link xlink:type="simple" xlink:href="../470/2073470.xml">
manifold</link>s</it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> give another framework for PCA generalization and extend the geometric interpretation of PCA by explicitly constructing a low-dimensional embedded manifold for data <link xlink:type="simple" xlink:href="../271/336271.xml">
approximation</link>.</p>
<p>

A completely different approach to nonlinear dimensionality reduction is through the use of <link xlink:type="simple" xlink:href="../612/6836612.xml">
autoencoder</link>s, a special kind of feed-forward <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link>s. Although the idea of autoencoders is quite old, training of the encoders has only recently become possible through the use of Restricted <link xlink:type="simple" xlink:href="../059/1166059.xml">
Boltzmann machine</link>s.</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
http://www.usatoday.com/tech/science/discoveries/2004-11-26-han-purple_x.htm "Scientists use Han Purple to research quantum behavior", <it>Associated Press</it>, <link xlink:type="simple" xlink:href="../581/21581.xml">
November 26</link> <link xlink:type="simple" xlink:href="../524/35524.xml">
2004</link></entry>
<entry id="2">
http://news-service.stanford.edu/news/2006/june7/flat-060706.html "3-D insulator called Han Purple loses a dimension to enter magnetic 'Flatland'", by Dawn Levy, Stanford Report, <link xlink:type="simple" xlink:href="../855/15855.xml">
June 2</link> <link xlink:type="simple" xlink:href="../164/36164.xml">
2006</link></entry>
<entry id="3">
A. Gorban, B. Kegl, D. Wunsch, A. Zinovyev  (Eds.),  <weblink xlink:type="simple" xlink:href="http://pca.narod.ru/contentsgkwz.htm">
Principal Manifolds for Data Visualisation and Dimension Reduction,</weblink> LNCSE 58, Springer, Berlin - Heidelberg - New York, 2007. ISBN 978-3-540-73749-0</entry>
</reflist>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../063/5245063.xml">
Feature space</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../253/42253.xml">
Data mining</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../950/1179950.xml">
Feature selection</link></entry>
<entry level="2" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../412/2507412.xml">
Information gain</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
<entry level="2" type="bullet">

 <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../424/113424.xml">
Chi-square distribution</link></distribution>
</arrangement>
</structure>
</entry>
<entry level="2" type="bullet">

 <link>
Recursive feature elimination</link> (SVM based)</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../190/242190.xml">
Feature extraction</link></entry>
<entry level="2" type="bullet">

 <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<component wordnetid="105868954" confidence="0.8">
<part wordnetid="105867413" confidence="0.8">
<link xlink:type="simple" xlink:href="../340/76340.xml">
Principal components analysis</link></part>
</component>
</concept>
</idea>
</entry>
<entry level="2" type="bullet">

 <link xlink:type="simple" xlink:href="../410/1187410.xml">
Semidefinite embedding</link></entry>
<entry level="2" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../720/3259720.xml">
Multifactor dimensionality reduction</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="2" type="bullet">

 <link xlink:type="simple" xlink:href="../261/309261.xml">
Nonlinear dimensionality reduction</link></entry>
<entry level="2" type="bullet">

 <link xlink:type="simple" xlink:href="../623/14628623.xml">
Isomap</link></entry>
<entry level="2" type="bullet">

 <link xlink:type="simple" xlink:href="../424/5978424.xml">
Kernel PCA</link></entry>
<entry level="2" type="bullet">

 <link xlink:type="simple" xlink:href="../427/689427.xml">
Latent semantic analysis</link></entry>
<entry level="2" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../911/50911.xml">
Wavelet compression</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="2" type="bullet">

 <link xlink:type="simple" xlink:href="../193/18546193.xml">
Semantic Mapping</link></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../009/17740009.xml">
Topological data analysis</link></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../939/22939.xml">
Physics</link></entry>
<entry level="2" type="bullet">

 <link xlink:type="simple" xlink:href="../716/19716.xml">
magnetism</link></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.llnl.gov/tid/lof/documents/pdf/240921.pdf">
A survey of dimension reduction techniques</weblink> (US DOE Office of Scientific and Technical Information, 2002)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://jmlr.csail.mit.edu/papers/special/feature03.html">
JMLR Special Issue on Variable and Feature Selection</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.seas.upenn.edu/~kilianw/sde">
Unsupervised Learning of Image Manifolds by Semidefinite Programming</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://bioinfo-out.curie.fr/projects/elmap/">
ELastic MAPs</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.toronto.edu/~roweis/lle">
Locally Linear Embedding</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.visumap.net/index.aspx?p=Resources/RpmOverview">
Relational Perspective Map</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://isomap.stanford.edu">
A Global Geometric Framework for Nonlinear Dimensionality Reduction</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://xxx.lanl.gov/abs/cond-mat/0606042">
Dimensional reduction at a quantum critical point (realisation of dimensional reduction in a magnet)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.unimaas.nl/l.vandermaaten/dr">
Matlab Toolbox for Dimensionality Reduction</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://cran.r-project.org/src/contrib/Descriptions/kernlab.html">
kernlab - R package for kernel-based machine learning (includes kernel PCA, and SVM)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://sy.lespi.free.fr/DD-HDS-homepage.html">
DD-HDS homepage</weblink></entry>
</list>
</p>



</sec>
</bdy>
</article>
