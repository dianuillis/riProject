<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:01:21[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Relevance feedback</title>
<id>5818361</id>
<revision>
<id>242632954</id>
<timestamp>2008-10-03T00:40:15Z</timestamp>
<contributor>
<username>Drmadskills</username>
<id>6332139</id>
</contributor>
</revision>
</header>
<bdy>

<b>Relevance <link xlink:type="simple" xlink:href="../545/11545.xml">
feedback</link></b> is a feature of some <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link> systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.
<sec>
<st>
 Explicit feedback </st>

<p>

Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as relevance judgments.</p>
<p>

Users may indicate relevance explicitly using a <it>binary</it> or <it>graded</it> relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.</p>
<p>

A performance <link xlink:type="simple" xlink:href="../697/20697.xml">
metric</link> which became popular around 2005 to measure the usefulness of a ranking <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> based on the explicit relevance feedback is <link xlink:type="simple" xlink:href="../049/19542049.xml">
NDCG</link>. Other measures include <link xlink:type="simple" xlink:href="../572/41572.xml">
precision</link> at <it>k</it> and mean average precision.</p>

</sec>
<sec>
<st>
 Implicit feedback </st>

<p>

Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions.</p>
<p>

The key differences of implicit relevance feedback from that of explicit include:</p>
<p>

<list>
<entry level="1" type="number">

 the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and </entry>
<entry level="1" type="number">

 the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback</entry>
</list>
</p>

</sec>
<sec>
<st>
 Blind feedback </st>

<p>

Blind or "pseudo" relevance feedback is obtained by assuming that the top <it>k</it> documents in the result set containing <it>n</it> results (usually where <it>k</it>  <it>n</it>) are relevant.</p>
<p>

Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.</p>

</sec>
<sec>
<st>
 Using relevance information </st>

<p>

Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the <link>
Rocchio algorithm</link>.</p>

</sec>
<sec>
<st>
Further reading</st>

<p>

<weblink xlink:type="simple" xlink:href="http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt">
Relevance feedback lecture notes</weblink> - Jimmy Lin's lecture notes, adapted from Doug Oard's</p>
<p>

<weblink xlink:type="simple" xlink:href="http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html">
http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html</weblink> - chapter from modern information retrieval</p>

</sec>
</bdy>
</article>
