<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:01:06[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>METEOR</title>
<id>5855043</id>
<revision>
<id>186709395</id>
<timestamp>2008-01-25T00:33:33Z</timestamp>
<contributor>
<username>CmdrObot</username>
<id>1079367</id>
</contributor>
</revision>
<categories>
<category>Natural language processing</category>
<category>Evaluation of machine translation</category>
</categories>
</header>
<bdy>

<b>METEOR</b> (<b>Metric for Evaluation of Translation with Explicit ORdering</b>) is a <link xlink:type="simple" xlink:href="../658/731658.xml">
metric</link> for the evaluation of <link xlink:type="simple" xlink:href="../980/19980.xml">
machine translation</link> output. The metric is based on the <link xlink:type="simple" xlink:href="../463/14463.xml">
harmonic mean</link> of unigram <link xlink:type="simple" xlink:href="../572/41572.xml">
precision</link> and <link xlink:type="simple" xlink:href="../499/159499.xml">
recall</link>, with recall weighted higher than precision. It also has several features that are not found in other metrics, such as <link xlink:type="simple" xlink:href="../964/475964.xml">
stemming</link> and <link xlink:type="simple" xlink:href="../396/67396.xml">
synonymy</link> matching, along with the standard exact word matching. The metric was designed to fix some of the problems found in the more popular <link xlink:type="simple" xlink:href="../276/3772276.xml">
BLEU</link> metric, and also produce good correlation with human judgement at the sentence or segment level This differs from the BLEU metric in that BLEU seeks correlation at the corpus level.
<image location="right" width="250px" src="METEOR-alignment-a.png" type="thumb">
<caption>

Example alignment (a).
</caption>
</image>

Results have been presented which give <link xlink:type="simple" xlink:href="../708/221708.xml">
correlation</link> of up to 0.964 with human judgement at the corpus level, compared to <link xlink:type="simple" xlink:href="../276/3772276.xml">
BLEU</link>'s achievement of 0.817 on the same data set. At the sentence level, the maximum correlation with human judgement achieved was 0.403.<weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php/METEOR#endnote_Banerjee2005">
http://localhost:18088/wiki/index.php/METEOR#endnote_Banerjee2005</weblink>
<image location="right" width="250px" src="METEOR-alignment-b.png" type="thumb">
<caption>

Example alignment (b).
</caption>
</image>

<sec>
<st>
Algorithm</st>

<p>

As with <link xlink:type="simple" xlink:href="../276/3772276.xml">
BLEU</link>, the basic unit of evaluation is the sentence, the algorithm first creates an <it>alignment</it> (see illustrations) between two <unit_of_measurement wordnetid="113583724" confidence="0.8">
<definite_quantity wordnetid="113576101" confidence="0.8">
<link xlink:type="simple" xlink:href="../352/870352.xml">
sentence</link></definite_quantity>
</unit_of_measurement>
s, the candidate translation string, and the reference translation string. The <it>alignment</it> is a set of <it>mappings</it> between <link xlink:type="simple" xlink:href="../182/986182.xml">
unigram</link>s. A mapping can be thought of as a line between a unigram in one string, and a unigram in another string. The constraints are as follows; every unigram in the candidate translation must map to zero or one unigram in the reference translation and <link xlink:type="simple" xlink:href="../735/17966735.xml">
vice versa</link>. In any alignment, a unigram in one string cannot map to more than one unigram in another string.</p>
<p>

An alignment is created incrementally through a series of <it>stages</it>, which are controlled by <it>modules</it>. A module is simply a matching algorithm, for example the "wn_synonymy" module maps <link xlink:type="simple" xlink:href="../396/67396.xml">
synonyms</link> using <link xlink:type="simple" xlink:href="../955/33955.xml">
WordNet</link>, while the "exact" module matches exact words. Examples are given as follows:</p>
<p>

Each stage is split up into two <it>phases</it>. In the first phase, all possible unigram mappings are collected for the module being used in this stage. In the second phase, the largest subset of these mappings is selected to produce an <it>alignment</it> as defined above. If there are two alignments with the same number of mappings, the alignment is chosen with the fewest <it>crosses</it>, that is, with fewer <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../157/48157.xml">
intersections</link></concept>
</idea>
 of two mappings. From the two alignments shown, alignment (a) would be selected at this point. Stages are run consecutively and each stage only adds to the alignment those unigrams which have not been matched in previous stages. Once the final alignment is computed, the score is computed as follows: Unigram precision <math>P</math> is calculated as:</p>
<p>

<table class="wikitable">
<row>
<col colspan="4" align="center" border="0">
<b>Examples of pairs of words whichwill be mapped by each module</b></col>
</row>
<row>
<header>
Module</header>
<col>
Candidate</col>
<col>
Reference</col>
<col>
Match</col>
</row>
<row>
<col>
Exact</col>
<col>
good</col>
<col>
good</col>
<col>
Yes</col>
</row>
<row>
<col>
Stemmer</col>
<col>
goods</col>
<col>
good</col>
<col>
Yes</col>
</row>
<row>
<col>
Synonymy</col>
<col>
well</col>
<col>
good</col>
<col>
Yes</col>
</row>
</table>
</p>
<p>

<indent level="1">

<math>P = \frac{m}{w_{t}}</math>
</indent>

Where <math>m</math> is the number of unigrams in the candidate translation that are also found in the reference translation, and <math>w_{t}</math> is the number of unigrams in the candidate translation. Unigram recall <math>R</math> is computed as:</p>
<p>

<indent level="1">

<math>R = \frac{m}{w_{r}}</math>
</indent>

Where <math>m</math> is as above, and <math>w_{r}</math> is the number of unigrams in the reference translation. Precision and recall are combined using the <link xlink:type="simple" xlink:href="../463/14463.xml">
harmonic mean</link> in the following fashion, with recall weighted 9 times more than precision:</p>
<p>

<indent level="1">

<math>F_{mean} = \frac{10PR}{R+9P}</math>
</indent>

The measures that have been introduced so far only account for congruity with respect to single words but not with respect to larger segments that appear in both the reference and the candidate sentence. In order to take these into account, longer <it>n</it>-gram matches are used to compute a penalty <math>p</math> for the alignment. The more mappings there are that are not adjacent in the reference and the candidate sentence, the higher the penalty will be. </p>
<p>

In order to compute this penalty, unigrams are grouped into the fewest possible <it>chunks</it>, where a chunk is defined as a set of unigrams that are adjacent in the hypothesis and in the reference. The longer the adjacent mappings between the candidate and the reference, the fewer chunks there are. A translation that is identical to the reference will give just one chunk. The penalty <math>p</math> is computed as follows, </p>
<p>

<indent level="1">

<math>p = 0.5 \left ( \frac{c}{u_{m}} \right )^3</math>
</indent>

Where <it>c</it> is the number of chunks, and <math>u_{m}</math> is the number of unigrams that have been mapped. The final score for a segment is calculated as <math>M</math> below. The penalty has the effect of reducing the <math>F_{mean}</math> by up to 50% if there are no bigram or longer matches.</p>
<p>

<indent level="1">

<math>M = F_{mean} (1 - p)</math>
</indent>

To calculate a score over a whole <link xlink:type="simple" xlink:href="../244/2890244.xml">
corpus</link>, or collection of segments, the aggregate values for <math>P</math>, <math>R</math> and <math>p</math> are taken and then combined using the same formula. The algorithm also works for comparing a candidate translation against more than one reference translations. In this case the algorithm compares the candidate against each of the references and selects the highest score.</p>

</sec>
<sec>
<st>
Examples</st>

<p>

<table>
<row>
<col>
Reference</col>
<col>
the</col>
<col>
cat</col>
<col>
sat</col>
<col>
on</col>
<col>
the</col>
<col>
mat</col>
</row>
<row>
<col>
Hypothesis</col>
<col>
on</col>
<col>
the</col>
<col>
mat</col>
<col>
sat</col>
<col>
the</col>
<col>
cat</col>
</row>
</table>
</p>
<p>


Score: 0.5000 = Fmean: 1.0000 * (1 - Penalty: 0.5000)
Fmean: 1.0000 = 10 * Precision: 1.0000 * Recall: 1.0000 / Recall: 1.0000 + 9 * Precision: 1.0000
Penalty: 0.5000 = 0.5 * (Fragmentation: 1.0000 ^3)
Fragmentation: 1.0000 = Chunks: 6.0000 / Matches: 6.0000
</p>
<p>

<table>
<row>
<col>
Reference</col>
<col>
the</col>
<col>
cat</col>
<col>
sat</col>
<col>
on</col>
<col>
the</col>
<col>
mat</col>
</row>
<row>
<col>
Hypothesis</col>
<col>
the</col>
<col>
cat</col>
<col>
sat</col>
<col>
on</col>
<col>
the</col>
<col>
mat</col>
</row>
</table>
</p>
<p>


Score: 0.9977 = Fmean: 1.0000 * (1 - Penalty: 0.0023)
Fmean: 1.0000 = 10 * Precision: 1.0000 * Recall: 1.0000 / Recall: 1.0000 + 9 * Precision: 1.0000
Penalty: 0.0023 = 0.5 * (Fragmentation: 0.1667 ^3) 
Fragmentation: 0.1667 = Chunks: 1.0000 / Matches: 6.0000
</p>
<p>

<table>
<row>
<col>
Reference</col>
<col>
the</col>
<col>
cat</col>

<col>
sat</col>
<col>
on</col>
<col>
the</col>
<col>
mat</col>
</row>
<row>
<col>
Hypothesis</col>
<col>
the</col>
<col>
cat</col>
<col>
was</col>
<col>
sat</col>
<col>
on</col>
<col>
the</col>
<col>
mat</col>
</row>
</table>
</p>
<p>


Score: 0.9654 = Fmean: 0.9836 * (1 - Penalty: 0.0185)
Fmean: 0.9836 = 10 * Precision: 0.8571 * Recall: 1.0000 / Recall: 1.0000 + 9 * Precision: 0.8571
Penalty: 0.0185 = 0.5 * (Fragmentation: 0.3333 ^3)
Fragmentation: 0.3333 = Chunks: 2.0000 / Matches: 6.0000
</p>

</sec>
<sec>
<st>
Notes</st>

<p>

<list>
<entry level="1" type="number">

  <cite id="endnote_Banerjee2005a" style="font-style: normal;">
<b>^</b></cite>&nbsp; Banerjee, S. and Lavie, A. (2005) </entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 Banerjee, S. and Lavie, A. (2005) "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments" in <it>Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43rd Annual Meeting of the Association of Computational Linguistics (ACL-2005), Ann Arbor, Michigan, June 2005''</it></entry>
<entry level="1" type="bullet">

 Lavie, A., Sagae, K. and Jayaraman, S. (2004) "The Significance of Recall in Automatic Metrics for MT Evaluation" in <it>Proceedings of AMTA 2004, Washington DC. September 2004''</it></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.cmu.edu/~alavie/METEOR/">
The METEOR Automatic Machine Translation Evaluation System</weblink> (including link for download)</entry>
</list>
</p>

</sec>
</bdy>
</article>
