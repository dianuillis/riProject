<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:21:05[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>QR algorithm</title>
<id>594072</id>
<revision>
<id>239752384</id>
<timestamp>2008-09-20T08:33:38Z</timestamp>
<contributor>
<username>Fph</username>
<id>1189212</id>
</contributor>
</revision>
<categories>
<category>Numerical linear algebra</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../660/7330660.xml">
numerical linear algebra</link>, the <b>QR algorithm</b> is an <link xlink:type="simple" xlink:href="../560/516560.xml">
eigenvalue algorithm</link>; that is, a procedure to calculate the <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalues and eigenvectors</link> of a <link xlink:type="simple" xlink:href="../728/19008728.xml">
matrix</link>. The QR transformation was developed in 1961 by John G.F. Francis (England) and by Vera N. Kublanovskaya (USSR), working independently.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>The basic idea is to perform a <link xlink:type="simple" xlink:href="../223/305223.xml">
QR decomposition</link>, writing the matrix as a product of an <link xlink:type="simple" xlink:href="../620/105620.xml">
orthogonal matrix</link> and an upper <link xlink:type="simple" xlink:href="../222/374222.xml">
triangular matrix</link>, multiply the factors in the other order, and iterate.
<sec>
<st>
The practical QR algorithm</st>

<p>

Formally, let <it>A</it> be the matrix of which we want to compute the eigenvalues, and let <it>A</it>0:=<it>A</it>. At the <it>k</it>-th step (starting with <it>k</it> = 0), we write <it>Ak</it> as the product of an orthogonal matrix <it>Qk</it> and a upper triangular matrix <it>Rk</it> and we form <it>Ak</it>+1 = <it>RkQk</it>. Note that
<indent level="1">

<math> A_{k+1} = R_k Q_k = Q_k^T Q_k R_k Q_k = Q_k^T A_k Q_k = Q_k^{-1} A_k Q_k, </math>
</indent>
so all the <it>Ak</it> are <link xlink:type="simple" xlink:href="../149/341149.xml">
similar</link> and hence they have the same eigenvalues. The algorithm is <link xlink:type="simple" xlink:href="../807/233807.xml">
numerically stable</link> because it proceeds by <it>orthogonal</it> similarity transforms.</p>
<p>

Under certain conditions<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>, the matrices <it>Ak</it> converge to a triangular matrix. The eigenvalues of a triangular matrix are listed on the diagonal, and the eigenvalue problem is solved. In testing for convergence it is impractical to require exact zeros, but the <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../414/2470414.xml">
Gershgorin circle theorem</link></proposition>
</theorem>
</message>
</statement>
 provides a bound on the error.</p>
<p>

In this crude form the iterations are relatively expensive. This can be mitigated by first bringing the matrix <it>A</it> to upper <link xlink:type="simple" xlink:href="../204/519204.xml">
Hessenberg form</link> (which costs <math>\begin{matrix}\frac{5}{3}\end{matrix} n^3 + O(n^2)</math> using <link xlink:type="simple" xlink:href="../424/485424.xml">
Householder reduction</link>) with a finite sequence of orthogonal similarity transforms, much like a QR decomposition. Determining the QR decomposition of an upper Hessenberg matrix costs <math>6 n^2 + O(n)</math>.</p>
<p>

If the original matrix is <link xlink:type="simple" xlink:href="../474/126474.xml">
symmetric</link>, then the upper Hessenberg matrix is also symmetric and thus <link xlink:type="simple" xlink:href="../218/519218.xml">
tridiagonal</link>, and so are all the <it>Ak</it>. This procedure costs <math>\begin{matrix}\frac{2}{3}\end{matrix} n^3 + O(n^2)</math> using Householder reduction. Determining the QR decomposition of a tridiagonal matrix costs <math>O(n)</math>.</p>
<p>

The rate of convergence depends on the separation between eigenvalues, so a practical algorithm will use shifts, either explicit or implicit, to increase separation and accelerate convergence. A typical symmetric QR algorithm isolates each eigenvalue (then reduces the size of the matrix) with only one or two iterations, making it efficient as well as robust.</p>

</sec>
<sec>
<st>
 The implicit QR algorithm </st>
<p>

In modern computational practice<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>, the QR algorithm is performed in an implicit version which makes the use of multiple shifts easier to introduce. The matrix is first brought to upper Hessenberg form <math>A_0=QAQ^T</math> as in the explicit version; then, at each step, the first column of <math>A_k</math> is transformed via a small-size Householder similarity transformation to the first column of <math>p(A_k)e_1</math>, where <math>p(A_k)</math>, of degree <math>r</math>, is the polynomial that defines the shifting strategy (often <math>p(x)=(x-\lambda)(x-\bar\lambda)</math>, where <math>\lambda</math> and <math>\bar\lambda</math> are the two eigenvalues of the trailing <math>2 \times 2</math> principal submatrix of <math>A_k</math>, the so-called <it>implicit double-shift</it>). Then successive Householder transformation of size <math>r+1</math> are performed in order to return the working matrix <math>A_k</math> to upper Hessenberg form. This operation is known as <it>bulge chasing</it>, due to the peculiar shape of the non-zero entries of the matrix along the steps of the algorithm. As in the first version,  deflation is performed as soon as one of the sub-diagonal entries of <math>A_k</math> is sufficiently small.</p>

<ss1>
<st>
Renaming proposal</st>
<p>

Since in the modern implicit version of the procedure no <link xlink:type="simple" xlink:href="../223/305223.xml">
QR decomposition</link>s are explicitly performed, some authors, for instance Watkins<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>, suggested changing its name to <it>Francis algorithm</it>. Golub and Van Loan use the term <it>Francis QR step</it>.</p>

</ss1>
</sec>
<sec>
<st>
 Interpretation and convergence </st>
<p>

The QR algorithm can be seen as a more sophisticated variation of the basic 'power' <link xlink:type="simple" xlink:href="../560/516560.xml">
eigenvalue algorithm</link>. Recall that the power algorithm repeatedly multiplies <it>A</it> times a single vector, normalizing after each iteration. The vector converges to the eigenvector of the largest eigenvalue. Instead, the QR algorithm works with a complete basis of vectors, using QR decomposition to renormalize (and orthogonalize). For a symmetric matrix <it>A</it>, upon convergence, <it>AQ</it> = <it>Q&amp;Lambda;</it>, where <it>&amp;Lambda;</it> is the diagonal matrix of eigenvalues to which <it>A</it> converged, and where <it>Q</it> is a composite of all the orthogonal similarity transforms required to get there. Thus the columns of <it>Q</it> are the eigenvectors.</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://planetmath.org/?op=getobj&amp;from=objects&amp;id=1474">
Eigenvalue problem</weblink> on <web_site wordnetid="106359193" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../623/161623.xml">
PlanetMath</link></web_site>
</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.math.umn.edu/~olver/aims_/qr.pdf">
Prof. Peter Olver's notes on orthogonal bases and the workings of the QR algorithm</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<reflist>
<entry id="1">
J.G.F. Francis, "The QR Transformation, I" (part 1) , <it>The Computer Journal</it>, vol. 4, no. 3, pages 265-271 (1961) <weblink xlink:type="simple" xlink:href="http://comjnl.oxfordjournals.org/cgi/content/abstract/4/3/265">
online at oxfordjournals.org</weblink>; "The QR Transformation, II" (part 2), <it>The Computer Journal</it>, vol. 4, no. 4, pages 332-345 (1962) <weblink xlink:type="simple" xlink:href="http://comjnl.oxfordjournals.org/cgi/content/abstract/4/4/332">
online at oxfordjournals.org</weblink>. <p>

Vera N. Kublanovskaya, "On some algorithms for the solution of the complete eigenvalue problem," <it>USSR Computational Mathematics and Mathematical Physics</it>, vol. 3, pages 637–657 (1961). Also published in: <it>Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki</it>, vol.1, no. 4, pages 555–570 (1961). </p>
<p>

For biographical information about John G.F. Francis, see “Notes” in Wikipedia article “<link xlink:type="simple" xlink:href="../429/2161429.xml">
Eigenvalue, eigenvector and eigenspace</link>”.   For biographical information about Vera Kublanovskaya, see Wikipedia article “<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../851/18387851.xml">
Vera Kublanovskaya</link></mathematician>
</scientist>
</causal_agent>
</person>
</physical_entity>
”.</p>
</entry>
<entry id="2">
Golub, G. H. and Van Loan, C. F.: Matrix Computations, 3rd ed., Johns Hopkins University Press, Baltimore, 1996, ISBN 0-8018-5414-8.</entry>
<entry id="3">
Golub and Van Loan, chapter 7</entry>
<entry id="4">
Watkins, David S.: The Matrix Eigenvalue Problem: GR and Krylov Subspace Methods, SIAM, Philadelphia, PA, USA, 2007, ISBN 0898716411, 9780898716412</entry>
</reflist>
</p>


</sec>
</bdy>
</article>
