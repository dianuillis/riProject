<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:07:38[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<know-how  confidence="0.8" wordnetid="105616786">
<method  confidence="0.8" wordnetid="105660268">
<header>
<title>Kernel principal component analysis</title>
<id>5978424</id>
<revision>
<id>233995825</id>
<timestamp>2008-08-24T20:42:23Z</timestamp>
<contributor>
<username>AlleborgoBot</username>
<id>3813685</id>
</contributor>
</revision>
<categories>
<category>Signal processing</category>
<category>Kernel methods for machine learning</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Kernel principal component analysis</b> (kernel PCA) is an extension of <link xlink:type="simple" xlink:href="../340/76340.xml">
principal component analysis</link> (PCA) using techniques of <link xlink:type="simple" xlink:href="../576/3424576.xml">
kernel methods</link>. Using a kernel, the originally linear operations of PCA are done in a <link xlink:type="simple" xlink:href="../196/651196.xml">
reproducing kernel Hilbert space</link> with a non-linear mapping.
<sec>
<st>
Example</st>
<p>

The two images show a number of data points before and after Kernel PCA. The color of the points is not part of the algorithm, it's only there to show how the data groups together before and after the transformation. Note in particular that the first principal component is enough to distinguish the three different groups, which is impossible using only linear PCA.</p>
<p>

The kernel used in this example was:</p>
<p>

<math>k(\boldsymbol{x},\boldsymbol{y}) = (\boldsymbol{x}^\mathrm{T}\boldsymbol{y} + 1)^2</math></p>
<p>

If instead a gaussian kernel is used:
<math>k(\boldsymbol{x},\boldsymbol{y}) = e^\frac{-||\boldsymbol{x} - \boldsymbol{y}||^2}{2\sigma^2},</math>
the result is shown in the next figure.</p>
<p>

<image location="left" width="300px" src="Kernel_pca_input.png" type="thumb">
<caption>

Input points before kernel PCA
</caption>
</image>

<image location="none" width="300px" src="Kernel_pca_output.png" type="thumb">
<caption>

Output after kernel PCA. The three groups are distinguishable using the first component only.
</caption>
</image>

<image location="none" width="300px" src="Kernel_pca_output_gaussian.png" type="thumb">
<caption>

Output after kernel PCA, with a <link xlink:type="simple" xlink:href="../918/299918.xml">
gaussian</link> kernel.
</caption>
</image>
</p>


</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.face-rec.org/algorithms/Kernel/kernelPCA_scholkopf.pdf">
Nonlinear Component Analysis as a Kernel Eigenvalue Problem</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../912/303912.xml">
Kernel trick</link></method>
</know-how>
</entry>
</list>
</p>




</sec>
</bdy>
</method>
</know-how>
</article>
