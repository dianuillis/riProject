<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:08:42[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Quantum mutual information</title>
<id>6011769</id>
<revision>
<id>113293006</id>
<timestamp>2007-03-07T11:27:17Z</timestamp>
<contributor>
<username>Jheald</username>
<id>141421</id>
</contributor>
</revision>
<categories>
<category>Quantum information theory</category>
<category>Quantum mechanical entropy</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../274/25274.xml">
quantum information theory</link>, <b>quantum mutual information</b>, or <b>von Neumann mutual information</b>, is a measure of correlation between subsystems of quantum state. It is the quantum mechanical analog of Shannon <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link>.
<sec>
<st>
 Motivation </st>

<p>

For simplicity, it will be assumed that all objects in the article are finite dimensional.</p>
<p>

The definition of quantum mutual entropy is motivated by the classical case. For a probability distribution of two variables <it>p</it>(<it>x</it>, <it>y</it>), the two marginal distributions are</p>
<p>

<indent level="1">

<math>p(x) = \sum_{y} p(x,y)\; , \; p(y) = \sum_{x} p(x,y).</math>
</indent>

The classical mutual information <it>I</it>(<it>X</it>, <it>Y</it>) is defined by</p>
<p>

<indent level="1">

<math>\;I(X,Y) = S(p(x)) + S(p(y)) - S(p(x,y))</math>
</indent>

where <it>S</it>(<it>q</it>) denotes the <link xlink:type="simple" xlink:href="../445/15445.xml">
Shannon entropy</link> of the probability distribution <it>q</it>.</p>
<p>

One can calculate directly</p>
<p>

<indent level="1">

<math>\; S(p(x)) + S(p(y))</math> 
</indent>

<indent level="1">

<math>\; = \sum_x p_x \log p(x) + \sum_y p_y \log p(y)</math> 
</indent>

<indent level="1">

<math>
\; = \sum_x \; ( \sum_{y'} p(x,y') \log \sum_{y'} p(x,y') ) + \sum_y ( \sum_{x'} p(x',y) \log \sum_{x'} p(x',y))</math>
</indent>

<indent level="1">

<math>\; = \sum_{x,y} p(x,y) (\log \sum_{y'} p(x,y') + \log \sum_{x'} p(x',y))</math>
</indent>

<indent level="1">

<math>\; = \sum_{x,y} p(x,y) \log p(x) p(y) .</math>
</indent>

So the mutual information is</p>
<p>

<indent level="1">

<math>I(X,Y) = \sum_{x,y} p(x,y) \log \frac{p(x) p(y)}{p(x,y)}.</math>
</indent>

But this is precisely the <link xlink:type="simple" xlink:href="../527/467527.xml">
relative entropy</link> between <it>p</it>(<it>x</it>, <it>y</it>) and <it>p</it>(<it>x</it>)<it>p</it>(<it>y</it>). In other words, if we assume the two variables <it>x</it> and <it>y</it> to be uncorrelated, mutual information is the <it>discrepancy in uncertainty</it> resulting from this (possibly erroneous) assumption.</p>
<p>

It follows from the property of relative entropy that <it>I</it>(<it>X</it>,<it>Y</it>) &amp;ge; 0 and equality holds if and only if <it>p</it>(<it>x</it>, <it>y</it>) = <it>p</it>(<it>x</it>)<it>p</it>(<it>y</it>).</p>

</sec>
<sec>
<st>
 Definition </st>

<p>

The quantum mechanical counterpart of classical probability distributions are <link>
 density matrices</link>.</p>
<p>

Consider a composite quantum system whose state space is the tensor product</p>
<p>

<indent level="1">

<math>H = H_A \otimes H_B.</math>
</indent>

Let <it>ρAB</it> be a density matrix acting on <it>H</it>. The <link xlink:type="simple" xlink:href="../425/2515425.xml">
von Neumann entropy</link> of <it>ρ</it>, which is the quantum mechanical analaog of the Shannon entropy, is given by</p>
<p>

<indent level="1">

<math>S(\rho^{AB}) = - \operatorname{Tr} \rho^{AB} \log \rho^{AB}.</math>
</indent>

For a probability distribution <it>p</it>(<it>x</it>,<it>y</it>), the marginal distributions are obtained by integrating away the variables <it>x</it> or <it>y</it>. The corresponding operation for density matrices is the <link xlink:type="simple" xlink:href="../148/814148.xml">
partial trace</link>. So one can assign to <it>&amp;rho;</it> a state on the subsystem <it>A</it> by</p>
<p>

<indent level="1">

<math>\rho^A = \operatorname{Tr}_B \; \rho^{AB}</math>
</indent>

where Tr<it>B</it> is partial trace with respect to system <it>B</it>. This is the <b>reduced state</b> of <it>&amp;rho;AB</it> on system <it>A</it>. The <b>reduced von Neumann entropy</b>' of <it>&amp;rho;AB</it> with respect to system <it>A</it> is</p>
<p>

<indent level="1">

<math>\;S(\rho^A).</math>
</indent>

<it>S</it>(<it>ρB</it>) is defined in the same way.</p>
<p>

<it>Technical Note:</it> In mathematical language, passing from the classical to quantum setting can be described as follows. The <it>algebra of observables</it> of a physical system is a <link xlink:type="simple" xlink:href="../184/7184.xml">
C*-algebra</link> and states are unital linear functionals on the algebra. Classical systems are described by commutative C*-algebras, therefore classical states are <link>
probability measure</link>s. Quantum mechanical systems have non-commutative observable algebras. In concrete considerations, quantum states are density operators. If the probability measure <it>μ</it> is a state on classical composite system consisting of two subsystem <it>A</it> and <it>B</it>, we project <it>μ</it> onto the system <it>A</it> to obtain the reduced state. As stated above, the quantum analog of this is the partial trace operation, which can be viewed as projection onto a tensor component. <it>End of note</it></p>
<p>

We can see now the appropriate definition of quantum mutual information should be</p>
<p>

<indent level="1">

<math>\; I(\rho^{AB}) = S(\rho^A) + S(\rho^B) - S(\rho^{AB}).</math>
</indent>

Quantum mutual information can be interpretated the same way as in the classical case: it can be shown that</p>
<p>

<indent level="1">

<math>I(\rho^{AB}) = S(\rho^{AB} \| \rho^A \otimes \rho^B)</math>
</indent>

where <math>S(\cdot \| \cdot)</math> denotes <link xlink:type="simple" xlink:href="../806/5993806.xml">
quantum relative entropy</link>.</p>

</sec>
</bdy>
</article>
