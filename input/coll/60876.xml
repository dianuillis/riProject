<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:46:09[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<physical_entity  confidence="0.8" wordnetid="100001930">
<person  confidence="0.8" wordnetid="100007846">
<model  confidence="0.8" wordnetid="110324560">
<assistant  confidence="0.8" wordnetid="109815790">
<worker  confidence="0.8" wordnetid="109632518">
<causal_agent  confidence="0.8" wordnetid="100007347">
<header>
<title>Markov chain</title>
<id>60876</id>
<revision>
<id>243365281</id>
<timestamp>2008-10-06T05:22:54Z</timestamp>
<contributor>
<username>Nanmus</username>
<id>5504541</id>
</contributor>
</revision>
<categories>
<category>Stochastic processes</category>
<category>Statistical models</category>
<category>Probability theory</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link>, a <b>Markov chain</b>, named after <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../915/915.xml">
Andrey Markov</link></scientist>
</person>
, is a <link xlink:type="simple" xlink:href="../895/47895.xml">
stochastic process</link> with the <link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link>. Having the Markov property means that, <it>given the present state</it>, future states are independent of the past states. In other words, the description of the present state fully captures all the information that could influence the future evolution of the process. Future states will be reached through a probabilistic process instead of a deterministic one. <p>

At each step the system may change its state from the current state to another state, or remain in the same state, according to a certain probability distribution. The changes of state are called transitions, and the probabilities associated with various state-changes are called transition probabilities. </p>
<p>

An example of a Markov chain is a <link xlink:type="simple" xlink:href="../451/235451.xml">
simple random walk</link> where the state space is a set of vertices of a <link xlink:type="simple" xlink:href="../401/12401.xml">
graph</link> and the transition steps involve moving to any of the neighbors of the current vertex with equal probability (regardless of the history of the walk).</p>

<sec>
<st>
Formal definition</st>
<p>

A Markov chain is a sequence of <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link>s <it>X</it>1, <it>X</it>2, <it>X</it>3, ... with the <link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link>, namely that, given the present state, the future and past states are independent. Formally,</p>
<p>

<indent level="1">

<math>\Pr(X_{n+1}=x|X_n=x_n, \ldots, X_1=x_1) = \Pr(X_{n+1}=x|X_n=x_n).\,</math>
</indent>

The possible values of <it>Xi</it> form a <link xlink:type="simple" xlink:href="../026/6026.xml">
countable set</link> <it>S</it> called the <b>state space</b> of the chain.</p>
<p>

Markov chains are often described by a <link xlink:type="simple" xlink:href="../986/19721986.xml">
directed graph</link>, where the edges are labeled by the probabilities of going from one state to the other states.</p>

<ss1>
<st>
Variations</st>
<p>

<link xlink:type="simple" xlink:href="../342/1125342.xml">
Continuous-time Markov process</link>es have a continuous index.</p>
<p>

<b>Time-homogeneous Markov chains</b> (or, Markov chains with time-homogeneous transition probabilities) are processes where</p>
<p>

<indent level="1">

<math>\Pr(X_{n+1}=x|X_n=y) = \Pr(X_{n}=x|X_{n-1}=y)\,</math>
</indent>

for all <it>n</it>.</p>
<p>

A <b>Markov chain of order <it>m</it></b> (or a Markov chain with memory <it>m</it>) where <it>m</it> is finite, is where</p>
<p>

<indent level="1">

<math>\Pr(X_n=x_n|X_{n-1}=x_{n-1}, X_{n-2}=x_{n-2}, \dots , X_{1}=x_{1})</math>
</indent>
:<math>  = \Pr(X_n=x_n|X_{n-1}=x_{n-1}, X_{n-2}=x_{n-2}, \dots, X_{n-m}=x_{n-m})</math></p>
<p>

for all <it>n</it>. It is possible to construct a chain (<it>Yn</it>) from (<it>Xn</it>) which has the 'classical' <link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link> as follows:
Let <it>Yn</it> = (<it>Xn</it>, <it>Xn</it>&amp;minus;1, ..., <it>Xn</it>&amp;minus;<it>m</it>+1), the ordered <it>m</it>-tuple of <it>X</it> values. Then <it>Yn</it> is a Markov chain with state space <it>Sm</it> and has the classical <link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link>.</p>

</ss1>
<ss1>
<st>
Example</st>

<p>

A <link xlink:type="simple" xlink:href="../931/10931.xml">
finite state machine</link> can be used as a representation of a Markov chain. Assuming a sequence of <link xlink:type="simple" xlink:href="../067/453067.xml">
independent and identically distributed</link> input signals (for example, symbols from a binary alphabet chosen by coin tosses), if the machine is in state <it>y</it> at time <it>n</it>, then the probability that it moves to state <it>x</it> at time <it>n</it>&nbsp;+&nbsp;1 depends only on the current state.</p>
<p>

A thorough development and many examples can be found in the on-line monograph 
Meyn &amp; Tweedie 2005 <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> 
The appendix of Meyn 2007 <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>, also available on-line, contains an abridged Meyn &amp; Tweedie.</p>

</ss1>
<ss1>
<st>
History</st>
<p>

Seneta<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> has provided an account of Markov's reasons for working on this theory, and of his early developments of it.</p>

</ss1>
</sec>
<sec>
<st>
 Properties of Markov chains </st>

<p>

Define the probability of going from state <it>i</it> to state <it>j</it> in <it>n</it> time steps as</p>
<p>

<indent level="1">

<math>p_{ij}^{(n)} = \Pr(X_n=j\mid X_0=i) \,</math>
</indent>

and the single-step transition as</p>
<p>

<indent level="1">

<math>p_{ij} = \Pr(X_1=j\mid X_0=i). \,</math>
</indent>

The <it>n</it>-step transition satisfies the <link xlink:type="simple" xlink:href="../075/526075.xml">
Chapman-Kolmogorov equation</link>, that for any <it>k</it> such that 0  <it>k</it>  <it>n</it>,</p>
<p>

<indent level="1">

<math>p_{ij}^{(n)} = \sum_{r \in S} p_{ir}^{(k)} p_{rj}^{(n-k)}.</math>
</indent>

The <link xlink:type="simple" xlink:href="../077/506077.xml">
marginal distribution</link> Pr (<it>Xn</it> = <it>x</it>) is the distribution over states at time <it>n</it>.  The initial distribution is Pr (<it>X</it>0 = <it>x</it>).  The evolution of the process through one time step is described by</p>
<p>

<indent level="1">

 <math> \Pr(X_{n}=j) = \sum_{r \in S} p_{rj} \Pr(X_{n-1}=r) = \sum_{r \in S} p_{rj}^{(n)} \Pr(X_0=r).</math>
</indent>

The superscript <math>(n)</math> is intended to be an integer-valued label only; however, if the Markov chain is time-homogeneous, then this superscript can also be interpreted as a "raising to the power of", discussed further below.  </p>

<ss1>
<st>
Reducibility</st>

<p>

A state <it>j</it> is said to be <b>accessible</b> from a different state <it>i</it> (written <it>i</it> → <it>j</it>) if, given that we are in state <it>i</it>, there is a non-zero probability that at some time in the future, we will be in state <it>j</it>.  Formally, state <it>j</it> is accessible from state <it>i</it> if there exists an integer <it>n</it>≥0 such that</p>
<p>

<indent level="1">

 <math> \Pr(X_{n}=j | X_0=i) &amp;gt; 0.\, </math>
</indent>

Allowing <it>n</it> to be zero means that every state is defined to be accessible from itself.</p>
<p>

A state <it>i</it> is said to <b>communicate</b> with state <it>j</it> (written <it>i</it> ↔ <it>j</it>) if it is true that both <it>i</it> is accessible from <it>j</it> and that <it>j</it> is accessible from <it>i</it>. A set of states <it>C</it> is a <b>communicating class</b> if every pair of states in <it>C</it> communicates with each other, and no state in <it>C</it> communicates with any state not in <it>C</it>. (It can be shown that communication in this sense is an <link xlink:type="simple" xlink:href="../259/9259.xml">
equivalence relation</link>). A communicating class is <b>closed</b> if the probability of leaving the class is zero, namely that if <it>i</it> is in <it>C</it> but <it>j</it> is not, then <it>j</it> is not accessible from <it>i</it>.</p>
<p>

Finally, a Markov chain is said to be <b>irreducible</b> if its state space is a communicating class; this means that, in an irreducible Markov chain, it is possible to get to any state from any state.</p>

</ss1>
<ss1>
<st>
Periodicity</st>
<p>

A state <it>i</it> has <b>period</b> <it>k</it> if any return to state <it>i</it> must occur in multiples of <it>k</it> time steps. For example, if it is only possible to return to state <it>i</it> in an even number of steps, then <it>i</it> is periodic with period 2. Formally, the period of a state is defined as
<indent level="1">

 <math> k = \operatorname{gcd}\{ n: \Pr(X_n = i | X_0 = i) &amp;gt; 0\}</math>
</indent>
(where "gcd" is the <link xlink:type="simple" xlink:href="../354/12354.xml">
greatest common divisor</link>). Note that even though a state has period <it>k</it>, it may not be possible to reach the state in <it>k</it> steps. For example, suppose it is possible to return to the state in {6,8,10,12,...} time steps; then <it>k</it> would be 2, even though 2 does not appear in this list.</p>
<p>

If <it>k</it> = 1, then the state is said to be <b>aperiodic</b>; otherwise (k&amp;gt;1), the state is said to be <b>periodic with period <it>k</it></b>.</p>
<p>

It can be shown that every state in a communicating class must have the same period.</p>
<p>

A finite state irreducible Markov chain is said to be <b>ergodic</b> if its states are aperiodic.</p>

</ss1>
<ss1>
<st>
Recurrence</st>
<p>

A state <it>i</it> is said to be <b>transient</b> if, given that we start in state <it>i</it>, there is a non-zero probability that we will never return to <it>i</it>. Formally, let the <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> <it>Ti</it> be the first return time to state <it>i</it> (the "hitting time"):</p>
<p>

<indent level="1">

 <math> T_i = \inf \{ n: X_n = i | X_0 = i\}.</math>
</indent>

Then, state <it>i</it> is transient <link xlink:type="simple" xlink:href="../922/14922.xml">
iff</link>:</p>
<p>

<indent level="1">

 <math> \Pr(T_i = {\infty}) &amp;gt; 0. </math>
</indent>

If a state <it>i</it> is not transient (it has finite hitting time with probability 1), then it is said to be <b>recurrent</b> or <b>persistent</b>. Although the hitting time is finite, it need not have a finite <link xlink:type="simple" xlink:href="../653/9653.xml">
expectation</link>. Let <it>Mi</it> be the expected return time,</p>
<p>

<indent level="1">

 <math> M_i = E[T_i].\, </math>
</indent>

Then, state <it>i</it> is <b>positive recurrent</b> if <it>Mi</it> is finite; otherwise, state <it>i</it> is <b>null recurrent</b> (the terms <b>non-null persistent</b> and <b>null persistent</b> are also used, respectively).</p>
<p>

It can be shown that a state is recurrent <link xlink:type="simple" xlink:href="../922/14922.xml">
if and only if</link> 
<indent level="1">

 <math>\sum_{n=0}^{\infty} p_{ii}^{(n)} = \infty.</math>
</indent>

A state <it>i</it> is called <b>absorbing</b> if it is impossible to leave this state.  Therefore,
the state <it>i</it> is absorbing if and only if</p>
<p>

<indent level="1">

 <math> p_{ii} = 1</math> and <math> p_{ij} = 0</math> for <math>i \not= j.</math>
</indent>

</p>
</ss1>
<ss1>
<st>
Ergodicity</st>
<p>

A state <it>i</it> is said to be <b><link xlink:type="simple" xlink:href="../986/258986.xml">
ergodic</link></b> if it is aperiodic and positive recurrent. If all states in a Markov chain are ergodic, then the chain is said to be ergodic.</p>

</ss1>
<ss1>
<st>
Steady-state analysis and limiting distributions</st>
<p>

If the Markov chain is a time-homogeneous Markov chain, so that the process is described by a single, time-independent matrix <math>p_{ij}</math>, then the vector <math>\boldsymbol{\pi}</math> is called a <b>stationary distribution</b> (or <b>equilibrium distribution</b> or <b><link xlink:type="simple" xlink:href="../201/7635201.xml">
invariant measure</link></b>) if its entries <math>\pi_j</math> sum to 1 and it satisfies</p>
<p>

<indent level="1">

 <math>\pi_j = \sum_{i \in S} \pi_i p_{ij}.</math>
</indent>

An irreducible chain has a stationary distribution if and only if all of its states are positive-recurrent. In that case, <it>π</it> is unique and is related to the expected return time:</p>
<p>

<indent level="1">

 <math>\pi_j = \frac{1}{M_j}.\,</math>
</indent>

Further, if the chain is both irreducible and aperiodic, then for any <it>i</it> and <it>j</it>,</p>
<p>

<indent level="1">

 <math>\lim_{n \rarr \infty} p_{ij}^{(n)} = \frac{1}{M_j}.</math>
</indent>

Note that there is no assumption on the starting distribution; the chain converges to the stationary distribution regardless of where it begins.</p>
<p>

If a chain is not irreducible, its stationary distributions will not be unique (consider any closed communicating class in the chain; each one will have its own unique stationary distribution. Any of these will extend to a stationary distribution for the overall chain, where the probability outside the class is set to zero). However, if a state <it>j</it> is aperiodic, then </p>
<p>

<indent level="1">

 <math>\lim_{n \rarr \infty} p_{jj}^{(n)} = \frac{1}{M_j}</math>
</indent>

and for any other state <it>i</it>, let <it>fij</it> be the probability that the chain ever visits state <it>j</it> if it starts at <it>i</it>,</p>
<p>

<indent level="1">

 <math>\lim_{n \rarr \infty} p_{ij}^{(n)} = \frac{f_{ij}}{M_j}.</math>
</indent>

</p>
</ss1>
</sec>
<sec>
<st>
 Markov chains with a finite state space </st>

<p>

If the state space is <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../742/11742.xml">
finite</link></concept>
</idea>
, the transition probability distribution can be represented by a <link xlink:type="simple" xlink:href="../728/19008728.xml">
matrix</link>, called the <b>transition matrix</b>, with the (<it>i</it>, <it>j</it>)th element of <b>P</b> equal to </p>
<p>

<indent level="1">

<math>p_{ij} = \Pr(X_{n+1}=j\mid X_n=i). \,</math>
</indent>

<b>P</b> is a <link xlink:type="simple" xlink:href="../313/217313.xml">
stochastic matrix</link>, which is an important fact to keep in mind for the rest of this discussion. When the Markov chain is a time-homogeneous Markov chain, so that the transition matrix <b>P</b> always remains the same at each step, then the <it>k</it>-step transition probability can be computed as the <it>k</it>'th power of the transition matrix, <b>P</b><it>k</it>.</p>
<p>

The stationary distribution <b>π</b> is a (row) vector whose entries sum to 1 that satisfies the equation</p>
<p>

<indent level="1">

<math> \pi = \pi\mathbf{P}.\,</math>
</indent>

In other words, the stationary distribution <b>π</b> is a normalized (meaning that the sum of its entries is 1) left <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link> of the transition matrix associated with the <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalue</link> 1.</p>
<p>

Alternatively, <b>π</b> can be viewed as a fixed point of the linear (hence continuous) transformation on the unit <link xlink:type="simple" xlink:href="../781/39781.xml">
simplex</link> associated to the matrix <b>P</b>. As any continuous transformation in the unit simplex has a fixed point, a stationary distribution always exists, but is not guaranteed to be unique, in general. However, if the Markov chain is irreducible and aperiodic, then there is a unique stationary distribution <b>π</b>. Additionally, in this case <b>P</b><it>k</it> converges to a rank-one matrix in which each row is the stationary distribution <b>π</b>, that is,</p>
<p>

<indent level="1">

<math>\lim_{k\rightarrow\infty}\mathbf{P}^k=\mathbf{1}\pi</math>
</indent>

where <b>1</b> is the column vector with all entries equal to 1. This is stated by the <link xlink:type="simple" xlink:href="../333/1540333.xml">
Perron-Frobenius theorem</link>. If, by whatever means, <math>\scriptstyle \lim_{k\to\infty}\mathbf{P}^k</math> is found, then the stationary distribution of the Markov chain in question can be easily determined for any starting distribution, as will be explained below.</p>
<p>

Since <b>P</b> is a stochastic matrix, <math>\scriptstyle \lim_{k\to\infty}\mathbf{P}^k</math> always exists. Because there are a number of different special cases to consider, the process of finding this limit can be a lengthy task. All the same, there are several general rules and guidelines to keep in mind. Let <b>P</b> be an <it>n</it>&amp;times;<it>n</it> matrix, and define <math>\scriptstyle \mathbf{Q} = \lim_{k\to\infty}\mathbf{P}^k.</math></p>
<p>

It is always true that</p>
<p>

<indent level="1">

<math>\mathbf{QP} = \mathbf{Q}.</math>
</indent>

Subtracting <b>Q</b> from both sides and factoring then yields</p>
<p>

<indent level="1">

<math>\mathbf{Q}(\mathbf{P} - \mathbf{I}_{n}) = \mathbf{0}_{n,n}</math>
</indent>

where <b>I</b><it>n</it> is the <link xlink:type="simple" xlink:href="../718/59718.xml">
identity matrix</link> of size <it>n</it>, and <b>0</b><it>n</it>,<it>n</it> is the <link xlink:type="simple" xlink:href="../888/1168888.xml">
zero matrix</link> of size <it>n</it>&amp;times;<it>n</it>. Multiplying together stochastic matrices always yields another stochastic matrix, so <b>Q</b> must be a stochastic matrix. It is sometimes sufficient to use the matrix equation above and the fact that <b>Q</b> is a stochastic matrix to solve for <b>Q</b>.</p>
<p>

Here is one method for doing so: first, define the function <it>f</it>(<b>A</b>) to return the matrix <b>A</b> with its right-most column replaced with all 1's. Then evaluate the following equation:</p>
<p>

<indent level="1">

<math>\mathbf{Q}=f(\mathbf{0}_{n,n})[f(\mathbf{P}-\mathbf{I}_n)]^{-1}.</math>
</indent>

This equation does not work when [''f''('''P''' – '''I'''n)]–1 does not exist. If this is the case, then it is necessary to take into account more information in order to find <b>Q</b>. One thing to notice is that if <b>P</b> has an element <b>P</b><it>i</it>,<it>i</it> on its main diagonal that is equal to 1 and the <it>i</it>th row or column is otherwise filled with 0's, then that row or column will remain unchanged in all of the subsequent powers <b>P</b><it>k</it>. Hence, the <it>i</it>th row or column of <b>Q</b> will have the 1 and the 0's in the same positions as in <b>P</b>.</p>
<p>

In most cases, <b>P</b><it>k</it> approaches but never actually equals its limit. There are numerous exceptions to this, however, such as the case in which</p>
<p>

<indent level="1">

<math>\mathbf{P} = \begin{bmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}.</math>
</indent>

If <b>A</b>0 (which is a row vector) represents the starting distribution, then the stationary distribution is equal to <b>A</b>0<b>Q</b>. Note that any distribution, regardless of the number of steps it is away from the starting distribution, can be used in place of <b>A</b>0 without affecting the result for the stationary distribution.</p>

</sec>
<sec>
<st>
 Reversible Markov chain </st>

<p>

The idea of a reversible Markov chain comes from the ability to "invert" a conditional probability using <link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' Rule</link>:</p>
<p>

<indent level="1">

<math>\Pr(X_{n}=i\mid X_{n+1}=j) = \frac{\Pr(X_n = i, X_{n+1} = j)}{\Pr(X_{n+1} = j)}</math>
</indent>
::::::<math> = \frac{\Pr(X_{n} = i)\Pr(X_{n+1} = j\mid X_n=i)}{\Pr(X_{n+1} = j)}. \,</math></p>
<p>

It now appears that time has been reversed.  Thus, a Markov chain is said to be <b>reversible</b> if there is a <b>π</b> such that</p>
<p>

<indent level="1">

<math>\pi_i p_{i,j} = \pi_j p_{j,i}.\,</math>
</indent>

This condition is also known as the <link xlink:type="simple" xlink:href="../867/2212867.xml">
detailed balance</link> condition.</p>
<p>

Summing over <math>i</math> gives</p>
<p>

<indent level="1">

<math>\sum_i \pi_i p_{i,j} = \pi_j\,</math>
</indent>

so for reversible Markov chains, <b>π</b> is always a stationary distribution.</p>

</sec>
<sec>
<st>
 Bernoulli scheme </st>

<p>

A <link xlink:type="simple" xlink:href="../555/2580555.xml">
Bernoulli scheme</link> is a special case of a Markov chain where the transition probability matrix has identical rows, which means that the next state is even independent of the current state (in addition to being independent of the past states). A Bernoulli scheme with only two possible states is known as a <link xlink:type="simple" xlink:href="../651/102651.xml">
Bernoulli process</link>.</p>

</sec>
<sec>
<st>
 Markov chains with general state space </st>

<p>

Many results for Markov chains with finite state space can be generalized to chains with uncountable state space through <link xlink:type="simple" xlink:href="../126/19485126.xml">
Harris chain</link>s. The main idea is to see if there is a point in the state space that the chain hits with probability one. Generally, it is not true for continuous state space, however, we can define sets <it>A</it> and <it>B</it> along with a positive number <it>ε</it> and a probability
measure <it>ρ</it>, such that</p>
<p>

<list>
<entry level="1" type="number">

 If <math>\tau_A = \inf\{n\geq 0: X_n \in A\}</math>, then <math>P_z(\tau_A&amp;lt;\infty)&amp;gt;0</math> for all <math>z</math>.</entry>
<entry level="1" type="number">

 If <math>x \in A</math> and <math>C\subset B</math>, then<math>p(x, C)\geq \epsilon \rho(C)</math>.</entry>
</list>
</p>
<p>

Then we could collapse the sets into an auxiliary point <it>α</it>, and a recurrent <link xlink:type="simple" xlink:href="../126/19485126.xml">
Harris chain</link> can be modified to contain <it>α</it>. Lastly, the collection of <link xlink:type="simple" xlink:href="../126/19485126.xml">
Harris chain</link>s is a comfortable level of generality, which is broad enough to contain a large number of interesting examples, yet restrictive enough to allow for a rich theory.</p>

</sec>
<sec>
<st>
Applications</st>

<ss1>
<st>
Physics</st>
<p>

Markovian systems appear extensively in <link xlink:type="simple" xlink:href="../939/22939.xml">
physics</link>, particularly <link xlink:type="simple" xlink:href="../481/28481.xml">
statistical mechanics</link>, whenever probabilities are used to represent unknown or unmodelled details of the system, if it can be assumed that the dynamics are time-invariant, and that no relevant history need be considered which is not already included in the state description.</p>

</ss1>
<ss1>
<st>
Testing</st>
<p>

Several theorists have proposed the idea of the Markov chain statistical test, a method of conjoining Markov chains to form a 'Markov blanket', arranging these chains in several recursive layers ('wafering') and producing more efficient test sets — samples — as a replacement for exhaustive testing.  MCSTs also have uses in temporal state-based networks; Chilukuri et al.'s paper entitled "Temporal Uncertainty Reasoning Networks for Evidence Fusion with Applications to Object Detection and Tracking" (ScienceDirect) gives an excellent background and case study for applying MCSTs to a wider range of applications.</p>

</ss1>
<ss1>
<st>
Queueing theory</st>
<p>

Markov chains can also be used to model various processes in <link xlink:type="simple" xlink:href="../578/50578.xml">
queueing theory</link> and <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.   <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../693/5693.xml">
Claude Shannon's</link></scientist>
 famous 1948 paper <it><link xlink:type="simple" xlink:href="../061/828061.xml">
A mathematical theory of communication</link></it>, which at a single step created the field of <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link>, opens by introducing the concept of <link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link> through Markov modeling of the English language.  Such idealized models can capture many of the statistical regularities of systems.  Even without describing the full structure of the system perfectly, such signal models can make possible very effective <link xlink:type="simple" xlink:href="../013/8013.xml">
data compression</link> through <link xlink:type="simple" xlink:href="../680/46680.xml">
entropy coding</link> techniques such as <link xlink:type="simple" xlink:href="../545/62545.xml">
arithmetic coding</link>.  They also allow effective <link xlink:type="simple" xlink:href="../170/1261170.xml">
state estimation</link> and <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link>.  The world's mobile telephone systems depend on the <link xlink:type="simple" xlink:href="../015/228015.xml">
Viterbi algorithm</link> for error-correction, while <link xlink:type="simple" xlink:href="../770/98770.xml">
hidden Markov models</link> are extensively used in <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link> and also in <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>, for instance for coding region/gene prediction. Markov chains also play an important role in <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link>.</p>

</ss1>
<ss1>
<st>
Internet applications</st>
<p>

The <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../724/23724.xml">
PageRank</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 of a webpage as used by <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../923/1092923.xml">
Google</link></company>
 is defined by a Markov chain. It is the probability to be at page <math>i</math> in the stationary distribution on the following Markov chain on all (known) webpages. If <math>N</math> is the number of known webpages, and a page <math>i</math> has <math>k_i</math> links then it has transition probability  <math>\frac{1-q}{k_i} + \frac{q}{N}</math> for all pages that are linked to and <math>\frac{q}{N}</math> for all pages  that are not linked to. The parameter <math>q</math> is taken to be about 0.15.</p>
<p>

Markov models have also been used to analyze web navigation behavior of users. A user's web link transition on a particular website can be modeled using first- or second-order Markov models and can be used to make predictions regarding future navigation and to personalize the web page for an individual user.</p>

</ss1>
<ss1>
<st>
Statistical</st>
<p>

Markov chain methods have also become very important for generating sequences of random numbers to accurately reflect very complicated desired probability distributions, via a process called <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../801/236801.xml">
Markov chain Monte Carlo</link></method>
</know-how>
 (MCMC). In recent years this has revolutionised the practicability of <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian inference</link> methods, allowing a wide range of <link xlink:type="simple" xlink:href="../672/357672.xml">
posterior distribution</link>s to be simulated and their parameters found numerically.</p>

</ss1>
<ss1>
<st>
Economics</st>
<p>

Dynamic macroeconomics heavily uses Markov chain.</p>

</ss1>
<ss1>
<st>
Mathematical biology</st>

<p>

Markov chains also have many applications in biological modelling, particularly <link xlink:type="simple" xlink:href="../384/239384.xml">
population process</link>es, which are useful in modelling processes that are (at least) analogous to biological populations.  The <link xlink:type="simple" xlink:href="../816/1545816.xml">
Leslie matrix</link> is one such example, though some of its entries
are not probabilities (they may be greater than 1). Another important example is the modeling of cell shape in dividing sheets of <link xlink:type="simple" xlink:href="../641/299641.xml">
epithelial cells</link>. The distribution of shapes -- predominantly hexagonal --  was a long standing mystery until it was explained by a simple Markov Model, where a cell's state is its number of sides. Empirical evidence from frogs, fruit flies, and hydra further suggests that the stationary distribution of cell shape is exhibited by almost all multicellular animals.<weblink xlink:type="simple" xlink:href="http://www.eecs.harvard.edu/~abpatel/drosophila_wing_model.htm">
http://www.eecs.harvard.edu/~abpatel/drosophila_wing_model.htm</weblink>.  Yet another example is the state of <substance wordnetid="100019613" confidence="0.8">
<compound wordnetid="114818238" confidence="0.8">
<protein wordnetid="114728724" confidence="0.8">
<macromolecule wordnetid="114944888" confidence="0.8">
<chemical wordnetid="114806838" confidence="0.8">
<part wordnetid="113809207" confidence="0.8">
<organic_compound wordnetid="114727670" confidence="0.8">
<molecule wordnetid="114682133" confidence="0.8">
<material wordnetid="114580897" confidence="0.8">
<link xlink:type="simple" xlink:href="../303/15303.xml">
Ion channel</link></material>
</molecule>
</organic_compound>
</part>
</chemical>
</macromolecule>
</protein>
</compound>
</substance>
s in cell membranes.</p>

</ss1>
<ss1>
<st>
Gambling</st>

<p>

Markov chains can be used to model many games of chance.  The children's games <link xlink:type="simple" xlink:href="../372/38372.xml">
Snakes and Ladders</link> and <link>
"Hi Ho! Cherry-O"</link>, for example, are represented exactly by Markov chains.  At each turn, the player starts in a given state (on a given square) and from there has fixed odds of moving to certain other states (squares).</p>

</ss1>
<ss1>
<st>
Music</st>
<p>

Markov chains are employed in <link xlink:type="simple" xlink:href="../884/479884.xml">
algorithmic music composition</link>, particularly in <link xlink:type="simple" xlink:href="../309/5309.xml">
software</link> programs such as <link xlink:type="simple" xlink:href="../998/149998.xml">
CSound</link> or <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../795/479795.xml">
Max</link></software>
. In a first-order chain, the states of the system become note or pitch values, and a <link xlink:type="simple" xlink:href="../133/217133.xml">
probability vector</link> for each note is constructed, completing a transition probability matrix (see below). An algorithm is constructed to produce and output note values based on the transition matrix weightings, which could be <link xlink:type="simple" xlink:href="../461/1692461.xml">
MIDI</link> note values, frequency (<link xlink:type="simple" xlink:href="../232/14232.xml">
Hz</link>), or any other desirable metric.</p>
<p>

<table style="float: left" class="wikitable">
<caption>
1st-order matrix</caption>
<header>
Note</header>
<header>
A</header>
<header>
C#</header>
<header>
Eb</header>
<row>
<header>
A</header>
<col>
0.1</col>
<col>
0.6</col>
<col>
0.3</col>
</row>
<row>
<header>
C#</header>
<col>
0.25</col>
<col>
0.05</col>
<col>
0.7</col>
</row>
<row>
<header>
Eb</header>
<col>
0.7</col>
<col>
0.3</col>
<col>
0</col>
</row>
</table>
</p>
<p>

<table style="float: left" class="wikitable">
<caption>
2nd-order matrix</caption>
<header>
Note</header>
<header>
A</header>
<header>
D</header>
<header>
G</header>
<row>
<header>
AA</header>
<col>
0.18</col>
<col>
0.6</col>
<col>
0.22</col>
</row>
<row>
<header>
AD</header>
<col>
0.5</col>
<col>
0.5</col>
<col>
0</col>
</row>
<row>
<header>
AG</header>
<col>
0.15</col>
<col>
0.75</col>
<col>
0.1</col>
</row>
<row>
<header>
DD</header>
<col>
0</col>
<col>
0</col>
<col>
1</col>
</row>
<row>
<header>
DA</header>
<col>
0.25</col>
<col>
0</col>
<col>
0.75</col>
</row>
<row>
<header>
DG</header>
<col>
0.9</col>
<col>
0.1</col>
<col>
0</col>
</row>
<row>
<header>
GG</header>
<col>
0.4</col>
<col>
0.4</col>
<col>
0.2</col>
</row>
<row>
<header>
GA</header>
<col>
0.5</col>
<col>
0.25</col>
<col>
0.25</col>
</row>
<row>
<header>
GD</header>
<col>
1</col>
<col>
0</col>
<col>
0</col>
</row>
</table>
</p>

<p>

A second-order Markov chain can be introduced by considering the current state <it>and</it> also the previous state, as indicated in the second table. Higher, <it>n</it>th-order chains tend to "group" particular notes together, while 'breaking off' into other patterns and sequences occasionally. These higher-order chains tend to generate results with a sense of <music wordnetid="107020895" confidence="0.8">
<auditory_communication wordnetid="107109019" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<section wordnetid="106392001" confidence="0.8">
<link xlink:type="simple" xlink:href="../613/624613.xml">
phrasal</link></section>
</writing>
</written_communication>
</auditory_communication>
</music>
 structure, rather than the 'aimless wandering' produced by a first-order system<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>.</p>

</ss1>
<ss1>
<st>
Baseball</st>
<p>

Markov chains models have been used in advanced baseball analysis since 1960, although their use is still rare.  Each half-inning of a baseball game fits the Markov chain state when the number of runners and outs are considered.  For each half-inning there are 24 possible run-out combinations.  Markov chain models can be used to evaluate runs created for both individual players as well as a team. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.</p>

</ss1>
<ss1>
<st>
 Markov text generators </st>
<p>

Markov processes can also be used to generate superficially "real-looking" text given a sample document: they are used in a variety of recreational "parody generator" software (see <link xlink:type="simple" xlink:href="../104/2284104.xml">
dissociated press</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<poet wordnetid="110444194" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<link xlink:type="simple" xlink:href="../665/73665.xml">
Jeff Harrison</link></writer>
</causal_agent>
</poet>
</person>
</communicator>
</physical_entity>
, <personality wordnetid="104617562" confidence="0.8">
<link xlink:type="simple" xlink:href="../091/174091.xml">
Mark V Shaney</link></personality>
,
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>                                                                                                                                                              <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>          
).</p>

</ss1>
</sec>
<sec>
<st>
 History </st>
<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../915/915.xml">
Andrey Markov</link></scientist>
</person>
 produced the first results (1906) for these processes, purely theoretically.
A generalization to countably infinite state spaces was given by <link xlink:type="simple" xlink:href="../161/91161.xml">
Kolmogorov</link> (1936).
Markov chains are related to <link xlink:type="simple" xlink:href="../436/4436.xml">
Brownian motion</link> and the <link xlink:type="simple" xlink:href="../980/258980.xml">
ergodic hypothesis</link>, two topics in physics which were important in the early years of the twentieth century, but Markov appears to have pursued this out of a mathematical motivation, namely the extension of the <link xlink:type="simple" xlink:href="../055/157055.xml">
law of large numbers</link> to dependent events. In 1913, he applied his findings for the first time, namely, to the first 20,000 letters of Pushkin's "Eugene Onegin".</p>

</sec>
<sec>
<st>
See also</st>

<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../770/98770.xml">
Hidden Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../196/195196.xml">
Examples of Markov chains</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../772/98772.xml">
Markov process</link></entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../029/17878029.xml">
Markov information source</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../801/236801.xml">
Markov chain Monte Carlo</link></method>
</know-how>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../736/1598736.xml">
Semi-Markov process</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../999/10770999.xml">
Variable-order Markov model</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../883/1125883.xml">
Markov decision process</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../265/1986265.xml">
Shift of finite type</link></entry>
<entry level="1" type="bullet">

 <personality wordnetid="104617562" confidence="0.8">
<link xlink:type="simple" xlink:href="../091/174091.xml">
Mark V Shaney</link></personality>
</entry>
<entry level="1" type="bullet">

 <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../151/3513151.xml">
Phase-type distribution</link></distribution>
</arrangement>
</structure>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../938/6101938.xml">
Markov chain mixing time</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../780/7941780.xml">
Quantum Markov chain</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../985/1323985.xml">
Markov network</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../010/800010.xml">
Belief propagation</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../186/2016186.xml">
Factor graph</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../013/12069013.xml">
Recurrence period density entropy</link></entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../530/3509530.xml">
Sequential analysis</link></method>
</know-how>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../160/1802160.xml">
Markov chain geostatistics</link></entry>
</list>
</p>


</sec>
<sec>
<st>
 References </st>


<p>

<reflist>
<entry id="1">
 S. P. Meyn and R.L. Tweedie, 2005.  <weblink xlink:type="simple" xlink:href="http://decision.csl.uiuc.edu/~meyn/pages/book.html">
Markov Chains and Stochastic Stability</weblink>. 
Second edition to appear, Cambridge University Press, 2008.  </entry>
<entry id="2">
 S. P. Meyn, 2007.  <weblink xlink:type="simple" xlink:href="http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html">
Control Techniques for Complex Networks</weblink>, Cambridge University Press, 2007.  </entry>
<entry id="3">
Seneta, E. (1996)  Markov and the Birth of Chain Dependence Theory.  International Statistical Review, 64(3), 255%ndash;263</entry>
<entry id="4">
 <cite style="font-style:normal" class="book">Curtis Roads (ed.)&#32;(1996). The Computer Music Tutorial.&#32;MIT Press. ISBN 0262181584.</cite>&nbsp;</entry>
<entry id="5">
 Pankin, Mark D..&#32;"<weblink xlink:type="simple" xlink:href="http://www.pankin.com/markov/theory.htm">
MARKOV CHAIN MODELS: THEORETICAL BACKGROUND</weblink>".&#32;Retrieved on <link>
2007-11-26</link>.
</entry>
<entry id="6">
                                                             
 <cite id="CITEREFKennerO.27Rourke1984" style="font-style:normal">Kenner, Hugh&#32;&amp;&#32;<link>
O'Rourke, Joseph</link>&#32;(November 1984),&#32;"A Travesty Generator for Micros",&#32;<it>BYTE</it>&#32;<b>9</b>(12):  129–131, 449–469</cite>&nbsp;                                                   
</entry>
<entry id="7">
                                                              
 <cite id="CITEREFHartman1996" style="font-style:normal">Hartman, Charles&#32;(1996),&#32;<it>The Virtual Muse: Experiments in Computer Poetry</it>, Hanover, NH: Wesleyan University Press, ISBN 0819522392</cite>&nbsp;                                                        
</entry>
</reflist>
</p>

<p>

<list>
<entry level="1" type="bullet">

  A.A. Markov. "Rasprostranenie zakona bol'shih chisel na velichiny, zavisyaschie drug ot druga". <it>Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete</it>, 2-ya seriya, tom 15, pp 135-156, 1906. </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 A.A. Markov. "Extension of the limit theorems of probability theory to a sum of variables connected in a chain". reprinted in Appendix B of: R. Howard. <it>Dynamic Probabilistic Systems, volume 1: Markov Chains</it>. John Wiley and Sons, 1971. </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Classical Text in Translation: A. A. Markov, An Example of Statistical Investigation of the Text Eugene Onegin Concerning the Connection of Samples in Chains, trans. David Link. Science in Context 19.4 (2006): 591-600. Online: http://journals.cambridge.org/production/action/cjoGetFulltext?fulltextid=637500</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Leo Breiman. <it>Probability</it>. Original edition published by Addison-Wesley, 1968; reprinted by Society for Industrial and Applied Mathematics, 1992. ISBN 0-89871-296-3. <it>(See Chapter 7.)''</it></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 J.L. Doob. <it>Stochastic Processes</it>. New York: John Wiley and Sons, 1953.  ISBN 0-471-52369-0.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 S. P. Meyn and R. L. Tweedie. <it>Markov Chains and Stochastic Stability</it>. London: Springer-Verlag, 1993.  ISBN 0-387-19832-6. online: http://decision.csl.uiuc.edu/~meyn/pages/book.html .  Second edition to appear, Cambridge University Press, 2008.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 S. P. Meyn. <it>Control Techniques for Complex Networks</it>. Cambridge University Press, 2007. ISBN-13: 9780521884419.  Appendix contains abridged Meyn &amp; Tweedie.  online:    http://decision.csl.uiuc.edu/~meyn/pages/CTCN/CTCN.html</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="Reference-Booth-1967" style="font-style:normal" class="book">Booth, Taylor L.&#32;(1967). Sequential Machines and Automata Theory, 1st,&#32;New York:&#32;John Wiley and Sons, Inc.. Library of Congress Card Catalog Number 67-25924.</cite>&nbsp; Extensive, wide-ranging book meant for specialists, written for both theoretical computer scientists as well as electrical engineers. With detailed explanations of state minimization techniques, FSMs, Turing machines, Markov processes, and undecidability. Excellent treatment of Markov processes pp.449ff. Discusses Z-transforms, D transforms in their context.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <cite id="Reference-Kemeny-1959" style="font-style:normal" class="book">Kemeny, John G.;&#32;Hazleton Mirkil, J. Laurie Snell, Gerald L. Thompson&#32;(1959). Finite Mathematical Structures, 1st,&#32;Englewood Cliffs, N.J.:&#32;Prentice-Hall, Inc.. Library of Congress Card Catalog Number 59-12841.</cite>&nbsp; Classical text. cf Chapter 6 <it>Finite Markov Chains</it> pp.384ff.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Nummelin. "General irreducible Markov chains and non-negative operators". Cambridge University Press, 1984, 2004. ISBN 0-521-60494-X</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf">
(pdf) Markov Chains chapter in American Mathematical Society's introductory probability book</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.utilitymill.com/utility/Markov_Chain_Parody_Text_Generator">
Generates random parodies in the style of another body of text using a Markov chain algorithm</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.fourteenminutes.com/fun/words/">
A generator that uses Markov Chains to create random words</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://crypto.mat.sbg.ac.at/~ste/diss/node6.html">
Markov Chains</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://planetmath.org/?op=getobj&amp;from=objects&amp;id=5765">
Class structure</weblink> on <web_site wordnetid="106359193" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../623/161623.xml">
PlanetMath</link></web_site>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.math.rutgers.edu/courses/338/coursenotes/chapter5.pdf">
Chapter 5: Markov Chain Models</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.bell-labs.com/cm/cs/pearls/sec153.html">
Generating Text</weblink> <it>(About generating random text using a Markov chain.)''</it></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mathworks.com/company/newsletters/news_notes/clevescorner/oct02_cleve.html">
The World's Largest Matrix Computation</weblink> <it>(Google's PageRank as the stationary distribution of a random walk through the Web.)''</it></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.gnu.org/software/emacs/manual/html_node/emacs/Dissociated-Press.html">
Dissociated Press</weblink> in <skilled_worker wordnetid="110605985" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<editor wordnetid="110044879" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../234/18933234.xml">
Emacs</link></causal_agent>
</worker>
</editor>
</person>
</physical_entity>
</skilled_worker>
 approximates a Markov process</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.vanguardsw.com/DpHelp4/dph00147.htm">
Markov Chain Example</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://en.kerouac3001.com/markov-chains-spam-that-search-engines-like-pt-1-5.htm">
Markov Chains for Search Engines</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.zentastic.com/entries/200503031618.html">
Steganography proof-of-concept using Markov Chains.</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.codeodor.com/index.cfm/2007/11/7/Fun-With-Markov-Models/1701">
nth order Markov Chain implementation in Ruby</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.hardballtimes.com/main/article/introducing-markov-chains/">
Baseball Run Modeler using Markov Chains</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.pankin.com/markov/theory.htm">
Theory of Markov chains in baseball</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://garnet.fsu.edu/~ajeong/DAT/index.htm">
Sequential analysis software for generating visual representations of probability models</weblink></entry>
</list>
</p>


</sec>
</bdy>
</causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</article>
