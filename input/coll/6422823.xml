<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:25:30[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<artifact  confidence="0.8" wordnetid="100021939">
<instrumentality  confidence="0.8" wordnetid="103575240">
<engine  confidence="0.8" wordnetid="103287733">
<motor  confidence="0.8" wordnetid="103789946">
<device  confidence="0.8" wordnetid="103183080">
<machine  confidence="0.8" wordnetid="103699975">
<header>
<title>Search engine technology</title>
<id>6422823</id>
<revision>
<id>174587508</id>
<timestamp>2007-11-29T13:09:57Z</timestamp>
<contributor>
<username>SmackBot</username>
<id>433328</id>
</contributor>
</revision>
<categories>
<category>Internet search engines</category>
<category>Information retrieval</category>
</categories>
</header>
<bdy>

Modern web search engines are complex software systems using the technology that has evolved over the years. The largest search engines such as <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../923/1092923.xml">
Google</link></company>
 and <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../213/188213.xml">
Yahoo!</link></company>
 utilize tens or hundreds of thousands of computers to process billions of web pages and return results for thousands of searches per second. High volume of queries and text processing requires the software to run in highly distributed environment with high degree of redundancy. Modern search engines have the following main components:
<sec>
<st>
 Crawl </st>

<p>

The first step in preparing <link xlink:type="simple" xlink:href="../063/34063.xml">
web page</link>s for search is to find and index them. In the past, search engines started with a small list of <link xlink:type="simple" xlink:href="../277/32277.xml">
URL</link>s as seed list, fetched the content, <link xlink:type="simple" xlink:href="../015/310015.xml">
parsed</link> for the <link xlink:type="simple" xlink:href="../547/49547.xml">
link</link>s on those pages, fetched the web pages pointed to by those links which provided new links and the cycle continued until enough pages were found. Most modern search engines now utilize a continuous <link xlink:type="simple" xlink:href="../120/33120.xml">
crawl</link> method rather than <b>discovery</b> based on a seed list. The continuous crawl method is just an extension of discovery method but there is no seed list because the crawl never stops. The current list of pages is visited on regular intervals and new pages are found when links are added or deleted from those pages. Many search engines use sophisticated scheduling algorithms to decide when to revisit a particular page. These algorithms range from constant visit-interval with higher priority for more frequently changing pages to adaptive visit-interval based on several criteria such as frequency of change, popularity and overall quality of site, speed of web server serving the page and resource constraints like amount of hardware and bandwidth of Internet connection. Search engines crawl many more pages than they make available for searching because crawler find lots duplicate content pages on the web and many pages don't have useful content. Duplicate and useless content often represents more than half the pages available for indexing.</p>

</sec>
<sec>
<st>
 Link Map </st>

<p>

Pages discovered by crawlers are fed into (often distributed) service that creates a <b>link map</b> of the pages. Link map is a <link xlink:type="simple" xlink:href="../931/557931.xml">
graph structure</link> in which pages are represented as <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<link xlink:type="simple" xlink:href="../074/998074.xml">
node</link></artifact>
</structure>
s connected by the links among those pages. This data is stored in <link xlink:type="simple" xlink:href="../519/8519.xml">
data structure</link>s that allow fast access to the data by certain <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>s which compute the popularity score of pages on the web, essentially based on how many links point to a web page and the quality of those links. One such algorithm, <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../724/23724.xml">
PageRank</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, proposed by Google founders <celebrity wordnetid="109903153" confidence="0.9508927676800064">
<person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../903/60903.xml">
Larry Page</link></person>
</celebrity>
 and <entrepreneur wordnetid="110060352" confidence="0.9173553029164789">
<celebrity wordnetid="109903153" confidence="0.9508927676800064">
<person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../904/60904.xml">
Sergey Brin</link></person>
</celebrity>
</entrepreneur>
, is well known and has attracted a lot of attention. The idea of doing link analysis to compute a popularity rank is older than PageRank and many variants of the same idea are currently in use. These ideas can be categorized in three main categories:  rank of individual pages, rank of web sites, and nature of web site content (<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../312/1280312.xml">
Jon Kleinberg</link></scientist>
's <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../223/1851223.xml">
HITS</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 algorithm). Search engines often differentiate between <link xlink:type="simple" xlink:href="../876/1176876.xml">
internal link</link>s and <link xlink:type="simple" xlink:href="../876/1176876.xml">
external link</link>s, with the assumption that links on a page pointing other pages on the same site are less valuable because they are often created by web site owners to artificially increase the rank of their web sites and pages. Link map data structures typically also store the <link xlink:type="simple" xlink:href="../393/274393.xml#xpointer(//*[./st=%22Links+and+anchors%22])">
anchor</link> text embedded in the links because anchor text often provides a very good quality short-summary of a web page's content.</p>

</sec>
<sec>
<st>
 Index </st>

<p>

<link xlink:type="simple" xlink:href="../386/7602386.xml">
Indexing</link> is the process of extracting text from web pages, tokenizing it and then creating an index structure (<link xlink:type="simple" xlink:href="../116/3125116.xml">
inverted index</link>) that can be used to quickly find which pages contain a particular word. Search engines differ quite a lot in tokenization process. The issues involved in tokenization are:  detecting the encoding used for the page, determining the language of the content (some pages use multiple languages), finding word, sentence and paragraph boundaries, combining multiple adjacent-words into one phrase and changing the case of text and <link xlink:type="simple" xlink:href="../964/475964.xml">
stemming</link> the words into their roots (lower-casing and stemming is applicable only to some languages). This phase also decides which sections of page to index and how much text from very large pages (such as technical manuals) to index. Search engines also differ in the document formats they interpret and extract the text from.</p>
<p>

Some search engines go through the indexing process every few weeks and refresh the complete index used for web search requests while others keep updating small fragments of the index continuously. Before web pages can be indexed, an algorithm decides which node (a server in a distributed service) will index any given page and makes the information available as <link xlink:type="simple" xlink:href="../632/18933632.xml">
metadata</link> for other components in the search engine. The index structure is complex and typically employs some compression algorithm. The choice of compression algorithm involves a trade-off between on-disk storage space and speed of decompression when needed to satisfy search requests. The largest search engines use thousands of computers to index pages in parallel.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../023/4059023.xml">
Search engine</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../120/33120.xml">
Web crawler</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../386/7602386.xml">
Search engine indexing</link></entry>
</list>
</p>

</sec>
</bdy>
</machine>
</device>
</motor>
</engine>
</instrumentality>
</artifact>
</article>
