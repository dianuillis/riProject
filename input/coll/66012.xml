<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:49:29[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<work  confidence="0.8" wordnetid="100575741">
<contradiction  confidence="0.8" wordnetid="107206887">
<scientific_research  confidence="0.8" wordnetid="100641820">
<falsehood  confidence="0.8" wordnetid="106756407">
<research  confidence="0.8" wordnetid="100636921">
<message  confidence="0.8" wordnetid="106598915">
<statement  confidence="0.8" wordnetid="106722453">
<paradox  confidence="0.8" wordnetid="106724559">
<event  confidence="0.8" wordnetid="100029378">
<experiment  confidence="0.8" wordnetid="100639556">
<investigation  confidence="0.8" wordnetid="100633864">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<activity  confidence="0.8" wordnetid="100407535">
<header>
<title>Newcomb&apos;s paradox</title>
<id>66012</id>
<revision>
<id>244015312</id>
<timestamp>2008-10-08T23:11:56Z</timestamp>
<contributor>
<username>McKay</username>
<id>19640</id>
</contributor>
</revision>
<categories>
<category> Mathematics paradoxes</category>
<category>Thought experiments</category>
<category>Paradoxes</category>
</categories>
</header>
<bdy>

<b>Newcomb's Paradox</b>, also referred to as <b>Newcomb's Problem</b>, is a <link xlink:type="simple" xlink:href="../535/49535.xml">
thought experiment</link> involving a game between two players, one of whom purports to be able to predict the future. Whether the problem is actually a <link xlink:type="simple" xlink:href="../390/24390.xml">
paradox</link> is disputed.<p>

Newcomb's paradox was created by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../327/536327.xml">
William Newcomb</link></educator>
</professional>
</adult>
</academician>
</causal_agent>
</person>
</physical_entity>
 of the <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../921/31921.xml">
University of California</link></university>
's <link xlink:type="simple" xlink:href="../039/39039.xml">
Lawrence Livermore Laboratory</link>. However, it was first analyzed and  was published in a philosophy paper spread to the philosophical community by <philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../275/26275.xml">
Robert Nozick</link></philosopher>
 in 1969, and appeared in <person wordnetid="100007846" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../435/20435.xml">
Martin Gardner</link></person>
's <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../507/29507.xml">
Scientific American</link></periodical>
</it> column in 1974.  Today it is a much debated problem in the philosophical branch of decision theory but has received little attention from the mathematical side.</p>

<sec>
<st>
The problem</st>
<p>

A person is playing a game operated by <it>the Predictor</it>, an entity somehow presented as being exceptionally skilled at predicting people's actions. The exact nature of the Predictor varies between retellings of the paradox.  Some assume that the character always has a reputation for being completely infallible and incapable of error. The Predictor can be presented as a <link xlink:type="simple" xlink:href="../323/58323.xml">
psychic</link>, as a superintelligent alien, as a <link xlink:type="simple" xlink:href="../438/39438.xml">
deity</link>, etc.  However, the original discussion by Nozick says only that the Predictor's predictions are "almost certainly" correct, and also specifies that "what you actually decide to do is not part of the explanation of why he made the prediction he made". With this original version of the problem, some of the discussion below is inapplicable.</p>
<p>

The player of the game is presented with two opaque boxes, labeled A and B. The player is permitted to take the contents of both boxes, or just of box B. (The option of taking only box A is ignored, for reasons soon to be obvious.) Box A contains $1,000. The contents of box B, however, are determined as follows: At some point before the start of the game, the Predictor makes a prediction as to whether the player of the game will take just box B, or both boxes. If the Predictor predicts that both boxes will be taken, then box B will contain nothing. If the Predictor predicts that only box B will be taken, then box B will contain $1,000,000.</p>
<p>

By the time the game begins, and the player is called upon to choose which boxes to take, the prediction has already been made, and the contents of box B have already been determined. That is, box B contains either $0 or $1,000,000 before the game begins, and once the game begins even the Predictor is powerless to change the contents of the boxes. Before the game begins, the player is aware of all the rules of the game, including the two possible contents of box B, the fact that its contents are based on the Predictor's prediction, and knowledge of the Predictor's infallibility. The only information withheld from the player is what prediction the Predictor made, and thus what the contents of box B are.</p>
<p>

<table style="float:left" class="wikitable">
<header>
Predicted choice</header>
<header>
Actual choice</header>
<header>
Payout</header>
<row>
<col>
A and B</col>
<col>
A and B</col>
<col>
$1,000</col>
</row>
<row>
<col>
A and B</col>
<col>
B only</col>
<col>
$0</col>
</row>
<row>
<col>
B only</col>
<col>
A and B</col>
<col>
$1,001,000</col>
</row>
<row>
<col>
B only</col>
<col>
B only</col>
<col>
$1,000,000</col>
</row>
</table>

The problem is called a <it>paradox</it> because two strategies that both sound intuitively logical give conflicting answers to the question of what choice maximizes the player's payout. The first strategy argues that, regardless of what prediction the Predictor has made, taking both boxes yields more money. That is, if the prediction is for both A and B to be taken, then the player's decision becomes a matter of choosing between $1,000 (by taking A and B) and $0 (by taking just B), in which case taking both boxes is obviously preferable. But, even if the prediction is for the player to take only B, then taking both boxes yields $1,001,000, and taking only B yields only $1,000,000&mdash;the difference is comparatively slight in the latter case, but taking both boxes is still better, regardless of which prediction has been made.</p>
<p>

The second strategy suggests taking only B. By this strategy, we can ignore the possibilities that return $0 and $1,001,000, as they both require that the Predictor has made an incorrect prediction, and the problem states that the Predictor is almost never wrong. Thus, the choice becomes whether to receive $1,000 (both boxes) or to receive $1,000,000 (only box B)&mdash;so taking only box B is better.</p>
<p>

In his 1969 article, Nozick noted that "To almost everyone, it is perfectly clear and obvious what should be done. The difficulty is that these people seem to divide almost evenly on the problem, with large numbers thinking that the opposing half is just being silly."</p>

</sec>
<sec>
<st>
Attempted resolutions</st>

<p>

Many argue that the paradox is primarily a matter of conflicting decision making models.  Using the <link xlink:type="simple" xlink:href="../803/736803.xml">
expected utility hypothesis</link> will lead one to believe that one should expect the most <link xlink:type="simple" xlink:href="../479/45479.xml">
utility</link> (or money) from taking only box B.  However if one uses the <link xlink:type="simple" xlink:href="../847/2352847.xml">
Dominance</link> principle, one would expect to benefit most from taking both boxes. </p>
<p>

Some argue that Newcomb's Problem is a <link xlink:type="simple" xlink:href="../390/24390.xml">
paradox</link> because it leads logically to self-contradiction. <work wordnetid="100575741" confidence="0.8">
<scientific_research wordnetid="100641820" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<experiment wordnetid="100639556" confidence="0.8">
<investigation wordnetid="100633864" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<research wordnetid="100636921" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../128/7506128.xml">
Reverse causation</link></activity>
</psychological_feature>
</research>
</act>
</investigation>
</experiment>
</event>
</scientific_research>
</work>
 is defined into the problem and therefore logically there can be no <link xlink:type="simple" xlink:href="../921/47921.xml">
free will</link>. However, <it>free will</it> is also defined in the problem; otherwise the chooser is not really making a choice. </p>
<p>

Other philosophers have proposed many solutions to the problem, many eliminating its seemingly paradoxical nature:</p>
<p>

Some suggest a rational person will choose both boxes, and an irrational person will choose just the one, therefore rational people fare better, since the Predictor cannot actually exist. Others have suggested that an irrational person will do better than a rational person and interpret this paradox as showing how people can be punished for making rational decisions. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>
<p>

It is possible to create a predictor similar to that proposed in the problem through the use of memory-blocking drugs like <link xlink:type="simple" xlink:href="../748/646748.xml">
Versed</link>.  Under such drugs, subjects are unable to lay down new memories, so it would be possible to run a subject through the problem a large number of times, producing for many subjects a highly accurate prediction of what they will do the next iteration, though today's drugs would generate a different mental state between the drugged trials and non-drugged "real" experiment.  This technique would fail with subjects who decide to deliberately act randomly in their response.</p>

<p>

Others have suggested that in a world with perfect predictors (or <link xlink:type="simple" xlink:href="../591/31591.xml">
time machine</link>s because a time machine could be the mechanism for making the prediction) causation can go backwards. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> If a person truly knows the future, and that knowledge affects his actions, then events in the future will be causing effects in the past. Chooser's choice will have already <it>caused</it> Predictor's action. Some have concluded that if time machines or perfect predictors can exist, then there can be no <link xlink:type="simple" xlink:href="../921/47921.xml">
free will</link> and Chooser will do whatever he's fated to do. Others conclude that the paradox shows that it is impossible to ever know the future. Taken together, the paradox is a restatement of the old contention that free will and <link xlink:type="simple" xlink:href="../922/47922.xml">
determinism</link> are incompatible, since determinism enables the existence of perfect predictors. Some philosophers argue this paradox is equivalent to the <link xlink:type="simple" xlink:href="../990/45990.xml">
grandfather paradox</link>.  Put another way, the paradox presupposes a perfect predictor, implying the "chooser" is not free to choose, yet simultaneously presumes a choice can be debated and decided. This suggests to some that the paradox is an artifact of these contradictory assumptions.
Note, however, that Nozick's exposition specifically excludes backward causation (such as time travel) and requires only that the predictions be of high accuracy, not that they are absolutely certain to be correct.  So the considerations just discussed are irrelevant to the paradox as seen by Nozick, which focuses on two principles of choice, one probabilistic and the other causal - assuming backward causation removes any conflict between these two principles.</p>
<p>

Newcomb's paradox can also be related to the question of machine consciousness, specifically if a perfect simulation of a person's brain will generate the consciousness of that person. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> Suppose we take the Predictor to be a machine that arrives at its prediction by simulating the brain of the Chooser when confronted with the problem of which box to choose. If that simulation generates the consciousness of the Chooser, then the Chooser cannot tell if he is standing in front of the boxes in the real world or in the virtual world generated by the simulation. The "virtual" Chooser would thus tell the Predictor which choice the "real" Chooser is going to make.</p>

</sec>
<sec>
<st>
See also</st>

<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../216/1937216.xml">
Kavka's toxin puzzle</link></entry>
</list>
</p>

</sec>
<sec>
<st>
Notes</st>

<p>

<reflist>
<entry id="1">
 <weblink xlink:type="simple" xlink:href="http://www.jstor.org/view/00294624/di982851/98p0133t/0">
Lewis, David (1981), "Why Ain'cha Rich?" Princeton University</weblink></entry>
<entry id="2">
<weblink xlink:type="simple" xlink:href="http://www.leaderu.com/offices/billcraig/docs/tachyons.html">
William, Craig (1988), "Tachyons, Time Travel, and Divine Omniscience." The Journal of Philosophy</weblink></entry>
<entry id="3">
R. M. Neal, <it>Puzzles of Anthropic Reasoning Resolved Using Full Non-indexical Conditioning</it>, <weblink xlink:type="simple" xlink:href="http://arxiv.org/abs/math.ST/0608592">
preprint</weblink></entry>
</reflist>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

Nozick, Robert (1969), "Newcomb's Problem and Two principles of Choice," in Essays in Honor of Carl G. Hempel, ed. Nicholas Rescher, Synthese Library (Dordrecht, the Netherlands: D. Reidel), p 115.</entry>
<entry level="1" type="bullet">

Bar-Hillel, Maya &amp; Margalit, Avishai (1972), <it>Newcomb's paradox revisited</it>. British Journal of Philosophy of Science, 23, 295-304.</entry>
<entry level="1" type="bullet">

Gardner, Martin (1974), "Mathematical Games," <it>Scientific American</it>, March 1974, p. 102; reprinted with an addendum and annotated bibliography in his book <it>The Colossal Book of Mathematics</it> (ISBN 0-393-02023-1)</entry>
<entry level="1" type="bullet">

Campbell, Richmond  and Lanning Sowden, ed. (1985), <it>Paradoxes of Rationality and Cooperation: Prisoners' Dilemma and Newcomb's Problem</it>, Vancouver: University of British Columbia Press. (an anthology discussing Newcomb's Problem, with an extensive bibliography)</entry>
<entry level="1" type="bullet">

Levi, Isaac (1982), "A Note on Newcombmania," <it>Journal of Philosophy</it> 79 (1982): 337-42. (a paper discussing the popularity of Newcomb's Problem)</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://collins.philo.columbia.edu/econphil/newcomb.pdf">
John Collins, "Newcomb's Problem", International Encyclopedia of the Social and Behavioral Sciences, Neil Smelser and Paul Baltes (eds), Elsevier Science (2001)</weblink> (Requires proper credentials)</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://members.aol.com/kiekeben/newcomb.html">
Newcomb's Paradox</weblink> by Franz Kiekeben</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.slate.com/?id=2061419">
Thinking Inside the Boxes</weblink> by Jim Holt, for <it><link xlink:type="simple" xlink:href="../214/44214.xml">
Slate</link>''</it></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://w3.ub.uni-konstanz.de/kops/volltexte/2000/524/">
Newcomb's Problem</weblink> by Marion Ledwig</entry>
</list>
</p>


</sec>
</bdy>
</activity>
</psychological_feature>
</act>
</investigation>
</experiment>
</event>
</paradox>
</statement>
</message>
</research>
</falsehood>
</scientific_research>
</contradiction>
</work>
</article>
