<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:49:21[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Reinforcement learning</title>
<id>66294</id>
<revision>
<id>243366754</id>
<timestamp>2008-10-06T05:33:54Z</timestamp>
<contributor>
<username>Fredbauder</username>
<id>744</id>
</contributor>
</revision>
<categories>
<category>All articles with dead external links</category>
<category>Articles with invalid date parameter in template</category>
<category>Articles with dead external links since June 2008</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

 <p>

For reinforcement learning in psychology, see <link xlink:type="simple" xlink:href="../960/211960.xml">
Reinforcement</link>.</p>
<p>

Inspired by related psychological theory, in <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>, <b>reinforcement learning</b> is a sub-area of <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> concerned with how an <it>agent</it> ought to take <it>actions</it> in an <it>environment</it> so as to maximize some notion of long-term <it>reward</it>.  Reinforcement learning algorithms attempt to find a <it>policy</it> that maps <it>states</it> of the world to the actions the agent ought to take in those states.  In <link xlink:type="simple" xlink:href="../223/9223.xml">
economics</link> and <link xlink:type="simple" xlink:href="../924/11924.xml">
game theory</link>, reinforcement learning is considered as a <link xlink:type="simple" xlink:href="../400/70400.xml">
boundedly rational</link> interpretation of how <link xlink:type="simple" xlink:href="../425/37425.xml">
equilibrium</link> may arise.</p>
<p>

The environment is typically formulated as a finite-state <link xlink:type="simple" xlink:href="../883/1125883.xml">
Markov decision process</link> (MDP), and reinforcement learning algorithms for this context are highly related to <link xlink:type="simple" xlink:href="../297/125297.xml">
dynamic programming</link> techniques. State transition probabilities and reward probabilities in the MDP are typically stochastic but stationary over the course of the problem.</p>
<p>

Reinforcement learning differs from the <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link> problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been mostly studied through the <link xlink:type="simple" xlink:href="../828/2854828.xml">
multi-armed bandit</link> problem.</p>
<p>

Formally, the basic reinforcement learning model consists of:</p>
<p>

<list>
<entry level="1" type="number">

 a set of environment states <math>S</math>;</entry>
<entry level="1" type="number">

 a set of actions <math>A</math>; and</entry>
<entry level="1" type="number">

 a set of scalar "rewards" in <math> \Bbb{R}</math>.</entry>
</list>
</p>
<p>

At each time <math>t</math>, the agent perceives its state <math>s_t \in S</math> and the set of possible actions <math>A(s_t)</math>. It chooses an action <math>a \in A(s_t)</math> and receives from the environment the new state <math>s_{t+1}</math> and a reward <math>r_{t+1}</math>. Based on these interactions, the reinforcement learning agent must develop a policy <math>\pi:S\rightarrow A</math> which maximizes the quantity <math>R=r_0 + r_1 + \cdots + r_n</math> for MDPs which have a terminal state, or the quantity 
<math> R = \sum_t \gamma^t r_t </math> 
for MDPs without terminal states (where <math>0\leq\gamma\leq1</math> is some "future reward" discounting factor).</p>
<p>

Thus, reinforcement learning is particularly well suited to problems which include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including <link xlink:type="simple" xlink:href="../885/175885.xml">
robot control</link>, elevator scheduling, <link xlink:type="simple" xlink:href="../322/30322.xml">
telecommunications</link>, <link xlink:type="simple" xlink:href="../329/4329.xml">
backgammon</link> and <link xlink:type="simple" xlink:href="../134/5134.xml">
chess</link> (<link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22refSutton1998%22])">
Sutton 1998</link>, Chapter 11).</p>

<sec>
<st>
 Algorithms </st>

<p>

After we have defined an appropriate return function to be maximized, we need to specify the algorithm that will be used to find the policy with the maximum return. </p>
<p>

The naive brute force approach entails the following two steps: a) For each possible policy, sample returns while following it. b) Choose the policy with the largest expected return. One problem with this is that the number of policies can be extremely large, or even infinite. Another is that returns might be stochastic, in which case a large number of samples will be required to accurately estimate the return of each policy.  
These problems can be ameliorated if we assume some structure and perhaps allow samples generated from one policy to influence the estimates made for another. The two main approaches for achieving this are value function estimation and direct policy optimization.</p>
<p>

Value function approaches do this by only maintaining a set of estimates of expected returns for one policy <math>\pi</math> (usually either the current or the optimal one).
In such approaches one attempts to estimate either the expected return starting from state 
<math>s</math> and following <math>\pi</math> thereafter, </p>
<p>

<indent level="1">

<math>V(s) = E[R|s,\pi]</math>,
</indent>

or the expected return when taking action <math>a</math> in state <math>s</math> and following <math>\pi</math>; thereafter, </p>
<p>

<indent level="1">

<math>Q(s,a) = E[R|s,\pi,a]</math>
</indent>

If someone gives us <math>Q</math> for the optimal policy, we can always choose optimal actions by simply choosing the action with the highest value at each state. In order to do this using <math>V</math>, we must either have a model of the environment, in the form of probabilities <math>P(s'|s,a)</math>, which allow us to calculate <math>Q</math> simply through </p>
<p>

<indent level="1">

<math>Q(s,a) = \sum_{s'} V(s')P(s'|s,a),</math>
</indent>

or we can employ so-called Actor-Critic methods, in which the model is split into two parts: the critic, which maintains the state value estimate <math>V</math>, and the actor, which is responsible for choosing the appropriate actions at each state.</p>
<p>

Given a fixed policy <math>\pi</math>, Estimating <math>E[R|\cdot]</math> for <math>\gamma=0</math> is trivial, as one only has to average the immediate rewards. The most obvious way to do this for <math>\gamma&amp;gt;0</math> is to average the total return after each state. However this type of Monte Carlo sampling requires the MDP to terminate.</p>
<p>

Thus carrying out this estimation for <math>\gamma&amp;gt;0</math> in the general does not seem obvious. In fact, it is quite simple once one realises that the expectation of <math>R</math> forms a recursive <link xlink:type="simple" xlink:href="../458/1236458.xml">
Bellman equation</link>:</p>
<p>

<math>E[R|s_t] = r_t + \gamma E[R|s_{t+1}]</math></p>
<p>

By replacing those expectations with our estimates, <math>V</math>, and performing <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link> with a squared error cost function, we obtain the <link xlink:type="simple" xlink:href="../759/1209759.xml">
temporal difference learning</link> algorithm TD(0). In the simplest case, the set of states and actions are both discrete and we maintain tabular estimates for each state. Similar state-action pair methods are Adaptive Heuristic Critic(AHC), <link xlink:type="simple" xlink:href="../297/10584297.xml">
SARSA</link> and <link xlink:type="simple" xlink:href="../850/1281850.xml">
Q-Learning</link>. All methods feature extensions whereby some approximating architecture is used, though in some cases <link xlink:type="simple" xlink:href="../779/349779.xml">
convergence</link> is not guaranteed. The estimates are usually updated with some form of gradient descent, though there have been recent developments with <link xlink:type="simple" xlink:href="../359/82359.xml">
least squares</link> methods for the linear approximation case.</p>
<p>

The above methods not only all converge to the correct estimates for a fixed policy, but can also be used to find the optimal policy. This is usually done by following a policy π that is somehow derived from the current value estimates, i.e. by choosing the action with the highest evaluation most of the time, while still occasionally taking random actions in order to explore the space. Proofs for convergence to the optimal policy also exist for the algorithms mentioned above, under certain conditions. However, all those proofs only demonstrate asymptotic convergence and little is known theoretically about the behaviour of RL algorithms in the small-sample case, apart from within very restricted settings.</p>
<p>

An alternative method to find the optimal policy is to search directly in policy space. Policy space methods define the policy as a parameterised function <math>\pi(s,\theta)</math> with parameters <math>\theta</math>. Commonly, a gradient method is employed to adjust the parameters. However, the application of gradient methods is not trivial, since no gradient information is assumed. Rather, the gradient itself must be estimated from noisy samples of the return. Since this greatly increases the computational cost, it can be advantageous to use a more powerful gradient method than steepest gradient descent.  Policy space gradient methods have received a lot of attention in the last 5 years and have now reached a relatively mature stage, but they remain an active field.  There are many other approaches, such as <link xlink:type="simple" xlink:href="../244/172244.xml">
simulated annealing</link>, that can be taken to explore the policy space.  Other direct optimization techniques, such as <link xlink:type="simple" xlink:href="../020/268020.xml">
evolutionary computation</link> are used in <link xlink:type="simple" xlink:href="../195/1050195.xml">
evolutionary robotics</link>.</p>

</sec>
<sec>
<st>
 Current research </st>

<p>

Current research topics include: Alternative representations (such as the <link xlink:type="simple" xlink:href="../852/11360852.xml">
Predictive State Representation</link> approach), <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link> in policy space, small-sample convergence results, algorithms and convergence results for <link xlink:type="simple" xlink:href="../552/3063552.xml">
partially observable MDPs</link>, modular and hierarchical reinforcement learning.
Recently, reinforcement learning has been used in the domain of Psychology to explain human learning and performance. In particular, it has been used in cognitive models that simulate human performance during problem solving and/or skill acquisition (e.g., Sun, Merril, &amp; Peterson, 2001; Sun, Slusarz, &amp; Terry, 2005; Gray, Sims, Fu, &amp; Schoelles, 2006; Fu &amp; Anderson, 2006).  It was also used to propose a model of the human error-processing system (Holroyd &amp; Coles, 2002). Multiagent or Distributed Reinforcement Learning is also a topic of interest in current research in this field.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../759/1209759.xml">
Temporal difference learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../850/1281850.xml">
Q learning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../297/10584297.xml">
SARSA</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../876/2435876.xml">
Fictitious play</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../565/362565.xml">
Optimal control</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../276/18365276.xml">
Kaelbling, Leslie P.</link></associate>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
; <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../933/16024933.xml">
Michael L. Littman</link></scholar>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</person>
</physical_entity>
; <link>
Andrew W. Moore</link>&#32;(1996).&#32;"<weblink xlink:type="simple" xlink:href="http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html">
Reinforcement Learning: A Survey</weblink>". <it>Journal of Artificial Intelligence Research</it>&#32;<b>4</b>: 237&ndash;285.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book"><physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../232/11050232.xml">
Sutton, Richard S.</link></associate>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
;&#32;<link>
Andrew G. Barto</link>&#32;(1998). <weblink xlink:type="simple" xlink:href="http://www.cs.ualberta.ca/~sutton/book/ebook/the-book.html">
Reinforcement Learning: An Introduction</weblink>.&#32;MIT Press. refSutton1998. ISBN 0-262-19398-1.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book"><link>
Bertsekas, Dimitri P.</link>;&#32;<link>
John Tsitsiklis</link>&#32;(1996). <weblink xlink:type="simple" xlink:href="http://www.athenasc.com/ndpbook.html">
Neuro-Dynamic Programming</weblink>.&#32;Nashua, NH:&#32;Athena Scientific. ISBN 1-886529-10-8.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../946/17651946.xml">
Ron Sun</link></psychologist>
</research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
, E. Merrill, and T. Peterson, From implicit skills to explicit knowledge: A bottom-up model of skill learning. Cognitive Science, Vol.25, No.2, pp.203-244. 2001. http://www.cogsci.rpi.edu/~rsun/sun.cs01.pdf</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../946/17651946.xml">
Ron Sun</link></psychologist>
</research_worker>
</scientist>
</causal_agent>
</person>
</physical_entity>
, P. Slusarz, and C. Terry, The interaction of the explicit and the implicit in skill learning: A dual-process approach . Psychological Review, Vol.112, No.1, pp.159-192. 2005. http://www.cogsci.rpi.edu/~rsun/sun-pr2005-f.pdf</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Peters, Jan;&#32;<link>
Sethu Vijayakumar</link>; <link>
Stefan Schaal</link>&#32;(2003). "<weblink xlink:type="simple" xlink:href="http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf">
Reinforcement Learning for Humanoid Robotics</weblink>".&#32;<it>IEEE-RAS International Conference on Humanoid Robots</it>.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><link xlink:type="simple" xlink:href="../675/8857675.xml">
Gray, Wayne D.</link>; <link>
Chris R. Sims</link>; <link>
Wai-Tat Fu</link>; <link>
Michael J. Schoelles</link>&#32;(2006).&#32;"<weblink xlink:type="simple" xlink:href="http://www.rpi.edu/~grayw/pubs/papers/GSFS06_PsycRvw/GSFS06_PsycRvw.htm">
The Soft Constraints Hypothesis: A Rational Analysis Approach to Resource Allocation for Interactive Behavior</weblink>"&#32;(&#91;&#93; &ndash; <weblink xlink:type="simple" xlink:href="http://scholar.google.co.uk/scholar?hl=en&amp;lr=&amp;q=author%3AGray+intitle%3AThe+Soft+Constraints+Hypothesis%3A+A+Rational+Analysis+Approach+to+Resource+Allocation+for+Interactive+Behavior&amp;as_publication=Psychological+Review&amp;as_ylo=&amp;as_yhi=&amp;btnG=Search">
Scholar search</weblink>). <it>Psychological Review</it>&#32;<b>113</b>&#32;(3): 461–482. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1037%2F0033-295X.113.3.461">
10.1037/0033-295X.113.3.461</weblink>.</cite>&nbsp;</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><link>
Fu, Wai-Tat</link>; <link xlink:type="simple" xlink:href="../823/8845823.xml">
John R. Anderson</link>&#32;(2006).&#32;"<weblink xlink:type="simple" xlink:href="http://www.humanfactors.uiuc.edu/Reports&amp;PapersPDFs/JournalPubs/Fu&amp;Anderson06-JEPG(published)(ReinforcementLearning).pdf">
From Recurrent Choice to Skill Learning: A Reinforcement-Learning Model</weblink>". <it>Journal of Experimental Psychology: General</it>&#32;<b>135</b>&#32;(2): 184 –206. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1037%2F0096-3445.135.2.184">
10.1037/0096-3445.135.2.184</weblink>.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>

<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www-anw.cs.umass.edu/rlr/">
Reinforcement Learning Repository</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://rlai.cs.ualberta.ca/">
Reinforcement Learning and Artificial Intelligence</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://glue.rl-community.org">
RL-Glue</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.dia.fi.upm.es/~jamartin/download.htm">
Software Tools for Reinforcement Learning (Matlab and Python)</weblink> </entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://rlai.cs.ualberta.ca/RLR/index.html">
The UofA Reinforcement Learning Library (texts)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.igi.tugraz.at/ril-toolbox">
The Reinforcement Learning Toolbox from the (Graz University of Technology) </weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cogsci.rpi.edu/~rsun/hybrid-rl.html">
Hybrid reinforcement learning</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://sourceforge.net/projects/piqle/">
 Piqle: a Generic Java Platform for Reinforcement Learning</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.user.cifnet.com/~lwebzem/ttt/ttt.html">
Reinforcement Learning applied to Tic-Tac-Toe Game</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.scholarpedia.org/article/Reinforcement_Learning">
Scholarpedia Reinforcement Learning</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
