<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:30:45[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<datum  confidence="0.9511911446218017" wordnetid="105816622">
<header>
<title>Cluster analysis</title>
<id>669675</id>
<revision>
<id>240685612</id>
<timestamp>2008-09-24T14:30:35Z</timestamp>
<contributor>
<username>Fnielsen</username>
<id>325</id>
</contributor>
</revision>
<categories>
<category>Knowledge discovery in databases</category>
<category>Data mining</category>
<category>Data clustering algorithms</category>
<category>Semi-protected</category>
<category>Multivariate statistics</category>
<category>Machine learning</category>
<category>Data analysis</category>
</categories>
</header>
<bdy>

<image width="150px" src="">

</image>


<b>Clustering</b> is the <link xlink:type="simple" xlink:href="../244/1579244.xml">
classification</link> of objects into different groups, or more precisely, the <link xlink:type="simple" xlink:href="../240/340240.xml">
partitioning</link> of a <link xlink:type="simple" xlink:href="../495/8495.xml">
data set</link> into <link xlink:type="simple" xlink:href="../631/27631.xml">
subset</link>s (clusters), so that the data in each subset (ideally) share some common trait - often proximity according to some defined <link xlink:type="simple" xlink:href="../467/1561467.xml">
distance measure</link>. Data clustering is a common technique for <link xlink:type="simple" xlink:href="../685/26685.xml">
statistical</link> <link xlink:type="simple" xlink:href="../954/2720954.xml">
data analysis</link>, which is used in many fields, including <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link>, <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link>, <link xlink:type="simple" xlink:href="../382/346382.xml">
image analysis</link> and <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>.  The computational task of classifying the data set into <it>k</it> clusters is often referred to as <b><it>k</it></b><b>-clustering</b><it>.</it><p>

Besides the term <it>data clustering</it> (or just <it>clustering</it>), there are a number of terms with similar meanings, including <it>cluster analysis</it>, <it>automatic classification</it>, <it>numerical taxonomy</it>, <it>botryology</it> and <it>typological analysis</it>. </p>

<sec>
<st>
 Types of clustering </st>
<p>

Data clustering algorithms can be <link xlink:type="simple" xlink:href="../998/13998.xml">
hierarchical</link>. Hierarchical algorithms find successive clusters using previously established clusters.  Hierarchical algorithms can be agglomerative ("bottom-up") or divisive ("top-down"). Agglomerative algorithms begin with each element as a separate cluster and merge them into successively larger clusters.  Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters. </p>
<p>

<link xlink:type="simple" xlink:href="../240/340240.xml">
Partitional</link> algorithms typically determine all clusters at once, 
but can also be used as divisive algorithms in the <link xlink:type="simple" xlink:href="../998/13998.xml">
hierarchical</link> clustering.  </p>
<p>

<it>Two-way clustering</it>, <it>co-clustering</it> or <link xlink:type="simple" xlink:href="../201/1810201.xml">
biclustering</link> are clustering methods where not only the objects are clustered but also the features of the objects, i.e.,  if the data is represented in a <link xlink:type="simple" xlink:href="../728/19008728.xml">
data matrix</link>, the rows and columns are clustered simultaneously.</p>
<p>

Another important distinction is whether the clustering uses symmetric or asymmetric distances. A property of <link xlink:type="simple" xlink:href="../697/9697.xml">
Euclidean space</link> is that distances are symmetric (the distance from object<it> A</it> to <it>B</it> is the same as the distance from <it>B</it> to <it>A</it>). In other applications (e.g., sequence-alignment methods, see Prinzie &amp; Van den Poel (2006)), this is not the case.</p>

</sec>
<sec>
<st>
 Distance measure </st>

<p>

An important step in any clustering is to select a <link xlink:type="simple" xlink:href="../378/39378.xml">
distance measure</link>, which will determine how the <it>similarity</it> of two elements is calculated. This will influence the shape of the clusters, as some elements may be close to one another according to one distance and further away according to another. For example, in a 2-dimensional space, the distance between the point (x=1, y=0) and the origin (x=0, y=0) is always 1 according to the usual norms, but the distance between the point (x=1, y=1) and the origin can be 2,<math>\sqrt 2</math> or 1 if you take respectively the 1-norm, 2-norm or infinity-norm distance.</p>
<p>

Common distance functions:
<list>
<entry level="1" type="bullet">

 The <link xlink:type="simple" xlink:href="../932/53932.xml">
Euclidean distance</link> (also called distance <link xlink:type="simple" xlink:href="../479/297479.xml">
as the crow flies</link> or 2-norm distance). A review of cluster analysis in health psychology research found that the most common distance measure in published studies in that research area is the Euclidean distance or the squared Euclidean distance.</entry>
<entry level="1" type="bullet">

 The <link xlink:type="simple" xlink:href="../354/408354.xml">
Manhattan distance</link> (also called taxicab norm or 1-norm)</entry>
<entry level="1" type="bullet">

 The <link xlink:type="simple" xlink:href="../755/349755.xml">
maximum norm</link></entry>
<entry level="1" type="bullet">

 The <link xlink:type="simple" xlink:href="../760/799760.xml">
Mahalanobis distance</link> corrects data for different scales and correlations in the variables</entry>
<entry level="1" type="bullet">

 The angle between two vectors can be used as a distance measure when clustering high dimensional data.  See <form wordnetid="106290637" confidence="0.8">
<word wordnetid="106286395" confidence="0.8">
<space wordnetid="100028651" confidence="0.8">
<part wordnetid="113809207" confidence="0.8">
<language_unit wordnetid="106284225" confidence="0.8">
<link xlink:type="simple" xlink:href="../856/14856.xml">
Inner product space</link></language_unit>
</part>
</space>
</word>
</form>
.</entry>
<entry level="1" type="bullet">

 The <link xlink:type="simple" xlink:href="../227/41227.xml">
Hamming distance</link> (sometimes edit distance) measures the minimum number of substitutions required to change one member into another.</entry>
</list>
</p>

</sec>
<sec>
<st>
Hierarchical clustering</st>

<ss1>
<st>
Creating clusters</st>

<p>

Hierarchical clustering builds (agglomerative), or breaks up (divisive), a hierarchy of clusters. The traditional representation of this hierarchy is a <link xlink:type="simple" xlink:href="../806/30806.xml">
tree</link> (called a <link xlink:type="simple" xlink:href="../693/1409693.xml">
dendrogram</link>), with individual elements at one end and a single cluster containing every element at the other. Agglomerative algorithms begin at the leaves of the tree, whereas divisive algorithms begin at the root. (In the figure, the arrows indicate an agglomerative clustering.)</p>
<p>

Cutting the tree at a given height will give a clustering at a selected precision. In the following example, cutting after the second row will yield clusters {a} {b c} {d e} {f}. Cutting after the third row will yield clusters {a} {b c} {d e f}, which is a coarser clustering, with a smaller number of larger clusters.</p>

</ss1>
<ss1>
<st>
Agglomerative hierarchical clustering</st>

<p>

For example, suppose this data is to be clustered, and the <link xlink:type="simple" xlink:href="../932/53932.xml">
euclidean distance</link> is the <link xlink:type="simple" xlink:href="../467/1561467.xml">
distance metric</link>.</p>
<p>

<image location="none" width="150px" src="Clusters.PNG" type="frame">
<caption>

Raw data
</caption>
</image>
</p>
<p>

The hierarchical clustering <link xlink:type="simple" xlink:href="../693/1409693.xml">
dendrogram</link> would be as such:</p>
<p>

<image location="none" width="150px" src="Hierarchical_clustering_diagram.png" type="frame">
<caption>

Traditional representation
</caption>
</image>
</p>
<p>

This method builds the hierarchy from the individual elements by progressively merging clusters. In our example, we have six elements {a} {b} {c} {d} {e} and {f}. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance.  </p>
<p>

Optionally, one can also construct a <link xlink:type="simple" xlink:href="../350/831350.xml">
distance matrix</link> at this stage, where the number in the <it>i</it>-th row <it>j</it>-th column is the distance between the <it>i</it>-th and <it>j</it>-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. A simple agglomerative clustering algorithm is described in the <link xlink:type="simple" xlink:href="../194/13029194.xml">
single-linkage clustering</link> page; it can easily be adapted to different types of linkage (see below).</p>
<p>

Suppose we have merged the two closest elements <it>b</it> and <it>c</it>, we now have the following clusters {<it>a</it>}, {<it>b</it>, <it>c</it>}, {<it>d</it>}, {<it>e</it>} and {<it>f</it>}, and want to merge them further. To do that, we need to take the distance between {a} and {b c}, and therefore define the distance between two clusters. 
Usually the distance between two clusters <math>\mathcal{A}</math> and <math>\mathcal{B}</math> is one of the following:
<list>
<entry level="1" type="bullet">

 The maximum distance between elements of each cluster (also called complete linkage clustering):</entry>
<entry level="2" type="indent">

<math> \max \{\, d(x,y) : x \in \mathcal{A},\, y \in \mathcal{B}\,\} </math></entry>
<entry level="1" type="bullet">

 The minimum distance between elements of each cluster (also called <link xlink:type="simple" xlink:href="../194/13029194.xml">
single-linkage clustering</link>):</entry>
<entry level="2" type="indent">

<math> \min \{\, d(x,y) : x \in \mathcal{A},\, y \in \mathcal{B} \,\} </math></entry>
<entry level="1" type="bullet">

 The mean distance between elements of each cluster (also called average linkage clustering, used e.g. in <link xlink:type="simple" xlink:href="../968/355968.xml">
UPGMA</link>):</entry>
<entry level="2" type="indent">

<math> {1 \over {|\mathcal{A}|\cdot|\mathcal{B}|}}\sum_{x \in \mathcal{A}}\sum_{ y \in \mathcal{B}} d(x,y) </math></entry>
<entry level="1" type="bullet">

 The sum of all intra-cluster variance</entry>
<entry level="1" type="bullet">

 The increase in variance for the cluster being merged (<link>
Ward's criterion</link>)</entry>
<entry level="1" type="bullet">

 The probability that candidate clusters spawn from the same distribution function (V-linkage)</entry>
</list>
</p>
<p>

Each agglomeration occurs at a greater distance between clusters than the previous agglomeration, and one can decide to stop clustering either when the clusters are too far apart to be merged (distance criterion) or when there is a sufficiently small number of clusters (number criterion).</p>

</ss1>
<ss1>
<st>
 Concept clustering </st>

<p>

Another variation of the agglomerative clustering approach is <link xlink:type="simple" xlink:href="../740/6979740.xml">
conceptual clustering</link>.</p>

</ss1>
</sec>
<sec>
<st>
Partitional clustering</st>

<ss1>
<st>
<it>K</it>-means and derivatives</st>

<ss2>
<st>
<it>K</it>-means clustering</st>

<p>

The <information wordnetid="105816287" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../407/1860407.xml">
<it>K</it>-means algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</datum>
</rule>
</event>
</information>
 assigns each point to the cluster whose center (also called centroid) is nearest. The center is the average of all the points in the cluster — that is, its coordinates are the arithmetic mean for each dimension separately over all the points in the cluster.</p>
<p>

<indent level="1">

<it>Example:</it> The data set has three dimensions and the cluster has two points: <it>X</it> = (<it>x</it>1, <it>x</it>2, <it>x</it>3) and <it>Y</it> = (<it>y</it>1, <it>y</it>2, <it>y</it>3). Then the centroid <it>Z</it> becomes <it>Z</it> = (<it>z</it>1, <it>z</it>2, <it>z</it>3), where <it>z</it>1 = (<it>x</it>1&nbsp;+&nbsp;<it>y</it>1)/2 and <it>z</it>2 = (<it>x</it>2&nbsp;+&nbsp;<it>y</it>2)/2 and <it>z</it>3 = (<it>x</it>3&nbsp;+&nbsp;<it>y</it>3)/2.
</indent>

The algorithm steps are (J. MacQueen, 1967):
<list>
<entry level="1" type="bullet">

 Choose the number of clusters, <it>k</it>.</entry>
<entry level="1" type="bullet">

 Randomly generate <it>k</it> clusters and determine the cluster centers, or directly generate <it>k</it> random points as cluster centers.</entry>
<entry level="1" type="bullet">

 Assign each point to the nearest cluster center.</entry>
<entry level="1" type="bullet">

 Recompute the new cluster centers.</entry>
<entry level="1" type="bullet">

 Repeat the two previous steps until some convergence criterion is met (usually that the assignment hasn't changed).</entry>
</list>
</p>
<p>

The main advantages of this algorithm are its simplicity and speed which allows it to run on large datasets.  Its disadvantage is that it does not yield the same result with each run, since the resulting clusters depend on the initial random assignments.  It minimizes intra-cluster variance, but does not ensure that the result has a global minimum of variance.</p>

</ss2>
<ss2>
<st>
Fuzzy <it>c</it>-means clustering</st>

<p>

In <link xlink:type="simple" xlink:href="../496/2422496.xml">
fuzzy clustering</link>, each point has a degree of belonging to clusters, as in <link xlink:type="simple" xlink:href="../180/49180.xml">
fuzzy logic</link>, rather than belonging completely to just one cluster.  Thus, points on the edge of a cluster, may be <it>in the cluster</it> to a lesser degree than points in the center of cluster.  For each point <it>x</it> we have a coefficient giving the degree of being in the <it>k</it>th cluster <math>u_k(x)</math>. Usually, the sum of those coefficients is defined to be 1:</p>
<p>

<indent level="1">

<math> \forall x \sum_{k=1}^{\mathrm{num.}\ \mathrm{clusters}} u_k(x) \ =1.</math>
</indent>

With fuzzy <it>c</it>-means, the centroid of a cluster is the mean of all points, weighted by their degree of belonging to the cluster:</p>
<p>

<indent level="1">

<math>\mathrm{center}_k = {{\sum_x u_k(x)^m x} \over {\sum_x u_k(x)^m}}.</math>
</indent>

The degree of belonging is related to the inverse of the distance to the cluster center:</p>
<p>

<indent level="1">

<math>u_k(x) = {1 \over d(\mathrm{center}_k,x)},</math>
</indent>

then the coefficients are normalized and fuzzyfied with a real parameter <math>m&amp;gt;1</math> so that their sum is 1.  So</p>
<p>

<indent level="1">

<math>u_k(x) = \frac{1}{\sum_j \left(\frac{d(\mathrm{center}_k,x)}{d(\mathrm{center}_j,x)}\right)^{2/(m-1)}}.</math>
</indent>

For <it>m</it> equal to 2, this is equivalent to normalising the coefficient linearly to make their sum 1. When <it>m</it> is close to 1, then cluster center closest to the point is given much more weight than the others, and the algorithm is similar to <it>k</it>-means. </p>
<p>

The fuzzy <it>c</it>-means algorithm is very similar to the <it>k</it>-means algorithm:
<list>
<entry level="1" type="bullet">

 Choose a number of clusters.</entry>
<entry level="1" type="bullet">

 Assign randomly to each point coefficients for being in the clusters.</entry>
<entry level="1" type="bullet">

 Repeat until the algorithm has converged (that is, the coefficients' change between two iterations is no more than <math>\epsilon</math>, the given sensitivity threshold) :</entry>
<entry level="2" type="bullet">

 Compute the centroid for each cluster, using the formula above.</entry>
<entry level="2" type="bullet">

 For each point, compute its coefficients of being in the clusters, using the formula above.</entry>
</list>
</p>
<p>

The algorithm minimizes intra-cluster variance as well, but has the same problems as <it>k</it>-means, the minimum is a local minimum, and the results depend on the initial choice of weights.
The <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<numerical_quantity wordnetid="105856066" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<value wordnetid="105856388" confidence="0.8">
<quantity wordnetid="105855125" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../752/470752.xml">
Expectation-maximization algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</quantity>
</value>
</rule>
</event>
</numerical_quantity>
</concept>
</idea>
 is a more statistically formalized method which includes some of these ideas: partial membership in classes. It has better convergence properties and is in general preferred to fuzzy-c-means.</p>

</ss2>
<ss2>
<st>
QT clustering algorithm</st>

<p>

QT (quality threshold) clustering (Heyer, Kruglyak, Yooseph, 1999) is an alternative method of partitioning data, invented for gene clustering. It requires more computing power than <it>k</it>-means, but does not require specifying the number of clusters <it>a priori</it>, and always returns the same result when run several times.</p>
<p>

The algorithm is:
<list>
<entry level="1" type="bullet">

 The user chooses a maximum diameter for clusters.</entry>
<entry level="1" type="bullet">

 Build a candidate cluster for each point by including the closest point, the next closest, and so on, until the diameter of the cluster surpasses the threshold.</entry>
<entry level="1" type="bullet">

 Save the candidate cluster with the most points as the first true cluster, and remove all points in the cluster from further consideration. Must clarify what happens if more than 1 cluster has the maximum number of points ?</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../407/25407.xml">
Recurse</link> with the reduced set of points.</entry>
</list>
</p>
<p>

The distance between a point and a group of points is computed using complete linkage, i.e. as the maximum distance from the point to any member of the group (see the "Agglomerative hierarchical clustering" section about distance between clusters).</p>

</ss2>
</ss1>
<ss1>
<st>
 Locality-sensitive hashing </st>
<p>

<link xlink:type="simple" xlink:href="../012/11634012.xml">
Locality-sensitive hashing</link> can be used for clustering. Feature space vectors are sets, and the metric used is the <link xlink:type="simple" xlink:href="../756/2203756.xml">
Jaccard distance</link>. The feature space can be considered high-dimensional. The <it>min-wise independent permutations</it> LSH scheme (sometimes MinHash) is then used to put similar items into buckets. With just one set of hashing methods, there are only clusters of very similar elements. By seeding the hash functions several times (eg 20), it is possible to get bigger clusters. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

</ss1>
<ss1>
<st>
 Graph-theoretic methods </st>

<p>

<link xlink:type="simple" xlink:href="../845/313845.xml">
Formal concept analysis</link> is a technique for generating clusters of objects and attributes, given a <link xlink:type="simple" xlink:href="../431/244431.xml">
bipartite graph</link> representing the relations between the objects and attributes. Other methods for generating <it>overlapping clusters</it> (a <link xlink:type="simple" xlink:href="../552/317552.xml">
cover</link> rather than a <link xlink:type="simple" xlink:href="../240/340240.xml">
partition</link>) are discussed by Jardine and Sibson (1968) and Cole and Wishart (1970).</p>

</ss1>
</sec>
<sec>
<st>
 Determining the number of clusters </st>
<p>
  
If the number of the clusters is not apparent from prior knowledge, it should chosen in some way. 
Several methods for this have been suggested within the statistical literature.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>:365 
where one <link xlink:type="simple" xlink:href="../538/56538.xml">
rule of thumb</link> sets the number to
<indent level="1">

 <math> k \approx (n/2)^{1/2} </math>
</indent>
with <it>n</it> as the number of objects (data points). </p>
<p>

<image location="right" width="300px" src="DataClustering_ElbowCriterion.JPG" type="thumb">
<caption>

Explained Variance. The "elbow" is indicated by the red circle. The number of clusters chosen should therefore be 4.
</caption>
</image>

Another rule of thumb looks at the percentage of variance explained as a function of the number of clusters:
You should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data.
More precisely, if you graph the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph.
The number of clusters are chosen at this point, hence the "elbow criterion".
This "elbow" cannot always be unambiguously identified.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>
Percentage of variance explained is the ratio of the between-group variance to the total variance.
A slight variation of this method plots the curvature of the within group variance.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>
The method can be traced to <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../967/5881967.xml">
Robert L. Thorndike</link></associate>
</psychologist>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
 in 1953.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>
<p>

Other ways to determine the number of clusters use <link xlink:type="simple" xlink:href="../512/690512.xml">
Akaike information criterion</link> (AIC) or <link xlink:type="simple" xlink:href="../272/2473272.xml">
Bayesian information criterion</link> (BIC) &mdash; if it is possible to make a likelihood function for the clustering model. 
For example: The k-means model is "almost" a <link xlink:type="simple" xlink:href="../681/871681.xml">
Gaussian mixture model</link> and one can construct a likelihood for the  Gaussian mixture model and thus also determine AIC and BIC values.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref></p>

</sec>
<sec>
<st>
 Spectral clustering </st>

<p>

Given a set of data points A, the <link xlink:type="simple" xlink:href="../743/1004743.xml">
similarity matrix</link> may be defined as a matrix <math>S</math> where <math>S_{ij}</math> represents a measure of the similarity between points <math>i, j\in A</math>. Spectral clustering techniques make use of the <link xlink:type="simple" xlink:href="../429/2161429.xml">
spectrum</link> of the similarity matrix of the data to perform <link xlink:type="simple" xlink:href="../867/579867.xml">
dimensionality reduction</link> for clustering in fewer dimensions.</p>
<p>

One such technique is the <it><link>
Shi-Malik algorithm</link></it>, commonly used for <link xlink:type="simple" xlink:href="../717/505717.xml">
image segmentation</link>. It partitions points into two sets <math>(S_1,S_2)</math> based on the <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link> <math>v</math> corresponding to the second-smallest <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalue</link> of the <link xlink:type="simple" xlink:href="../472/1448472.xml">
Laplacian matrix</link></p>
<p>

<indent level="1">

<math>L = I - D^{-1/2}SD^{-1/2}</math>
</indent>

of <math>S</math>, where <math>D</math> is the diagonal matrix</p>
<p>

<indent level="1">

<math>D_{ii} = \sum_{j} S_{ij}.</math>
</indent>

This partitioning may be done in various ways, such as by taking the median <math>m</math> of the components in <math>v</math>, and placing all points whose component in <math>v</math> is greater than <math>m</math> in <math>S_1</math>, and the rest in <math>S_2</math>. The algorithm can be used for hierarchical clustering by repeatedly partitioning the subsets in this fashion. </p>
<p>

A related algorithm is the <it><link>
Meila-Shi algorithm</link></it>, which takes the <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link>s corresponding to the <it>k</it> largest <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalue</link>s of the matrix <math>P = SD^{-1}</math> for some <it>k</it>, and then invokes another (e.g. <it>k</it>-means) to cluster points by their respective <it>k</it> components in these eigenvectors.</p>

</sec>
<sec>
<st>
Applications</st>

<ss1>
<st>
 Biology </st>
<p>

In <link xlink:type="simple" xlink:href="../632/9127632.xml">
biology</link> <b>clustering</b> has many applications 
<list>
<entry level="1" type="bullet">

In imaging, data clustering may take different form based on the data dimensionality. For example, the <weblink xlink:type="simple" xlink:href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture">
SOCR EM Mixture model segmentation activity and applet</weblink> shows how to obtain point, region or volume classification using the online <link xlink:type="simple" xlink:href="../480/3217480.xml">
SOCR</link> computational libraries. </entry>
<entry level="1" type="bullet">

In the fields of <link xlink:type="simple" xlink:href="../208/23208.xml">
plant</link> and <link xlink:type="simple" xlink:href="../790/11039790.xml">
animal</link> <link xlink:type="simple" xlink:href="../630/9630.xml">
ecology</link>, clustering is used to describe and to make spatial and temporal comparisons of communities (assemblages) of organisms in heterogeneous environments; it is also used in <link xlink:type="simple" xlink:href="../813/27813.xml">
plant systematics</link> to generate artificial <link xlink:type="simple" xlink:href="../962/23962.xml">
phylogenies</link> or clusters of organisms (individuals) at the species, genus or higher level that share a number of attributes</entry>
<entry level="1" type="bullet">

In computational biology and <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>:</entry>
<entry level="2" type="bullet">

 In <link xlink:type="simple" xlink:href="../071/1075071.xml">
transcriptomics</link>, clustering is used to build groups of <link xlink:type="simple" xlink:href="../553/4250553.xml">
genes</link> with related expression patterns (also known as coexpressed genes). Often such groups contain functionally related proteins, such as <link xlink:type="simple" xlink:href="../257/9257.xml">
enzyme</link>s for a specific <link xlink:type="simple" xlink:href="../941/20941.xml">
pathway</link>, or genes that are co-regulated. High throughput experiments using <link xlink:type="simple" xlink:href="../426/477426.xml">
expressed sequence tag</link>s (ESTs) or <link xlink:type="simple" xlink:href="../954/255954.xml">
DNA microarray</link>s can be a powerful tool for <link xlink:type="simple" xlink:href="../792/323792.xml">
genome annotation</link>, a general aspect of <link xlink:type="simple" xlink:href="../170/55170.xml">
genomics</link>.</entry>
<entry level="2" type="bullet">

 In <link xlink:type="simple" xlink:href="../550/235550.xml">
sequence analysis</link>, clustering is used to group homologous sequences into <link xlink:type="simple" xlink:href="../094/342094.xml">
gene families</link>. This is a very important concept in bioinformatics, and <link xlink:type="simple" xlink:href="../101/418101.xml">
evolutionary biology</link> in general. See evolution by <link xlink:type="simple" xlink:href="../554/477554.xml">
gene duplication</link>.</entry>
<entry level="2" type="bullet">

 In high-throughput genotyping platforms clustering algorithms are used to automatically assign <link xlink:type="simple" xlink:href="../796/12796.xml">
genotypes</link>.</entry>
<entry level="1" type="bullet">

 In QSAR and molecular modeling studies as also chemoinformatics</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Medicine </st>
<p>

In <link xlink:type="simple" xlink:href="../714/234714.xml">
medical imaging</link>, such as <link xlink:type="simple" xlink:href="../032/24032.xml">
PET scan</link>s, cluster analysis can be used to differentiate between different types of <link xlink:type="simple" xlink:href="../915/103915.xml">
tissue</link> and <link xlink:type="simple" xlink:href="../997/3997.xml">
blood</link> in a three dimensional image. In this application, actual position does not matter, but the <link xlink:type="simple" xlink:href="../573/222573.xml">
voxel</link> intensity is considered as a <link xlink:type="simple" xlink:href="../358/879358.xml">
vector</link>, with a dimension for each image that was taken over time. This technique allows, for example, accurate measurement of the rate a radioactive tracer is delivered to the area of interest, without a separate sampling of <link xlink:type="simple" xlink:href="../790/36790.xml">
arterial</link> blood, an intrusive technique that is most common today.</p>

</ss1>
<ss1>
<st>
 Market research </st>

<p>

Cluster analysis is widely used in <link xlink:type="simple" xlink:href="../583/240583.xml">
market research</link> when working with multivariate data from <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../134/272134.xml">
surveys</link></method>
</know-how>
 and test panels. Market researchers use cluster analysis to partition the general <link xlink:type="simple" xlink:href="../949/22949.xml">
population</link> of <link xlink:type="simple" xlink:href="../818/7818.xml">
consumers</link> into market segments and to better understand the relationships between different groups of consumers/potential <link xlink:type="simple" xlink:href="../206/234206.xml">
customers</link>.</p>
<p>

<list>
<entry level="1" type="bullet">

 Segmenting the market and determining <link xlink:type="simple" xlink:href="../885/15383885.xml">
target market</link>s</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../194/234194.xml">
Product positioning</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../192/216192.xml">
New product development</link></entry>
<entry level="1" type="bullet">

 Selecting test markets (see : <link xlink:type="simple" xlink:href="../889/267889.xml">
experimental techniques</link>)</entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Other applications </st>

<p>

<b>Social network analysis</b>: In the study of <link xlink:type="simple" xlink:href="../726/325726.xml">
social networks</link>, clustering may be used to recognize <link xlink:type="simple" xlink:href="../695/5695.xml">
communities</link> within large groups of people.</p>
<p>

<b>Image segmentation</b>: Clustering can be used to divide a <link xlink:type="simple" xlink:href="../276/8276.xml">
digital</link> <link xlink:type="simple" xlink:href="../925/71925.xml">
image</link> into distinct regions for <link>
border detection</link> or <link xlink:type="simple" xlink:href="../466/14661466.xml">
object recognition</link>.</p>
<p>

<b>Data mining</b>: Many <link xlink:type="simple" xlink:href="../253/42253.xml">
data mining</link> applications involve partitioning data items into related subsets; the marketing applications discussed above represent some examples. Another common application is the division of documents, such as <invention wordnetid="105633385" confidence="0.8">
<link xlink:type="simple" xlink:href="../139/33139.xml">
World Wide Web</link></invention>
 pages, into genres.</p>
<p>

<b>Search result grouping</b>: In the process of intelligent grouping of the files and websites, clustering may be used to create a more relevant set of search results compared to normal search engines like <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../923/1092923.xml">
Google</link></company>
. There are currently a number of web based clustering tools such as <web_site wordnetid="106359193" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../678/1080678.xml">
Clusty</link></web_site>
.</p>
<p>

<b>Slippy map optimization</b>: <web_site wordnetid="106359193" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../458/1178458.xml">
Flickr</link></web_site>
's map of photos and other map sites use clustering to reduce the number of markers on a map. This makes it both faster and reduces the amount of visual clutter.</p>
<p>

<b>IMRT segmentation</b>: Clustering can be used to divide a fluence map into distinct regions for conversion into deliverable fields in MLC-based Radiation Therapy.</p>
<p>

<b>Grouping of Shopping Items</b>: Clustering can be used to group all the shopping items available on the web into a set of unique products. For example, all the items on eBay can be grouped into unique products. (eBay doesn't have the concept of a <link xlink:type="simple" xlink:href="../704/1644704.xml">
SKU</link>)</p>
<p>

<b><link xlink:type="simple" xlink:href="../072/1570072.xml">
Mathematical chemistry</link></b>: To find structural similarity, etc., for example, 3000 chemical compounds were clustered in the space of 90 <link xlink:type="simple" xlink:href="../614/14221614.xml">
topological indices</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref></p>
<p>

<b>Petroleum Geology</b>: Cluster Analysis is used to reconstruct missing bottom hole core data or missing log curves in order to evaluate reservoir properties.</p>

</ss1>
</sec>
<sec>
<st>
 Comparisons between data clusterings </st>
<p>
 
There have been several suggestions for a measure of similarity between two clusterings.   Such a measure can be used to compare how well different data clustering algorithms perform on a set of data. 
Many of these measures are derived from the <link xlink:type="simple" xlink:href="../558/847558.xml">
matching matrix</link> (aka <link xlink:type="simple" xlink:href="../558/847558.xml">
confusion matrix</link>), e.g., the <link xlink:type="simple" xlink:href="../359/4813359.xml">
Rand measure</link> and the Fowlkes-Mallows <it>Bk</it> measures.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>  </p>
<p>

Several different clustering systems based on <link xlink:type="simple" xlink:href="../282/427282.xml">
mutual information</link> have been proposed. One is Marina Meila's 'Variation of Information' metric (see ref below); another provides hierarchical clustering<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>.</p>

</sec>
<sec>
<st>
Algorithms</st>
<p>

In recent years considerable effort has been put into improving algorithm performance (Z. Huang, 1998). Among the most popular are <it><link>
CLARANS</link></it> (Ng and Han,1994), <it><link xlink:type="simple" xlink:href="../309/13747309.xml">
DBSCAN</link></it> (Ester et al., 1996) and <it>BIRCH</it> (Zhang et al., 1996).</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial neural network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (ANN)</entry>
<entry level="1" type="bullet">

 <information wordnetid="105816287" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../742/14526742.xml">
Canopy clustering algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</datum>
</rule>
</event>
</information>
</entry>
<entry level="1" type="bullet">

 <link>
Cluster-weighted modeling</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../553/9303553.xml">
Constrained clustering</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../455/10113455.xml">
Cophenetic correlation</link></entry>
<entry level="1" type="bullet">

 <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<numerical_quantity wordnetid="105856066" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<value wordnetid="105856388" confidence="0.8">
<quantity wordnetid="105855125" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../752/470752.xml">
Expectation maximization</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</quantity>
</value>
</rule>
</event>
</numerical_quantity>
</concept>
</idea>
 (EM)</entry>
<entry level="1" type="bullet">

 <information wordnetid="105816287" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../807/14448807.xml">
FLAME clustering</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</datum>
</rule>
</event>
</information>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../407/1860407.xml">
K-means</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../786/398786.xml">
Multidimensional scaling</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../996/76996.xml">
Self-organizing map</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../659/17813659.xml">
Structured data analysis (statistics)</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Bibliography </st>
<p>

<reflist>
<entry id="1">
<weblink xlink:type="simple" xlink:href="http://www2007.org/program/paper.php?id=570">
Google News personalization: scalable online collaborative filtering</weblink></entry>
<entry id="2">
 <cite style="font-style:normal" class="book"><link xlink:type="simple" xlink:href="../910/7105910.xml">
Kanti Mardia</link> et al.&#32;(1979). Multivariate Analysis.&#32;Academic Press.</cite>&nbsp;</entry>
<entry id="3">
See, e.g.,  <cite style="font-style:normal">David J. Ketchen, Jr &amp; Christopher L. Shook&#32;(1996).&#32;"<weblink xlink:type="simple" xlink:href="http://www3.interscience.wiley.com/cgi-bin/fulltext/17435/PDFSTART">
The application of cluster analysis in Strategic Management Research: An analysis and critique</weblink>". <it><link>
Strategic Management Journal</link></it>&#32;<b>17</b>&#32;(6): 441&ndash;458.</cite>&nbsp;</entry>
<entry id="4">
See, e.g., Figure 6 in
<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Cyril Goutte, Peter Toft, Egill Rostrup, Finn Årup Nielsen, <link>
Lars Kai Hansen</link>&#32;(March 1999).&#32;"On Clustering fMRI Time Series". <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../788/6330788.xml">
NeuroImage</link></periodical>
</it>&#32;<b>9</b>&#32;(3): 298&ndash;310. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1006%2Fnimg.1998.0391">
10.1006/nimg.1998.0391</weblink>. PMID 10075900.</cite>&nbsp;</entry>
</list>
</entry>
<entry id="5">
 <cite style="font-style:normal"><physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../967/5881967.xml">
Robert L. Thorndike</link></associate>
</psychologist>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
&#32;(December 1953).&#32;"Who Belong in the Family?". <it><link>
Psychometrika</link></it>&#32;<b>18</b>&#32;(4).</cite>&nbsp;</entry>
<entry id="6">
 <cite style="font-style:normal">Cyril Goutte, <link>
Lars Kai Hansen</link>, Matthew G. Liptrot &amp; Egill Rostrup&#32;(2001).&#32;"<weblink xlink:type="simple" xlink:href="http://www3.interscience.wiley.com/cgi-bin/fulltext/82002382/">
Feature-Space Clustering for fMRI Meta-Analysis</weblink>". <it><link xlink:type="simple" xlink:href="../360/18948360.xml">
Human Brain Mapping</link></it>&#32;<b>13</b>&#32;(3): 165&ndash;183.</cite>&nbsp; see especially Figure 14 and appendix.</entry>
<entry id="7">
Basak  S.C.,  Magnuson  V.R.,  Niemi  C.J.,   Regal   R.R. "Determing Structural Similarity of Chemicals  Using  Graph Theoretic Indices". <it>Discr. Appl. Math.</it>, <b>19</b>, 1988: 17-44.</entry>
<entry id="8">
 <cite style="font-style:normal">E. B. Fowlkes &amp; C. L. Mallows&#32;(September <link xlink:type="simple" xlink:href="../755/34755.xml">
1983</link>).&#32;"A Method for Comparing Two Hierarchical Clusterings". <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../313/16179313.xml">
Journal of the American Statistical Association</link></periodical>
</it>&#32;<b>78</b>&#32;(383): 553&ndash;584. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.2307%2F2288117">
10.2307/2288117</weblink>.</cite>&nbsp;</entry>
<entry id="9">
Alexander Kraskov, Harald Stögbauer, Ralph G. Andrzejak, and Peter Grassberger, "Hierarchical Clustering Based on Mutual Information", (2003) <it><weblink xlink:type="simple" xlink:href="http://arxiv.org/abs/q-bio/0311039">
ArXiv q-bio/0311039</weblink>''</it></entry>
</reflist>
</p>

<ss1>
<st>
 Others </st>
<p>

<list>
<entry level="1" type="bullet">

 Clatworthy, J., Buick, D., Hankins, M., Weinman, J., &amp; Horne, R. (2005). The use and reporting of cluster analysis in health psychology: A review. <it>British Journal of Health Psychology</it> 10: 329-358. </entry>
<entry level="1" type="bullet">

 Cole, A. J. &amp; Wishart, D. (1970). An improved algorithm for the Jardine-Sibson method of generating overlapping clusters.  <it>The Computer Journal</it> 13(2):156-163.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

Ester, M., Kriegel, H.P., Sander, J., and Xu, X. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, Oregon, USA: AAAI Press, pp. 226–231.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Heyer, L.J., Kruglyak, S. and Yooseph, S., Exploring Expression Data: Identification and Analysis of Coexpressed Genes, <it>Genome Research</it> 9:1106-1115.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 S. Kotsiantis, P. Pintelas, Recent Advances in Clustering: A Brief Survey, WSEAS Transactions on Information Science and Applications, Vol 1, No 1 (73-81), 2004.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Huang, Z. (1998). Extensions to the K-means Algorithm for Clustering Large Datasets with Categorical Values. <it>Data Mining and Knowledge Discovery</it>, 2, p. 283-304.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Jardine, N. &amp; Sibson, R. (1968). The construction of hierarchic and non-hierarchic classifications. <it>The Computer Journal</it> 11:177.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/">
The on-line textbook: Information Theory, Inference, and Learning Algorithms</weblink>, by <link xlink:type="simple" xlink:href="../315/2679315.xml">
David J.C. MacKay</link> includes chapters on k-means clustering, soft k-means clustering, and derivations including the E-M algorithm and the variational view of the E-M algorithm. </entry>
<entry level="1" type="bullet">

 MacQueen, J. B. (1967). Some Methods for classification and Analysis of Multivariate Observations, Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, University of California Press, 1:281-297</entry>
<entry level="1" type="bullet">

 Ng, R.T. and Han, J. 1994. Efficient and effective clustering methods for spatial data mining. Proceedings of the 20th VLDB Conference, Santiago, Chile, pp. 144–155.</entry>
<entry level="1" type="bullet">

 Prinzie A., D. Van den Poel (2006), <weblink xlink:type="simple" xlink:href="http://econpapers.repec.org/paper/rugrugwps/05_2F292.htm">
Incorporating sequential information into traditional classification models by using an element/position-sensitive SAM</weblink>. <it>Decision Support Systems</it> 42 (2): 508-526.</entry>
<entry level="1" type="bullet">

 Romesburg, H. Clarles, <it>Cluster Analysis for Researchers</it>, 2004, 340 pp. ISBN 1-4116-0617-5, reprint of 1990 edition published by <link>
Krieger Pub. Co.</link>.. A Japanese language translation is available from <link>
Uchida Rokakuho Publishing Co.</link>, Ltd., Tokyo, Japan.</entry>
<entry level="1" type="bullet">

Sheppard, A. G. (1996). The sequence of factor analysis and cluster analysis: Differences in segmentation and dimensionality through the use of raw and factor scores. Tourism Analysis, 1(Inaugural Volume), 49-57.</entry>
<entry level="1" type="bullet">

Zhang, T., Ramakrishnan, R., and Livny, M. 1996. BIRCH: An efficient data clustering method for very large databases. Proceedings of ACM SIGMOD Conference, Montreal, Canada, pp. 103–114.</entry>
</list>
</p>
<p>

For spectral clustering:
<list>
<entry level="1" type="bullet">

 Jianbo Shi and Jitendra Malik, "Normalized Cuts and Image Segmentation", IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8), 888-905, August 2000. Available on <weblink xlink:type="simple" xlink:href="http://www.cs.berkeley.edu/~malik/malik-pubs-ptrs.html">
Jitendra Malik's homepage</weblink></entry>
<entry level="1" type="bullet">

 Marina Meila and Jianbo Shi, "Learning Segmentation with Random Walk", Neural Information Processing Systems, NIPS, 2001. Available from <weblink xlink:type="simple" xlink:href="http://www.cis.upenn.edu/~jshi/jshi_publication.htm">
Jianbo Shi's homepage</weblink></entry>
<entry level="1" type="bullet">

 see referenced articles <weblink xlink:type="simple" xlink:href="http://www.luigidragone.com/datamining/spectral-clustering.html#references">
here</weblink></entry>
</list>
</p>
<p>

For estimating number of clusters:
<list>
<entry level="1" type="bullet">

 I. O. Kyrgyzov, O. O. Kyrgyzov, H. Maître and M. Campedel. <weblink xlink:type="simple" xlink:href="http://www.tsi.enst.fr/~kyrgyzov/publications.html">
Kernel MDL to Determine the Number of Clusters</weblink>, <weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/j646uqx4p435j530/">
MLDM, pp. 203-217, 2007</weblink>. </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Stan Salvador and Philip Chan, <weblink xlink:type="simple" xlink:href="http://cs.fit.edu/~pkc/papers/ictai04salvador.pdf">
Determining the Number of Clusters/Segments in Hierarchical Clustering/Segmentation Algorithms</weblink>, Proc. 16th IEEE Intl. Conf. on Tools with AI, pp. 576-584, 2004.</entry>
<entry level="1" type="bullet">

 Can, F., Ozkarahan, E. A.  (1990) "Concepts and effectiveness of the cover coefficient-based clustering methodology for text databases."  ACM Transactions on Database Systems.  15 (4) 483-517.</entry>
</list>
</p>
<p>

For discussion of the elbow criterion:
<list>
<entry level="1" type="bullet">

 Aldenderfer, M.S., Blashfield, R.K, <it>Cluster Analysis</it>, (1984), Newbury Park (CA): Sage.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<it><weblink xlink:type="simple" xlink:href="http://adios.tau.ac.il/compact/">
COMPACT - Comparative Package for Clustering Assessment</weblink></it>. A free Matlab package, 2006.</entry>
<entry level="1" type="bullet">

 P. Berkhin, <it><weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/berkhin02survey.html">
Survey of Clustering Data Mining Techniques</weblink></it>, Accrue Software, 2002.</entry>
<entry level="1" type="bullet">

 Jain, Murty and Flynn: <it><weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/jain99data.html">
Data Clustering: A Review</weblink></it>, ACM Comp. Surv., 1999.</entry>
<entry level="1" type="bullet">

 for another presentation of hierarchical, k-means and fuzzy c-means see this <weblink xlink:type="simple" xlink:href="http://www.elet.polimi.it/upload/matteucc/Clustering/tutorial_html/index.html">
 introduction to clustering</weblink>. Also has an explanation on mixture of <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussians</link>.</entry>
<entry level="1" type="bullet">

 David Dowe, <it><weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/cluster.html">
Mixture Modelling page</weblink></it> - other clustering and mixture model links.</entry>
<entry level="1" type="bullet">

 A tutorial on clustering <weblink xlink:type="simple" xlink:href="http://gauss.nmsu.edu/~lludeman/video/ch6pr.html">
http://gauss.nmsu.edu/~lludeman/video/ch6pr.html</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.inference.phy.cam.ac.uk/mackay/itila/">
The on-line textbook: Information Theory, Inference, and Learning Algorithms</weblink>, by <link xlink:type="simple" xlink:href="../315/2679315.xml">
David J.C. MacKay</link> includes chapters on k-means clustering, soft k-means clustering, and derivations including the E-M algorithm and the variational view of the E-M algorithm.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.nerd-cam.com/cluster-results/">
An overview of non-parametric clustering and computer vision</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://blog.peltarion.com/2007/04/10/the-self-organized-gene-part-1/">
"The Self-Organized Gene"</weblink>, tutorial explaining clustering through competitive learning and self-organizing maps.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://cran.r-project.org/web/packages/kernlab/index.html">
kernlab</weblink> - R package for kernel based machine learning (includes spectral clustering implementation)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/">
Tutorial</weblink> - Tutorial with introduction of Clustering Algorithms (k-means, fuzzy-c-means, hierarchical, mixture of gaussians) + some interactive demos (java applets)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://dmoz.org/Computers/Software/Databases/Data_Mining/Public_Domain_Software/">
Data Mining Software</weblink> - Data mining software frequently utilizes clustering techniques.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://homepages.feis.herts.ac.uk/~nngroup/software.html">
Java Competitive Learning Application</weblink> A suite of Unsupervised Neural Networks for clustering.  Written in Java.  Complete with all source code.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Software/">
Machine Learning Software</weblink> - Also contains much clustering software.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://cism.kingston.ac.uk/people/shihab/dissertation.pdf">
<it>Fuzzy Clustering Algorithms and their Application to Medical Image Analysis</it> PhD Thesis, 2001, by AI Shihab.</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.youtube.com/watch?v=1ZDybXl212Q">
Cluster Computing and MapReduce Lecture 4</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://factominer.free.fr/">
FactoMineR</weblink> (free exploratory multivariate data analysis software linked to <link xlink:type="simple" xlink:href="../707/376707.xml">
R</link>)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.springer.com/statistics/statistical+theory+and+methods/journal/357">
The Journal of Classification</weblink>. A publication of the <weblink xlink:type="simple" xlink:href="http://thames.cs.rhul.ac.uk/~fionn/classification-society">
Classification Society of North America</weblink> that specializes on the mathematical and statistical theory of cluster analysis.</entry>
</list>
</p>


</sec>
</bdy>
</datum>
</article>
