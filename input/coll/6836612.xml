<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:38:34[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Auto-encoder</title>
<id>6836612</id>
<revision>
<id>243517401</id>
<timestamp>2008-10-06T21:22:29Z</timestamp>
<contributor>
<username>Robina Fox</username>
<id>1765847</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>
<p>

An <b>auto-encoder</b> is an <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link> used for learning efficient codings.
The aim of an auto-encoder is to learn a compressed representation (encoding) for a set of data.
This means it is being used for <link xlink:type="simple" xlink:href="../867/579867.xml">
dimensionality reduction</link>. More specifically, it is a feature extraction method.
Auto-encoders use three or more layers:</p>
<p>

<list>
<entry level="1" type="bullet">

 An input layer. For example, in a face recognition task, the neurons in the input layer could map to pixels in the photograph.</entry>
<entry level="1" type="bullet">

 A number of considerably smaller hidden layers, which will form the encoding.</entry>
<entry level="1" type="bullet">

 An output layer, where each neuron has the same meaning as in the input layer.</entry>
</list>
</p>
<p>

If linear neurons are used, then an auto-encoder is very similar to <link xlink:type="simple" xlink:href="../340/76340.xml">
PCA</link>.</p>

<sec>
<st>
 Training </st>

<p>

An auto-encoder is often trained using one of the many <link xlink:type="simple" xlink:href="../091/1360091.xml">
backpropagation</link> variants (<link xlink:type="simple" xlink:href="../821/1448821.xml">
conjugate gradient method</link>, <link xlink:type="simple" xlink:href="../489/201489.xml">
steepest descent</link>, etc.) Though often reasonably effective, there are fundamental problems with using backpropagation to train networks with many hidden layers. Once the errors get backpropagated to the first few layers, they are minuscule, and quite ineffectual. This causes the network to almost always learn to reconstruct the average of all the training data. Though more advanced backpropagation methods (such as the conjugate gradient method) help with this to some degree, it still results in very slow learning and poor solutions. This problem is remedied by using initial weights that approximate the final solution. The process to find these initial weights is often called pretraining. </p>
<p>

A pretraining technique developed by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../174/507174.xml">
Geoffrey Hinton</link></associate>
</research_worker>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
 for training many-layered "deep" auto-encoders involves treating each neighboring set of two layers like a <link xlink:type="simple" xlink:href="../059/1166059.xml#xpointer(//*[./st=%22Restricted+Boltzmann+Machine%22])">
Restricted Boltzmann Machine</link> for pre-training to approximate a good solution and then using a backpropagation technique to fine-tune.</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://hebb.mit.edu/people/seung/talks/continuous/sld007.htm">
Presentation introducing auto-encoders for number recognition</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.sciencemag.org/cgi/content/abstract/313/5786/504">
Reducing the Dimensionality of Data with Neural Networks</weblink> (Science, 28 July 2006, Hinton &amp; Salakhutdinov)</entry>
</list>
</p>


</sec>
</bdy>
</article>
