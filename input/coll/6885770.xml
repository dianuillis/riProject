<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:35:19[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Bootstrapping (statistics)</title>
<id>6885770</id>
<revision>
<id>242552142</id>
<timestamp>2008-10-02T18:31:52Z</timestamp>
<contributor>
<username>Cydebot</username>
<id>1215485</id>
</contributor>
</revision>
<categories>
<category>Resampling (statistics)</category>
<category>Computational statistics</category>
<category>Data analysis</category>
<category>Statistical inference</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, <b>bootstrapping</b> is a modern, computer-intensive, general purpose approach to <link xlink:type="simple" xlink:href="../577/27577.xml">
statistical inference</link>, falling within a broader class of <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../850/3763850.xml">
resampling</link></higher_cognitive_process>
</trial>
</method>
</experiment>
</know-how>
</problem_solving>
</thinking>
</inquiry>
</process>
 methods.<p>

Bootstrapping is the practice of estimating properties of an <link xlink:type="simple" xlink:href="../043/10043.xml">
estimator</link> (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution of the observed data. In the case where a set of observations can be assumed to be from an <link xlink:type="simple" xlink:href="../067/453067.xml">
independent and identically distributed</link> population, this can be implemented by constructing a number of  <process wordnetid="105701363" confidence="0.8">
<inquiry wordnetid="105797597" confidence="0.8">
<thinking wordnetid="105770926" confidence="0.8">
<problem_solving wordnetid="105796750" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<experiment wordnetid="105798043" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<trial wordnetid="105799212" confidence="0.8">
<higher_cognitive_process wordnetid="105770664" confidence="0.8">
<link xlink:type="simple" xlink:href="../850/3763850.xml">
resamples</link></higher_cognitive_process>
</trial>
</method>
</experiment>
</know-how>
</problem_solving>
</thinking>
</inquiry>
</process>
 of the observed dataset (and of equal size to the observed dataset), each of which is obtained by <link xlink:type="simple" xlink:href="../361/160361.xml">
random sampling with replacement</link> from the original dataset.</p>
<p>

It may also be used for constructing hypothesis tests. It is often used as an alternative to inference based on parametric assumptions when those assumptions are in doubt, or where parametric inference is impossible or requires very complicated formulas for the calculation of standard errors.</p>
<p>

The advantage of bootstrapping over analytical methods is its great simplicity - 
it is straightforward to apply the bootstrap to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients.</p>
<p>

The disadvantage of bootstrapping is that while (under some conditions) it is asymptotically consistent, it does not provide general finite sample guarantees, and has a tendency to be overly optimistic. The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples) where these would be more formally stated in other approaches.</p>

<sec>
<st>
 Example: Fisher's iris data </st>
<p>

To introduce the basic ideas and the value of the method, Fisher's famous <set wordnetid="107996689" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../224/10477224.xml">
Iris flower data set</link></group>
</collection>
</set>
 will be used, where only the species <it>virginica</it> and <it>versicolor</it> are considered. The analysis was performed in <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../707/376707.xml">
R</link></programming_language>
.</p>
<p>

The species can be modelled by <link xlink:type="simple" xlink:href="../631/226631.xml">
logistic regression</link> as a function of sepal length (that is, the other variables available are ignored). Fitting the logistic regression model by <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood</link> gives the following parameter estimates and their <link xlink:type="simple" xlink:href="../543/226543.xml">
standard error</link>s:</p>
<p>

<table class="wikitable">
<row>

<header>
Estimate</header>
<header>
Std. Error</header>
</row>
<row>
<header align="right">
Intercept</header>
<col align="right">
-12.57</col>
<col align="right">
2.91</col>
</row>
<row>
<header align="right ">
Sepal Length</header>
<col align="right">
2.01</col>
<col align="right">
0.47</col>
</row>
</table>
</p>
<p>

It is known that maximum likelihood estimators are asymptotically normally distributed. We can check this assumption using a bootstrap procedure as follows:</p>
<p>

<list>
<entry level="1" type="number">

 Sample <it>n</it> observations <it>with replacement</it> from the original data, where <it>n</it> is the number of observations.</entry>
<entry level="1" type="number">

 Fit the logistic regression model by maximum likelihood.</entry>
<entry level="1" type="number">

 Repeat this bootstrap sampling very often (<it>B</it> rounds).</entry>
<entry level="1" type="number">

 Use the <link xlink:type="simple" xlink:href="../670/520670.xml">
sampling distribution</link> of the estimates thus computed to be an approximation to the 'true' population sampling distribution.</entry>
</list>
</p>
<p>

The plot below contains kernel density plots of the two parameters in the model, as estimated from 10 000 bootstrap samples.</p>
<p>

<image width="150px" src="IrisBootstraps.png">
<caption>

IrisBootstraps.png
</caption>
</image>
</p>
<p>

The distributions of the parameter estimates are clearly not normal. That is, the asymptotic assumptions about the maximum likelihood estimates cannot be relied on, and quantities such as <link xlink:type="simple" xlink:href="../911/280911.xml">
confidence interval</link>s and <link>
hypothesis test</link>s that rely on those assumptions will be suspect.</p>
<p>

One way to estimate confidence intervals from bootstrap samples is to take the <math>\alpha</math> and <math>1-\alpha</math> quantiles of the estimated values. These are called <it>bootstrap</it> percentile intervals. In this case, for the intercept and for sepal length, the bootstrap 95% <link xlink:type="simple" xlink:href="../907/354907.xml">
percentile</link> intervals are (-20.02, -7.08) and (1.26, 3.20) respectively. These can be contrasted with the asymptotic intervals derived from the maximum likelihood estimates plus or minus 1.96 standard errors: (-18.26, -6.87) and (1.10, 2.93). The intervals from the asymptotic theory are apparently too narrow (as well as being symmetric).</p>
<p>

This simple bootstrap method is not the only way of making improved inferences over the asymptotic approach. Other bootstrap schemes are available, as are approaches based on likelihood or Bayesian considerations. (In fact, the simple bootstrap scheme used here can be quite easily criticized.)</p>
<p>

There are more complicated bootstraps for sampling without replacement, two-sample problems, regression, time series, hierarchical sampling, mediation analyses, and other statistical problems.</p>

</sec>
<sec>
<st>
 How many bootstrap samples is enough? </st>
<p>

In the example with Fisher's iris data, 10 000 bootstrap samples were used. However, no explanation was given for this number. It seems that the number of bootstrap samples recommended in the literature has increased as available computing power has increased. Whereas a few years ago, 10 000 samples would have seemed excessive, the above example ran in just a few minutes.</p>
<p>

As a general guideline, 1000 samples is often enough for a first look. However, if the results really matter, as many samples as is reasonable given available computing power and time should be used.</p>

</sec>
<sec>
<st>
 Types of bootstrap scheme </st>
<p>

In univariate problems, it is usually acceptable to resample the individual observations with replacement. However, in small samples, a parametric bootstrap approach might be preferred, and for some problems a <it>smooth bootstrap</it> will likely be preferred.</p>
<p>

For regression problems, various other alternatives are available.</p>

<ss1>
<st>
 Smooth bootstrap </st>
<p>

Under this scheme, a small amount of (usually normally distributed) zero-centered random noise is added on to each resampled observation. This is equivalent to sampling from a <link xlink:type="simple" xlink:href="../057/2090057.xml">
kernel density</link> estimate of the data.</p>

</ss1>
<ss1>
<st>
 Parametric bootstrap </st>
<p>

In this case, a parametric model is fit to the data, often by maximum likelihood, and samples of <link xlink:type="simple" xlink:href="../627/41627.xml">
random number</link>s are drawn from this parametric model. Then, the quantity, or estimate, of interest is calculated from these samples.</p>

</ss1>
<ss1>
<st>
 Case resampling </st>
<p>

In regression problems, <it>case resampling</it> refers to the simple scheme of resampling individual cases - often rows of a <link xlink:type="simple" xlink:href="../495/8495.xml">
data set</link>. For regression problems, so long as the data set is fairly large, this simple scheme is often acceptable (and this is the method used in the iris example above). However, the method is open to criticism.</p>
<p>

In regression problems, the <link xlink:type="simple" xlink:href="../701/437701.xml">
explanatory variable</link>s are often fixed, or at least observed with more control than the response variable. Also, the range of the explanatory variables defines the information available from them. Therefore, to resample cases means that each bootstrap sample will lose some information. As such, alternative bootstrap procedures should be considered.</p>

</ss1>
<ss1>
<st>
 Resampling residuals </st>
<p>

Another approach to bootstrapping in regression problems is to resample <link xlink:type="simple" xlink:href="../509/461509.xml">
residual</link>s. The method proceeds as follows.</p>
<p>

<list>
<entry level="1" type="number">

 Fit the model and retain the fitted values <math>\hat y_i</math> and the residual errors <math>\epsilon_i = y_i - \hat{y}_i, (i = 1,\dots, n)</math>.</entry>
<entry level="1" type="number">

 For each pair, <math>(x_i, y_i)</math>, in which <math>x_i</math> is the (possibly multivariate) explanatory variable, add a randomly resampled residual error, <math>\epsilon_i</math>, to the response variable <math>y_i</math>. In other words create synthetic response variables <math>y^*_i = y_i + \epsilon_j</math> where <it>j</it> is selected randomly from the list <math>(1,\dots ,n)</math> for every <it>i</it>.</entry>
<entry level="1" type="number">

 Refit the model using the fictitious response variables <math>y^*_i</math>, and retain the quantities of interest (often the parameters, <math>\hat\mu^*_i</math>, estimated from the synthetic <math>y^*_i</math>).</entry>
<entry level="1" type="number">

 Repeat steps 2 and 3 many, many, times.</entry>
</list>
</p>
<p>

This scheme has the advantage that it retains the information in the explanatory variables. However, a question arises as to which residuals to resample. Raw residuals are one option, another is <link xlink:type="simple" xlink:href="../197/609197.xml">
studentized residuals</link> (in linear regression). Whilst there are arguments in favour of using studentized residuals, in practice it often makes little difference and it is easy to run both schemes and compare the results against each other.</p>

</ss1>
<ss1>
<st>
 Wild bootstrap </st>
<p>

This is the same as resampling residuals but with the additional step that each randomly resampled residual is randomly multiplied by 1 or -1. This method assumes that the 'true' residual distribution is symmetric and can offer advantages over simple residual sampling for smaller sample sizes.</p>

</ss1>
<ss1>
<st>
 Choice of statistic - pivoting </st>
<p>

In situations where it is essential to extract as much information as possible from a data-set, consideration needs to be given to exactly what estimate or statistic should be the subject of the  bootstrapping. Suppose inference is required about the mean of some observations. Then two possibilities are:
<list>
<entry level="1" type="bullet">

 generate bootstrap samples of the sample mean to construct a confidence interval for the mean;</entry>
<entry level="1" type="bullet">

 generate bootstrap samples of the new statistic (mean divided by sample standard deviation), construct a confidence interval for this, then derive the final confidence interval for the mean by multiplying the end-points of the initial interval by the sample standard deviation of the original sample.</entry>
</list>

The results will be different, and simulations results suggest that the second approach is better. The approach may derive partly from the standard parametric approach for Normal distributions, but is rather more general. The idea is to try to make use of a <link xlink:type="simple" xlink:href="../205/6081205.xml">
pivotal quantity</link>, or to find a derived statistic that is approximately pivotal. See also <link xlink:type="simple" xlink:href="../398/670398.xml">
ancillary statistic</link>.</p>

</ss1>
</sec>
<sec>
<st>
 Example applications </st>

<ss1>
<st>
 Application to testing for mediation </st>

<p>

Bootstrapping is becoming the most popular method of testing <link xlink:type="simple" xlink:href="../682/7072682.xml">
mediation</link> <weblink xlink:type="simple" xlink:href="http://www.comm.ohio-state.edu/ahayes/sobel.htm">
http://www.comm.ohio-state.edu/ahayes/sobel.htm</weblink> <weblink xlink:type="simple" xlink:href="http://www.psych.ku.edu/preacher/sobel/sobel.htm">
http://www.psych.ku.edu/preacher/sobel/sobel.htm</weblink> because it does not require the normality assumption to be met, and because it can be effectively utilized with smaller sample sizes (N  20).  However, mediation continues to be (perhaps inappropriately) most frequently determined using (1) the logic of Baron and Kenny <weblink xlink:type="simple" xlink:href="http://davidakenny.net/cm/mediate.htm">
http://davidakenny.net/cm/mediate.htm</weblink> or (2) the <link>
Sobel test</link>.</p>

</ss1>
</sec>
<sec>
<st>
 Example: smoothed bootstrap </st>
<p>

Newcomb's speed of light data are used in the book <it>Bayesian Data Analysis</it> by Gelman et al. and can be found via the <link xlink:type="simple" xlink:href="../495/8495.xml">
classic data sets</link> page. Some analysis of these data appears on the <link xlink:type="simple" xlink:href="../691/2885691.xml">
robust statistics</link> page.</p>
<p>

The data set contains two obvious outliers so that, as an estimate of location, the median is to be preferred over the mean. Bootstrapping is a method often employed for estimating confidence intervals for medians. However, the median is a discrete statistic, and this fact shows up in the bootstrap distribution.</p>
<p>

In order to smooth over the discreteness of the median, we can add a small amount of <math>N(0,\sigma^2)</math> random noise to each bootstrap sample. We choose <math>\sigma = 1/\sqrt n</math> for sample size <math>n</math>.</p>
<p>

Histograms of the bootstrap distribution and the smooth bootstrap distribution appear below. The bootstrap distribution is very jagged because there are only a small number of values that the median can take. The smoothed bootstrap distribution overcomes this jaggedness.</p>
<p>

<image width="150px" src="MedianHists.png">
<caption>

MedianHists.png
</caption>
</image>
</p>
<p>

Although the bootstrap distribution of the median looks ugly and intuitively wrong, confidence intervals from it are not bad in this example. The simple 95% percentile interval is (26, 28.5) for the simple bootstrap and (25.98, 28.46) for the smoothed bootstrap.</p>

</sec>
<sec>
<st>
 Relationship to other resampling methods </st>
<p>

The bootstrap is distinguished from :
<list>
<entry level="1" type="bullet">

 the <link xlink:type="simple" xlink:href="../850/3763850.xml#xpointer(//*[./st=%22Jackknife%22])">
jackknife</link> procedure, used to estimate biases of sample statistics and to estimate variances, and</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../612/416612.xml">
cross-validation</link>, used when the outcome of the basic analysis is the result of a search for the best of many possibilities, with the judgement being based on the sample of data available. </entry>
</list>
</p>
<p>

For more details see <link xlink:type="simple" xlink:href="../850/3763850.xml#xpointer(//*[./st=%22Bootstrap%22])">
bootstrap resampling</link>.</p>
<p>

<link xlink:type="simple" xlink:href="../911/1307911.xml">
Bootstrap aggregating</link> (bagging) is a <link xlink:type="simple" xlink:href="../458/774458.xml">
meta-algorithm</link> based on averaging the results of multiple bootstrap samples.</p>

</sec>
<sec>
<st>
 References and further reading </st>

<p>

<list>
<entry level="1" type="bullet">

 <cite id="Reference-Chernick-1999" style="font-style:normal" class="book">Chernick, Michael R.&#32;(1999). Bootstrap Methods, A practitioner's guide.&#32;Wiley Series in Probability and Statistics.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite id="Reference-Davison-1997" style="font-style:normal" class="book">Davison, A. C.;&#32;Hinkley, D. Bootstrap Methods and their Applications.&#32;(1997). Bootstrap Methods and their Applications.&#32;Cambridge:&#32;Cambridge Series in Statistical and Probabilistic Mathematics.</cite>&nbsp; <weblink xlink:type="simple" xlink:href="http://statwww.epfl.ch/davison/BMA/library.html">
software</weblink>.</entry>
<entry level="1" type="bullet">

 <cite id="Reference-Davison-2006" style="font-style:normal" class="book">Davison, A. C.;&#32;Hinkley, D. Bootstrap Methods and their Applications.&#32;(2006). Bootstrap Methods and their Applications, 8th,&#32;Cambridge:&#32;Cambridge Series in Statistical and Probabilistic Mathematics.</cite>&nbsp; </entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Diaconis, P. &amp; Efron, B.&#32;(May <link xlink:type="simple" xlink:href="../755/34755.xml">
1983</link>).&#32;"Computer-intensive methods in statistics". <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../507/29507.xml">
Scientific American</link></periodical>
</it>: 116&ndash;130.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><peer wordnetid="109626238" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<president wordnetid="110468559" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<executive wordnetid="110069645" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<administrator wordnetid="109770949" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<corporate_executive wordnetid="109966255" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<head wordnetid="110162991" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../868/7250868.xml">
Efron, B.</link></mathematician>
</head>
</causal_agent>
</corporate_executive>
</leader>
</administrator>
</statistician>
</executive>
</associate>
</scientist>
</colleague>
</person>
</president>
</physical_entity>
</peer>
&#32;(<link xlink:type="simple" xlink:href="../754/34754.xml">
1979</link>).&#32;"Bootstrap Methods: Another Look at the Jackknife". <it><link>
The Annals of Statistics</link></it>&#32;<b>7</b>&#32;(1): 1&ndash;26.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><peer wordnetid="109626238" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<president wordnetid="110468559" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<executive wordnetid="110069645" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<administrator wordnetid="109770949" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<corporate_executive wordnetid="109966255" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<head wordnetid="110162991" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../868/7250868.xml">
Efron, B.</link></mathematician>
</head>
</causal_agent>
</corporate_executive>
</leader>
</administrator>
</statistician>
</executive>
</associate>
</scientist>
</colleague>
</person>
</president>
</physical_entity>
</peer>
&#32;(<link xlink:type="simple" xlink:href="../776/34776.xml">
1981</link>).&#32;"Nonparametric estimates of standard error: The jackknife, the bootstrap and other methods". <it><periodical wordnetid="106593296" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../178/1734178.xml">
Biometrika</link></periodical>
</it>&#32;<b>68</b>: 589&ndash;599. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1093%2Fbiomet%2F68.3.589">
10.1093/biomet/68.3.589</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book"><peer wordnetid="109626238" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<president wordnetid="110468559" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<executive wordnetid="110069645" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<administrator wordnetid="109770949" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<corporate_executive wordnetid="109966255" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<head wordnetid="110162991" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../868/7250868.xml">
Efron, B.</link></mathematician>
</head>
</causal_agent>
</corporate_executive>
</leader>
</administrator>
</statistician>
</executive>
</associate>
</scientist>
</colleague>
</person>
</president>
</physical_entity>
</peer>
&#32;(<link xlink:type="simple" xlink:href="../756/34756.xml">
1982</link>). The jackknife, the bootstrap, and other resampling plans&#32;<b>38</b>.&#32;Society of Industrial and Applied Mathematics CBMS-NSF Monographs.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Efron, B.;&#32;Tibshirani, R.&#32;(1993). An Introduction to the Bootstrap.&#32;Chapman &amp; Hall/CRC.</cite>&nbsp; <weblink xlink:type="simple" xlink:href="http://lib.stat.cmu.edu/S/bootstrap.funs">
software</weblink>.</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book">Edgington, E. S.&#32;(<link xlink:type="simple" xlink:href="../658/34658.xml">
1995</link>). Randomization tests.&#32;New York:&#32;<link xlink:type="simple" xlink:href="../893/7871893.xml">
M. Dekker</link>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 Hesterberg, T. C., D. S. Moore, S. Monaghan, A. Clipson, and R. Epstein (2005):  <weblink xlink:type="simple" xlink:href="http://bcs.whfreeman.com/ips5e/content/cat_080/pdf/moore14.pdf">
Bootstrap Methods and Permutation Tests</weblink>, <weblink xlink:type="simple" xlink:href="http://www.insightful.com/Hesterberg/bootstrap">
software</weblink>.</entry>
<entry level="1" type="bullet">

Mooney, C Z &amp; Duval, R D (1993). Bootstrapping. A Nonparametric Approach to Statistical Inference.  Sage University Paper series on Quantitative Applications in the Social Sciences, 07-095. Newbury Park, CA: Sage</entry>
<entry level="1" type="bullet">

 Simon, J. L. (1997): <weblink xlink:type="simple" xlink:href="http://www.resample.com/content/text/index.shtml">
Resampling: The New Statistics</weblink>.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://people.revoledu.com/kardi/tutorial/Bootstrap/index.html">
Bootstrap Sampling Tutorial</weblink>: Introduction to Bootstrap sampling, a tutorial using MS Excel.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.nt.tu-darmstadt.de/nt/index.php?id=227">
Bootstrap tutorial from ICASSP 99</weblink>:  Tutorial from a signal processing perspective</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://animation.yihui.name/dmml:bootstrap_i.i.d">
Animations for bootstrapping i.i.d data</weblink> by Yihui Xie using the <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../707/376707.xml">
R</link></programming_language>
 package <weblink xlink:type="simple" xlink:href="http://cran.r-project.org/package=animation">
animation</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
