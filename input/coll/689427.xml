<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:31:13[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Latent semantic analysis</title>
<id>689427</id>
<revision>
<id>240806399</id>
<timestamp>2008-09-25T01:54:43Z</timestamp>
<contributor>
<username>Nowa</username>
<id>303089</id>
</contributor>
</revision>
<categories>
<category>Information retrieval</category>
<category>Natural language processing</category>
</categories>
</header>
<bdy>

<b>Latent semantic analysis (LSA)</b> is a technique in <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link>, in particular in <link xlink:type="simple" xlink:href="../134/1256134.xml">
vectorial semantics</link>, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  <p>

LSA was patented in <link xlink:type="simple" xlink:href="../670/34670.xml">
1988</link> (<weblink xlink:type="simple" xlink:href="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853">
US Patent 4,839,853</weblink>) by <link xlink:type="simple" xlink:href="../898/2772898.xml">
Scott Deerwester</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../087/2232087.xml">
Susan Dumais</link></associate>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
, <link xlink:type="simple" xlink:href="../116/2232116.xml">
George Furnas</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../143/2232143.xml">
Richard Harshman</link></psychologist>
</scientist>
</causal_agent>
</person>
</physical_entity>
, <physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../134/2232134.xml">
Thomas Landauer</link></educator>
</professional>
</writer>
</adult>
</causal_agent>
</person>
</communicator>
</physical_entity>
, <link>
Karen Lochbaum</link> and <link>
Lynn Streeter</link>. In the context of its application to <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link>, it is sometimes called <b>latent semantic indexing (LSI)</b>.</p>

<sec>
<st>
 Occurrence matrix </st>
<p>

LSA can use a <link xlink:type="simple" xlink:href="../327/1234327.xml">
term-document matrix</link> which describes the occurrences of terms in documents; it is a <link xlink:type="simple" xlink:href="../015/341015.xml">
sparse matrix</link> whose rows correspond to <link xlink:type="simple" xlink:href="../476/430476.xml">
terms</link> and whose columns correspond to documents, typically <link xlink:type="simple" xlink:href="../964/475964.xml">
stemmed</link> words that appear in the documents. A typical example of the weighting of the elements of the matrix is <link xlink:type="simple" xlink:href="../290/2057290.xml">
tf-idf</link> (term frequencyâ€“inverse document frequency): the element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.</p>
<p>

This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.</p>
<p>

LSA transforms the occurrence matrix into a relation between the terms and some <it>concepts</it>, and a relation between those concepts and the documents. Thus the terms and documents are now indirectly related through the concepts.</p>

</sec>
<sec>
<st>
 Applications </st>

<p>

The new concept space typically can be used to:
<list>
<entry level="1" type="bullet">

 Compare the documents in the concept space (<link xlink:type="simple" xlink:href="../675/669675.xml">
data clustering</link>, <link xlink:type="simple" xlink:href="../441/1331441.xml">
document classification</link>)......</entry>
<entry level="1" type="bullet">

 Find similar documents across languages, after analyzing a base set of translated documents (<link>
cross language retrieval</link>).</entry>
<entry level="1" type="bullet">

 Find relations between terms (<link xlink:type="simple" xlink:href="../396/67396.xml">
synonymy</link> and <link xlink:type="simple" xlink:href="../327/155327.xml">
polysemy</link>).</entry>
<entry level="1" type="bullet">

 Given a query of terms, translate it into the concept space, and find matching documents (<link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link>).</entry>
</list>
</p>
<p>

Synonymy and polysemy are fundamental problems in <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link>: 
<list>
<entry level="1" type="bullet">

 Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "physicians", even though the words have the same meaning.</entry>
<entry level="1" type="bullet">

 Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Rank lowering </st>
<p>

After the construction of the occurrence matrix, LSA finds a low-<link xlink:type="simple" xlink:href="../561/26561.xml">
rank</link> approximation to the <link xlink:type="simple" xlink:href="../327/1234327.xml">
term-document matrix</link>. There could be various reasons for these approximations:</p>
<p>

<list>
<entry level="1" type="bullet">

 The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an <it>approximation</it> (a "least and necessary evil").</entry>
<entry level="1" type="bullet">

 The original term-document matrix is presumed <it>noisy</it>: for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a <it>de-noisified matrix</it> (a better matrix than the original).</entry>
<entry level="1" type="bullet">

 The original term-document matrix is presumed overly <link xlink:type="simple" xlink:href="../015/341015.xml">
sparse</link> relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually <it>in</it> each document, whereas we might be interested in all words <it>related to</it> each document--generally a much larger set due to <link xlink:type="simple" xlink:href="../396/67396.xml">
synonymy</link>.</entry>
</list>
</p>
<p>

The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:</p>
<p>

<indent level="2">

 {(car), (truck), (flower)} --&amp;gt;  {(1.3452 * car + 0.2828 * truck), (flower)}
</indent>

This mitigates synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates polysemy, since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.</p>

</sec>
<sec>
<st>
 Derivation </st>
<p>

Let <math>X</math> be a matrix where element <math>(i,j)</math> describes the occurrence of term <math>i</math> in document <math>j</math> (this can be, for example, the frequency). <math>X</math> will look like this:</p>
<p>

<indent level="1">

<math>
\begin{matrix} 
 &amp; \textbf{d}_j \\
 &amp; \downarrow \\
\textbf{t}_i^T \rightarrow &amp;
\begin{bmatrix} 
x_{1,1} &amp; \dots &amp; x_{1,n} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{m,1} &amp; \dots &amp; x_{m,n} \\
\end{bmatrix}
\end{matrix}
</math>
</indent>

Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document: </p>
<p>

<indent level="1">

<math>\textbf{t}_i^T = \begin{bmatrix} x_{i,1} &amp; \dots &amp; x_{i,n} \end{bmatrix}</math>
</indent>

Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:</p>
<p>

<indent level="1">

<math>\textbf{d}_j = \begin{bmatrix} x_{1,j} \\ \vdots \\ x_{m,j} \end{bmatrix}</math>
</indent>

Now the <link xlink:type="simple" xlink:href="../093/157093.xml">
dot product</link> <math>\textbf{t}_i^T \textbf{t}_p</math> between two term vectors gives the <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation</link> between the terms over the documents. The <link xlink:type="simple" xlink:href="../280/125280.xml">
matrix product</link> <math>X X^T</math> contains all these dot products. Element <math>(i,p)</math> (which is equal to element <math>(p,i)</math>) contains the dot product <math>\textbf{t}_i^T \textbf{t}_p</math> (<math> = \textbf{t}_p^T \textbf{t}_i</math>). Likewise, the matrix <math>X^T X</math> contains the dot products between all the document vectors, giving their correlation over the terms: <math>\textbf{d}_j^T \textbf{d}_q = \textbf{d}_q^T \textbf{d}_j</math>.</p>
<p>

Now assume that there exists a decomposition of <math>X</math> such that <math>U</math> and <math>V</math> are <link xlink:type="simple" xlink:href="../620/105620.xml">
orthonormal matrices</link> and <math>\Sigma</math> is a <link xlink:type="simple" xlink:href="../080/174080.xml">
diagonal matrix</link>. This is called a <link xlink:type="simple" xlink:href="../207/142207.xml">
singular value decomposition</link> (SVD):</p>
<p>

<indent level="1">

<math>
X = U \Sigma V^T
</math>
</indent>

The matrix products giving us the term and document correlations then become</p>
<p>

<indent level="1">

<math>
\begin{matrix}
X X^T &amp;=&amp; (U \Sigma V^T) (U \Sigma V^T)^T = (U \Sigma V^T) (V^{T^T} \Sigma^T U^T) = U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T \\
X^T X &amp;=&amp; (U \Sigma V^T)^T (U \Sigma V^T) = (V^{T^T} \Sigma^T U^T) (U \Sigma V^T) = V \Sigma U^T U \Sigma V^T = V \Sigma^T \Sigma V^T
\end{matrix}
</math>
</indent>

Since <math>\Sigma \Sigma^T</math> and <math>\Sigma^T \Sigma</math> are diagonal we see that <math>U</math> must contain the <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link>s of <math>X X^T</math>, while <math>V</math> must be the eigenvectors of <math>X^T X</math>. Both products have the same non-zero eigenvalues, given by the non-zero entries of <math>\Sigma \Sigma^T</math>, or equally, by the non-zero entries of <math>\Sigma^T\Sigma</math>. Now the decomposition looks like this:</p>
<p>

<indent level="1">

<math>
\begin{matrix} 
 &amp; X &amp; &amp; &amp; U &amp; &amp; \Sigma &amp; &amp; V^T \\
 &amp; (\textbf{d}_j) &amp; &amp; &amp; &amp; &amp; &amp; &amp; (\hat \textbf{d}_j) \\
 &amp; \downarrow &amp; &amp; &amp; &amp; &amp; &amp; &amp; \downarrow \\
(\textbf{t}_i^T) \rightarrow 
&amp;
\begin{bmatrix} 
x_{1,1} &amp; \dots &amp; x_{1,n} \\
\\
\vdots &amp; \ddots &amp; \vdots \\
\\
x_{m,1} &amp; \dots &amp; x_{m,n} \\
\end{bmatrix}
&amp;
=
&amp;
(\hat \textbf{t}_i^T) \rightarrow
&amp;
\begin{bmatrix} 
\begin{bmatrix} \, \\ \, \\ \textbf{u}_1 \\ \, \\ \,\end{bmatrix} 
\dots
\begin{bmatrix} \, \\ \, \\ \textbf{u}_l \\ \, \\ \, \end{bmatrix}
\end{bmatrix}
&amp;
\cdot
&amp;
\begin{bmatrix} 
\sigma_1 &amp; \dots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \dots &amp; \sigma_l \\
\end{bmatrix}
&amp;
\cdot
&amp;
\begin{bmatrix} 
\begin{bmatrix} &amp; &amp; \textbf{v}_1 &amp; &amp; \end{bmatrix} \\
\vdots \\
\begin{bmatrix} &amp; &amp; \textbf{v}_l &amp; &amp; \end{bmatrix}
\end{bmatrix}
\end{matrix}
</math>
</indent>

The values <math>\sigma_1, \dots, \sigma_l</math> are called the singular values, and <math>u_1, \dots, u_l</math> and <math>v_1, \dots, v_l</math> the left and right singular vectors.
Notice how the only part of <math>U</math> that contributes to <math>\textbf{t}_i</math> is the <math>i\textrm{'th}</math> row. 
Let this row vector be called <math>\hat \textrm{t}_i</math>. 
Likewise, the only part of <math>V^T</math> that contributes to <math>\textbf{d}_j</math> is the <math>j\textrm{'th}</math> column, <math>\hat \textrm{d}_j</math>. 
These are <it>not</it> the eigenvectors, but <it>depend</it> on <it>all</it> the eigenvectors. </p>
<p>

It turns out that when you select the <math>k</math> largest singular values, and their corresponding singular vectors from <math>U</math> and <math>V</math>, you get the rank <math>k</math> approximation to X with the smallest error (<link>
Frobenius norm</link>). The amazing thing about this approximation is that not only does it have a minimal error, but it translates the term and document vectors into a concept space. The vector <math>\hat \textbf{t}_i</math> then has <math>k</math> entries, each giving the occurrence of term <math>i</math> in one of the <math>k</math> concepts. Likewise, the vector <math>\hat \textbf{d}_j</math> gives the relation between document <math>j</math> and each concept. We write this approximation as </p>
<p>

<indent level="1">

<math>X_k = U_k \Sigma_k V_k^T</math>
</indent>

You can now do the following:
<list>
<entry level="1" type="bullet">

 See how related documents <math>j</math> and <math>q</math> are in the concept space by comparing the vectors <math>\hat \textbf{d}_j</math> and <math>\hat \textbf{d}_q</math> (typically by <link xlink:type="simple" xlink:href="../134/1256134.xml">
cosine similarity</link>). This gives you a clustering of the documents.</entry>
<entry level="1" type="bullet">

 Comparing terms <math>i</math> and <math>p</math> by comparing the vectors <math>\hat \textbf{t}_i</math> and <math>\hat \textbf{t}_p</math>, giving you a clustering of the terms in the concept space.</entry>
<entry level="1" type="bullet">

 Given a query, view this as a mini document, and compare it to your documents in the concept space.</entry>
</list>
</p>
<p>

To do the latter, you must first translate your query into the concept space. It is then intuitive that you must use the same transformation that you use on your documents:</p>
<p>

<indent level="1">

<math>\textbf{d}_j = U_k \Sigma_k \hat \textbf{d}_j</math>
</indent>

<indent level="1">

<math>\hat \textbf{d}_j = \Sigma_k^{-1} U_k^T \textbf{d}_j</math>
</indent>

This means that if you have a query vector <math>q</math>, you must do the translation <math>\hat \textbf{q} = \Sigma_k^{-1} U_k^T \textbf{q}</math> before you compare it with the document vectors in the concept space. You can do the same for pseudo term vectors:</p>
<p>

<indent level="1">

<math>\textbf{t}_i^T = \hat \textbf{t}_i^T \Sigma_k V_k^T</math>
</indent>

<indent level="1">

<math>\hat \textbf{t}_i^T = \textbf{t}_i^T V_k^{-T} \Sigma_k^{-1} = \textbf{t}_i^T V_k \Sigma_k^{-1}</math>
</indent>

<indent level="1">

<math>\hat \textbf{t}_i = \Sigma_k^{-1}  V_k^T \textbf{t}_i</math>
</indent>

</p>
</sec>
<sec>
<st>
 Implementation </st>

<p>

The <link xlink:type="simple" xlink:href="../207/142207.xml">
SVD</link> is typically computed using large matrix methods (for example, <link xlink:type="simple" xlink:href="../852/2593852.xml">
Lanczos method</link>s) but may also be computed incrementally and with greatly reduced resources via a <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link>-like approach, which does not require the large, full-rank matrix to be held in memory (<weblink xlink:type="simple" xlink:href="http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf">
Gorrell and Webb, 2005</weblink>).</p>
<p>

A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed (<weblink xlink:type="simple" xlink:href="http://www.merl.com/publications/TR2006-059/">
Brand, 2006</weblink>). Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's (2006) algorithm provides an exact solution.</p>

</sec>
<sec>
<st>
 Limitations </st>
<p>

LSA has two drawbacks:</p>
<p>

<list>
<entry level="1" type="bullet">

 The resulting dimensions might be difficult to interpret. For instance, in </entry>
<entry level="2" type="indent">

 {(car), (truck), (flower)} --&amp;gt;  {(1.3452 * car + 0.2828 * truck), (flower)}</entry>
<entry level="1" type="indent">

the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to </entry>
<entry level="2" type="indent">

 {(car), (bottle), (flower)} --&amp;gt;  {(1.3452 * car + 0.2828 * bottle), (flower)}</entry>
<entry level="1" type="indent">

will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 The <link xlink:type="simple" xlink:href="../576/27576.xml">
probabilistic model</link> of LSA does not match observed data: LSA assumes that words and documents form a joint <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian</link> model (<link xlink:type="simple" xlink:href="../980/258980.xml">
ergodic hypothesis</link>), while a <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../561/41561.xml">
Poisson distribution</link></distribution>
</arrangement>
</structure>
 has been observed.  Thus, a newer alternative is <link xlink:type="simple" xlink:href="../675/2088675.xml">
probabilistic latent semantic analysis</link>, based on a <link xlink:type="simple" xlink:href="../553/1045553.xml">
multinomial</link> model, which is reported to give better results than standard LSA .</entry>
</list>
</p>

</sec>
<sec>
<st>
Commercial Applications</st>

<p>

LSA has been used to assist in performing <link xlink:type="simple" xlink:href="../906/572906.xml">
prior art</link> searches for <link xlink:type="simple" xlink:href="../273/23273.xml">
patents</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../134/1256134.xml">
Vectorial semantics</link></entry>
<entry level="1" type="bullet">

 <link>
DSIR model</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../351/4605351.xml">
Latent Dirichlet allocation</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../651/43651.xml">
Spamdexing</link></entry>
<entry level="1" type="bullet">

 An example of the application of <weblink xlink:type="simple" xlink:href="http://blog.targetwoman.com/latent-semantic-analysis/">
http://blog.targetwoman.com/latent-semantic-analysis/</weblink> Latent Semantic Analysis in Natural language Processing </entry>
<entry level="1" type="bullet">

 <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../675/2088675.xml">
Probabilistic latent semantic analysis</link></datum>
</information>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../095/11989095.xml">
Latent semantic mapping</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../347/13615347.xml">
Latent Semantic Structure Indexing</link></entry>
<entry level="1" type="bullet">

 <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<component wordnetid="105868954" confidence="0.8">
<part wordnetid="105867413" confidence="0.8">
<link xlink:type="simple" xlink:href="../340/76340.xml">
Principal components analysis</link></part>
</component>
</concept>
</idea>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../649/18046649.xml">
Compound term processing</link></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.seobook.com/lsi/lsa_definition.htm">
Latent Semantic Indexing</weblink>, a non mathematical introduction and explanation of LSI</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://knowledgesearch.org/">
The Semantic Indexing Project</weblink>, an open source program for latent semantic indexing</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://senseclusters.sourceforge.net">
SenseClusters</weblink>, an open source package for Latent Semantic Analysis and other methods for clustering similar contexts</entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 "<weblink xlink:type="simple" xlink:href="http://lsa.colorado.edu/">
The Latent Semantic Indexing home page</weblink>".</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Matthew Brand&#32;(2006).&#32;"<weblink xlink:type="simple" xlink:href="http://www.merl.com/publications/TR2006-059/">
Fast Low-Rank Modifications of the Thin Singular Value Decomposition</weblink>". <it>Linear Algebra and Its Applications</it>&#32;<b>415</b>: 20â€“30. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2Fj.laa.2005.07.021">
10.1016/j.laa.2005.07.021</weblink>.</cite>&nbsp; -- a <weblink xlink:type="simple" xlink:href="http://www.eecs.umich.edu/~wingated/resources.html">
MATLAB implementation of Brand's algorithm</weblink> is available</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../134/2232134.xml">
Thomas Landauer</link></educator>
</professional>
</writer>
</adult>
</causal_agent>
</person>
</communicator>
</physical_entity>
, P. W. Foltz, &amp; D. Laham&#32;(1998).&#32;"<weblink xlink:type="simple" xlink:href="http://lsa.colorado.edu/papers/dp1.LSAintro.pdf">
Introduction to Latent Semantic Analysis</weblink>". <it>Discourse Processes</it>&#32;<b>25</b>: 259â€“284.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><link xlink:type="simple" xlink:href="../898/2772898.xml">
S. Deerwester</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../087/2232087.xml">
Susan Dumais</link></associate>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
, <link xlink:type="simple" xlink:href="../116/2232116.xml">
G. W. Furnas</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../134/2232134.xml">
T. K. Landauer</link></educator>
</professional>
</writer>
</adult>
</causal_agent>
</person>
</communicator>
</physical_entity>
, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<psychologist wordnetid="110488865" confidence="0.8">
<link xlink:type="simple" xlink:href="../143/2232143.xml">
R. Harshman</link></psychologist>
</scientist>
</causal_agent>
</person>
</physical_entity>
&#32;(1990).&#32;"<weblink xlink:type="simple" xlink:href="http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf">
Indexing by Latent Semantic Analysis</weblink>". <it>Journal of the American Society for Information Science</it>&#32;<b>41</b>&#32;(6): 391â€“407. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1002%2F%28SICI%291097-4571%28199009%2941%3A6391%3A%3AAID-ASI1%3E3.0.CO%3B2-9">
10.1002/(SICI)1097-4571(199009)41:6391::AID-ASI1&amp;gt;3.0.CO;2-9</weblink>.</cite>&nbsp; Original article where the model was first exposed.</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Michael Berry, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../087/2232087.xml">
S.T. Dumais</link></associate>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
, G.W. O'Brien&#32;(1995).&#32;"<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/berry95using.html">
Using Linear Algebra for Intelligent Information Retrieval</weblink>".</cite>&nbsp; <weblink xlink:type="simple" xlink:href="http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf">
PDF</weblink>. Illustration of the application of LSA to document retrieval.</entry>
<entry level="1" type="bullet">

 "<weblink xlink:type="simple" xlink:href="http://iv.slis.indiana.edu/sw/lsa.html">
Latent Semantic Analysis</weblink>".&#32;  InfoVis.</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">T. Hofmann&#32;(1999). "<weblink xlink:type="simple" xlink:href="http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf">
Probabilistic Latent Semantic Analysis</weblink>".&#32;<it>Uncertainty in Artificial Intelligence</it>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">G. Gorrell and B. Webb&#32;(2005). "<weblink xlink:type="simple" xlink:href="http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf">
Generalized Hebbian Algorithm for Latent Semantic Analysis</weblink>".&#32;<it>Interspeech</it>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 Fridolin Wild&#32;(<link xlink:type="simple" xlink:href="../806/21806.xml">
November 23</link> <link xlink:type="simple" xlink:href="../984/35984.xml">
2005</link>).&#32;"<weblink xlink:type="simple" xlink:href="http://cran.at.r-project.org/web/packages/lsa/index.html">
An Open Source LSA Package for R</weblink>".&#32;  CRAN.&#32;Retrieved on <link>
2006-11-20</link>.</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<communicator wordnetid="109610660" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<writer wordnetid="110794014" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../134/2232134.xml">
Thomas Landauer</link></educator>
</professional>
</writer>
</adult>
</causal_agent>
</person>
</communicator>
</physical_entity>
.&#32;"<weblink xlink:type="simple" xlink:href="http://www.welchco.com/02/14/01/60/96/02/2901.HTM">
A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge</weblink>".&#32;Retrieved on <link>
2007-07-02</link>.</entry>
<entry level="1" type="bullet">

 Dimitrios Zeimpekis and E. Gallopoulos&#32;(September 11, 2005).&#32;"<weblink xlink:type="simple" xlink:href="http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/">
A MATLAB Toolbox for generating term-document matrices from text collections</weblink>".&#32;Retrieved on <link>
2006-11-20</link>.</entry>
</list>
</p>


</sec>
</bdy>
</article>
