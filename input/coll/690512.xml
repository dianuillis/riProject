<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:32:42[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Akaike information criterion</title>
<id>690512</id>
<revision>
<id>228610038</id>
<timestamp>2008-07-29T14:37:09Z</timestamp>
<contributor>
<username>Baccyak4H</username>
<id>1964832</id>
</contributor>
</revision>
<categories>
<category>Regression analysis</category>
</categories>
</header>
<bdy>

<b>Akaike's information criterion</b>, developed by <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../862/4291862.xml">
Hirotsugu Akaike</link></associate>
</mathematician>
</scientist>
</causal_agent>
</colleague>
</statistician>
</person>
</peer>
</physical_entity>
 under the name of "an information criterion" (<b>AIC</b>) in 1971 and proposed in Akaike (1974), is a measure of the goodness of fit of an estimated <link xlink:type="simple" xlink:href="../576/27576.xml">
statistical model</link>. It is grounded in the concept of <link xlink:type="simple" xlink:href="../445/15445.xml">
entropy</link>, in effect offering a relative measure of the <link>
information lost</link> when a given model is used to describe reality and can be said to describe the tradeoff between <link xlink:type="simple" xlink:href="../786/40786.xml">
bias</link> and <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link> in model construction, or loosely speaking that of precision and complexity of the model. <p>

The AIC is not a test on the model in the sense of hypothesis testing, rather it is a tool for <link xlink:type="simple" xlink:href="../073/3664073.xml">
model selection</link>. Given a data set, several competing models may be ranked according to their AIC, with the one having the lowest AIC being the best. From the AIC value one may infer that e.g the top three models are in a tie and the rest are far worse, but one should not assign a value above which a given model is 'rejected'.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

<sec>
<st>
Definition</st>
<p>

In the general case, the AIC is</p>
<p>

<indent level="1">

<math>AIC = 2k - 2\ln(L)\,</math>
</indent>

where <it>k</it> is the number of <link xlink:type="simple" xlink:href="../065/25065.xml">
parameter</link>s in the <link xlink:type="simple" xlink:href="../576/27576.xml">
statistical model</link>, and <it>L</it> is the maximized value of the <link xlink:type="simple" xlink:href="../968/44968.xml">
likelihood</link> function for the estimated model.</p>
<p>

Over the remainder of this entry, it will be assumed that the model errors are normally and independently distributed. Let <it>n</it> be the number of <link xlink:type="simple" xlink:href="../649/22649.xml">
observation</link>s and <it>RSS</it> be 
<indent level="1">

<math>RSS =  \sum_{i=1}^n \hat{\varepsilon}_i^2,</math>
</indent>
the <link xlink:type="simple" xlink:href="../303/2473303.xml">
residual sum of squares</link>. Then AIC becomes</p>
<p>

<indent level="1">

<math>AIC=2k + n[\ln(2\pi RSS/n) + 1]\,.</math>
</indent>

Increasing the number of free parameters to be estimated improves the goodness of fit, regardless of the number of free parameters in the data generating process. Hence AIC not only rewards goodness of fit, but also includes a penalty that is an increasing function of the number of estimated parameters. This penalty discourages <link xlink:type="simple" xlink:href="../332/173332.xml">
overfitting</link>. <it>The preferred model is the one with the lowest AIC value.</it> The AIC methodology attempts to find the model that <b>best explains the data with a minimum of free parameters</b>. By contrast, more traditional approaches to modeling start from a <link xlink:type="simple" xlink:href="../673/226673.xml">
null hypothesis</link>.  The AIC penalizes free parameters less strongly than does the <link xlink:type="simple" xlink:href="../272/2473272.xml">
Schwarz criterion</link>.</p>
<p>

AIC judges a model by how close its fitted values tend to be to the true values, in terms of a certain expected value.</p>

</sec>
<sec>
<st>
Relevance to <math>\chi^2</math> fitting (maximum likelihood)</st>

<p>

Often, one wishes to select amongst competing models where the likelihood function assumes that the underlying errors are normally distributed. This assumption leads to <math>\chi^2</math> data fitting.</p>
<p>

For any set of models where the number of data points, <it>n</it>, is the same, one can use a slightly altered AIC. For the purposes of this article, this will be called <math>AIC_{\chi^2}</math>. It differs from the AIC only through an additive constant, which is a function only of <it>n</it>. As only differences in the AIC are relevant, this constant can be ignored. <math>AIC_{\chi^2}</math> is given by
<indent level="1">

<math>AIC_{\chi^2}=\chi^2+2k</math>
</indent>

This form is often convenient in that data fitting programs produce <math>\chi^2</math> as a statistic for the fit. For models with the same number of data points, the one with the lowest <math>AIC_{\chi^2}</math> should be preferred.</p>

</sec>
<sec>
<st>
AICc and AICu</st>
<p>

AICc is AIC with a second order correction for small sample sizes, to start with:</p>
<p>

<indent level="1">

<math>AICc = AIC + \frac{2k(k + 1)}{n - k - 1}.\,</math>
</indent>

Since AICc converges to AIC as <it>n</it> gets large, AICc should be employed regardless of sample size (Burnham and Anderson, 2004).  </p>
<p>

McQuarrie and Tsai (1998: 22) define AICc as:</p>
<p>

<indent level="1">

<math>AICc = \ln{\frac{RSS}{n}} + \frac{n+k}{n-k-2}\ ,</math>
</indent>

and propose (p. 32) the closely related measure:</p>
<p>

<indent level="1">

<math>AICu = \ln{\frac{RSS}{n-k}} + \frac{n+k}{n-k-2}\ .</math>
</indent>

McQuarrie and Tsai ground their high opinion of AICc and AICu on extensive simulation work.</p>

</sec>
<sec>
<st>
QAIC</st>
<p>

QAIC (the quasi-AIC) is defined as:</p>
<p>

<indent level="1">

<math>QAIC = 2k-\frac{1}{c}2\ln{L}\,</math>
</indent>

where <it>c</it> is a variance inflation factor. QAIC adjusts for over-dispersion or lack of fit. The small sample version of QAIC is</p>
<p>

<indent level="1">

<math>QAICc = QAIC + \frac{2k(k + 1)}{n - k - 1}.\,</math>
</indent>

</p>
</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
Burnham, Anderson, 1998, "Model Selection and Inference - A practical information-theoretic approach" ISBN 0-387-98504-2</entry>
</reflist>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Akaike, Hirotugu&#32;(1974).&#32;"A new look at the statistical model identification". <it>IEEE Transactions on Automatic Control</it>&#32;<b>19</b>&#32;(6): 716â€“723. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1109%2FTAC.1974.1100705">
10.1109/TAC.1974.1100705</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

Burnham, K. P., and D. R. Anderson, 2002. <it>Model Selection and Multimodel Inference: A Practical-Theoretic Approach</it>, 2nd ed. Springer-Verlag. ISBN 0-387-95364-7. </entry>
<entry level="1" type="bullet">

--------, 2004. <it><weblink xlink:type="simple" xlink:href="http://www2.fmg.uva.nl/modelselection/presentations/AWMS2004-Burnham-paper.pdf">
Multimodel Inference: understanding AIC and BIC in Model Selection</weblink></it>, Amsterdam Workshop on Model Selection.</entry>
<entry level="1" type="bullet">

 Hurvich, C. M., and Tsai, C.-L., 1989. <it>Regression and time series model selection in small samples</it>. Biometrika, Vol 76. pp. 297-307</entry>
<entry level="1" type="bullet">

 McQuarrie, A. D. R., and Tsai, C.-L., 1998. <it>Regression and Time Series Model Selection</it>. World Scientific.</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../272/2473272.xml">
Bayesian information criterion</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../609/432609.xml">
deviance</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../500/3103500.xml">
deviance information criterion</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../075/12583075.xml">
Hannan-Quinn information criterion</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../573/4019573.xml">
Jensen-Shannon divergence</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../527/467527.xml">
Kullback-Leibler divergence</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../797/36797.xml">
Occam's Razor</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.garfield.library.upenn.edu/classics1981/A1981MS54100001.pdf">
Hirotogu Akaike comments on how he arrived at the AIC in This Week's Citation Classic</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
