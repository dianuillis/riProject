<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:39:22[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Concept learning</title>
<id>6968451</id>
<revision>
<id>238760841</id>
<timestamp>2008-09-16T07:17:09Z</timestamp>
<contributor>
<username>HLGallon</username>
<id>351274</id>
</contributor>
</revision>
<categories>
<category>Learning theory (education)</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>Concept learning</b>, also known as <link>
category learning</link> and concept attainment, is largely based on the works of the cognitive psychologist <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../226/261226.xml">
Jerome Bruner</link></scientist>
. Bruner, Goodnow, &amp; Austin (1967) defined concept attainment (or concept learning) as "the search for and listing of attributes that can be used to distinguish exemplars from non exemplars of various categories." More simply put, concepts are the mental categories that help us classify objects, events, or ideas and each object, event, or idea has a set of common relevant features. Thus, concept learning is a strategy which requires a learner to compare and contrast groups or categories that contain concept-relevant features with groups or categories that do not contain concept-relevant features.<p>

Concept learning also refers to a learning task in which a human or machine learner is trained to classify objects by being shown a set of example objects along with their class labels. The learner will simplify what has been observed in an example. This simplified version of what has been learned will then be applied to future examples. Concept learning ranges in simplicity and complexity because learning takes place over many areas. When a concept is more difficult, it will be less likely that the learner will be able to simplify, and therefore they will be less likely to learn. Colloquially, this task is known as <it>learning from examples.</it> Most theories of concept learning are based on the storage of exemplars and avoid summarization or overt abstraction of any kind.</p>

<sec>
<st>
 Types of Concepts </st>

<p>

<list>
<entry level="1" type="number">

<b>Not a Concept</b>. Learning through reciting something from memory (recall) or discriminating between two things that differ (discrimination) is not the same as concept learning. </entry>
<entry level="1" type="number">

<b>Concrete or Perceptual Concepts</b></entry>
<entry level="1" type="number">

<b>Defined (or Relational) and Associated Concepts</b></entry>
<entry level="1" type="number">

<b>Complex Concepts</b>. Constructs such as a <link xlink:type="simple" xlink:href="../031/48031.xml">
schema</link> and a script are examples of complex concepts. A schema is an organization of smaller concepts (or features) and is revised by situational information to assist in comprehension. A script on the other hand is a list of actions that a person follows in order to complete a desired goal. An example of a script would be buying a CD. There are several actions that must occur before the actual act of purchasing the CD and a script provides you with the necessary actions and proper order of these actions in order to be successful in purchasing the CD.</entry>
</list>
</p>

</sec>
<sec>
<st>
 The Theoretical Issues </st>

<p>

The theoretical issues underlying concept learning are those underlying <link xlink:type="simple" xlink:href="../877/14877.xml">
induction</link> in general. These issues are addressed in many diverse literatures, including <link xlink:type="simple" xlink:href="../809/7578809.xml">
Version Spaces</link>, <link xlink:type="simple" xlink:href="../303/1053303.xml">
Statistical Learning Theory</link>, <link xlink:type="simple" xlink:href="../008/380008.xml">
PAC Learning</link>, <link xlink:type="simple" xlink:href="../773/14773.xml">
Information Theory</link>, and <link xlink:type="simple" xlink:href="../647/2829647.xml">
Algorithmic Information Theory</link>. Some of the broad theoretical ideas are also discussed by Watanabe (1969,1985), Solomonoff (1964a,1964b), and Rendell (1986).</p>

</sec>
<sec>
<st>
 Modern Psychological Theories of Concept Learning </st>

<p>

It is difficult to make any general statements about human (or animal) concept learning without already assuming a particular psychological theory of concept learning. Although the classical views of <link xlink:type="simple" xlink:href="../978/6978.xml">
concepts</link> and concept learning in philosophy speak of a process of <link xlink:type="simple" xlink:href="../ury/30th_century.xml">
abstraction</link>, <link xlink:type="simple" xlink:href="../013/8013.xml">
data compression</link>, simplification, and summarization, currently popular psychological theories of concept learning diverge on all these basic points. </p>

<ss1>
<st>
 Rule-Based Theories of Concept Learning </st>
<p>

Rule-based theories of concept learning take classification data and a rule-based theory as input, which are the result of a rule-based learner with the hopes of producing a more accurate model of the data (Hekenaho 1997). The majority of rule-based models that have been developed are heuristic, meaning that rational analyses have not been provided and the models are not related to statistical approaches to induction. A rational analysis for rule-based models could presume that concepts are represented as rules, and would then ask what degree of belief a rational agent should be in agreement with each rule, provided some observed examples (Goodman, Griffiths, Feldman, and Tenenbaum). Rule based theories of concept learning are focused more so on perceptual learning and less on definition learning. Rules can be used in learning when the stimuli are confusable as opposed to simple. When rules are used in learning, the decisions are made based on properties alone and rely on simple criteria that do require a lot of memory ( Rouder and Ratcliff, 2006). </p>
<p>

Example of Rule based theory:</p>
<p>

"A radiologist using rule-based categorization would observe
whether specific properties of the X-ray meet certain
criteria; for example, is there an extreme difference in brightness
in a suspicious region relative to the other regions? A decision is
then based on this property alone" (Rouder and Ratcliff 2006)</p>

</ss1>
<ss1>
<st>
 Prototype Theory of Concept Learning </st>

<p>

The prototype view on concept learning holds that people abstract out the central tendency (or prototype) of the experienced examples, and use this as a basis for their categorization decisions.</p>
<p>

<link xlink:type="simple" xlink:href="../464/1042464.xml">
Prototype theory</link>:</p>
<p>

The prototype view on concept learning holds that people categorize based on one or more central examples of a given category followed by a penumbra of decreasingly typical examples. This implies that people do not categorize based on a list of things that all correspond to a definition; rather, a hierarchical inventory based on semantic similarity to the central example(s). </p>
<p>

To illustrate this, imagine the following mental representations of the category: Sports</p>
<p>

The first illustration may demonstrate a mental representation if we were to categorize by definition:</p>
<p>

Definition of Sports: an athletic activity requiring skill or physical prowess and often of a competitive nature.</p>

<p>

Basketball   Football    Bowling
Baseball                                     Skiing
Track and field	                             	   Snowboarding
Lacrosse                                                                    rugby   
Soccer				  Sports                       Skateboarding    
Golf				                    Bike-Racing
Hockey				                Surfing	
Weightlifting              Tennis</p>
<p>

The second illustration may demonstrate a mental representation that Prototype Theory would predict:</p>
<p>

1. Baseball</p>
<p>

2. Football</p>
<p>

3. Basketball</p>
<p>

4. Soccer</p>
<p>

5. Hockey</p>
<p>

6. Tennis</p>
<p>

7. Golf</p>
<p>

...</p>
<p>

15. Bike-racing</p>
<p>

16. Weightlifting</p>
<p>

17. Skateboarding</p>
<p>

18. Snowboarding</p>
<p>

19. Boxing</p>
<p>

20. Wrestling</p>
<p>

...</p>
<p>

32. Fishing</p>
<p>

33. Hunting</p>
<p>

34. Hiking</p>
<p>

35. sky-diving</p>
<p>

36. bunji-jumping</p>
<p>

...</p>
<p>

62. cooking</p>
<p>

63. walking</p>
<p>

...</p>
<p>

82. Gatorade</p>
<p>

83. water</p>
<p>

84. protein</p>
<p>

85. diet</p>
<p>

As you can see the Prototype theory hypothesizes a more continuous (less discrete) way of categorization in which we don’t limit the list to things that match the category’s definition.</p>

</ss1>
<ss1>
<st>
 Exemplar Theories of Concept Learning </st>

<p>

Exemplar theory is the storage of specific instances (exemplars), with new objects evaluated only with respect to how closely they resemble specific known members (and nonmembers) of the category. This theory hypothesizes that learners store examples <it>verbatim</it>. This theory views concept learning as highly simplistic. Only individual properties are represented. These individual properties are not abstract and they do not create rules. An example of what Exemplar theory would look at is, “water is wet;” it simply knows that some (or one, or all) stored examples of water have the property wet. Exemplar based theories have become more empirically popular over the years with some evidence suggesting that human learners use exemplar based strategies only in early learning, forming prototypes and generalizations later in life. An important result of exemplar models in psychological literature has been a de-emphasis of complexity in concept learning. Some of the best known exemplar theory of concept learning are the 
Generalized Context Model (GCM), Nosofsky's (1986) generalization of Medin ans Schaffer's (1978) Context Model. A <link xlink:type="simple" xlink:href="../636/263636.xml">
connectionist</link> version of the GCM, called ALCOVE, has been developed by Kruschke (1992). The ALCOVE model addresses trial-by-trial concept learning. On each training trial, ALCOVE is presented with a stimulus, makes a prediction of the distribution of category choices, is presented with the correct classification, and then adjusts its associative weights and dimensional attention strengths. All of these models are matching models that exemplar sets for a category contains all of the category's exemplars.</p>
<p>

Problems with Exemplar Theory</p>
<p>

Exemplar models critically depend on two measures:</p>
<p>

1. Similarity between exemplars</p>
<p>

2. Rule to determine Group Membership</p>
<p>

Sometimes it is difficult to attain or distinguish these measures.</p>

</ss1>
<ss1>
<st>
 Multiple-Prototype Theories of Concept Learning </st>

<p>

More recently, cognitive psychologists have begun to explore the idea that the prototype and exemplar models form two extremes. It has been suggested that people are able to form a multiple prototype representation, besides the two extreme representations. For example, consider the category spoon. There are two distinct subgroups or conceptual clusters: spoons tend to be either large and wooden or small and made of steel. The prototypical spoon would then be a medium-size object made of a mixture of steel and wood, which is clearly an unrealistic proposal. A more natural representation of the category spoon would instead consist of multiple (at least two) prototypes, one for each cluster. A number of different proposals have been made in this regard (Anderson, 1991; Griffiths, Canini, Sanborn &amp; Navarro, 2007; Love, Medin &amp; Gureckis, 2004; Vanpaemel &amp; Storms, 2008). These models can be regarded as providing a compromise between exemplar and prototype models. </p>


</ss1>
<ss1>
<st>
 Explanation-Based Theories of Concept Learning </st>

<p>

The basic idea of explanation-based learning suggests that a new concept is acquired by experiencing examples of it and forming a basic outline<link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_1%22])">
1</link>. Put simply, by observing or receiving the qualities of a thing the mind forms a concept which possesses and is identified by those qualities. </p>
<p>

The original theory proposed by Mitchell, Keller, and Kedar-Cabelli in 1986, called explanation-based generalization, is that learning occurs through progressive generalizing<link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_2%22])">
2</link>. This theory was first developed to program machines to learn. When applied to human cognition, it translates as such - the mind actively separates information that applies to more than one thing and enters it into a broader description of a category of things.  This is done by identifying sufficient conditions for a thing fitting a category, similar to schematizing.  </p>
<p>

The revised model revolves around the integration of four mental processes – generalization, chunking, operationalization, and analogy<link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_3%22])">
3</link>. </p>
<p>

- Generalization is the process by which the characteristics of a concept which are fundamental to it are recognized and labeled. For example, birds have feathers and wings. Any thing with feathers and wings will be identified as ‘bird’. </p>
<p>

- When information is grouped mentally, whether by similarity or relatedness, the group is called a chunk. Chunks can vary in size from a single item with parts or many items with many parts<link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_4%22])">
4</link>. </p>
<p>

- A concept is operationalized when the mind is able to actively recognize examples of it by characteristics and label it appropriately<link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_5%22])">
5</link>.</p>
<p>

- Analogy is the recognition of similarities between potential examples<link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_6%22])">
6</link>.</p>
<p>

This particular theory of concept learning is relatively new and more research is now being conducted to test it.</p>

</ss1>
<ss1>
<st>
 Bayesian Theories of Concept Learning </st>

<p>

Bayesian theories are those which directly apply normative probability theory to achieve optimal learning.  They generally base their categorization of data on the posterior probability for each category, where for category <math>i</math>, this posterior is given by Bayes rule,</p>
<p>

<indent level="1">

<math>
P(C_{i}|D) = \frac{P(D|C_{i})P(C_{i})}{P(D)}
</math> 
</indent>

where <math>P(D|C_{i})</math> is the probability of observing the given data on the assumption it was generated from category <math>C_{i}</math>, <math>P(C_{i})</math> is the prior probability of category <math>C_{i}</math>, and <math>P(D)</math> is the marginal probability of observing the data, which usually does not enter into consideration.  In general, the category possessing the maximum posterior <math>P(C_{i}|D)</math> would be the category selected for the given data. </p>
<p>

<statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
 is important because it provides a powerful tool for understanding, manipulating and controlling data5 that takes a larger view that is not limited to data analysis alone6.  The approach is subjective and this requires the assessment of prior probabilities6, making it also very complex.  However, if Bayesian's show that the accumulated evidence and the application of Bayes's law are sufficient the work will overcome the subjectivity of the inputs involved7.  Bayesian inference can be used for any honestly collected data and has a major advantage because of its scientific focus6. </p>
<p>

One model that incorporates the Bayesian theory of concept learning is the <link xlink:type="simple" xlink:href="../071/821071.xml">
ACT-R</link> model, developed by <link xlink:type="simple" xlink:href="../823/8845823.xml">
John R. Anderson</link>.  The ACT-R model is a programming language that works to define the basic cognitive and perceptual operations that enable the human mind by producing a step-by-step simulation of human behavior.  This theory works along with the idea that each task humans perform should consist of a series of discrete operations.  The model has been applied to learning and memory, higher level cognition, natural language, perception and attention, human-computer interaction, education and computer generated forces. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>
<p>

In addition to the work of John R. Anderson, Joshua Tenenbaum has been a contributor to the field of concept learning; studying the computational basis of human learning and inference using behavioral testing of adults, children, and machines from Bayesian statistics and probability theory, but also from geometry, graph theory, and liner algebra.  Tenenbaum is working to achieve a better understanding of human learning in computational terms and trying to build computational systems that come closer to the capacities of human learners. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>


</ss1>
<ss1>
<st>
 Component Display Theory </st>

<p>

M. D. Merrill's Component Display Theory (CDT) is a cognitive matrix that focuses on the interaction between two dimensions: the level of performance expected from the learner and the types of content of the material to be learned.  Merrill classifies learner's level of performance as find, use, remember and material content as facts, concepts, procedures, and principles.  The theory also calls upon four primary presentation forms, and several other secondary presentation forms.  The primary presentation forms include: rules, examples, recall, and practice.  Secondary presentation forms include: prerequisites, objectives, helps, mnemonics, and feedback.  A complete lesson should include a combination of these primary and secondary presentation forms, but the most effective combination varies from learner to learner and also from concept to concept. Another significant aspect of the CDT model is that it allows for the learner to control the instructional strategies used and adapt them to meet his or her own learning style and preference. A major goal of this model was to reduce three common errors in concept formation: over-generalization, under-generalization and misconception.</p>
<p>

Main principles of this theory are:</p>
<p>

1.	Having all three primary learner level of performance forms (find, use, and remember) present yields the most effective instruction.</p>
<p>

2.	Primary presentation forms can either be presented through an explanation learning strategy or through an investigation learning strategy.</p>
<p>

3.	As long as all of the primary presentation forms are present in the instruction, the order in which the primary presentation forms are presented does not matter.</p>
<p>

4.	Learners should have control over the number of instances or practice items that they receive.</p>

</ss1>
</sec>
<sec>
<st>
 Machine Learning Approaches to Concept Learning </st>

<p>

This is a budding field due to recent progress in algorithms, computational power, and the expansion of information on the internet.  Unlike the situation in Psychology, the problem of concept learning within <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> is not one of finding the "right" theory of concept learning, but one of finding the most effective method for a given task.  As such, there has been a huge proliferation of concept learning theories.  In the <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> literature, this concept learning is more typically called <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link> or supervised classification, in contrast to <link xlink:type="simple" xlink:href="../497/233497.xml">
unsupervised learning</link> or unsupervised classification, in which the learner is not provided with class labels.  In machine learning, algorithms of in Exemplar theory are also known as instance learners or lazy learners.  </p>
<p>

There are three important roles for machine learning.  </p>
<p>

1.	Data Mining: this is using historical data to improve decisions.  An example is looking at medical records and applying it to medical knowledge when making a diagnoses.</p>
<p>

2.	Software applications that we cannot program by hand: Examples of this are autonomous driving and speech recognition</p>
<p>

3.	Self-customizing programs: An example of this is a newsreader that learns a readers particular interests and highlights these when the reader visits the site.  </p>
<p>

Machine learning has an exciting future.  Some future advantages include; learning across full mixed-media data, learning across multiple internal databases (including the internet and newsfeeds), learning by active experimentation, learning decisions rather than predictions, and the possibility of programming languages with learning embedded.</p>

<ss1>
<st>
 Minimum Description Length Theories: </st>

<p>

The <link xlink:type="simple" xlink:href="../325/331325.xml">
minimum description length</link> principle is a formalization of <link xlink:type="simple" xlink:href="../797/36797.xml">
Occam's Razor</link> in which the best <link xlink:type="simple" xlink:href="../281/14281.xml">
hypothesis</link> for a given set of data is the one that leads to the largest compression of the data. In short, data that shows a lot of regularities and/or patterns, may be compressed without losing any important information. Applying this to learning, we can conclude that the more regularity and/or patterns we are able to find within data, the more we have learned about the data. </p>
<p>

To illustrate this imagine the following as representations of two sets of data:</p>
<p>

Set 1:   100110111011011001010110110010001100101101 Set 2:   011011011011011011011011011011011011011</p>
<p>

Set 1 appears as to be random, but we with set 2 we are able to detect a pattern, thereby allowing us to describe it as "011 repeating 13 times.</p>
<p>

For more information see: http://learningtheory.org/articles/mdlintro.pdf</p>

</ss1>
</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../717/72717.xml">
Categorization</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../877/14877.xml">
Induction</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../319/7517319.xml">
Rule induction</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Footnotes </st>
<p>

<list>
<entry level="1" type="bullet">

  <cite id="fn_1"><link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_1_back%22])">
Note 1</link>: </cite>http://portal.acm.org/citation.cfm?id=66445</entry>
<entry level="1" type="bullet">

  <cite id="fn_2"><link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_2_back%22])">
Note 2</link>: </cite>Dejong, Gerald and Raymond Mooney. "Explanation-based Learning: An Alternative View." <it>Machine Learning.</it> Volume 1. 1986. pp. 145-176.</entry>
<entry level="1" type="bullet">

  <cite id="fn_3"><link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_3_back%22])">
Note 3</link>: </cite>See 1.</entry>
<entry level="1" type="bullet">

  <cite id="fn_4"><link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_4_back%22])">
Note 4</link>: </cite>Ashcraft, Mark A. <it>Cognition.</it>Fourth Edition. Pearson Education, Inc: Upper Saddle River, NJ. 2006.</entry>
<entry level="1" type="bullet">

  <cite id="fn_5"><link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_5_back%22])">
Note 5</link>: </cite>See 2.</entry>
<entry level="1" type="bullet">

  <cite id="fn_6"><link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22fn_6_back%22])">
Note 6</link>: </cite>http://dictionary.reference.com/browse/analogy</entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
http://en.wikipedia.org/wiki/ACT-R</entry>
<entry id="2">
<weblink xlink:type="simple" xlink:href="http://web.mit.edu/bcs/people/tenenbaum.shtml">
MIT : Brain and Cognitive Sciences : People : Faculty : Joshua Tenenbaum</weblink></entry>
</reflist>
</p>
<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Ratcliff, Roger&#32;(2006).&#32;"Comparing Exemplar and Rule-Based Theories of Categorization". <it>Current Directions in Psychological Science</it>&#32;<b>15</b>: 9–13. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1111%2Fj.0963-7214.2006.00397.x">
10.1111/j.0963-7214.2006.00397.x</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 "<weblink xlink:type="simple" xlink:href="http://www.mit.edu/~ndg/papers/op322-goodman.pdf">
A Rational Analysis of Rule-based Concept Learning</weblink>"&#32;(PDF).&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

"<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/85278.html">
GA-based Rule Enhancement in Concept Learning</weblink>".&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

 Bruner, J., Goodnow, J. J., &amp; Austin, G. A. (1967). A study of thinking. New York: Science Editions.</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Feldman, Jacob&#32;(2003).&#32;"The Simplicity Principle in Human Concept Learning". <it>Psychology Science</it>&#32;<b>12</b>: 227–232.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Rendell, Larry&#32;(1986).&#32;"A general framework for induction and a study of selective induction". <it>Machine Learning</it>&#32;<b>1</b>: 177–226. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF00114117">
10.1007/BF00114117</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite id="Reference-Watanabe-1969" style="font-style:normal" class="book">Watanabe, Satosi&#32;(1969). Knowing and Guessing: A Quantitative Study of Inference and Information.&#32;New York:&#32;Wiley.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite id="Reference-Watanabe-1985" style="font-style:normal" class="book">Watanabe, Satosi&#32;(1985). Pattern Recognition: Human and Mechanical.&#32;New York:&#32;Wiley.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Solomonoff, R. J.&#32;(1964).&#32;"A formal theory of inductive inference. Part I". <it>Information and Control</it>&#32;<b>7</b>&#32;(1): 1–22. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2FS0019-9958%2864%2990223-2">
10.1016/S0019-9958(64)90223-2</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Solomonoff, R. J.&#32;(1964).&#32;"A formal theory of inductive inference. Part II". <it>Information and Control</it>&#32;<b>7</b>&#32;(2): 224–254. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2FS0019-9958%2864%2990131-7">
10.1016/S0019-9958(64)90131-7</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

"<weblink xlink:type="simple" xlink:href="http://en.wikipedia.org/wiki/ACT-R">
ACT-R</weblink>".&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

"<weblink xlink:type="simple" xlink:href="http://web.mit.edu/bcs/people/tenenbaum.shtml">
Brain and Cognitive Sciences</weblink>".&#32;  Massachusetts Institute of Technology.&#32;Retrieved on <link>
2007-11-23</link>.</entry>
<entry level="1" type="bullet">

Kearsley, Greg&#32;(Copyright 1994-2007).&#32;"<weblink xlink:type="simple" xlink:href="http://tip.psychology.org/merrill.html">
Component Display Theory (M.D. Merrill)</weblink>".&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

Kearsley, Greg&#32;(Copyright 1994-2007).&#32;"<weblink xlink:type="simple" xlink:href="http://tip.psychology.org/concept.html">
Concept</weblink>".&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

"<weblink xlink:type="simple" xlink:href="http://moogl.wordpress.com/2007/04/10/component-display-theory/">
Component Display Theory</weblink>"&#32;(<link>
2007-04-10</link>).&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

"<weblink xlink:type="simple" xlink:href="http://www.lovinlearning.org/concept/">
Concept Attainment</weblink>"&#32;(Copyright 1999).&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

"<weblink xlink:type="simple" xlink:href="http://edutechwiki.unige.ch/en/Concept_learning">
Concept Learning</weblink>"&#32;(<link>
2007-11-07</link>).&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

"<weblink xlink:type="simple" xlink:href="http://edutechwiki.unige.ch/en/Concept_learning">
Concept Formation</weblink>".&#32;  The McGraw-Hill Companies&#32;(Copyright 2007).&#32;Retrieved on <link>
2007-12-04</link>.</entry>
<entry level="1" type="bullet">

6 <cite style="font-style:normal">Berry, Donald A.&#32;(1997-1998).&#32;"Teaching Elementary Bayesian Statistics with Real Applications in Science". <it>The American Statistician</it>&#32;<b>5</b>&#32;(3): 241–246.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

7 <cite style="font-style:normal">Brown, Harold I.&#32;(1994).&#32;"Reason, Judgment and Bayes's Law". <it>Philosophy of Science</it>&#32;<b>61</b>&#32;(3): 351–369. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1086%2F289808">
10.1086/289808</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

5 <cite style="font-style:normal">Lindley, Dennis V.&#32;(1983).&#32;"Theory and Practice of Bayesian Statistics". <it>The Statistician</it>&#32;<b>32</b>&#32;(1/2): 1–11. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.2307%2F2987587">
10.2307/2987587</weblink>.</cite>&nbsp;</entry>
</list>
</p>

</sec>
</bdy>
</article>
