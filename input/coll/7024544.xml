<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:40:50[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Statistical parsing</title>
<id>7024544</id>
<revision>
<id>221473275</id>
<timestamp>2008-06-24T17:32:49Z</timestamp>
<contributor>
<username>KathrynLybarger</username>
<id>4496249</id>
</contributor>
</revision>
<categories>
<category>Wikify from October 2007</category>
<category>Articles with invalid date parameter in template</category>
<category>All pages needing to be wikified</category>
<category>Statistical natural language processing</category>
<category>Natural language parsing</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Wikitext.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 <b>Please  this article or section.</b>
Help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Statistical_parsing&amp;action=edit">
improve this article</weblink> by adding  . <it>(October 2007)''</it></col>
</row>
</table>

<p>

<b>Statistical parsing</b> is a group of parsing methods within <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link>.  The methods have in common that they associate <link xlink:type="simple" xlink:href="../569/12569.xml">
grammar</link> rules with a probability.  Grammar rules are traditionally viewed in <link xlink:type="simple" xlink:href="../561/5561.xml">
computational linguistics</link> as defining the valid sentences in a language.  Within this mindset, the idea of associating each rule with a probability then provides the relative frequency of any given grammar rule and, by deduction, the probability of a complete parse for a sentence.  (The probability associated with a grammar rule may be induced, but the application of that grammar rule within a parse tree and the computation of the probability of the parse tree based on its component rules is a form of deduction.)  Using this concept, statistical parsers make use of a procedure to search over a space of all candidate parses, and the computation of each candidate's probability, to derive the most probable parse of a sentence.  The <link xlink:type="simple" xlink:href="../752/470752.xml">
expectation maximization</link> algorithm is one popular method of searching for the most probable parse.</p>
<p>

"Search" in this context is an application of the very useful <link xlink:type="simple" xlink:href="../249/28249.xml">
search algorithm</link> in <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link>.</p>
<p>

By way of example, think about the sentence "The can can hold water."  A reader would instantly see that there is an object called "The can" and that this object is performing the action 'can' (i.e. is able to); and the thing the object is able to do is "hold"; and the thing the object is able to hold is "water".  Using more linguistic terminology, "The can" is a noun phrase composed of a determiner followed by a noun, and "can hold water" is a verb phrase which is itself composed of a verb followed by a verb phrase.   But is this the only interpretation of the sentence?  Certainly "The can can" is a perfectly valid noun-phrase referring to a type of dance, and "hold water" is also a valid verb-phrase, although the coerced meaning of the combined sentence is non-obvious.  This lack of meaning is not seen as a problem by most linguists (for a discussion on this point, see <link xlink:type="simple" xlink:href="../438/46438.xml">
Colorless green ideas sleep furiously</link>)  but from a pragmatic point of view it is desirable to obtain the first interpretation rather than the second and statistical parsers achieve this by ranking the interpretations based on their probability.</p>
<p>

(In this example I have made various assumptions about the grammar, such as a simple left-to-right derivation rather than head-driven, its use of noun-phrases rather than the currently fashionable determiner-phrases, and no type-check preventing a concrete noun being combined with an abstract verb phrase.  None of these assumptions affect the thesis of my argument and a comparable argument can be made using any other grammatical formalism.)</p>
<p>

There are a number of methods that statistical parsing algorithms frequently use.  While few algorithms will use all of these they give a good overview of the general field.  Most statistical parsing algorithms are based on a modified form of <link xlink:type="simple" xlink:href="../490/106490.xml">
chart parsing</link>.  The modifications are necessary to support an extremely large number of grammatical rules and therefore search space, and essentially involve applying classical <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> algorithms to the traditionally exhaustive search.  Some examples of the optimisations are only searching a likely subset of the search space (<link xlink:type="simple" xlink:href="../783/2229783.xml">
stack search</link>), for optimising the search probability (<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../778/822778.xml">
Baum-Welch algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
) and for discarding parses that are too similar to be treated separately (<link xlink:type="simple" xlink:href="../015/228015.xml">
Viterbi algorithm</link>).</p>

<sec>
<st>
Notable people in statistical parsing</st>
<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../678/7398678.xml">
Eugene Charniak</link></scientist>
</causal_agent>
</person>
</physical_entity>
 Author of Statistical techniques for natural language parsing <weblink xlink:type="simple" xlink:href="http://www.cs.brown.edu/people/ec/home.html">
http://www.cs.brown.edu/people/ec/home.html</weblink> amongst many other contributions</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../188/4561188.xml">
Fred Jelinek</link> Applied and developed numerous techniques from Information Theory to build the field <weblink xlink:type="simple" xlink:href="http://www.clsp.jhu.edu/people/jelinek/">
http://www.clsp.jhu.edu/people/jelinek/</weblink></entry>
<entry level="1" type="bullet">

 <link>
David Magerman</link> Major contributor to turning the field from theoretical to practical by managing data <weblink xlink:type="simple" xlink:href="http://www-cs-students.stanford.edu/~magerman/">
http://www-cs-students.stanford.edu/~magerman/</weblink></entry>
<entry level="1" type="bullet">

 <link>
James Curran</link> Applying the MaxEnt alogrithm, word representation, and other contributions <weblink xlink:type="simple" xlink:href="http://www.it.usyd.edu.au/about/people/staff/james.shtml">
http://www.it.usyd.edu.au/about/people/staff/james.shtml</weblink></entry>
<entry level="1" type="bullet">

 <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../734/11172734.xml">
Michael Collins (computational linguist)</link></scientist>
 First very high performance statistical parser <weblink xlink:type="simple" xlink:href="http://people.csail.mit.edu/mcollins/">
http://people.csail.mit.edu/mcollins/</weblink></entry>
<entry level="1" type="bullet">

 <link>
Joshua Goodman</link> Hypergraphs, and other generalizations between different methods <weblink xlink:type="simple" xlink:href="http://research.microsoft.com/~joshuago/">
http://research.microsoft.com/~joshuago/</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<language wordnetid="106282651" confidence="0.8">
<link xlink:type="simple" xlink:href="../329/299329.xml">
Stochastic context-free grammar</link></language>
</entry>
</list>
</p>



</sec>
</bdy>
</article>
