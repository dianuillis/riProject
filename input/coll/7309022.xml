<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:49:22[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Nearest neighbor search</title>
<id>7309022</id>
<revision>
<id>242747627</id>
<timestamp>2008-10-03T14:05:20Z</timestamp>
<contributor>
<username>User A1</username>
<id>2062655</id>
</contributor>
</revision>
<categories>
<category>Information retrieval</category>
<category>Data mining</category>
<category>Approximation algorithms</category>
<category>Image processing</category>
<category>Geometric algorithms</category>
<category>Discrete geometry</category>
<category>Classification algorithms</category>
<category>Search algorithms</category>
<category>Numerical analysis</category>
<category>Optimization algorithms</category>
<category>Searching</category>
<category>Machine learning</category>
<category>Analysis of algorithms</category>
<category>Mathematical optimization</category>
</categories>
</header>
<bdy>

<b>Nearest neighbor search</b> (<b>NNS</b>), also known as <b>proximity search</b>, <b>similarity search</b> or <b>closest point search</b>, is an optimization problem for finding closest points in <link xlink:type="simple" xlink:href="../018/20018.xml">
metric space</link>s.  The problem is: given a set <it>S</it> of points in a metric space <it>M</it> and a query point <it>q</it> âˆˆ <it>M</it>, find the closest point in <it>S</it> to <it>q</it>.  In many cases, <it>M</it> is taken to be <it>d</it>-dimensional <link xlink:type="simple" xlink:href="../697/9697.xml">
Euclidean space</link> and distance is measured by <link xlink:type="simple" xlink:href="../932/53932.xml">
Euclidean distance</link> or <link xlink:type="simple" xlink:href="../354/408354.xml">
Manhattan distance</link>.<p>

<person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../095/8095.xml">
Donald Knuth</link></scientist>
</person>
 in vol. 3 of <it><work wordnetid="104599396" confidence="0.8">
<product wordnetid="104007894" confidence="0.8">
<creation wordnetid="103129123" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<book wordnetid="106410904" confidence="0.8">
<publication wordnetid="106589574" confidence="0.8">
<link xlink:type="simple" xlink:href="../358/31358.xml">
The Art of Computer Programming</link></publication>
</book>
</artifact>
</creation>
</product>
</work>
</it> (1973) called it the <b>post-office problem</b>, referring to an application of assigning a residence to the nearest post office. </p>

<sec>
<st>
Applications</st>

<p>

The nearest neighbor search problem arises in numerous fields of application, including:
<list>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../706/126706.xml">
Pattern recognition</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 - in particular for <link xlink:type="simple" xlink:href="../091/49091.xml">
optical character recognition</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../244/1579244.xml">
Statistical classification</link>- see <link xlink:type="simple" xlink:href="../388/1775388.xml">
k-nearest neighbor algorithm</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../596/6596.xml">
Computer vision</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../377/8377.xml">
Database</link>s - e.g. <link xlink:type="simple" xlink:href="../971/743971.xml">
content-based image retrieval</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../869/321869.xml">
Coding theory</link> - see <link xlink:type="simple" xlink:href="../782/1940782.xml">
maximum likelihood decoding</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../013/8013.xml">
Data compression</link> - see <link xlink:type="simple" xlink:href="../060/20060.xml">
MPEG-2</link> standard</entry>
<entry level="1" type="bullet">

<computer wordnetid="103082979" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<web_site wordnetid="106359193" confidence="0.8">
<link xlink:type="simple" xlink:href="../646/596646.xml">
Recommendation systems</link></web_site>
</machine>
</device>
</instrumentality>
</artifact>
</computer>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../177/1618177.xml">
Internet marketing</link> - see <link xlink:type="simple" xlink:href="../328/2963328.xml">
contextual advertising</link> and <link xlink:type="simple" xlink:href="../552/8923552.xml">
behavioral targeting</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../125/1158125.xml">
DNA sequencing</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../869/605869.xml">
Spell checking</link> - suggesting correct spelling</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../404/9368404.xml">
Plagiarism detection</link></entry>
<entry level="1" type="bullet">

<link>
Contact searching algorithms in FEA</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../804/3672804.xml">
Similarity score</link>s for predicting career paths of professional athletes.</entry>
</list>
</p>

</sec>
<sec>
<st>
Methods</st>

<p>

Various solutions to the NNS problem have been proposed.  The quality and usefulness of the algorithm are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. Informal observation usually referred as <link xlink:type="simple" xlink:href="../776/787776.xml">
curse of dimensionality</link> states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.</p>

<ss1>
<st>
Linear search</st>
<p>

The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the "best so far".  This algorithm, sometimes referred to as the naive approach, works for small databases but quickly becomes intractable as either the size or the dimensionality of the problem becomes large.  Linear search has a running time of O(<it>Nd</it>) where N is the <link xlink:type="simple" xlink:href="../er)/6174_(number).xml">
cardinality</link> of <it>S</it> and d is the dimensionality of <it>S</it>.  There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database.</p>

</ss1>
<ss1>
<st>
Space partitioning</st>

<p>

Starting from 1970s <link xlink:type="simple" xlink:href="../580/456580.xml">
branch and bound</link> methodology was applied to the problem. In the case of Euclidean space this approach is known as <link xlink:type="simple" xlink:href="../717/4533717.xml">
spatial index</link> or spatial access methods. Several space-partitioning methods have been developed for solving the NNS problem.  Perhaps the simplest is the <link xlink:type="simple" xlink:href="../725/1676725.xml">
kd-tree</link>, which iteratively bisects the search space into two regions containing half of the points of the parent region.  Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split.  For constant dimension query time, average complexity is O(log <it>N</it>) <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> in the case of randomly distributed points, worst case complexity analyses have been performed <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>. 
. Alternatively the <link xlink:type="simple" xlink:href="../249/865249.xml">
R-tree</link> data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions.  </p>
<p>

In case of general metric space branch and bound approach is known under the name of <link xlink:type="simple" xlink:href="../216/3417216.xml">
metric trees</link>. Particular examples include <link xlink:type="simple" xlink:href="../067/2879067.xml">
VP-tree</link> and <link xlink:type="simple" xlink:href="../338/3417338.xml">
Bk-tree</link>.</p>

</ss1>
<ss1>
<st>
Locality sensitive hashing</st>

<p>

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../012/11634012.xml">
Locality sensitive hashing</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.</p>

</ss1>
<ss1>
<st>
Nearest neighbor search in spaces with small intrinsic dimension</st>

<p>

The <link xlink:type="simple" xlink:href="../009/10912009.xml">
cover tree</link> has a theoretical bound that is based on the dataset's <link>
doubling constant</link>. The bound on search time is O(c12 log n) where <it>c</it>  is the <link xlink:type="simple" xlink:href="../726/5508726.xml">
expansion constant</link> of the dataset.</p>

</ss1>
</sec>
<sec>
<st>
Variants</st>

<p>

There are numerous variants of the NNS problem and the two most well-known are the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../388/1775388.xml">
<it>k</it>-nearest neighbor search</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 and the <link>
&amp;epsilon;-approximate nearest neighbor search</link>.</p>

<ss1>
<st>
K-nearest neighbor</st>


<p>

<indent level="1">

<it>Main article: <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../388/1775388.xml">
K-nearest neighbor algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</it>
</indent>

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../388/1775388.xml">
<it>k</it>-nearest neighbor search</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 identifies the top <it>k</it> nearest neighbors to the query.  This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors.</p>

</ss1>
<ss1>
<st>
Approximate nearest neighbor</st>

<p>

In some applications it may be acceptable to retrieve a "good guess" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.</p>
<p>

Algorithms which support the approximate nearest neighbor search include <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../323/15585323.xml">
Best Bin First</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 and the <link xlink:type="simple" xlink:href="../038/5150038.xml">
kd-trie</link>.</p>
<p>

<link>
&amp;epsilon;-approximate nearest neighbor search</link> is becoming an increasingly popular tool for fighting the <link xlink:type="simple" xlink:href="../776/787776.xml">
curse of dimensionality</link>.</p>

</ss1>
<ss1>
<st>
Nearest neighbor distance ratio</st>

<p>

<link>
Nearest neighbor distance ratio</link> do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in <software wordnetid="106566077" confidence="0.8">
<application wordnetid="106570110" confidence="0.8">
<program wordnetid="106568978" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106359877" confidence="0.8">
<code wordnetid="106355894" confidence="0.8">
<coding_system wordnetid="106353757" confidence="0.8">
<link xlink:type="simple" xlink:href="../971/743971.xml">
CBIR</link></coding_system>
</code>
</writing>
</written_communication>
</program>
</application>
</software>
 to retrieve pictures through a "query by example" using the similarity between local features. More generally it is involved in several <link xlink:type="simple" xlink:href="../797/581797.xml">
matching</link> problems.</p>

</ss1>
<ss1>
<st>
All nearest neighbors</st>

<p>

For some applications (e.g. <link xlink:type="simple" xlink:href="../628/18246628.xml">
entropy estimation</link>), we may have N data-points and wish to know which is the nearest neighbor <it>for every one of those N points</it>. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these N queries to produce a more efficient search. As a simple example: when we find the distance from point X to point Y, that also tells us the distance from point Y to point X, so the same calculation can be reused in two different queries.</p>
<p>

Given a fixed dimension, a semi-definite positive norm (thereby including every <link>
 Lp norm</link>), and n points in this space, the m nearest neighbours of every point can be found in <math>O(mn \log n)</math> time.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>

</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../388/1775388.xml">
K-nearest neighbor algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../498/2905498.xml">
Nearest-neighbor interpolation</link></entry>
<entry level="1" type="bullet">

<software wordnetid="106566077" confidence="0.8">
<application wordnetid="106570110" confidence="0.8">
<program wordnetid="106568978" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106359877" confidence="0.8">
<code wordnetid="106355894" confidence="0.8">
<coding_system wordnetid="106353757" confidence="0.8">
<link xlink:type="simple" xlink:href="../971/743971.xml">
Content-based image retrieval</link></coding_system>
</code>
</writing>
</written_communication>
</program>
</application>
</software>
</entry>
<entry level="1" type="bullet">

<link>
Near Duplicate Algorithms</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../012/11634012.xml">
Locality sensitive hashing</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../668/177668.xml">
Voronoi diagram</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
</list>
</p>

</sec>
<sec>
<st>
Notes</st>
<p>

<reflist>
<entry id="1">
</entry>
<entry id="2">
 <cite id="CITEREFLeeWong1977" style="font-style:normal">Lee, D. T.&#32;&amp;&#32;Wong, C. K.&#32;(1977),&#32;"Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees",&#32;<it>Acta Informatica</it>&#32;<b>9</b>(1):  23â€“29, <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF00263763">
10.1007/BF00263763</weblink></cite>&nbsp;</entry>
<entry id="3">
 <cite id="CITEREFVaidya1989" style="font-style:normal">Vaidya, P. M.&#32;(1989),&#32;"An O(n logn) Algorithm for the All-Nearest-Neighbors Problem",&#32;<it>Discrete and Computational Geometry</it>&#32;<b>4</b>(1):  101-115</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 Arya, S., D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu.  An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions.  <it>Journal of the ACM</it>, vol. 45, no. 6, pp. 891-923</entry>
<entry level="1" type="bullet">

 Zezula, P., Amato, G., Dohnal, V., and Batko, M. Similarity Search - The Metric Space Approach. Springer, 2006. <link xlink:type="simple" xlink:href="../919/14919.xml">
ISBN</link>: 0-387-29146-6</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://simsearch.yury.name">
Nearest Neighbors and Similarity Search</weblink> - a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://sisap.org/?f=library">
Metric Spaces Library</weblink> - An open-source C-based library for metric space indexing by Karina Figueroa, Gonzalo Navarro, Edgar ChÃ¡vez.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.umd.edu/~mount/ANN/">
ANN</weblink> - A Library for Approximate Nearest Neighbor Searching by David M. Mount and Sunil Arya.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.compgeom.com/~stann/">
STANN</weblink> - A Simple Threaded Approximate Nearest Neighbor Search Library in C++ by Michael Connor and Piyush Kumar.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://lsd.fi.muni.cz/trac/messif">
MESSIF</weblink> - Metric Similarity Search Implementation Framework by Michal Batko and David Novak.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.obsearch.net/">
OBSearch</weblink> - Metric Similarity Search (distributed, GPL). Implementation by Arnoldo Muller, developed during Google Summer of Code 2007.</entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
