<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 22:49:59[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Version space</title>
<id>7578809</id>
<revision>
<id>217564758</id>
<timestamp>2008-06-06T16:33:45Z</timestamp>
<contributor>
<username>DOI bot</username>
<id>6652755</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
</categories>
</header>
<bdy>

A <b>version space</b> in <link xlink:type="simple" xlink:href="../451/6968451.xml">
concept learning</link> or <link xlink:type="simple" xlink:href="../877/14877.xml">
induction</link> is the subset of all hypotheses that are <link xlink:type="simple" xlink:href="../802/75802.xml">
consistent</link> with the observed training examples (Mitchell 1997).  This set contains all hypotheses that have not been eliminated as a result of being in conflict with observed data.  <p>

<image location="right" width="300px" src="Version_space.png" type="thumb">
<caption>

Version space for a "rectangle" hypothesis language in two dimensions.  Green pluses are positive examples, and red circles are negative examples.  GB is the maximally <b>general</b> positive hypothesis boundary, and SB is the maximally <b>specific</b> positive hypothesis boundary.  The intermediate (thin) rectangles represent the hypotheses in the version space.
</caption>
</image>

</p>
<sec>
<st>
The Version Space algorithm</st>
<p>

In settings where there is a generality-ordering on hypotheses, it is possible to represent the version space by two sets of hypotheses: (1) the <b>most specific</b> consistent hypotheses, and (2) the <b>most general</b> consistent hypotheses, where "consistent" indicates agreement with observed data.   The most specific hypotheses (i.e., the specific boundary <b>SB</b>) are the hypotheses that cover the observed positive training examples, and as little of the remaining feature space as possible.  These are hypotheses which if reduced any further would <it>exclude</it> a <it>positive</it> training example, and hence become inconsistent.  These minimal hypotheses essentially constitute a (pessimistic) claim that the true concept is defined just by the <it>positive</it> data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be negative.  (I.e., if data has not previously been ruled in, then it's ruled out.) </p>
<p>

The most general hypotheses (i.e., the general boundary <b>GB</b>) are those which cover the observed positive training examples, but also cover as much of the remaining feature space without including any negative training examples.  These are hypotheses which if enlarged any further would <it>include</it> a <it>negative</it> training example, and hence become inconsistent.  These maximal hypotheses essentially constitute a (optimistic) claim  that the true concept is defined just by the <it>negative</it> data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be positive. (I.e., if data has not previously been ruled out, then it's ruled in.) </p>
<p>

Thus, during the learning process, the version space (which itself is a set – possibly infinite – containing <it>all</it> consistent hypotheses) can be represented by just its lower and upper bounds (maximally general and maximally specific hypothesis sets), and learning operations can be performed just on these representative sets.</p>

</sec>
<sec>
<st>
Historical background</st>
<p>

The notion of Version Spaces was introduced by Mitchell as a framework for understanding the basic problem of supervised learning within the context of <it>solution search</it>.  Although the basic "candidate elimination" search method that accompanies the Version Space framework is <it>not</it> a popular learning algorithm (for various reasons, including the issue of <b>noise</b> (Russell &amp; Norvig 2002)), there <it>are</it> some practical implementations that have been developed (e.g., Sverdlik &amp; Reynolds 1992, Hong &amp; Tsang 1997, Dubois &amp; Quafafou 2002).   </p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../778/1634778.xml">
Rough set</link></instrumentality>
</artifact>
</system>
. [The rough set framework focuses on the case where ambiguity is introduced by an impoverished '''feature set'''. That is, the target concept cannot be decisively described because the available feature set fails to disambiguate objects belonging to different categories.  The version space framework focuses on the (classical induction) case where the ambiguity is introduced by an impoverished '''data set'''.  That is, the target concept cannot be decisively described because the available data fails to uniquely pick out a hypothesis.  Naturally, both types of ambiguity can occur in the same learning problem.]</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="#xpointer(//cite[@id=%22CITEREFRendell1986%22])">
Rendell (1986)</link> provides an interesting discussion of the general problem of induction.</entry>
<entry level="1" type="bullet">

 Mill (1843/2002) is the classic source on inductive logic.</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Dubois, Vincent;&#32;Quafafou, Mohamed&#32;(2002). "Concept learning with approximation: Rough version spaces".&#32;<it>Rough Sets and Current Trends in Computing: Proceedings of the Third International Conference, RSCTC 2002</it>: 239–246.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Hong, Tzung-Pai; Shian-Shyong Tsang&#32;(1997).&#32;"A generalized version space learning algorithm for noisy and uncertain data". <it>IEEE Transactions on Knowledge and Data Engineering</it>&#32;<b>9</b>&#32;(2): 336–340. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1109%2F69.591457">
10.1109/69.591457</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Mill, John Stuart&#32;(1843/2002). A System of Logic, Ratiocinative and Inductive: Being a Connected View of the Principles of Evidence and the Methods of Scientific Investigation.&#32;Honolulu, HI:&#32;University Press of the Pacific.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Mitchell, Tom M.&#32;(1997). Machine Learning.&#32;Boston:&#32;McGraw-Hill.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Rendell, Larry&#32;(1986).&#32;"A general framework for induction and a study of selective induction". <it>Machine Learning</it>&#32;<b>1</b>: 177–226. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF00114117">
10.1007/BF00114117</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite id="CITEREFRussellNorvig2003" style="font-style:normal"><link>
Russell, Stuart J.</link>&#32;&amp;&#32;<link>
Norvig, Peter</link>&#32;(2003),&#32;<it><weblink xlink:type="simple" xlink:href="http://aima.cs.berkeley.edu/">
</weblink></it>&#32;(2nd ed.), Upper Saddle River, NJ: Prentice Hall, ISBN 0-13-790395-2, </cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Sverdlik, W.;&#32;Reynolds, R.G.&#32;(1992). "Dynamic version spaces in machine learning".&#32;<it>Proceedings, Fourth International Conference on Tools with Artificial Intelligence (TAI '92)</it>: 308 - 315.</cite>&nbsp;</entry>
</list>
</p>





</sec>
</bdy>
</article>
