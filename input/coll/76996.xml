<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:51:07[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Self-organizing map</title>
<id>76996</id>
<revision>
<id>239250682</id>
<timestamp>2008-09-18T11:38:10Z</timestamp>
<contributor>
<username>BOTijo</username>
<id>3729068</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

A <b>self-organizing map (SOM)</b> is a type of <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link> that is trained using <link xlink:type="simple" xlink:href="../497/233497.xml">
unsupervised learning</link> to produce a low-dimensional (typically two dimensional), discretized representation of the input space of the training samples, called a <b>map</b>.  The map seeks to preserve the <link xlink:type="simple" xlink:href="../954/29954.xml">
topological</link> properties of the input space. <p>

<image location="right" width="300px" src="Synapse_Self-Organizing_Map.png" type="thumb">
<caption>

Self-Organizing Map showing <link>
US Congress</link> voting patterns visualized in <weblink xlink:type="simple" xlink:href="http://www.peltarion.com/products/products.html">
Synapse</weblink>. The first two boxes show clustering and distances while the remaining ones show the component planes. Red means a yes vote while blue means a no vote in the component planes (except the party component where red is <party wordnetid="108256968" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../070/32070.xml">
Republican</link></party>
 and blue is <party wordnetid="108256968" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../544/5043544.xml">
Democrat</link></party>
.)
</caption>
</image>

This makes SOM useful for <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../963/575963.xml">
visualizing</link></method>
</know-how>
</technique>
 low-dimensional views of high-dimensional data, akin to <link xlink:type="simple" xlink:href="../786/398786.xml">
multidimensional scaling</link>. The model was first described as an artificial neural network by the <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../577/10577.xml">
Finnish</link></country>
 professor <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../591/1685591.xml">
Teuvo Kohonen</link></scientist>
</causal_agent>
</person>
</physical_entity>
, and is sometimes called a <b>Kohonen map</b>.</p>
<p>

Like most artificial neural networks, SOMs operate in two modes: training and mapping.  Training builds the map using input examples. It is a competitive process, also called <link xlink:type="simple" xlink:href="../805/47805.xml">
vector quantization</link>.  Mapping automatically classifies a new input vector.</p>

<sec>
<st>
 Network structure </st>

<p>

A self-organizing map consists of components called nodes or neurons.  Associated with each node is a weight vector of the same dimension as the input data vectors and a position in the map space. The usual arrangement of nodes is a regular spacing in a hexagonal or rectangular grid.  The self-organizing map describes a mapping from a higher dimensional input space to a lower dimensional map space.  The procedure for placing a vector from data space onto the map is to find the node with the closest weight vector to the vector taken from data space and to assign the map coordinates of this node to our vector.</p>
<p>

While it is typical to consider this type of network structure as related to <link xlink:type="simple" xlink:href="../332/1706332.xml">
feedforward networks</link> where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation.</p>
<p>

Useful extensions include using <link xlink:type="simple" xlink:href="../800/74800.xml">
toroidal</link> grids where opposite edges are connected and using large numbers of nodes.  It has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to <information wordnetid="105816287" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../407/1860407.xml">
K-means</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</datum>
</rule>
</event>
</information>
, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.  </p>
<p>

It is also common to use the U-matrix.  The U-matrix value of a particular node is the average distance between the node and its closest neighbors.  In a rectangular grid for instance, we might consider the closest 4 or 8 nodes. </p>
<p>

Large SOMs display properties which are emergent.  Therefore, large maps are preferable to smaller ones. In maps consisting of thousands of nodes, it is possible to perform cluster operations on the map itself.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>

</sec>
<sec>
<st>
 Learning algorithm </st>

<p>

The goal of learning in the self-organizing map is to cause different parts of the network to respond similarly to certain input patterns. This is partly motivated by how visual, auditory or other <link xlink:type="simple" xlink:href="../912/43912.xml">
sensory</link> information is handled in separate parts of the <link xlink:type="simple" xlink:href="../686/58686.xml">
cerebral cortex</link> in the <link xlink:type="simple" xlink:href="../620/490620.xml">
human brain</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

The weights of the neurons are initialized either to small random values or sampled evenly from the subspace spanned by the two largest <link xlink:type="simple" xlink:href="../340/76340.xml">
principal component</link> <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvectors</link>. With the latter alternative, learning is much faster because the initial weights already give good approximation of SOM weights.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

The network must be fed a large number of example vectors that represent, as close as possible, the kinds of vectors expected during mapping. The examples are usually administered several times.</p>
<p>

The training utilizes <link>
competitive learning</link>. When a training example is fed to the network, its <link xlink:type="simple" xlink:href="../932/53932.xml">
Euclidean distance</link> to all weight vectors is computed. The neuron with weight vector most similar to the input is called the best matching unit (BMU). The weights of the BMU and neurons close to it in the SOM lattice are adjusted towards the input vector. The magnitude of the change decreases with time and with distance from the BMU. The update formula for a neuron with weight vector <b>Wv</b>(t) is
<indent level="1">

<b>Wv</b>(t + 1) = <b>Wv</b>(t) + Θ (v, t) α(t)(<b>D</b>(t) - <b>Wv</b>(t)),
</indent>
where α(t) is a <link xlink:type="simple" xlink:href="../260/48260.xml">
monotonically decreasing</link> learning coefficient and <b>D</b>(t) is the input vector. The neighborhood function Θ (v, t) depends on the lattice distance between the BMU and neuron <it>v</it>. In the simplest form it is one for all neurons close enough to BMU and zero for others, but a <link xlink:type="simple" xlink:href="../552/245552.xml">
gaussian function</link> is a common choice, too. Regardless of the functional form, the neighborhood function shrinks with time.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> At the beginning when the neighborhood is broad, the self-organizing takes place on the global scale. When the neighborhood has shrunk to just a couple of neurons the weights are converging to local estimates.</p>
<p>

This process is repeated for each input vector for a (usually large) number of cycles <b>λ</b>. The network winds up associating output nodes with groups or patterns in the input data set. If these patterns can be named, the names can be attached to the associated nodes in the trained net.</p>
<p>

During mapping, there will be one single <it>winning</it> neuron: the neuron whose weight vector lies closest to the input vector. This can be simply determined by calculating the Euclidean distance between input vector and weight vector.</p>
<p>

While representing input data as vectors has been emphasized in this article, it should be noted that any kind of object which can be represented digitally and which has an appropriate distance measure associated with it and in which the necessary operations for training are possible can be used to construct a self-organizing map.  This includes matrices, continuous functions or even other self-organizing maps.</p>

</sec>
<sec>
<st>
 Example </st>

<ss1>
<st>
 Preliminary definitions </st>

<p>

Consider a 10×10 array of nodes each of which contains a weight vector and is aware of its location in the array. Each weight vector is of the same dimension as the node's input vector. The weights are initially set to random values.</p>
<p>

Now we need input to feed the map. (The generated map and the given input exist in separate subspaces)  We will create three vectors to represent colors. Colors can be represented by their red, green, and blue components. Consequently our input vectors will have three components, each corresponding to a color space. The input vectors will be:
R = 255, 0, 0&amp;gt;
G = 0, 255, 0&amp;gt;
B = 0, 0, 255&amp;gt;</p>

</ss1>
<ss1>
<st>
 Variables </st>
<p>

Vectors are in <b>bold</b>
t = current iteration
λ = limit on time iteration
<b>Wv</b> = current weight vector
<b>D</b> = target input
Θ(t) = restraint due to distance from BMU - usually called the neighbourhood function
α(t) = learning restraint due to time</p>

</ss1>
<ss1>
<st>
 Stepping through the algorithm </st>
<p>

<list>
<entry level="1" type="number">

 Randomize the map's nodes' weight vectors</entry>
<entry level="1" type="number">

 Grab an input vector</entry>
<entry level="1" type="number">

 Traverse each node in the map</entry>
<entry level="2" type="number">

 Use <link xlink:type="simple" xlink:href="../932/53932.xml">
Euclidean distance</link> formula to find similarity between the input vector and the map's node's weight vector</entry>
<entry level="2" type="number">

 Track the node that produces the smallest distance (this node is the best matching unit, BMU)</entry>
<entry level="1" type="number">

 Update the nodes in the neighbourhood of BMU by pulling them closer to the input vector</entry>
<entry level="2" type="number">

 <b>Wv</b>(t + 1) = <b>Wv</b>(t) + Θ(t)α(t)(<b>D</b>(t) - <b>Wv</b>(t))</entry>
<entry level="1" type="number">

 Increment t and repeat while t  λ</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 Interpretation </st>

<p>

There are two ways to interpret a SOM. Because in the training phase
weights of the whole neighborhood are moved in the same direction,
similar items tend to excite adjacent neurons. Therefore, SOM forms a semantic map where similar samples are mapped close together and
dissimilar apart.</p>
<p>

The other way is to think of neuronal weights as pointers to the input space. They form a discrete approximation of the distribution of training samples. More neurons point to regions with high training sample concentration and fewer where the samples are scarce.</p>
<p>

SOM may be considered a nonlinear generalization of <link xlink:type="simple" xlink:href="../340/76340.xml">
principal component analysis</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>.</p>

</sec>
<sec>
<st>
 Alternatives </st>

<p>

<link xlink:type="simple" xlink:href="../054/1091054.xml">
Generative topographic map</link>s (GTM) are a potential alternative to SOMs.  In the sense that GTM explicitly requires a smooth and continuous mapping from the input space to the map space, it is topology preserving.  However, in a practical sense, this measure of topological preservation is lacking.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>

</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
 <cite id="Reference-Ultsch-2007" style="font-style:normal" class="book">Ultsch, Alfred&#32;(2007). Emergence in Self-Organizing Feature Maps, In Proceedings Workshop on Self-Organizing Maps (WSOM '07).&#32;Bielefeld, Germany. ISBN 978-3-00-022473-7.</cite>&nbsp;</entry>
<entry id="2">
 <cite id="Reference-Haykin-1999" style="font-style:normal" class="book">Haykin, Simon&#32;(1999).&#32;"9. Self-organizing maps", Neural networks - A comprehensive foundation, 2nd edition,&#32;Prentice-Hall. ISBN 0-13-908385-5.</cite>&nbsp;</entry>
<entry id="3">
"<weblink xlink:type="simple" xlink:href="http://www.cis.hut.fi/projects/somtoolbox/theory/somalgorithm.shtml">
Intro to SOM by Teuvo Kohonen</weblink>".&#32;<it>SOM Toolbox</it>.&#32;Retrieved on <link>
2006-06-18</link>.
</entry>
<entry id="4">
<it>Yin H.</it> <weblink xlink:type="simple" xlink:href="http://pca.narod.ru/contentsgkwz.htm">
Learning Nonlinear Principal Manifolds by Self-Organising Maps</weblink>, In: Gorban A. N. et al (Eds.), LNCSE 58, Springer, 2007 ISBN 978-3-540-73749-0</entry>
<entry id="5">
Kaski, S..&#32;"<it>Data exploration using self-organizing maps, Acta Polytechnica Scandinavica, Mathematics, Computing and Management in Engineering Series No. 82, Espoo 1997, 57 pp.</it>".</entry>
</reflist>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Rustum R., Adeloye A.J., and Scholz M. , 2008. Applying Kohonen Self-organizing Map as a Software Sensor to Predict the   Biochemical Oxygen Demand. Water Environment Research, 80 (1), 2008. </entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 Rustum R and A.J.Adeloye, 2007. Replacing outliers and missing values from activated sludge data using Kohonen Self Organizing Map. Journal of Environmental Engineering, Vol. 133,No. 9, September 1, 2007, pp. 909-916.</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../164/1164.xml">
Artificial intelligence</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../157/361157.xml">
Biologically-inspired computing</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../636/263636.xml">
Connectionism</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../253/42253.xml">
Data mining</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../059/5254059.xml">
Growing self-organizing map</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../632/4922632.xml">
Growing neural gas</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../054/1091054.xml">
Generative topographic map</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../261/309261.xml">
Nonlinear dimensionality reduction</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/233488.xml">
Machine learning</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../706/126706.xml">
Pattern recognition</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../563/4141563.xml">
Predictive analytics</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../443/9651443.xml">
Radial basis function network</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../523/21523.xml">
Artificial Neural Network</link></entry>
<entry level="1" type="bullet">

 <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../571/13345571.xml">
Sammon's projection</link></function>
</mathematical_relation>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../024/4435024.xml">
J. Brant Arseneau</link></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 Chapter 15 <weblink xlink:type="simple" xlink:href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K15.pdf">
Kohonen networks</weblink> of <weblink xlink:type="simple" xlink:href="http://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/pmwiki/pmwiki.php?n=Books.NeuralNetworksBook">
<it>Neural Networks - A Systematic Introduction''</it></weblink> by <link>
Raúl Rojas</link> (ISBN 978-3540605058)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cis.hut.fi/teuvo/">
Prof. Kohonen's website in Helsinki University of Technology</weblink> See -&amp;gt;research -&amp;gt;software for Toolkits and C and Matlab code for SOM's</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.scholarpedia.org/wiki/index.php?title=Kohonen_Network">
Kohonen network</weblink> a Scholarpedia article on the Self-Organizing Map</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://blog.peltarion.com/2007/04/10/the-self-organized-gene-part-1">
"The Self-Organized Gene, Part 1"</weblink>, and <weblink xlink:type="simple" xlink:href="http://blog.peltarion.com/2007/06/13/the-self-organized-gene-part-2">
Part 2</weblink> beginners' level introduction to competitive learning and self-organizing maps.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.shef.ac.uk/psychology/gurney/notes/l7/l7.html">
Chapter 7: <it>Competition and self organisation: Kohonen nets''</it></weblink>, part of Kevin Gurney's web-book on Neural Nets.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://neurondotnet.freehostia.com/index.html">
NeuronDotNet</weblink>, Implementation in C# along with sample applications</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.neuroinformatik.ruhr-uni-bochum.de/ini/VDM/research/gsn/DemoGNG/GNG.html">
Nice application to visualize some neural-network algorithms.</weblink> Written in Java, so you need a Java-plugin in your browser.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.heatonresearch.com/articles/6/page1.html">
Programming a Kohonen Neural Network in Java</weblink> part of a Java Neural Network web book.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.mathematik.uni-marburg.de/~databionics/de">
Databionics</weblink> Prof. A. Ultsch's websites on Visualization and Datamining with SOM </entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.e-nuts.net/en/self-organizing-map">
Kohonen Neural Network</weblink> applied to the Traveling Salesman Problem (using three dimensions).</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.len.ro/work/ai/som-neural-networks">
Python SOM example</weblink>: Simple SOM python library along with a 2D implementation and some very suggestive images. </entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.viscovery.net/somine/index.php?sprache=en">
Viscovery SOMine</weblink>: SOM Technology Tool from Viscovery Software (formerly Eudaptics Software)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://ai-depot.com/Tutorial/SomColour.html">
SomColour tutorial</weblink>: Self-Organising Maps For Colour Recognition</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://websom.hut.fi/websom/">
WEBSOM</weblink>, a Kohonen network project</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.staffmapper.com/">
Staff-Mapper</weblink>, Solution for mapping knowledge based on survey results</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.samhill.co.uk/kohonen/">
Kohonen Neural Network Simulation</weblink>, Demonstration of and detailed discussion about Kohonen Neural Networks, a form of SOM</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://netzspannung.org/about/tools/semantic-map/index.xsp?lang=en">
The Semantic Map Interface based on Kohonen Map</weblink> for Text analysis and visualization on <weblink xlink:type="simple" xlink:href="http://netzspannung.org/index_en_flash.html">
netzspannung.org</weblink>- the German platform for media art &amp; electronic culture.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://homepages.feis.herts.ac.uk/~nngroup/software.html">
Java Competitve Learning Application</weblink> A suite of Unsupervised Neural Networks (including Self-organizing map) written in Java.  Complete with all source code.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.sarstech.com/kohonen/">
Kohonen Map Java Applet</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.visipoint.fi/">
SOM and Sammon's mapping software for multivariate data analysis</weblink></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">
</entry>
</list>
</p>


</sec>
</bdy>
</article>
