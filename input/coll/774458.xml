<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:39:59[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Metaheuristic</title>
<id>774458</id>
<revision>
<id>244552252</id>
<timestamp>2008-10-11T11:35:11Z</timestamp>
<contributor>
<username>Diego Moya</username>
<id>124142</id>
</contributor>
</revision>
<categories>
<category>Heuristics</category>
<category>Operations research</category>
<category>Applied mathematics</category>
<category>Mathematical optimization</category>
</categories>
</header>
<bdy>

A <b>metaheuristic</b> is a <link xlink:type="simple" xlink:href="../452/63452.xml">
heuristic</link> method for solving a very general class of <link xlink:type="simple" xlink:href="../213/5213.xml">
computational</link> problems by combining user-given <link xlink:type="simple" xlink:href="../778/3957778.xml">
black-box procedure</link>s—usually heuristics themselves—in a hopefully efficient way. The name combines the <language wordnetid="106282651" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../887/11887.xml">
Greek</link></language>
 prefix "<link xlink:type="simple" xlink:href="../814/1018814.xml">
meta</link>" ("beyond", here in the sense of "higher level") and "heuristic" (from ευρισκειν, <it>heuriskein</it>, "to find").<p>

Metaheuristics are generally applied to problems for which there is no satisfactory problem-specific <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> or heuristic; or when it is not practical to implement such a method. Most commonly used metaheuristics are targeted to <link xlink:type="simple" xlink:href="../555/420555.xml">
combinatorial optimization</link> problems, but of course can handle any problem that can be recast in that form, such as solving <link xlink:type="simple" xlink:href="../239/2701239.xml">
boolean equations</link>.</p>

<sec>
<st>
 Overview </st>
<p>

The goal of combinatorial optimization is to find a discrete mathematical object (such as a <link xlink:type="simple" xlink:href="../937/1189937.xml">
bit string</link> or <link xlink:type="simple" xlink:href="../027/44027.xml">
permutation</link>) that maximizes (or minimizes) an arbitrary <link xlink:type="simple" xlink:href="../427/185427.xml">
function</link> specified by the user of the metaheuristic. These objects are generically called <it>states</it>, and the <link xlink:type="simple" xlink:href="../691/26691.xml">
set</link> of all candidate states is the <it>search space</it>.  The nature of the states and the search space are usually problem-specific.</p>
<p>

The function to be optimized is called the <it>goal function</it>, or <it>objective function</it>, and is usually provided by the user as a black-box procedure that evaluates the function on a given state.  Depending on the meta-heuristic, the user may have to provide other black-box procedures that, say, produce a new random state, produce variants of a given state, pick one state among several, provide upper or lower <link xlink:type="simple" xlink:href="../693/42693.xml">
bound</link>s for the goal function over a set of states, and the like. </p>
<p>

Some metaheuristics maintain at any instant a single <it>current state</it>, and replace that state by a new one. This basic step is sometimes called a <it>state transition</it> or <it>move</it>.  The move is <it>uphill</it> or <it>downhill</it> depending on whether the goal function value increases or decreases. The new state may be constructed from scratch by a user-given <it>generator</it> procedure. Alternatively, the new state be derived from the current state by a user-given <it>mutator</it> procedure; in this case the new state is called a <it>neighbour</it> of the current one.  Generators and mutators are often <link>
probabilistic procedures</link>. The set of new states that can be produced by the mutator is the <it>neighbourhood</it> of the current state.</p>
<p>

More sophisticated meta-heuristics maintain, instead of a single current state, a <it>current pool</it> with several candidate states.  The basic step then may add or delete states from this pool. User-given procedures may be called to select the states to be discarded, and to generate the new ones to be added. The latter may be generated by <it>combination</it> or <it>crossover</it> of two or more states from the pool.</p>
<p>

A metaheuristic also keep track of the <it>current optimum</it>, the optimum state among those already evaluated so far.</p>
<p>

Since the set of candidates is usually very large, metaheuristics are typically implemented so that they can be interrupted after a client-specified <it>time budget</it>.  If not interrupted, some <it>exact metaheuristics</it> will eventually check all candidates, and use heuristic methods only to choose the order of enumeration; therefore, they will always find the true optimum, if their time budget is large enough.  Other metaheuristics give only a weaker probabilistic guarantee, namely that, as the time budget goes to infinity, the probability of checking every candidate tends to 1.</p>

</sec>
<sec>
<st>
Timeline</st>






<p>

Timeline of main metaheuristics.</p>


<p>

<list>
<entry level="1" type="bullet">

 1952: first works on stochastics optimization methods<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.</entry>
<entry level="1" type="bullet">

 1954: Barricelli carry out the first simulations of the <link xlink:type="simple" xlink:href="../236/9236.xml">
evolution</link> process and use them on general optimization problems<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>.</entry>
<entry level="1" type="bullet">

 1965: Rechenberg conceives the first algorithm using <link xlink:type="simple" xlink:href="../033/940033.xml">
evolution strategies</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.</entry>
<entry level="1" type="bullet">

 1966: Fogel, Owens et Walsh propose <link xlink:type="simple" xlink:href="../689/460689.xml">
evolutionary programming</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>.</entry>
<entry level="1" type="bullet">

 1970: Hastings conceives the <link xlink:type="simple" xlink:href="../107/56107.xml">
Metropolis-Hastings algorithm</link>, which can sample any <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.</entry>
<entry level="1" type="bullet">

 1975: <link xlink:type="simple" xlink:href="../868/147868.xml">
John Holland</link> proposes the first <link xlink:type="simple" xlink:href="../254/40254.xml">
genetic algorithm</link>s<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>.</entry>
<entry level="1" type="bullet">

 1980: Smith describes <link xlink:type="simple" xlink:href="../424/12424.xml">
genetic programming</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>.</entry>
<entry level="1" type="bullet">

 1983: based on Hastings's work, Kirkpatrick, Gelatt and Vecchi conceive <link xlink:type="simple" xlink:href="../244/172244.xml">
simulated annealing</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>.</entry>
<entry level="1" type="bullet">

 1985: independently, Černý proposes the same algorithm<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>.</entry>
<entry level="1" type="bullet">

 1986: first mention of the term "meta-heuristic" by <physical_entity wordnetid="100001930" confidence="0.8">
<champion wordnetid="109906704" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<player wordnetid="110439851" confidence="0.8">
<contestant wordnetid="109613191" confidence="0.8">
<rival wordnetid="110533013" confidence="0.8">
<coach wordnetid="109931640" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<trainer wordnetid="110722575" confidence="0.8">
<link xlink:type="simple" xlink:href="../926/6820926.xml">
Fred Glover</link></trainer>
</causal_agent>
</leader>
</coach>
</rival>
</contestant>
</player>
</person>
</champion>
</physical_entity>
, during the conception of <it><link xlink:type="simple" xlink:href="../937/381937.xml">
tabu search</link></it><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref> :</entry>
<entry level="1" type="indent">

 <cite id="CITEREF" style="font-style:normal"></cite>&nbsp;</entry>
<entry level="1" type="bullet">

 1986: Farmer, Packard and Perelson work on <link xlink:type="simple" xlink:href="../987/1589987.xml">
artificial immune system</link>s<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref>.</entry>
<entry level="1" type="bullet">

 1988: the first conference on genetic algorithms is organized at the <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../695/384695.xml">
University of Illinois at Urbana-Champaign</link></university>
.</entry>
<entry level="1" type="bullet">

 1988: works on the collective behaviour of ants finds an application in <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref>.</entry>
<entry level="1" type="bullet">

 1988: Koza register his first patent on genetic programming<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref>.</entry>
<entry level="1" type="bullet">

 1989: Goldberg publishes one of the best known books on genetic algorithms<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref>.</entry>
<entry level="1" type="bullet">

 1989: <link xlink:type="simple" xlink:href="../872/3119872.xml">
Evolver</link>, the first optimisation software using the genetic algorithm, is released by the <link xlink:type="simple" xlink:href="../343/5497343.xml">
Axcelis</link> company.</entry>
<entry level="1" type="bullet">

 1989: The term "<link xlink:type="simple" xlink:href="../208/3989208.xml">
memetic algorithm</link>" is first used by Moscato<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2215%22])">15</ref>.</entry>
<entry level="1" type="bullet">

 1991: the <link xlink:type="simple" xlink:href="../615/588615.xml">
ant colony algorithm</link>s are proposed by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../360/1183360.xml">
Marco Dorigo</link></educator>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
, in his Ph.D. thesis<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2216%22])">16</ref>.</entry>
<entry level="1" type="bullet">

 1993: The journal <it><link xlink:type="simple" xlink:href="../020/268020.xml">
Evolutionary Computation</link></it> begins to be published by the <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../879/18879.xml">
MIT</link></university>
.</entry>
<entry level="1" type="bullet">

 1995: Feo and Resende propose the <link xlink:type="simple" xlink:href="../034/1804034.xml">
greedy randomized adaptive search procedure</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2217%22])">17</ref>.</entry>
<entry level="1" type="bullet">

 1995: Kennedy and Eberhart conceive <link xlink:type="simple" xlink:href="../083/337083.xml">
particle swarm optimization</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2218%22])">18</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2219%22])">19</ref>.</entry>
<entry level="1" type="bullet">

 1996: Mühlenbein and Paaß work on <link xlink:type="simple" xlink:href="../637/3062637.xml">
estimation of distribution algorithm</link>s<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2220%22])">20</ref>.</entry>
<entry level="1" type="bullet">

 1997: Storn and Price propose a <link xlink:type="simple" xlink:href="../241/3132241.xml">
differential evolution</link> algorithm<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2221%22])">21</ref>.</entry>
<entry level="1" type="bullet">

 1997: Rubinstein works on the <link xlink:type="simple" xlink:href="../980/5767980.xml">
cross entropy method</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2222%22])">22</ref>.</entry>
<entry level="1" type="bullet">

 1999 : Boettcher propose the <link xlink:type="simple" xlink:href="../828/4832828.xml">
extremal optimization</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2223%22])">23</ref>.</entry>
<entry level="1" type="bullet">

 2000: First <link xlink:type="simple" xlink:href="../162/901162.xml">
interactive genetic algorithms</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2224%22])">24</ref>.</entry>
<entry level="1" type="bullet">

 2001: Geem, Kim, and Longanathan propose <link xlink:type="simple" xlink:href="../902/9485902.xml">
harmony search</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2225%22])">25</ref>.</entry>
<entry level="1" type="bullet">

 2004: Nakrani and Tovey describe the <link>
honey bee algorithm</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2226%22])">26</ref>.</entry>
<entry level="1" type="bullet">

 2008: Yang describes the <link>
firefly algorithm</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2227%22])">27</ref>.</entry>
<entry level="1" type="bullet">

 2008: Karaboga and Basturk describe the <link>
artificial bee algorithm</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2228%22])">28</ref>.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Meta-heuristics concepts</st>
<p>

Some well-known meta heuristics are 
<list>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../864/147864.xml">
Random optimization</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../942/313942.xml">
Local search</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../247/89247.xml">
Greedy algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 and <link xlink:type="simple" xlink:href="../002/364002.xml">
hill-climbing</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../002/364002.xml">
Random-restart hill climbing</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../271/148271.xml">
Best-first search</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../254/40254.xml">
Genetic algorithms</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../244/172244.xml">
Simulated annealing</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../937/381937.xml">
Tabu search</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../615/588615.xml">
Ant colony optimization</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../034/1804034.xml">
GRASP</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../827/4958827.xml">
Stochastic Diffusion Search</link></entry>
<entry level="1" type="bullet">

<link>
Generalized extremal optimization</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../902/9485902.xml">
Harmony search</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link>
Variable Neighbourhood Search</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../558/100558.xml">
A*</link></entry>
</list>
</p>
<p>

Innumerable variants and hybrids of these techniques have been proposed, and many more applications of metaheuristics to specific problems have been reported. This is an active field of research, with a considerable literature, a large community of researchers and users, and a wide range of applications.</p>

</sec>
<sec>
<st>
General criticisms</st>
<p>

While there are many computer scientists who are enthusiastic advocates of metaheuristics, there are also many who are highly critical of the concept and have little regard for much of the research that is done on it. </p>
<p>

Those critics point out, for one thing, that the general goal of the typical metaheuristic &mdash; the efficient optimization of an arbitrary black-box function—cannot be solved efficiently, since for any metaheuristic <it>M</it> one can easily build a function <it>f</it> that will force <it>M</it> to enumerate the whole search space (or worse). Indeed, the "<link xlink:type="simple" xlink:href="../402/1297402.xml">
no-free-lunch theorem</link>" says that over the set of all mathematically possible problems, each optimization algorithm will do on average as well as any other. Thus, at best, a specific metaheuristic can be efficient only for restricted classes of goal functions (usually those that are partially "smooth" in some sense).  However, when these restrictions are stated at all, they either exclude most applications of interest, or make the problem amenable to specific solution methods that are much more efficient than the meta-heuristic.  </p>
<p>

Moreover, all metaheuristics rely on auxiliary procedures (producers, mutators, etc.) that are given by the user as black-box functions. It turns out that the effectiveness of a metaheuristic on a particular problem depends almost exclusively on these auxiliary functions, and very little on the metaheuristic itself.  Given any two distinct metaheuristics <it>M</it> and <it>N</it>, and almost any goal function <it>f</it>, it is usually possible to write a set of auxiliary procedures that will make <it>M</it> find the optimum much more efficient than <it>N</it>, by many orders of magnitude; or vice-versa. In fact, since the auxiliary procedures are usually unrestricted, one can submit the basic step of metaheuristic <it>M</it> as the generator or mutator for <it>N</it>.  Because of this extreme generality, one cannot say that any metaheuristic is better than any other, <it>not even for a specific class of problems</it>.  In particular, no meta-heuristic can be shown to be better for any specific problem than <link>
brute force search</link>, or the following "banal metaheuristic":
<list>
<entry level="1" type="number">

 Call the user-provided state generator.</entry>
<entry level="1" type="number">

 Print the resulting state.</entry>
<entry level="1" type="number">

 Stop.</entry>
</list>
</p>
<p>

Finally, all metaheuristic optimization techniques are extremely crude when evaluated by the standards of (<link xlink:type="simple" xlink:href="../122/6122.xml">
continuous</link>) <link>
nonlinear optimization</link>. Within this area, it is well-known that to find the optimum of a smooth function on <it>n</it> variables one must essentially obtain its <link xlink:type="simple" xlink:href="../108/412108.xml">
Hessian matrix</link>, the <it>n</it> by <it>n</it> matrix of its second derivatives.  If the function is given as a black-box procedure, then one must call it about <it>n</it>2/2 times, and solve an <it>n</it> by <it>n</it> <link xlink:type="simple" xlink:href="../087/113087.xml">
system of linear equations</link>, before one can make the first useful step towards the minimum. However, none of the common metaheuristics incorporate or accommodate this procedure. At best, they can be seen as computing some crude approximation to the local <link xlink:type="simple" xlink:href="../461/12461.xml">
gradient</link> of the goal function, and moving more or less "downhill".  But gradient-descent is can be extremely inefficient for non-linear optimization.  For example, consider the problem of finding a pair of numbers <it>x</it>,<it>y</it> that minimizes the quadratic function <it>Q</it>(<it>x</it>,<it>y</it>) = 1000000(<it>x</it> + <it>y</it> - 1000)2 + (<it>x</it> - <it>y</it> - 10)2.  Gradient-descent methods will generally take a very long time to reach the minimum from, say, (1000,0); whereas Hessian-based methods will reach it in one step.  Unfortunately, "narrow valley" functions like this one are increasingly likely to occur as the dimension of the space increases.</p>
<p>

Even though meta-heuristics are often used for discrete or non-differentiable functions, or black-box functions whose derivatives are not available, they cannot be expected to be of any value unless there is some correlation between goal function values at nearby candidate solutions—in other words, unless the  goal function has a globally smooth continuous component more or less hidden by the jumps and bumps created by the discreteness constraints.  Yet none of the popular meta-heuristics uses the know-how of continuous optimization when trying to exploit that continuous component.  For example, if the problem is to find two <it>integers</it> that minimize the <it>Q</it> function above, known meta-heuristics (including genetic ones) will fail to notice the overall quadratic behavior of <it>Q</it>, and will essentially behave as a random local search—or worse.  (Note that this remark refers to the <it>global</it> behavior of the goal function, not the <it>local</it> smoothness of a continuous goal function with many local minima.  Such local smoothness is most effectively exploited by using continuous optimization methods inside the generator/mutator procedures, so that the meta-heuristic only sees a discrete search space consisting of the local minima.)</p>

</sec>
<sec>
<st>
Pragmatics</st>
<p>

Independently of whether those criticisms are valid or not, metaheuristics can be terribly wasteful if used indiscriminately (so would be classical heuristics).  Since their performance is critically dependent on the user-provided generators and mutators, one should concentrate on improving these procedures, rather than twiddling the parameters of sophisticated metaheuristics.  A trivial metaheuristic with a good mutator will usually run circles around a sophisticated one with a poor mutator (and a good problem-specific heuristic will often do much better than both). In this area, more than in any other, a few hours of reading, thinking and programming can easily save months of computer time. On the other hand, this generalization does not necessarily extend equally to all problem domains. The use of <link xlink:type="simple" xlink:href="../254/40254.xml">
genetic algorithms</link>, for example, has produced evolved design solutions that exceed the best human-produced solutions despite years of theory and research. Problem domains falling into this category are often problems of <link xlink:type="simple" xlink:href="../555/420555.xml">
combinatorial optimization</link> and include the design of <link xlink:type="simple" xlink:href="../061/562061.xml">
sorting networks</link>, and <link xlink:type="simple" xlink:href="../175/3105175.xml">
evolved antenna</link>s, among others.</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../518/11220518.xml">
Search-based software engineering</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../553/17667553.xml">
Hyper-heuristic</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
Robbins, H. and Monro, S., <it>A Stochastic Approximation Method</it>, Annals of Mathematical Statistics, vol. 22, pp. 400-407, 1951</entry>
<entry id="2">
Barricelli, Nils Aall, <it>Esempi numerici di processi di evoluzione</it>, Methodos, pp. 45-68, 1954</entry>
<entry id="3">
Rechenberg, I., <it>Cybernetic Solution Path of an Experimental Problem</it>, Royal Aircraft Establishment Library Translation, 1965</entry>
<entry id="4">
Fogel, L., Owens, A.J., Walsh, M.J., <it>Artificial Intelligence through Simulated Evolution</it>, Wiley, 1966</entry>
<entry id="5">
 W.K. Hastings. <it>Monte Carlo Sampling Methods Using Markov Chains and Their Applications</it>, Biometrika, volume 57, 1, pages 97-109, 1970</entry>
<entry id="6">
Holland, John H., <it>Adaptation in Natural and Artificial Systems</it>, University of Michigan Press, Ann Arbor, 1975</entry>
<entry id="7">
Smith, S.F., <it>A Learning System Based on Genetic Adaptive Algorithms</it>, PhD dissertation (University of Pittsburgh), 1980</entry>
<entry id="8">
S. Kirkpatrick, C. D. Gelatt et M. P. Vecchi, <it>Optimization by Simulated Annealing</it>, Science, volume 220,  4598, pages 671-680, 1983</entry>
<entry id="9">
V. Černý <it>A thermodynamical approach to the travelling salesman problem : an efficient simulation algorithm</it> Journal of Optimization Theory and Applications, volume45, pages 41-51, 1985</entry>
<entry id="10">
Fred GLover, Future Paths for Integer Programming and Links to Artificial Intelligence, Comput. &amp; Ops. Res.Vol. 13, No.5, pp. 533-549, 1986</entry>
<entry id="11">
J.D. Farmer, N. Packard and A. Perelson, <it>The immune system, adaptation and machine learning</it>, Physica D, vol. 22, pp. 187--204, 1986</entry>
<entry id="12">
F. Moyson, B. Manderick, <it>The collective behaviour of Ants : an Example of Self-Organization in Massive Parallelism</it>, Actes de AAAI Spring Symposium on Parallel Models of Intelligence, Stanford, Californie, 1988</entry>
<entry id="13">
Koza, John R. <it>Non-Linear Genetic Algorithms for Solving Problems</it>. United States Patent 4,935,877. Filed May 20, 1988. Issued June 19, 1990</entry>
<entry id="14">
Goldberg, David E., <it>Genetic Algorithms in Search, Optimization and Machine Learning</it>, Kluwer Academic Publishers, Boston, MA., 1989</entry>
<entry id="15">
P. Moscato, <it>On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts : Towards Memetic Algorithms</it>, Caltech Concurrent Computation Program, C3P Report 826, 1989.</entry>
<entry id="17">
Feo, T., Resende, M., <it>Greedy randomized adaptive search procedure</it>, Journal of Global Optimization, tome 42, page 32--37, 1992</entry>
<entry id="16">
M. Dorigo, <it>Optimization, Learning and Natural Algorithms</it>, Ph.D. Thesis, Politecnico di Milano, Italy, 1992.</entry>
<entry id="19">
Kennedy, J. et Eberhart, R. C., <it>Particle swarm optimization</it>, Proceedings of IEEE International Conference on Neural Networks, Piscataway, NJ. pp. 1942-1948, 1995</entry>
<entry id="18">
Eberhart, R. C. et Kennedy, J., <it>A new optimizer using particle swarm theory</it>, Proceedings of the Sixth International Symposium on Micromachine and Human Science, Nagoya, Japan. pp. 39-43, 1995</entry>
<entry id="21">
Rainer Storn, Kenneth Price, <it>Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces</it>, Journal of Global Optimization, volume 11, 4, pages 341-359, 1997</entry>
<entry id="20">
Mülhenbein, H., Paaß, G., <it>From recombination of genes to the estimation of distribution I. Binary parameters</it>, Lectures Notes in Computer Science 1411: Parallel Problem Solving from Nature, tome PPSN IV, pages 178--187, 1996</entry>
<entry id="23">
Stefan Boettcher, Allon G. Percus, "Extremal Optimization : Methods derived from Co-Evolution", Proceedings of the Genetic and Evolutionary Computation Conference (1999)</entry>
<entry id="22">
Rubinstein, R.Y., <it>Optimization of Computer simulation Models with Rare Events</it>, European Journal of Operations Research, 99, 89-112, 1997</entry>
<entry id="25">
Geem Z. W., Kim J. H., and Loganathan G. V.,A new heuristic optimization algorithm: harmony search, Simulation, vol. 76, 60 (2001)</entry>
<entry id="24">
Takagi, H., <it>Active user intervention in an EC Search</it>, Proceesings of the JCIS 2000</entry>
<entry id="27">
Yang X. S., Firefly algorithm (chapter 8) in: Nature-inspired Metaheuristic Algorithms, Luniver Press, (2008) </entry>
<entry id="26">
Nakrani S. and Tovey S., On honey bees and dynamic server allocation in Internet hosting centers, Adaptive Behaviour, vol. 12, 223 (2004)</entry>
<entry id="28">
Karaboga D. and Basturk B., On the performance of artificial
bee colony algorithm, Applied Soft Computing, vol. 8, 687 (2008) </entry>
</reflist>
</p>

</sec>
<sec>
<st>
 Further reading </st>
<p>

<list>
<entry level="1" type="bullet">

 C. Blum and A. Roli (2003). Metaheuristics in combinatorial optimization: Overview and conceptual comparison. <it>ACM Computing Surveys</it> 35(3) 268&ndash;308.</entry>
<entry level="1" type="bullet">

 Geem Z. W., Kim J. H., and Loganathan G. V., A new heuristic optimization algorithm: harmony search, Simulation, vol. 76, 60 (2001)</entry>
<entry level="1" type="bullet">

 Yang X. S., Firefly algorithm (chapter 8) in: Nature-inspired Metaheuristic Algorithms, Luniver Press, (2008).</entry>
<entry level="1" type="bullet">

 Karaboga D. and Basturk B., On the performance of artificial bee colony algorithm, Applied Soft Computing, vol. 8, 687 (2008).</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://paradiseo.gforge.inria.fr/">
ParadisEO</weblink>: a C++ framework dedicated to the reusable design of metaheuristics, as well as hybrid, parallel and distributed metaheuristics. </entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.metaheuristics.eu">
EU/ME</weblink> EU/ME (the EURO chapter on metaheuristics) is the largest working group on this topic. The website of EU/ME is the main platform for communication among metaheuristics researchers.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://dgpf.sourceforge.net/">
DGPF</weblink> A distributed framework for randomized, heuristic searches like GA and Hill Climbing which comes with a specialization for Genetic Programming and allows to combine different search algorithms.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://neo.lcc.uma.es/Software/MHTB/">
MHTB</weblink> A toolbox of metaheuristic algorithms for MATLAB. It offers single-solution, population-based and hybrids metaheuristics. With this toolbox you can solve optimization problems defined in the MATLAB language using metaheuristic algorithms implemented in C++ and Java.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://neo.lcc.uma.es/metal">
jMetal</weblink> jMetal is an object-oriented Java-based framework aimed at facilitating the development of metaheuristics for solving multi-objective optimization problems (MOPs). </entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://iridia.ulb.ac.be/sls-forum">
Metaheuristic / Stochastic Local Search Forum</weblink> A forum where practitioners and researchers can discuss and share knowledge about metaheuristics and stochastic local search algorithms.</entry>
</list>
</p>



</sec>
</bdy>
</article>
