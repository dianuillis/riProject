<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:40:37[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Curse of dimensionality</title>
<id>787776</id>
<revision>
<id>236473125</id>
<timestamp>2008-09-05T16:19:57Z</timestamp>
<contributor>
<username>RussBot</username>
<id>279219</id>
</contributor>
</revision>
<categories>
<category>numerical analysis</category>
<category>Machine learning</category>
<category>Dynamic programming</category>
</categories>
</header>
<bdy>

The <b><it>curse of dimensionality</it></b> is a term coined by <link xlink:type="simple" xlink:href="../173/730173.xml">
Richard Bellman</link> to describe the problem caused by the exponential increase in <link xlink:type="simple" xlink:href="../498/32498.xml">
volume</link> associated with adding extra dimensions to a (mathematical) space. <p>

For example, 100 evenly-spaced sample points suffice to sample a <link xlink:type="simple" xlink:href="../324/49324.xml">
unit interval</link> with no more than 0.01 distance between points; an equivalent sampling of a 10-dimensional <link xlink:type="simple" xlink:href="../763/407763.xml">
unit hypercube</link> with a lattice with a spacing of 0.01 between adjacent points would require 1020 sample points: thus, in some sense, the 10-dimensional hypercube can be said to be a factor of 1018 "larger" than the unit interval. (Adapted from an example by R. E. Bellman, see below.)</p>
<p>

Another way to envisage the "vastness" of high-dimensional Euclidean space is to compare the size of the <link xlink:type="simple" xlink:href="../443/606443.xml">
unit sphere</link> with the <link xlink:type="simple" xlink:href="../763/407763.xml">
unit cube</link> as the dimension of the space increases: as the dimension increases, the unit sphere becomes an insignificant volume relative to that of the unit cube; thus, in some sense, nearly all of the high-dimensional space is "far away" from the centre, or, to put it another way, the high-dimensional unit space can be said to consist almost entirely of the "corners" of the hypercube, with almost no "middle". (This is an important intuition for understanding the <link xlink:type="simple" xlink:href="../424/113424.xml">
chi-squared distribution</link>.)</p>

<sec>
<st>
The curse of dimensionality in optimization and learning</st>
<p>

The curse of dimensionality is a significant obstacle to solving dynamic <link xlink:type="simple" xlink:href="../033/52033.xml">
optimization</link> problems by numerical <link xlink:type="simple" xlink:href="../912/2060912.xml">
backwards induction</link> when the dimension of the 'state variable' is large. It also complicates <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> problems that involve learning a 'state-of-nature' (maybe infinite distribution) from a finite (low) number of data samples in a high-dimensional <link xlink:type="simple" xlink:href="../063/5245063.xml">
feature space</link> and <link xlink:type="simple" xlink:href="../022/7309022.xml">
nearest neighbor search</link> in high dimensional space.</p>

</sec>
<sec>
<st>
See also</st>
<p>
 
<list>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../297/125297.xml">
Dynamic programming</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../458/1236458.xml">
Bellman equation</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../912/2060912.xml">
Backwards induction</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../601/478601.xml">
Quasi-random</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../738/7835738.xml">
Combinatorial explosion</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../022/7309022.xml">
Nearest neighbor search</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

<link>
Îµ-Approximate Nearest Neighbor Search</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

Bellman, R.E. 1957. <it>Dynamic Programming</it>. Princeton University Press, Princeton, NJ.</entry>
<entry level="2" type="bullet">

Republished 2003: Dover, ISBN 0486428095.</entry>
<entry level="1" type="bullet">

Bellman, R.E. 1961. <it>Adaptive Control Processes</it>. Princeton University Press, Princeton, NJ.</entry>
<entry level="1" type="bullet">

Powell, Warren B. 2007. <it>Approximate Dynamic Programming: Solving the Curses of Dimensionality</it>. Wiley, ISBN 0470171553.</entry>
</list>
</p>


</sec>
</bdy>
</article>
