<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:40:39[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<measure  confidence="0.9511911446218017" wordnetid="100174412">
<header>
<title>Mahalanobis distance</title>
<id>799760</id>
<revision>
<id>229738324</id>
<timestamp>2008-08-04T08:52:52Z</timestamp>
<contributor>
<username>Melcombe</username>
<id>4682566</id>
</contributor>
</revision>
<categories>
<category>Multivariate statistics</category>
<category>Statistical terminology</category>
<category>Statistical distance measures</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, <b>Mahalanobis distance</b> is a <link xlink:type="simple" xlink:href="../378/39378.xml">
distance</link> measure introduced by <link xlink:type="simple" xlink:href="../931/359931.xml">
P. C. Mahalanobis</link> in <link xlink:type="simple" xlink:href="../673/34673.xml">
1936</link>. It is based on <link xlink:type="simple" xlink:href="../057/157057.xml">
correlation</link>s between variables by which different patterns can be identified and analyzed. It is a useful way of determining <it>similarity</it> of an unknown <link xlink:type="simple" xlink:href="../586/27586.xml">
sample set</link> to a known one. It differs from <link xlink:type="simple" xlink:href="../932/53932.xml">
Euclidean distance</link> in that it takes into account the correlations of the <link xlink:type="simple" xlink:href="../495/8495.xml">
data set</link> and is <process wordnetid="100029677" confidence="0.8">
<physical_entity wordnetid="100001930" confidence="0.8">
<phenomenon wordnetid="100034213" confidence="0.8">
<spatial_property wordnetid="105062748" confidence="0.8">
<property wordnetid="104916342" confidence="0.8">
<symmetry wordnetid="105064827" confidence="0.8">
<link>
scale-invariant</link></symmetry>
</property>
</spatial_property>
</phenomenon>
</physical_entity>
</process>
, i.e. not dependent on the scale of measurements.
<sec>
<st>
Definition</st>
<p>

Formally, the Mahalanobis distance from a group of values with mean <math>\mu = ( \mu_1, \mu_2, \mu_3, \dots , \mu_p )^T</math> and <link xlink:type="simple" xlink:href="../752/191752.xml">
covariance matrix</link> <math>\Sigma</math> 
for a multivariate vector <math>x = ( x_1, x_2, x_3, \dots, x_p )^T</math> is defined as:</p>
<p>

<indent level="1">

<math>D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x-\mu)}.\, </math>
</indent>

Mahalanobis distance can also be defined as dissimilarity measure between two <link xlink:type="simple" xlink:href="../821/49821.xml">
random vector</link>s <math> \vec{x}</math> and <math> \vec{y}</math> of the same <link xlink:type="simple" xlink:href="../543/23543.xml">
distribution</link> with the covariance matrix 
<math>\Sigma</math> :</p>
<p>

<indent level="1">

<math> d(\vec{x},\vec{y})=\sqrt{(\vec{x}-\vec{y})^T \Sigma^{-1} (\vec{x}-\vec{y})}.\,
</math>
</indent>

If the covariance matrix is the identity matrix, the Mahalanobis distance reduces to the 
<link xlink:type="simple" xlink:href="../932/53932.xml">
Euclidean distance</link>. If the covariance matrix is diagonal, then the resulting distance measure is called the <it>normalized Euclidean distance</it>:</p>
<p>

<indent level="1">

<math> d(\vec{x},\vec{y})=
\sqrt{\sum_{i=1}^p  {(x_i - y_i)^2 \over \sigma_i^2}},
</math>
</indent>

where <math>\sigma_i</math> is the <link xlink:type="simple" xlink:href="../590/27590.xml">
standard deviation</link> of the <math> x_i </math> over the sample set.</p>

</sec>
<sec>
<st>
Intuitive explanation</st>
<p>

Consider the problem of estimating the probability
that a test point in <it>N</it>-dimensional <link xlink:type="simple" xlink:href="../697/9697.xml">
Euclidean space</link> belongs to a set, 
where we are given sample points that definitely belong to that set.
Our first step would be to find the average or center of mass of
the sample points. Intuitively, the closer the point in question
is to this center of mass, the more likely it is to belong to the set.</p>
<p>

However, we also need to know how large the set is.
The simplistic approach is to estimate the <link xlink:type="simple" xlink:href="../590/27590.xml">
standard deviation</link>
of the distances of the sample points from the center of mass.
If the distance between the test point and the center of mass
is less than one standard deviation, then we conclude that
it is highly probable that the test point belongs to the set.
The further away it is, the more likely that the test point
should not be classified as belonging to the set.</p>
<p>

This intuitive approach can be made quantitative by 
defining the normalized distance between the test point and the set
to be <math> {x - \mu} \over \sigma </math>.
By plugging this into the normal distribution
we get the probability of the test point belonging to the set.</p>
<p>

The drawback of the above approach was that we assumed
that the sample points are distributed about the center of mass
in a spherical manner. Were the distribution to be decidedly
non-spherical, for instance ellipsoidal, then we would expect
the probability of the test point belonging to the set
to depend not only on the distance from the center of mass,
but also on the direction. In those directions where the
ellipsoid has a short axis the test point must be closer,
while in those where the axis is long the test point can be
further away from the center.</p>
<p>

Putting this on a mathematical basis, the ellipsoid that
best represents the set's probability distribution
can be estimated by building the covariance matrix of the
samples. The Mahalanobis distance is simply the distance
of the test point from the center of mass divided by the width
of the ellipsoid in the direction of the test point.</p>

</sec>
<sec>
<st>
Relationship to leverage</st>
<p>

Mahalanobis distance is closely related to the <link>
leverage statistic</link> <it>h</it>. The Mahalanobis distance of a data point from the centroid of a multivariate data set is (<it>N</it>&nbsp;&amp;minus;&nbsp;1) times the leverage of that data point, where <it>N</it> is the number of data points in the set.</p>

</sec>
<sec>
<st>
Applications</st>
<p>

Mahalanobis distance is widely used in <link xlink:type="simple" xlink:href="../675/669675.xml">
cluster analysis</link> and other <link xlink:type="simple" xlink:href="../244/1579244.xml">
classification</link> techniques. It is closely related to <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../711/934711.xml">
Hotelling's T-square distribution</link></distribution>
</arrangement>
</structure>
 used for multivariate statistical testing.</p>
<p>

In order to use the Mahalanobis distance to classify a test point as belonging to one of N
classes, one first estimates the covariance matrix of each class, 
usually based on samples known to belong to each class.
Then, given a test sample, one computes the Mahalanobis distance to each
class, and classifies the test point as belonging to that class
for which the Mahalanobis distance is minimal.
Using the probabilistic interpretation given above,
this is equivalent to selecting the class with the highest probability.</p>
<p>

Also, Mahalanobis distance and leverage are often used to detect <link xlink:type="simple" xlink:href="../951/160951.xml">
outlier</link>s especially in the development of <link xlink:type="simple" xlink:href="../903/17903.xml">
linear regression</link> models. A point that has a greater Mahalanobis distance from the rest of the sample population of points is said to have higher leverage since it has a greater influence on the slope or coefficients of the regression equation.</p>

</sec>
<sec>
<st>
References</st>



</sec>
</bdy>
</measure>
</article>
