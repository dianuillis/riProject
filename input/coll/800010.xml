<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:42:44[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Belief propagation</title>
<id>800010</id>
<revision>
<id>237649130</id>
<timestamp>2008-09-11T04:45:52Z</timestamp>
<contributor>
<username>Adoniscik</username>
<id>266283</id>
</contributor>
</revision>
<categories>
<category>Articles lacking in-text citations</category>
<category>Coding theory</category>
<category>Probability theory</category>
<category>Graph models (statistics)</category>
<category>Graph algorithms</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="50px" src="Text_document_with_red_question_mark.svg">
</image>
</p>
</col>
<col style="" class="mbox-text">
 This article or section includes a  or , but its sources remain unclear because it has <b>insufficient  .</b>
You can  this article by introducing more precise citations .</col>
</row>
</table>


<b>Belief propagation</b>, also known as the <b>sum-product algorithm</b>, is an iterative <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> for computing <link xlink:type="simple" xlink:href="../077/506077.xml">
marginal</link>s of functions on a <link xlink:type="simple" xlink:href="../298/447298.xml">
graphical model</link> most commonly used in <link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link> and <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link>.  <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../964/699964.xml">
Judea Pearl</link></scientist>
</person>
 in 1982<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> formulated this algorithm on trees, and Kim and Pearl (in 1983)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref> on polytrees.
Pearl (1988)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref> has then suggested this algorithm
as an approximation for general (loopy) network. It is an  efficient inference algorithm on <link xlink:type="simple" xlink:href="../560/48560.xml">
tree</link>s and has demonstrated empirical success in numerous applications including <link xlink:type="simple" xlink:href="../393/516393.xml">
low-density parity-check codes</link>, <link xlink:type="simple" xlink:href="../535/497535.xml">
turbo codes</link>, <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../221/39221.xml">
free energy</link></concept>
</idea>
 approximation, and <link xlink:type="simple" xlink:href="../715/4715.xml">
satisfiability</link>.  It is commonly used in pairwise <link xlink:type="simple" xlink:href="../985/1323985.xml">
Markov random field</link>s (which have a maximum <link xlink:type="simple" xlink:href="../466/524466.xml">
clique</link> size of 2), <link xlink:type="simple" xlink:href="../996/203996.xml">
Bayesian networks</link>, and <link xlink:type="simple" xlink:href="../186/2016186.xml">
factor graph</link>s.  <p>

Recall that the <link xlink:type="simple" xlink:href="../077/506077.xml">
marginal distribution</link> of a single <link xlink:type="simple" xlink:href="../685/25685.xml">
random variable</link> <math>X_i</math> is simply the summation of a <link xlink:type="simple" xlink:href="../637/879637.xml">
joint distribution</link> over all variables except <math>X_i</math>, and let 
<math>\mathbf{x}</math> be an assignment of all variables in the joint distribution:</p>
<p>

<indent level="1">

<math>P(x_i) = \sum_{\mathbf{x}: X_i=x_i} P(\mathbf{x}).</math>
</indent>

For the purposes of explaining this algorithm, consider the marginal function, which is simply an <link>
 unnormalized</link> marginal distribution with a generic global function <math>g(\mathbf{x})</math>:</p>
<p>

<indent level="1">

<math>z(x_i) = \sum_{\mathbf{x}: X_i=x_i} g(\mathbf{x}).</math>
</indent>

</p>
<sec>
<st>
Exact algorithm for trees</st>

<p>

This algorithm functions by passing positive <link>
 real</link> vector valued messages across edges in a graphical model.  More precisely, in <link xlink:type="simple" xlink:href="../560/48560.xml">
tree</link>s: a <link xlink:type="simple" xlink:href="../899/638899.xml">
vertex</link> sends a message to an <link xlink:type="simple" xlink:href="../508/1651508.xml">
adjacent</link> vertex if (a) it has received messages from all of its other <link xlink:type="simple" xlink:href="../508/1651508.xml">
adjacent</link> vertices and (b) hasn't already sent one.  So in the first iteration, the algorithm sends messages from all <link xlink:type="simple" xlink:href="../228/18228.xml">
leaf node</link>s to each of the lone vertices adjacent to those respective leaves and continues sending messages in this manner until all messages have been sent exactly once, hence explaining the term propagation.  It is easily proven that all messages will be sent (there are twice the number of edges of them).  Upon termination, the marginal of a variable is simply the product of the incoming messages of all its adjacent vertices. A simple proof of this fact, though somewhat messy, can be done by <link xlink:type="simple" xlink:href="../881/18881.xml">
mathematical induction</link>.</p>
<p>

The message definitions will be described in the <link xlink:type="simple" xlink:href="../186/2016186.xml">
factor graph</link> setting, as the algorithms for other <link xlink:type="simple" xlink:href="../298/447298.xml">
graphical model</link>s are nearly identical.  Since <link xlink:type="simple" xlink:href="../186/2016186.xml">
factor graph</link>s have variable and factor nodes, there are two types of messages to define:</p>
<p>

A <it>variable message</it> is a real-valued <link xlink:type="simple" xlink:href="../427/185427.xml">
function</link> that is a message sent from a variable to a factor, and defined as </p>
<p>

<indent level="1">

<math>X_n\rightarrow f_m(x_n) = \prod_{f_i\in N(X_n)\setminus \{f_m\}} f_i\rightarrow X_n(x_n).</math>
</indent>

A <it>factor message</it> is a real-valued <link xlink:type="simple" xlink:href="../427/185427.xml">
function</link> that is a message sent from a factor to a variable, and defined as </p>
<p>

<indent level="1">

<math>f_m\rightarrow X_n(x_n) = \sum_{\mathbf{x_m}:X_n=x_n} f_m(\mathbf{x_m}) \prod_{X_i\in N(f_m)\setminus \{X_n\}} X_i\rightarrow f_m(x_i),</math>
</indent>

where <math>N(u)</math> is defined as the set of neighbours (adjacent vertices in a <link xlink:type="simple" xlink:href="../806/325806.xml">
graph</link>) of a vertex <math>u</math>.  <math>\mathbf{x_m}</math> is an assignment to the vertices affecting <math>f_m</math> (i.e. vertices in <math>N(f_m)</math>).</p>
<p>

As mentioned in the description of the algorithm, the marginal of <math>X_i</math> can be computed in the following manner:</p>
<p>

<indent level="1">

<math>z(x_i) = \prod_{f_j\in N(X_i)} f_j\rightarrow X_i(x_i).</math>
</indent>

One can also compute the marginal of a factor <math>f_j</math>, equivalently, the marginal of the subset of variables <math>X_j</math> in the following manner:</p>
<p>

<indent level="1">

<math>z(\mathbf{x_j}) = f_j(\mathbf{x_j})\prod_{X_i\in N(f_j)} X_i\rightarrow f_j(x_i).</math>
</indent>

</p>
</sec>
<sec>
<st>
Approximate algorithm for general graphs</st>

<p>

Curiously, nearly the same algorithm is used in general <link xlink:type="simple" xlink:href="../806/325806.xml">
graph</link>s. The algorithm is then sometimes called "loopy" belief propagation, because graphs typically contain <link xlink:type="simple" xlink:href="../609/168609.xml">
cycle</link>s, or loops.  The procedure must be adjusted slightly because graphs might not contain any leaves.  Instead, one initializes all variable messages to 1 and uses the same message definitions above, updating all messages at every iteration (although messages coming from known leaves or tree-structured subgraphs may no longer need updating after sufficient iterations).  It is easy to show that in a tree, the message definitions of this modified procedure will converge to the set of message definitions given above within a number of iterations equal to the <link xlink:type="simple" xlink:href="../007/8007.xml">
diameter</link> of the tree.</p>
<p>

The precise conditions under which loopy belief propagation will converge are still not well understood; it is known that graphs containing a single loop will converge to a correct solution. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref> Several sufficient (but not necessary) conditions for convergence of loopy belief propagation to a unique fixed point exist. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> There exist graphs which will fail to converge, or which will oscillate between multiple states over repeated iterations.  Techniques like <link xlink:type="simple" xlink:href="../100/8594100.xml">
EXIT charts</link> can provide an approximate visualisation of the progress of belief propagation and an approximate test for convergence. </p>
<p>

There are other approximate methods for marginalization including <link xlink:type="simple" xlink:href="../112/1405112.xml">
variational method</link>s and <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo method</link></method>
</know-how>
</technique>
s. </p>
<p>

One method of exact marginalization in general graphs is called the <link xlink:type="simple" xlink:href="../682/4855682.xml">
junction tree algorithm</link>, which is simply belief propagation on a modified graph guaranteed to be a tree.  The basic premise is to eliminate cycles by clustering them into single nodes.</p>

</sec>
<sec>
<st>
Related algorithm and complexity issues</st>

<p>

A similar algorithm is commonly referred to as the <link xlink:type="simple" xlink:href="../015/228015.xml">
Viterbi algorithm</link>, but also known as the max-product or min-sum algorithm, which solves the related problem of maximization, or most probable explanation.  Instead of attempting to solve the marginal, the goal here is to find the  values <math>\mathbf{x}</math> that maximises the global function (i.e. most probable values in a probabilistic setting), and it can be defined using the <link xlink:type="simple" xlink:href="../004/348004.xml">
arg max</link>:</p>
<p>

<indent level="1">

<math>\arg\max_{\mathbf{x}} g(\mathbf{x}).</math>
</indent>

An algorithm that solves this problem is nearly identical to belief propagation, with the sums replaced by maxima in the definitions.</p>
<p>

It is worth noting that <link xlink:type="simple" xlink:href="../465/317465.xml">
inference</link> problems like marginalization and maximization are <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../681/54681.xml">
NP-hard</link></group>
</collection>
</class>
 to solve exactly and approximately (at least for <link>
 relative error</link>) in a graphical model.  More precisely, the marginalization problem defined above is <link>
 #P</link>-complete and maximization is <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../466/39466.xml">
NP-complete</link></group>
</collection>
</class>
.</p>

</sec>
<sec>
<st>
Relation to free energy</st>

<p>

The sum-product algorithm is related to the calculation of <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../221/39221.xml">
free energy</link></concept>
</idea>
 in <link xlink:type="simple" xlink:href="../952/29952.xml">
thermodynamics</link>. Let <it>Z</it> be the <link xlink:type="simple" xlink:href="../849/16846849.xml">
partition function</link>. A probability distribution </p>
<p>

<indent level="1">

<math>P(\mathbf{X}) = \frac{1}{Z} \prod_{f_j} f_j(x_j)</math> 
</indent>

(as per the factor graph representation) can be viewed as a measure of the <link xlink:type="simple" xlink:href="../757/340757.xml">
internal energy</link> present in a system, computed as </p>
<p>

<indent level="1">

<math>E(\mathbf{X}) = \log \prod_{f_j} f_j(x_j).</math>
</indent>

The free energy of the system is then </p>
<p>

<indent level="1">

<math>F = U - H = \sum_{\mathbf{X}} P(\mathbf{X}) E(\mathbf{X}) + \sum_{\mathbf{X}}  P(\mathbf{X}) \log P(\mathbf{X}).</math>
</indent>

It can then be shown that the points of convergence of the sum-product algorithm represent the points where the free energy in such a system is minimized.  Similarly, it can be shown that a fixed point of the iterative belief propagation algorithm in graphs with cycles is a stationary point of a free energy approximation.</p>

</sec>
<sec>
<st>
Generalized belief propagation (GBP)</st>

<p>

Belief propagation algorithms are normally presented as messages update equations on a factor graph, involving messages between variable nodes and their neighboring factor nodes and vice versa. Considering messages between <it>regions</it> in a graph is one way of generalizing the belief propagation algorithm. There are several ways of defining the set of regions in a graph that can exchange messages. One method uses ideas introduced by <link>
Kikuchi</link> in the physics literature, and is known as Kikuchi's <link>
cluster variation method</link>. </p>
<p>

Improvements in the performance of belief propagation algorithms are also achievable by breaking the replicas symmetry in the distributions of the fields (messages). This generalization leads to a new kind of algorithm called Survey Propagation (SP), which have proved to be very efficient in <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../466/39466.xml">
NP-complete</link></group>
</collection>
</class>
 problems like <link xlink:type="simple" xlink:href="../715/4715.xml">
satisfiability</link> and <link xlink:type="simple" xlink:href="../743/426743.xml">
graph coloring</link>.</p>
<p>

The cluster variational method and the survey propagation algorithms are two different improvements to belief propagation. The name <link>
generalized survey propagation</link> (GSP) is waiting to be assigned to the algorithm that merges both generalizations.</p>

</sec>
<sec>
<st>
Footnotes</st>

<p>

<reflist>
<entry id="1">
Pearl, J. (1982) Reverend Bayes on inference engines:  A distributed hierarchical approach. <it>Proceedings American Association of Artificial Intelligence National Conference on AI,</it> Pittsburgh, PA, 133--136.</entry>
<entry id="2">
Kim, J.H. and Pearl, J., (1983) A computational model for combined causal and diagnostic reasoning in inference systems, <it>Proceedings IJCAI-83,</it> Karlsruhe, Germany, 190--193.</entry>
<entry id="3">
Pearl, J. (1988) <it>Probabilistic Reasoning in Intelligent Systems:  Networks of Plausible Inference</it> (Revised Second Printing) San Francisco, CA: Morgan Kaufmann.</entry>
<entry id="4">
Y. Weiss. <it>Correctness of Local Probability Propagation in Graphical Models with Loops</it>. Neural Computation, 2000.</entry>
<entry id="5">
J. Mooij &amp; H. Kappen. <it>Sufficient Conditions for Convergence of the Sum–Product Algorithm</it>. IEEE Transactions on Information Theory 53(12):4422-4437, 2007</entry>
</reflist>
</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 Frey, Brendan (1998). <it>Graphical Models for Machine Learning and Digital Communication</it>.  MIT Press</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../315/2679315.xml">
David J.C. MacKay</link> (2003). Exact Marginalization in Graphs. In David J.C. MacKay, <it>Information Theory, Inference, and Learning Algorithms</it>, pp. 334–340. Cambridge: Cambridge University Press.</entry>
<entry level="1" type="bullet">

 Mackenzie, Dana (2005). <weblink xlink:type="simple" xlink:href="http://www.newscientist.com/channel/info-tech/mg18725071.400">
<it>Communication Speed Nears Terminal Velocity''</it></weblink>  New Scientist. 9 July 2005. Issue 2507 (Registration required)</entry>
<entry level="1" type="bullet">

 Yedidia, J.S.; Freeman, W.T.; Weiss, Y. Constructing free-energy approximations and generalized belief propagation algorithms, <it>IEEE Transactions on Information Theory</it>, vol.51(7), pp.2282-2312, July 2005.</entry>
<entry level="1" type="bullet">

 Yedidia, J.S.; Freeman, W.T.; Weiss, Y. <weblink xlink:type="simple" xlink:href="http://www.merl.com/publications/TR2001-022/">
<it>Understanding Belief Propagation and Its Generalizations''</it></weblink>, Exploring Artificial Intelligence in the New Millennium, ISBN 1558608117, Chap. 8, pp. 239-236, January 2003 (Science &amp; Technology Books)</entry>
<entry level="1" type="bullet">

 Bishop, Christopher M. <it>Pattern Recognition and Machine Learning</it>, <weblink xlink:type="simple" xlink:href="http://research.microsoft.com/%7Ecmbishop/PRML/Bishop-PRML-sample.pdf">
Chapter 8: Graphical models</weblink></entry>
<entry level="1" type="bullet">

 Koch, Volker M. (2007). <weblink xlink:type="simple" xlink:href="http://www.volker-koch.com/diss/''A">
Factor Graph Approach to Model-Based Signal Separation''</weblink> --- A tutorial-style dissertation</entry>
</list>
</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
