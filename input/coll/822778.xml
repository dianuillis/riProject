<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:43:47[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Baum-Welch algorithm</title>
<id>822778</id>
<revision>
<id>215637729</id>
<timestamp>2008-05-29T01:39:27Z</timestamp>
<contributor>
<username>Sethbot</username>
<id>68471</id>
</contributor>
</revision>
<categories>
<category>Statistical algorithms</category>
<category>Bioinformatics algorithms</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../323/5323.xml">
computer science</link>, <link xlink:type="simple" xlink:href="../717/15832717.xml">
statistical computing</link> and <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>, the <b>Baum-Welch algorithm</b> is used to find the unknown parameters of a <link xlink:type="simple" xlink:href="../770/98770.xml">
hidden Markov model</link> (HMM).  It makes use of the <link xlink:type="simple" xlink:href="../749/9292749.xml">
forward-backward algorithm</link> and is named for <link>
Leonard E. Baum</link> and <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<theorist wordnetid="110706812" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../153/14151153.xml">
Lloyd R. Welch</link></causal_agent>
</intellectual>
</theorist>
</person>
</physical_entity>
.
<sec>
<st>
 Explanation </st>

<p>

The Baum-Welch algorithm is a generalized <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<numerical_quantity wordnetid="105856066" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<value wordnetid="105856388" confidence="0.8">
<quantity wordnetid="105855125" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../752/470752.xml">
expectation-maximization</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</quantity>
</value>
</rule>
</event>
</numerical_quantity>
</concept>
</idea>
 (GEM) algorithm.  It can compute <link xlink:type="simple" xlink:href="../806/140806.xml">
maximum likelihood</link> estimates and <link xlink:type="simple" xlink:href="../433/1792433.xml">
posterior mode</link> estimates for the parameters (transition and emission probabilities) of an HMM, when given only emissions as training data.</p>
<p>

The algorithm has two steps: 
<list>
<entry level="1" type="number">

 Calculating the forward probability and the backward probability for each HMM state; </entry>
<entry level="1" type="number">

 On the basis of this, determining the frequency of the transition-emission pair values and dividing it by the probability of the entire string.  This amounts to calculating the expected count of the particular transition-emission pair. Each time a particular transition is found, the value of the quotient of the transition divided by the probability of the entire string goes up, and this value can then be made the new value of the transition.</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

The algorithm was introduced in the paper:
<list>
<entry level="1" type="bullet">

L. E. Baum, T. Petrie, G. Soules, and N. Weiss, <weblink xlink:type="simple" xlink:href="http://links.jstor.org/sici?sici=0003-4851%28197002%2941%3A1%3C164%3AAMTOIT%3E2.0.CO%3B2-V">
"A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains"</weblink>, Ann. Math. Statist., vol. 41, no. 1, pp. 164--171, 1970.</entry>
</list>

The Shannon Lecture by Welch:
<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.itsoc.org/publications/nltr/it_dec_03final.pdf">
Hidden Markov Models and the Baum-Welch Algorithm</weblink>, IEEE Information Theory Society Newsletter, Dec. 2003.</entry>
</list>

The path-counting algorithm, an alternative to the Baum-Welch algorithm:
<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/677948.html">
Comparing and Evaluating HMM Ensemble Training Algorithms Using Train and Test and Condition Number Criteria</weblink>, Journal of Pattern Analysis and Applications, 2003.</entry>
</list>
</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../749/9292749.xml">
Forward-backward algorithm</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../015/228015.xml">
Viterbi algorithm</link></entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.jhu.edu/~jason/papers/#tnlp02">
An Interactive Spreadsheet for Teaching the Forward-Backward Algorithm</weblink> (spreadsheet and article with step-by-step walkthrough)</entry>
</list>
</p>




</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
