<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:42:04[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>LVQ</title>
<id>827635</id>
<revision>
<id>234399798</id>
<timestamp>2008-08-26T18:12:25Z</timestamp>
<contributor>
<username>AlleborgoBot</username>
<id>3813685</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<b>LVQ</b>, or <b>Learning Vector Quantization</b>, is a <link xlink:type="simple" xlink:href="../718/72718.xml">
prototype-based</link> <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised</link> <link xlink:type="simple" xlink:href="../244/1579244.xml">
classification</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>. <p>

LVQ can be understood as a special case of an <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural network</link>, more precisely, it applies a <link xlink:type="simple" xlink:href="../175/1853175.xml">
winner-take-all</link> <link xlink:type="simple" xlink:href="../084/404084.xml">
Hebbian learning</link>-based approach. It is a precursor to <link xlink:type="simple" xlink:href="../996/76996.xml">
Self-organizing map</link>s (SOM) and related to <link xlink:type="simple" xlink:href="../021/2457021.xml">
Neural gas</link>, and to the <link xlink:type="simple" xlink:href="../388/1775388.xml">
k-Nearest Neighbor algorithm</link> (k-NN). LVQ was invented by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../591/1685591.xml">
Teuvo Kohonen</link></scientist>
</causal_agent>
</person>
</physical_entity>
.</p>
<p>

The network has two layers: a layer of input neurons, and a layer of output neurons. The network is given by prototypes W=(w(i),...,w(n)). It changes the weights of the network in order to classify the data correctly. For each data point, the prototype (neuron) that is closest to it is determined (called the winner neuron). The weights of the connections to this neuron are then adapted, i.e. made closer if it correcly classifies the data point or made less similar if it incorrectly classifies it.</p>
<p>

An advantage of LVQ is that it creates prototypes that are easy to interpret for experts in the field.</p>
<p>

LVQ can be a source of great help in classifying text documents.</p>

<sec>
<st>
 References </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cis.hut.fi/panus/papers/dtwsom.pdf">
Self-Organizing Maps and Learning Vector Quantization for Feature Sequences, Somervuo and Kohonen. 2004</weblink> (pdf)</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.ansijournals.com/itj/2007/154-159.pdf">
Classification of Textual Documents using LVQ, Fahad and Sikander. 2007</weblink> (pdf)</entry>
</list>
</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../996/76996.xml">
Self-organizing map</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://fuzzy.cs.uni-magdeburg.de/~borgelt/doc/lvqd/">
Introduction to the algorithm. Plus download of codes and programs implementing it (University of Magdeburg)</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://wekaclassalgos.sourceforge.net/">
LVQ for WEKA</weblink>: Implementation of LVQ variants (LVQ1, OLVQ1, LVQ2.1, LVQ3, OLVQ3) for the WEKA Machine Learning Workbench.</entry>
</list>
</p>




</sec>
</bdy>
</article>
