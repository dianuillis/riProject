<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 23:10:18[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Random neural network</title>
<id>8278198</id>
<revision>
<id>147440037</id>
<timestamp>2007-07-27T12:54:30Z</timestamp>
<contributor>
<username>Tabletop</username>
<id>173687</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
</categories>
</header>
<bdy>

The <b>random neural network</b> (RNN) is a mathematical representation of <link xlink:type="simple" xlink:href="../120/21120.xml">
neuron</link>s or <link xlink:type="simple" xlink:href="../230/4230.xml">
cells</link> which exchange <link xlink:type="simple" xlink:href="../998/156998.xml">
spiking signals</link>.  Each cell is represented by an integer whose value rises when the cell receives an excitatory spike and drops when it receives an inhibitory spike.  The spikes can originate outside the <link xlink:type="simple" xlink:href="../542/1729542.xml">
network</link> itself, or they can come from other cells in the networks.  Cells whose internal excitatory state has a positive value are allowed to send out spikes of either kind to other cells in the network according to specific cell-dependent spiking rates.  The model has a mathematical solution in steady-state which provides the joint <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> of the network in terms of the individual probabilities that each cell is excited and able to send out spikes. Computing this solution is based on solving a set of <link xlink:type="simple" xlink:href="../103/146103.xml">
non-linear algebraic equations</link> whose <link xlink:type="simple" xlink:href="../065/25065.xml">
parameter</link>s are related to the spiking rates of individual cells and their connectivity to other cells, as well as the arrival rates of spikes from outside the network. The RNN also has a gradient-based <link xlink:type="simple" xlink:href="../488/233488.xml">
learning algorithm</link> whose computational complexity is proportional to the cube of the number of cells, and other <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>s such as <link xlink:type="simple" xlink:href="../294/66294.xml">
reinforcement learning</link> can also be used.  The RNN has also been shown to be a universal approximator for <link xlink:type="simple" xlink:href="../509/311509.xml">
bounded</link> and <link xlink:type="simple" xlink:href="../122/6122.xml">
continuous function</link>s. 
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, <it>Random neural networks with negative and positive signals and product form solution,</it> Neural Computation, vol. 1, no. 4, pp. 502-511, 1989.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe,  <it>Stability of the random neural network model,</it> Neural Computation, vol. 2, no. 2, pp. 239-247, 1990.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, A. Stafylopatis, and A. Likas, <it>Associative memory operation of the random network model,</it> in Proc. Int. Conf. Artificial Neural  Networks, Helsinki, pp. 307-312, 1991.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, F. Batty, <it>Minimum cost graph covering with the random neural network,</it> Computer Science and Operations Research, O. Balci (ed.), New York, Pergamon, pp. 139-147, 1992.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, <it>Learning in the recurrent random neural network,</it> Neural Computation, vol. 5, no. 1, pp. 154-164, 1993.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, V. Koubi, F. Pekergin, <it>Dynamical random neural network approach to the traveling salesman problem,</it> Proc. IEEE Symp. Syst., Man, Cybern., pp. 630-635, 1993.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, C. Cramer, M. Sungur, P. Gelenbe <it>Traffic and video quality</it></entry>
</list>

in adaptive neural compression<it>, Multimedia Systems, Vol. 4, pp. 357-369, 1996.</it></p>
<p>

<list>
<entry level="1" type="bullet">

 C. Cramer, E. Gelenbe, H. Bakircioglu <it>Low bit rate video compression with neural networks and temporal sub-sampling,</it> Proceedings of the IEEE, Vol. 84, No. 10, pp. 1529-1543, October 1996.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, T. Feng, K.R.R. Krishnan <it>Neural network methods for volumetric magnetic resonance imaging of the human brain,</it> Proceedings of the IEEE, Vol. 84, No. 10, pp. 1488-1496, October 1996.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, A. Ghanwani, V. Srinivasan,  <it>Improved neural heuristics for multicast routing,</it> IEEE J. Selected Areas in Communications, vol. 15, no. 2, pp. 147-155, 1997.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, Z. H. Mao, and Y. D. Li, <it>Function approximation with the random neural network,</it>  IEEE Trans. Neural Networks, vol. 10, no. 1, January 1999.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, J.M. Fourneau <it>Random neural networks with multiple classes of signals,</it> Neural Computation, vol. 11, pp. 721-731, 1999.</entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 E. Gelenbe, Z.-H. Mao and Y-D. Li "Functions approximation by random neural networks with a bounded number of layers", Differential Equations and Dynamical Systems, Vol. 12, 1&amp;2, pp. 143-170, Jan. April 2004.</entry>
</list>
</p>


</sec>
</bdy>
</article>
