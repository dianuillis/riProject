<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:24:28[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Distributed computing</title>
<id>8501</id>
<revision>
<id>243627306</id>
<timestamp>2008-10-07T10:53:36Z</timestamp>
<contributor>
<username>Machete97</username>
<id>6463086</id>
</contributor>
</revision>
<categories>
<category>All articles with dead external links</category>
<category>Articles with invalid date parameter in template</category>
<category>Articles with dead external links since June 2008</category>
<category>Distributed computing</category>
</categories>
</header>
<bdy>

<b>Distributed computing</b> deals with <link xlink:type="simple" xlink:href="../615/13615.xml">
hardware</link> and <link xlink:type="simple" xlink:href="../309/5309.xml">
software</link> <link xlink:type="simple" xlink:href="../675/8286675.xml">
systems</link> containing more than one processing element or <link xlink:type="simple" xlink:href="../904/41904.xml">
storage</link> element, <link xlink:type="simple" xlink:href="../605/2581605.xml">
concurrent</link> processes, or multiple programs, running under a loosely or tightly controlled <link xlink:type="simple" xlink:href="../522/250522.xml">
regime</link>.<p>

In distributed computing a program is split up into parts that run simultaneously on multiple computers communicating over a network.  Distributed computing is a form of <link xlink:type="simple" xlink:href="../162/145162.xml">
parallel computing</link>, but parallel computing is most commonly used to describe program parts running simultaneously on multiple processors in the same computer.  Both types of processing require dividing a program into parts that can run simultaneously, but distributed programs often must deal with heterogeneous environments, network links of varying latencies, and unpredictable failures in the network or the computers.</p>

<sec>
<st>
 Organization </st>
<p>

Organizing the interaction between the computers that execute distributed computations is of prime importance.  In order to be able to use the widest possible variety of computers, the protocol or communication channel should not contain or use any information that may not be understood by certain machines.  Special care must also be taken that messages are indeed delivered correctly and that invalid messages, which would otherwise bring down the system and perhaps the rest of the network, are rejected.</p>
<p>

Another important factor is the ability to send software to another computer in a portable way so that it may execute and interact with the existing network.  This may not always be practical when using differing hardware and resources, in which case other methods, such as cross-compiling or manually porting this software, must be used.</p>

</sec>
<sec>
<st>
 Goals and advantages </st>
<p>

There are many different types of distributed computing systems and many challenges to overcome in successfully designing one. The main goal of a distributed computing system is to connect users and resources in a <link xlink:type="simple" xlink:href="../091/351091.xml">
transparent</link>, open, and <link xlink:type="simple" xlink:href="../529/185529.xml">
scalable</link> way. Ideally this arrangement is drastically more <link xlink:type="simple" xlink:href="../720/2573720.xml">
fault tolerant</link> and more powerful than many combinations of <link xlink:type="simple" xlink:href="../878/17946878.xml">
stand-alone</link> computer systems.</p>

<ss1>
<st>
 Openness </st>
<p>

Openness is the property of distributed systems such that each subsystem is continually open to interaction with other systems (see references).  <link xlink:type="simple" xlink:href="../483/93483.xml">
Web services</link> <link xlink:type="simple" xlink:href="../615/23615.xml">
protocols</link> are standards which enable distributed systems to be extended and scaled. In general, an open system that scales has an advantage over a perfectly closed and self-contained system. Openness cannot be archived unless the specification and documentation of the key software interface of the component of a system are made available to the software developer.</p>
<p>

Consequently, open distributed systems are required to meet the following challenges:</p>
<p>

<list>
<entry level="1" type="definition">

 Monotonicity</entry>
<entry level="1" type="indent">

 Once something is published in an open  system, it cannot be taken back.  </entry>
<entry level="1" type="definition">

 Pluralism</entry>
<entry level="1" type="indent">

 Different subsystems of an open distributed system include heterogeneous, overlapping and possibly conflicting information.  There is no central arbiter of truth in open distributed systems.</entry>
<entry level="1" type="definition">

 <link xlink:type="simple" xlink:href="../475/2647475.xml">
Unbounded Nondeterminism</link></entry>
<entry level="1" type="indent">

 Asynchronously, different subsystems can come up and go down and communication links can come in and go out between subsystems of an open distributed system.  Therefore the time that it will take to complete an operation cannot be bounded in advance.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 Drawbacks and disadvantages </st>

<p>

<indent level="1">

<it>See also: <link xlink:type="simple" xlink:href="../868/3308868.xml">
Fallacies of Distributed Computing</link></it>
</indent>
=== Technical issues ===
If not planned properly, a distributed system can decrease the overall <link xlink:type="simple" xlink:href="../651/41651.xml">
reliability</link> of computations if the un<link xlink:type="simple" xlink:href="../760/40760.xml">
availability</link> of a node can cause disruption of the other nodes.  <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../671/195671.xml">
Leslie Lamport</link></scientist>
 famously quipped that: "A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable."<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>
<p>

<link xlink:type="simple" xlink:href="../274/1714274.xml">
Troubleshoot</link>ing and diagnosing problems in a distributed system can also become more difficult, because the analysis may require connecting to remote nodes or inspecting communication between nodes.</p>
<p>

Many types of <link xlink:type="simple" xlink:href="../926/5926.xml">
computation</link> are not well suited for distributed environments, typically owing to the amount of network communication or <link xlink:type="simple" xlink:href="../738/28738.xml">
synchronization</link> that would be required between <link xlink:type="simple" xlink:href="../116/998116.xml">
node</link>s.  If <link xlink:type="simple" xlink:href="../827/15612827.xml">
bandwidth</link>, <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<link xlink:type="simple" xlink:href="../933/17933.xml">
latency</link></concept>
</idea>
, or communication requirements are too significant, then the benefits of distributed computing may be negated and the <link xlink:type="simple" xlink:href="../515/224515.xml">
performance</link> may be worse than a non-distributed environment.</p>

</sec>
<sec>
<st>
 Architecture </st>
<p>

Various  hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely-coupled devices and cables. At a higher level, it is necessary to interconnect <link>
 processes</link> running on those CPUs with some sort of <link xlink:type="simple" xlink:href="../925/40925.xml">
communication system</link>.</p>
<p>

Distributed programming typically falls into one of several basic architectures or categories: <link xlink:type="simple" xlink:href="../513/6513.xml">
Client-server</link>, <link xlink:type="simple" xlink:href="../003/20003.xml">
3-tier architecture</link>, <link xlink:type="simple" xlink:href="../003/20003.xml">
N-tier architecture</link>, <link xlink:type="simple" xlink:href="../893/1941893.xml">
Distributed object</link>s, <link xlink:type="simple" xlink:href="../470/1639470.xml">
loose coupling</link>, or <link xlink:type="simple" xlink:href="../896/18949896.xml">
tight coupling</link>.</p>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../513/6513.xml">
Client-server</link> &mdash; Smart client code contacts the server for data, then formats and displays it to the user.  Input at the client is committed back to the server when it represents a permanent change.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../003/20003.xml">
3-tier architecture</link> &mdash; Three tier systems move the client intelligence to a middle tier so that stateless clients can be used.  This simplifies application deployment.  Most web applications are 3-Tier.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../003/20003.xml">
N-tier architecture</link> &mdash; N-Tier refers typically to web applications which further forward their requests to other enterprise services.  This type of application is the one most responsible for the success of <link xlink:type="simple" xlink:href="../154/165154.xml">
application server</link>s.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../896/18949896.xml">
Tightly coupled</link> (clustered) &mdash; refers typically to a cluster of machines that closely work together, running a shared process in parallel. The task is subdivided in parts that are made individually by each one and then put back together to make the final result.</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../107/24107.xml">
Peer-to-peer</link> &mdash; an architecture where there is no special machine or machines that provide a service or manage the network resources. Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and servers.</entry>
<entry level="1" type="bullet">

 <link>
Space based</link>  &mdash; refers to an infrastructure that creates the illusion (virtualization) of one single address-space. Data are transparently replicated according to application needs. Decoupling in time, space and reference is achieved.</entry>
</list>
</p>
<p>

Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various <link xlink:type="simple" xlink:href="../867/1324867.xml">
message passing</link> protocols, processes may communicate directly with one another, typically in a <link>
 master/slave</link> relationship. Alternatively, a <link>
 "database-centric" architecture</link> can enable distributed computing to be done without any form of direct <link xlink:type="simple" xlink:href="../106/152106.xml">
inter-process communication</link>, by utilizing a shared <link xlink:type="simple" xlink:href="../377/8377.xml">
database</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

</sec>
<sec>
<st>
 Concurrency </st>
<p>

Distributed computing implements a kind of <link xlink:type="simple" xlink:href="../467/928467.xml">
concurrency</link>. It interrelates tightly with <link xlink:type="simple" xlink:href="../605/2581605.xml">
concurrent programming</link> so much that they are sometimes not taught as distinct subjects <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.</p>

<ss1>
<st>
 Multiprocessor systems </st>

<p>

A <link xlink:type="simple" xlink:href="../020/64020.xml">
multiprocessor</link> system is simply a computer that has more than one  CPU on its motherboard. If the operating system is built to take advantage of this, it can run different <link xlink:type="simple" xlink:href="../178/45178.xml">
processes</link> (or different threads belonging to the same process) on different CPUs.</p>

</ss1>
<ss1>
<st>
Multicore systems </st>

<p>

Intel CPUs from the late <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../228/165228.xml">
Pentium 4</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
 era (Northwood and Prescott cores) employed a technology called <link xlink:type="simple" xlink:href="../443/151443.xml">
Hyper-threading</link> that allowed more than one <link xlink:type="simple" xlink:href="../303/45303.xml">
thread</link> (usually two) to run on the same CPU. The more recent Sun <link xlink:type="simple" xlink:href="../615/2580615.xml">
UltraSPARC T1</link>, AMD <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../251/1863251.xml">
Athlon 64 X2</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
, AMD <link xlink:type="simple" xlink:href="../944/188944.xml">
Athlon FX</link>, AMD <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../237/165237.xml">
Opteron</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
, Intel <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/1560876.xml">
Pentium D</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
, <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../080/3429080.xml">
Intel Core</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
, <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../988/5048988.xml">
Intel Core 2</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
, <link xlink:type="simple" xlink:href="../988/5048988.xml">
Intel Core 2 Quad</link>, and Intel <chip wordnetid="103020034" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<microprocessor wordnetid="103760310" confidence="0.8">
<conductor wordnetid="103088707" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<semiconductor_device wordnetid="104171831" confidence="0.8">
<link xlink:type="simple" xlink:href="../920/269920.xml">
Xeon</link></semiconductor_device>
</device>
</conductor>
</microprocessor>
</instrumentality>
</artifact>
</chip>
 processors feature multiple processor cores to also increase the number of concurrent threads they can run.</p>

</ss1>
<ss1>
<st>
 Multicomputer systems </st>
<p>

A multicomputer may be considered to be either a loosely coupled <link xlink:type="simple" xlink:href="../643/40643.xml">
NUMA</link> computer or a tightly coupled <link xlink:type="simple" xlink:href="../896/18949896.xml">
cluster</link>. Multicomputers are commonly used when strong compute power is required in an environment with restricted physical space or electrical power.</p>
<p>

Common suppliers include <company wordnetid="108058098" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../062/2525062.xml">
Mercury Computer Systems</link></institution>
</company>
, CSPI, and SKY Computers.</p>
<p>

Common uses include 3D medical imaging devices and mobile radar.</p>

</ss1>
<ss1>
<st>
 Computing taxonomies </st>
<p>

The types of distributed systems are based on <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../349/222349.xml">
Flynn's taxonomy</link></group>
</collection>
</class>
 of systems; <link xlink:type="simple" xlink:href="../630/1103630.xml">
single instruction, single data</link> (SISD), <link xlink:type="simple" xlink:href="../359/55359.xml">
single instruction, multiple data</link> (SIMD), <link xlink:type="simple" xlink:href="../666/991666.xml">
multiple instruction, single data</link> (MISD), and <link xlink:type="simple" xlink:href="../139/157139.xml">
multiple instruction, multiple data</link> (MIMD). Other taxonomies and architectures available at <link xlink:type="simple" xlink:href="../509/6509.xml">
Computer architecture</link> and in 
Computer architecture.</p>

</ss1>
<ss1>
<st>
 Computer clusters </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../896/18949896.xml">
Cluster computing</link></it>
</indent>
A cluster consists of multiple stand-alone machines acting in parallel across a local high speed network. Distributed computing differs from <link xlink:type="simple" xlink:href="../896/18949896.xml">
cluster computing</link> in that computers in a distributed computing environment are typically not exclusively running "group" tasks, whereas clustered computers are usually much more tightly coupled.  Distributed computing also often consists of machines which are widely separated geographically.</p>

</ss1>
<ss1>
<st>
 Grid computing </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../373/49373.xml">
Grid computing</link></it>
</indent>
A grid uses the resources of many separate computers, loosely connected by a network (usually the Internet), to solve large-scale computation problems.  Public grids may use idle time on many thousands of computers throughout the world.  Such arrangements permit handling of data that would otherwise require the power of expensive <link xlink:type="simple" xlink:href="../153/37153.xml">
supercomputer</link>s or would have been impossible to analyze.</p>

</ss1>
</sec>
<sec>
<st>
Languages</st>
<p>

Nearly any <link xlink:type="simple" xlink:href="../015/23015.xml">
programming language</link> that has access to the full <link xlink:type="simple" xlink:href="../615/13615.xml">
hardware</link> of the system could handle distributed programming given enough time and code. <message wordnetid="106598915" confidence="0.8">
<protocol wordnetid="106665108" confidence="0.8">
<direction wordnetid="106786629" confidence="0.8">
<rule wordnetid="106652242" confidence="0.8">
<link xlink:type="simple" xlink:href="../346/26346.xml">
Remote procedure call</link></rule>
</direction>
</protocol>
</message>
s distribute <link xlink:type="simple" xlink:href="../194/22194.xml">
operating system</link> commands over a network connection. Systems like <link xlink:type="simple" xlink:href="../799/4096799.xml">
CORBA</link>, Microsoft <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<component wordnetid="105868954" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<part wordnetid="105867413" confidence="0.8">
<link xlink:type="simple" xlink:href="../492/93492.xml">
DCOM</link></part>
</causal_agent>
</worker>
</component>
</assistant>
</concept>
</idea>
</model>
</person>
</physical_entity>
, <link xlink:type="simple" xlink:href="../284/43284.xml">
Java RMI</link> and others, try to map <link xlink:type="simple" xlink:href="../757/22757.xml">
object oriented</link> design to the network. Loosely coupled systems communicate through intermediate documents that are typically human readable (e.g. <format wordnetid="106636806" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../138/34138.xml">
XML</link></format>
, <format wordnetid="106636806" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../191/13191.xml">
HTML</link></format>
, <link>
SGML</link>, <message wordnetid="106598915" confidence="0.8">
<protocol wordnetid="106665108" confidence="0.8">
<standard wordnetid="107260623" confidence="0.8">
<direction wordnetid="106786629" confidence="0.8">
<rule wordnetid="106652242" confidence="0.8">
<proposal wordnetid="107162194" confidence="0.8">
<recommendation wordnetid="106671637" confidence="0.8">
<advice wordnetid="106671484" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../366/61366.xml">
X.500</link></system_of_measurement>
</advice>
</recommendation>
</proposal>
</rule>
</direction>
</standard>
</protocol>
</message>
, and <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../790/9790.xml">
EDI</link></datum>
</information>
).</p>

</sec>
<sec>
<st>
 Examples </st>

<ss1>
<st>
 Projects </st>

<p>

<indent level="1">

<it>Main article: <link xlink:type="simple" xlink:href="../522/1336522.xml">
List of distributed computing projects</link></it>
</indent>
A variety of distributed computing projects have grown up in recent years. Many are run on a volunteer basis, and involve users donating their unused computational power to work on interesting computational problems. Examples of such projects include the <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../977/26977.xml">
Stanford University</link></university>
 <link xlink:type="simple" xlink:href="../180/5180.xml">
Chemistry</link> Department <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../102/413102.xml">
Folding@home</link></software>
 project, which is focused on simulations of <link xlink:type="simple" xlink:href="../085/52085.xml">
protein folding</link> to find disease cures and to understand biophysical systems; <organization wordnetid="108008335" confidence="0.9508927676800064">
<undertaking wordnetid="100795720" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../517/1176517.xml">
World Community Grid</link></undertaking>
</organization>
, an effort to create the world's largest public computing grid to tackle scientific research projects that benefit humanity, run and funded by <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../259/18622259.xml">
IBM</link></company>
; <link xlink:type="simple" xlink:href="../768/309768.xml">
SETI@home</link>, which is focused on analyzing radio-telescope data to find evidence of intelligent signals from space, hosted by the <link xlink:type="simple" xlink:href="../648/2229648.xml">
Space Sciences Laboratory</link> at the <university wordnetid="108286163" confidence="0.9508927676800064">
<ranking wordnetid="114429484" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../922/31922.xml">
University of California, Berkeley</link></ranking>
</university>
; <link xlink:type="simple" xlink:href="../027/1191027.xml">
LHC@home</link>, which is used to help design and tune the <artifact wordnetid="100021939" confidence="0.8">
<facility wordnetid="103315023" confidence="0.8">
<link xlink:type="simple" xlink:href="../353/357353.xml">
Large Hadron Collider</link></facility>
</artifact>
, hosted by <organization wordnetid="108008335" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../351/37351.xml">
CERN</link></organization>
 in <town wordnetid="108665504" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../521/12521.xml">
Geneva</link></town>
; and <link xlink:type="simple" xlink:href="../011/337011.xml">
distributed.net</link>, which is focused on breaking various cryptographic ciphers.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></p>
<p>

Distributed computing projects also often involve competition with other distributed systems.  This competition may be for prestige, or it may be a matter of enticing users to donate processing power to a specific project.  For example, stat races are a measure of the work a distributed computing project has been able to compute over the past day or week.  This has been found to be so important in practice that virtually all distributed computing projects offer online statistical analyses of their performances, updated at least daily if not in real-time.</p>

</ss1>
</sec>
<sec>
<st>
 See also </st>


<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../868/3308868.xml">
Fallacies of Distributed Computing</link></entry>
<entry level="1" type="bullet">


Concurrent programming languages</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../351/454351.xml#xpointer(//*[./st=%22Distributed+computing%22])">
List of distributed computing publications</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../162/145162.xml">
Parallel computing</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../197/16695197.xml">
Sideband computing</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../826/9646826.xml">
Network Agility</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../154/165154.xml">
Application server</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../674/2816674.xml">
Software componentry</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../528/938528.xml">
Distributed computing environment</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../164/2348164.xml">
Distributed Resource Management</link> System</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../549/11948549.xml">
High-Throughput Computing</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../522/1336522.xml">
List of distributed computing projects</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../175/12943175.xml">
Service-Oriented Modeling</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../807/11347807.xml">
Active message</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../011/337011.xml">
distributed.net</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../896/346896.xml">
BOINC</link></entry>
</list>
</p>


</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../671/195671.xml">
Leslie Lamport</link></scientist>
.&#32;"<weblink xlink:type="simple" xlink:href="http://research.microsoft.com/users/lamport/pubs/distributed-system.txt">
Subject: distribution (Email message sent to a DEC SRC bulletin board at 12:23:29 PDT on 28 May 87)</weblink>".&#32;Retrieved on <link>
2007-04-28</link>.</entry>
<entry id="2">
<weblink xlink:type="simple" xlink:href="http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed&amp;list_uids=16711722&amp;cmd=Retrieve">
A database-centric virtual chemistry system, J Chem Inf Model. 2006 May-Jun;46(3):1034-9</weblink></entry>
<entry id="3">
<weblink xlink:type="simple" xlink:href="http://www.cs.technion.ac.il/~cs236370/main.html">
CS236370 Concurrent and Distributed Programming 2002</weblink></entry>
<entry id="4">
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../417/2531417.xml">
David P. Anderson</link></scholar>
</causal_agent>
</alumnus>
</intellectual>
</person>
</physical_entity>
&#32;(2005-05-23).&#32;"<it><weblink xlink:type="simple" xlink:href="http://www.ngp.org.sg/seminars/Slides/DavidAnderson-Seminar@NLB.pdf">
A Million Years of Computing</weblink></it>". &#32;Retrieved on <link>
2006-08-11</link>.</entry>
</reflist>
</p>


</sec>
<sec>
<st>
 Further reading</st>
<p>

<list>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Attiya, Hagit and Welch, Jennifer&#32;(2004). Distributed Computing: Fundamentals, Simulations, and Advanced Topics.&#32;Wiley-Interscience.</cite>&nbsp; ISBN 0471453242.</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal" class="book">Lynch, Nancy A&#32;(1997). Distributed Algorithms.&#32;Morgan Kaufmann.</cite>&nbsp; ISBN 1558603484.</entry>
<entry level="1" type="bullet">

 <cite id="Reference-Tel-1994" style="font-style:normal" class="book">Tel, Gerard&#32;(1994). Introduction to Distributed Algorithms.&#32;Cambridge University Press.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal"><link>
Davies, Antony</link>&#32;(June 2004).&#32;"<weblink xlink:type="simple" xlink:href="http://www.business.duq.edu/faculty/davies/research/EconomicsOfComputation.pdf">
Computational Intermediation and the Evolution of Computation as a Commodity</weblink>"&#32;(&#91;&#93; &ndash; <weblink xlink:type="simple" xlink:href="http://scholar.google.co.uk/scholar?hl=en&amp;lr=&amp;q=author%3ADavies+intitle%3AComputational+Intermediation+and+the+Evolution+of+Computation+as+a+Commodity&amp;as_publication=Applied+Economics&amp;as_ylo=2004&amp;as_yhi=2004&amp;btnG=Search">
Scholar search</weblink>). <it>Applied Economics</it>. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1080%2F0003684042000247334">
10.1080/0003684042000247334</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Kornfeld, William&#32;(January 1981).&#32;"[https://dspace.mit.edu/handle/1721.1/5693 The Scientific Community Metaphor]". <it>MIT AI</it>&#32;(Memo 641).</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Hewitt, Carl&#32;(August 1983). "Analyzing the Roles of Descriptions and Actions in Open Systems".&#32;<it>Proceedings of the National Conference on Artificial Intelligence</it>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Hewitt, Carl&#32;(April 1985).&#32;"The Challenge of Open Systems". <it>Byte Magazine</it>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Hewitt, Carl&#32;(1999-10-23&ndash;1999-10-27). "Towards Open Information Systems Semantics".&#32;<it>Proceedings of 10th International Workshop on Distributed Artificial Intelligence</it>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Hewitt, Carl&#32;(January 1991).&#32;"Open Information Systems Semantics". <it>Journal of Artificial Intelligence</it>&#32;<b>47</b>: 79. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1016%2F0004-3702%2891%2990051-K">
10.1016/0004-3702(91)90051-K</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 <cite style="font-style:normal">Nadiminti, Dias de Assunção, Buyya&#32;(September 2006).&#32;"<weblink xlink:type="simple" xlink:href="http://www.gridbus.org/~raj/papers/InfoNet-Article06.pdf">
Distributed Systems and Recent Innovations: Challenges and Benefits</weblink>". <it>InfoNet Magazine, Volume 16, Issue 3, Melbourne, Australia</it>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 Bell, Michael&#32;(2008).&#32;"<weblink xlink:type="simple" xlink:href="http://www.amazon.com/Service-Oriented-Modeling-Service-Analysis-Architecture/dp/0470141115/ref=pd_bbs_2">
Service-Oriented Modeling: Service Analysis, Design, and Architecture</weblink>".&#32;  Wiley.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.disc-conference.org/">
DISC: International Symposium on Distributed Computing</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.bacchae.co.uk/docs/dist.html">
A primer on distributed computing</weblink> </entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.dmoz.org/Computers/Computer_Science/Distributed_Computing//">
Distributed computing</weblink> at the <work wordnetid="100575741" confidence="0.8">
<possession wordnetid="100032613" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<company wordnetid="108058098" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<property wordnetid="113244109" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<subsidiary_company wordnetid="108003935" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../501/18949501.xml">
Open Directory Project</link></institution>
</subsidiary_company>
</activity>
</psychological_feature>
</act>
</property>
</undertaking>
</company>
</event>
</possession>
</work>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.dmoz.org/Computers/Computer_Science/Distributed_Computing/Publications//">
Distributed computing journals</weblink> at the <work wordnetid="100575741" confidence="0.8">
<possession wordnetid="100032613" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<company wordnetid="108058098" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<property wordnetid="113244109" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<subsidiary_company wordnetid="108003935" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../501/18949501.xml">
Open Directory Project</link></institution>
</subsidiary_company>
</activity>
</psychological_feature>
</act>
</property>
</undertaking>
</company>
</event>
</possession>
</work>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.dmoz.org/Computers/Computer_Science/Distributed_Computing/Conferences//">
Distributed computing conferences</weblink> at the <work wordnetid="100575741" confidence="0.8">
<possession wordnetid="100032613" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<company wordnetid="108058098" confidence="0.8">
<undertaking wordnetid="100795720" confidence="0.8">
<property wordnetid="113244109" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<subsidiary_company wordnetid="108003935" confidence="0.8">
<institution wordnetid="108053576" confidence="0.8">
<link xlink:type="simple" xlink:href="../501/18949501.xml">
Open Directory Project</link></institution>
</subsidiary_company>
</activity>
</psychological_feature>
</act>
</property>
</undertaking>
</company>
</event>
</possession>
</work>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://ocw.mit.edu/OcwWeb/Electrical-Engineering-and-Computer-Science/6-852JFall-2005/CourseHome/index.htm">
MIT's Open Course - Distributed Algorithms</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.mu.oz.au/courses/mbc/medc.html">
Melbourne's Masters Course in Distributed Computing</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.akaas.net/distributed-computing/distributed-computing-1.htm">
Distributed Computing Frequently Asked Questions (FAQs)</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.hoozi.com/Articles/DynaOS.htm">
DynaOS - Description of a conceptual Distributed Operating System</weblink></entry>
</list>
</p>
<p>

<table style=";" class="navbox" cellspacing="0">
<row>
<col style="padding:2px;">
<table style="width:100%;background:transparent;color:inherit;;" class="nowraplinks collapsible autocollapse " cellspacing="0">
<row>
<header colspan="2" style=";" class="navbox-title">
<link xlink:type="simple" xlink:href="../162/145162.xml">
Parallel computing</link>topics</header>
</row>
<row style="height:2px;">

</row>
<row>
<col style=";;" class="navbox-group">
General</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<condition wordnetid="113920835" confidence="0.8">
<state wordnetid="100024720" confidence="0.8">
<problem wordnetid="114410605" confidence="0.8">
<difficulty wordnetid="114408086" confidence="0.8">
<link xlink:type="simple" xlink:href="../527/832527.xml">
High-performance computing</link></difficulty>
</problem>
</state>
</condition>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Parallelism</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../148/14229148.xml">
Bit-level parallelism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../960/245960.xml">
Instruction level parallelism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../420/9467420.xml">
Data parallelism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../070/9468070.xml">
Task parallelism</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Threads</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../877/313877.xml">
Superthreading</link>&nbsp;·  <link xlink:type="simple" xlink:href="../443/151443.xml">
Hyperthreading</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Theory</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../612/1448612.xml">
Speedup</link>&nbsp;·  <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<link xlink:type="simple" xlink:href="../ury/24th_century.xml">
Amdahl's law</link></rule>
</concept>
</idea>
&nbsp;·  <class wordnetid="107997703" confidence="0.8">
<collection wordnetid="107951464" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<link xlink:type="simple" xlink:href="../349/222349.xml">
Flynn's taxonomy</link></group>
</collection>
</class>
 (<link xlink:type="simple" xlink:href="../630/1103630.xml">
SISD</link>&nbsp;&amp;bull;  <link xlink:type="simple" xlink:href="../359/55359.xml">
SIMD</link>&nbsp;&amp;bull;  <link xlink:type="simple" xlink:href="../666/991666.xml">
MISD</link>&nbsp;&amp;bull;  <link xlink:type="simple" xlink:href="../139/157139.xml">
MIMD</link>)&nbsp;·  <link xlink:type="simple" xlink:href="../721/3505721.xml">
Cost efficiency</link>&nbsp;·  <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<rule wordnetid="105846054" confidence="0.8">
<link xlink:type="simple" xlink:href="../252/4243252.xml">
Gustafson's law</link></rule>
</concept>
</idea>
&nbsp;·  <link xlink:type="simple" xlink:href="../042/9453042.xml">
Karp-Flatt metric</link>&nbsp;·  <link xlink:type="simple" xlink:href="../068/15167068.xml">
Parallel slowdown</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Elements</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../178/45178.xml">
Process</link>&nbsp;·  <link xlink:type="simple" xlink:href="../303/45303.xml">
Thread</link>&nbsp;·  <link xlink:type="simple" xlink:href="../712/5533712.xml">
Fiber</link>&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../675/956675.xml">
Parallel Random Access Machine</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Coordination</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../020/64020.xml">
Multiprocessing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../679/10520679.xml">
Multithreading</link>&nbsp;·  <link xlink:type="simple" xlink:href="../857/6857.xml">
Multitasking</link>&nbsp;·  <link xlink:type="simple" xlink:href="../818/399818.xml">
Memory coherency</link>&nbsp;·  <link xlink:type="simple" xlink:href="../865/176865.xml">
Cache coherency</link>&nbsp;·  <link xlink:type="simple" xlink:href="../263/4736263.xml">
Barrier</link>&nbsp;·  <link xlink:type="simple" xlink:href="../017/4726017.xml">
Synchronization</link>&nbsp;·  <link xlink:type="simple" xlink:href="../501/8501.xml">
Distributed computing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../373/49373.xml">
Grid computing</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../311/5311.xml">
Programming</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<paradigm wordnetid="113804375" confidence="0.8">
<linguistic_relation wordnetid="113797142" confidence="0.8">
<inflection wordnetid="113803782" confidence="0.8">
<grammatical_relation wordnetid="113796779" confidence="0.8">
<link xlink:type="simple" xlink:href="../375/2242375.xml">
Programming model</link></grammatical_relation>
</inflection>
</linguistic_relation>
</paradigm>
&nbsp;·  <link xlink:type="simple" xlink:href="../888/3453888.xml">
Implicit parallelism</link>&nbsp;·  <link xlink:type="simple" xlink:href="../332/3095332.xml">
Explicit parallelism</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../310/5310.xml">
Hardware</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<link xlink:type="simple" xlink:href="../896/18949896.xml">
Computer cluster</link>&nbsp;·  <link xlink:type="simple" xlink:href="../542/66542.xml">
Beowulf</link>&nbsp;·  <link xlink:type="simple" xlink:href="../318/50318.xml">
Symmetric multiprocessing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../643/40643.xml">
Non-Uniform Memory Access</link>&nbsp;·  <link xlink:type="simple" xlink:href="../307/910307.xml">
Cache only memory architecture</link>&nbsp;·  <link xlink:type="simple" xlink:href="../506/2576506.xml">
Asymmetric multiprocessing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../021/315021.xml">
Simultaneous multithreading</link>&nbsp;·  <link xlink:type="simple" xlink:href="../653/825653.xml">
Shared memory</link>&nbsp;·  <link xlink:type="simple" xlink:href="../887/234887.xml">
Distributed memory</link>&nbsp;·  <link xlink:type="simple" xlink:href="../049/584049.xml">
Massive parallel processing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../702/51702.xml">
Superscalar processing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../205/58205.xml">
Vector processing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../153/37153.xml">
Supercomputer</link>&nbsp;·  <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<paradigm wordnetid="113804375" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<linguistic_relation wordnetid="113797142" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<inflection wordnetid="113803782" confidence="0.8">
<grammatical_relation wordnetid="113796779" confidence="0.8">
<link xlink:type="simple" xlink:href="../727/2786727.xml">
Stream processing</link></grammatical_relation>
</inflection>
</causal_agent>
</linguistic_relation>
</worker>
</paradigm>
</assistant>
</model>
</person>
</physical_entity>
&nbsp;·  <substance wordnetid="100019613" confidence="0.8">
<paper wordnetid="114974264" confidence="0.8">
<card wordnetid="102962545" confidence="0.8">
<part wordnetid="113809207" confidence="0.8">
<material wordnetid="114580897" confidence="0.8">
<link xlink:type="simple" xlink:href="../939/1268939.xml">
GPGPU</link></material>
</part>
</card>
</paper>
</substance>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../309/5309.xml">
Software</link></col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<link xlink:type="simple" xlink:href="../843/399843.xml">
Distributed shared memory</link> &nbsp;·  <link xlink:type="simple" xlink:href="../765/416765.xml">
Application checkpointing</link>&nbsp;·  <link xlink:type="simple" xlink:href="../129/2738129.xml">
Warewulf</link></col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
<link xlink:type="simple" xlink:href="../ury/24th_century.xml">
API</link>s</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-even">
<standard wordnetid="107260623" confidence="0.8">
<system_of_measurement wordnetid="113577171" confidence="0.8">
<link xlink:type="simple" xlink:href="../671/634671.xml">
POSIX Threads</link></system_of_measurement>
</standard>
&nbsp;·  <link xlink:type="simple" xlink:href="../842/381842.xml">
OpenMP</link>&nbsp;·  <link xlink:type="simple" xlink:href="../466/221466.xml">
Message Passing Interface (MPI)</link>&nbsp;·  <link xlink:type="simple" xlink:href="../616/1057616.xml">
UPC</link>&nbsp;·  <link xlink:type="simple" xlink:href="../077/11625077.xml">
Intel Threading Building Blocks</link>&nbsp;·  <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../324/711324.xml#xpointer(//*[./st=%22Multithreading+=E2=80=93+Boost.Thread%22])">
Boost.Thread</link></room>
</library>
</area>
</artifact>
</structure>
</col>
</row>
<row style="height:2px">

</row>
<row>
<col style=";;" class="navbox-group">
Problems</col>
<col style="text-align:left;border-left:2px solid #fdfdfd;width:100%;padding:0px;;;" class="navbox-list navbox-odd">
<system wordnetid="104377057" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<link xlink:type="simple" xlink:href="../712/1738712.xml">
Embarrassingly parallel</link></instrumentality>
</artifact>
</system>
&nbsp;·  <condition wordnetid="113920835" confidence="0.8">
<state wordnetid="100024720" confidence="0.8">
<problem wordnetid="114410605" confidence="0.8">
<difficulty wordnetid="114408086" confidence="0.8">
<link xlink:type="simple" xlink:href="../754/439754.xml">
Grand Challenge</link></difficulty>
</problem>
</state>
</condition>
&nbsp;·  <plant_part wordnetid="113086908" confidence="0.8">
<natural_object wordnetid="100019128" confidence="0.8">
<kernel wordnetid="113137010" confidence="0.8">
<link xlink:type="simple" xlink:href="../798/12332798.xml">
Software lockout</link></kernel>
</natural_object>
</plant_part>
</col>
</row>
</table>
</col>
</row>
</table>
</p>


</sec>
</bdy>
</article>
