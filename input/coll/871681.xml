<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:46:51[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<physical_entity  confidence="0.8" wordnetid="100001930">
<structure  confidence="0.8" wordnetid="105726345">
<person  confidence="0.8" wordnetid="100007846">
<model  confidence="0.8" wordnetid="110324560">
<arrangement  confidence="0.8" wordnetid="105726596">
<assistant  confidence="0.8" wordnetid="109815790">
<worker  confidence="0.8" wordnetid="109632518">
<causal_agent  confidence="0.8" wordnetid="100007347">
<distribution  confidence="0.8" wordnetid="105729036">
<header>
<title>Mixture model</title>
<id>871681</id>
<revision>
<id>243733251</id>
<timestamp>2008-10-07T20:40:05Z</timestamp>
<contributor>
<username>Twri</username>
<id>7976492</id>
</contributor>
</revision>
<categories>
<category>Probability distributions</category>
<category>Statistical models</category>
<category>Wikipedia articles needing clarification</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link>, the term <b>mixture model</b> is a model in which independent variables are fractions of a total.
<sec>
<st>
Examples</st>

<p>

<image location="right" width="250px" src="Normal_distribution_pdf.png" type="thumb">
<caption>

The <link xlink:type="simple" xlink:href="../462/21462.xml">
normal distribution</link> is plotted using different means and variances
</caption>
</image>

Suppose researchers are trying to find the optimal mixture of ingredients for a fruit punch consisting of grape juice, mango juice, and pineapple juice.  A mixture model is suitable here because the results of the taste tests will not depend on the amount of ingredients used to make the batch but rather on the fraction of each ingredient present in the punch.  The components always sum to a whole, which a mixture model takes into account.</p>
<p>

As another example, financial returns often behave differently in normal situations and during crisis times. A mixture model for return data seems reasonable.</p>

<ss1>
<st>
Direct and indirect applications of mixture models</st>

<p>

The financial example above is one direct application of the mixture model, a situation in which we assume an underlying mechanism so that each observation belongs to one of some number of different sources or categories. This underlying mechanism may or may not, however, be observable. In this form of mixture, each of the sources is described by a component probability density function, and its mixture weight is the probability that an observation comes from this component.</p>
<p>

In an indirect application of the mixture model we do not assume such a mechanism. The mixture model is simply used for its mathematical flexibilities. For example, a mixture of two <link xlink:type="simple" xlink:href="../462/21462.xml">
normal distribution</link>s with different means may result in a density with two <link xlink:type="simple" xlink:href="../127/1432127.xml">
modes</link>, which is not modeled by standard parametric distributions.</p>

</ss1>
</sec>
<sec>
<st>
 Types of mixture model </st>

<ss1>
<st>
Probability mixture model</st>
<p>

In <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, a <b>probability mixture model</b> is a <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> that is a <link xlink:type="simple" xlink:href="../534/794534.xml">
convex combination</link> of other probability distributions.</p>
<p>

Suppose that the <link xlink:type="simple" xlink:href="../015/3285015.xml">
discrete random variable</link> <math>X</math> is a mixture of <math>n</math> component discrete random variables <math>Y_i</math>.  Then, the <link xlink:type="simple" xlink:href="../725/154725.xml">
probability mass function</link> of <math>X</math>, <math>f_{X}(x)</math>, is a weighted sum of its component distributions:</p>
<p>

<indent level="1">

<math>f_{X}(x) = \sum_{i=1}^{n} a_i f_{Y_i}(x)</math>
</indent>

for some mixture proportions <math>0 &amp;lt; a_{i}&amp;lt; 1</math> where <math>a_{1} +\cdots+ a_{n} = 1</math>.</p>
<p>

The definition is the same for <link xlink:type="simple" xlink:href="../792/5792.xml">
continuous random variable</link>s, except that the functions <math>f</math> are <link xlink:type="simple" xlink:href="../487/43487.xml">
probability density function</link>s.</p>

</ss1>
<ss1>
<st>
Parametric mixture model</st>
<p>

In the <b>parametric mixture model</b>, the component distributions are from a <link xlink:type="simple" xlink:href="../337/19653337.xml">
parametric family</link>, with unknown parameters <math>\theta_i</math>:</p>
<p>

<indent level="1">

<math>f_{X}(x) = \sum_{i=1}^{n} a_i f_Y(x ; \theta_i)</math>
</indent>

</p>
</ss1>
<ss1>
<st>
Continuous mixture</st>
<p>

A <b>continuous mixture</b> is defined similarly:</p>
<p>

<indent level="1">

<math>f_{X}(x) = \int_\Theta h(\theta) f_Y(x ; \theta) \, d\theta</math>
</indent>

where
<indent level="1">

<math>0 \le h(\theta) \quad \forall \theta \in \Theta</math>
</indent>

and
<indent level="1">

<math>\int_\Theta h(\theta) \, d\theta = 1</math>
</indent>

</p>
</ss1>
</sec>
<sec>
<st>
 Identifiability </st>

<p>

Identifiability refers to the existence of a unique characterization for any one of the models in the class being considered. Estimation procedure may not be well-defined and asymptotic theory may not hold if a model is not identifiable.</p>

<ss1>
<st>
 Example </st>
<p>

Let <math>J</math> be the class of all binomial distributions with <math> n=2</math>. Then a mixture of two members of <math>J</math> would have</p>
<p>

<indent level="1">

<math>p_0=\pi(1-\theta_1)^2+(1-\pi)(1-\theta_2)^2</math>
</indent>
:<math>p_1=2\pi\theta_1(1-\theta_1)+2(1-\pi)\theta_2(1-\theta_2)</math></p>
<p>

and <math>p_2=1-p_0-p_1</math>. Clearly, given <math>p_0</math> and <math>p_1</math>, it is not possible to determine the above mixture model uniquely, as there are three parameters (<math>\pi,\theta_1,\theta_2</math>) to be determined.</p>

</ss1>
<ss1>
<st>
 Definition </st>
<p>

Consider a mixture of parametric distributions of the same class. Let</p>
<p>

<indent level="1">

<math>J=\{f(\cdot ; \theta):\theta\in\Omega\}</math>
</indent>

be the class of all component distributions. Then the <link xlink:type="simple" xlink:href="../634/40634.xml">
convex hull</link> <math>K</math> of <math>J</math> defines the class of all finite mixture of distributions in <math>J</math>:</p>
<p>

<indent level="1">

<math>K=\{p(\cdot):p(\cdot)=\sum_{i=1}^n a_i f_i(\cdot ; \theta_i), a_i&amp;gt;0, \sum_{i=1}^n a_i=1, f_i(\cdot ; \theta_i)\in J\ \forall i,n\}</math>
</indent>

<math>K</math> is said to be identifiable if all its members are unique, that is, given two members <math>p</math> and <math>p'</math> in <math>K</math>, being mixtures of <math>k</math> distributions and <math>k'</math> distributions respectively in <math>J</math>, we have <math>p=p'</math> if and only if, first of all, <math>k=k'</math> and secondly we can reorder the summations such that <math>a_i=a_i'</math> and <math>f_i=f_i'</math> for all <math>i</math>.</p>

</ss1>
</sec>
<sec>
<st>
 Common approaches for estimation in mixture models </st>
<p>

Parametric mixture models are often used when we know the distribution <math>Y</math> and we can sample from <math>X</math>, but we would like to determine the <math>a_{i}</math> and <math>\theta_i</math> values.  Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations.</p>
<p>

It is common to think of probability mixture modeling as a missing data problem.  One way to understand this is to assume that the data points under consideration have "membership" in one of the distributions we are using to model the data.  When we start, this membership is unknown, or missing.  The job of estimation is to devise appropriate parameters for the model functions we choose, with the connection to the data points being represented as their membership in the individual model distributions.</p>

<ss1>
<st>
 Expectation maximization </st>
<p>
 
The <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<numerical_quantity wordnetid="105856066" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<value wordnetid="105856388" confidence="0.8">
<quantity wordnetid="105855125" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../752/470752.xml">
Expectation-maximization algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</quantity>
</value>
</rule>
</event>
</numerical_quantity>
</concept>
</idea>
 can be used to compute the parameters of a parametric mixture model distribution (the <math>a_{i}</math>'s and <math>\theta_{i}</math>'s).  It is an <link xlink:type="simple" xlink:href="../237/15237.xml">
iterative algorithm</link> with two steps: an <it>expectation step</it> and a <it>maximization step</it>. <weblink xlink:type="simple" xlink:href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture">
Practical examples of EM and Mixture Modeling</weblink> are included in the <link xlink:type="simple" xlink:href="../480/3217480.xml">
SOCR</link> demonstrations.</p>

<ss2>
<st>
 The expectation step </st>
<p>

With initial guesses for the parameters of our mixture model, "partial membership" of each data point in each constituent distribution is computed by calculating <link xlink:type="simple" xlink:href="../653/9653.xml">
expectation value</link>s for the membership variables of each data point.  That is, for each data point <math>x_j</math> and distribution <math>Y_i</math>, the membership value <math>y_{i,j}</math> is:</p>
<p>

<indent level="1">

<math> y_{i,j} = \frac{a_i f_Y(x_j;\theta_i)}{f_{X}(x_j)}</math>
</indent>

</p>
</ss2>
<ss2>
<st>
 The maximization step </st>
<p>

With expectation values in hand for group membership, <link>
plug-in estimates</link> are recomputed for the distribution parameters.</p>
<p>

The mixing coefficients <math>a_i</math> are the <link xlink:type="simple" xlink:href="../612/612.xml">
mean</link>s of the membership values over the <math>N</math> data points.</p>
<p>

<indent level="1">

<math> a_i = \frac{1}{N}\sum_{j=1}^N y_{i,j}</math>
</indent>

The component model parameters <math>\theta_{i}</math> are also calculated by expectation maximization using data points <math>x_j</math> that have been weighted using the membership values.  For example, if <math>\theta</math> is a mean <math>\mu</math></p>
<p>

<indent level="1">

<math> \mu_{i} = \frac{\sum_{j} y_{i,j}x_{j}}{\sum_{j} y_{i,j}}</math>
</indent>

With new estimates for <math>a_i</math> and the <math>\theta_i</math>'s, the expectation step is repeated to recompute new membership values.  The entire procedure is repeated until model parameters converge.</p>

</ss2>
</ss1>
<ss1>
<st>
 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov-chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
 <technique wordnetid="105665146" confidence="0.8">
<know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../098/56098.xml">
Monte Carlo</link></method>
</know-how>
</technique>
 </st>
<p>

As an alternative to the EM algorithm, the mixture model parameters can be deduced using <link>
posterior sampling</link> as indicated by <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
.  This is still regarded as an incomplete data problem whereby membership of data points is the missing data.  A two-step iterative procedure known as <know-how wordnetid="105616786" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<link xlink:type="simple" xlink:href="../709/509709.xml">
Gibbs sampling</link></method>
</know-how>
 can be used.</p>
<p>

The previous example of a mixture of two <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian distribution</link>s can demonstrate how the method works.  As before, initial guesses of the parameters for the mixture model are made.  Instead of computing partial memberships for each elemental distribution, a membership value for each data point is drawn from a <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../189/199189.xml">
Bernoulli distribution</link></distribution>
</arrangement>
</structure>
 (that is, it will be assigned to either the first or the second Gaussian).  The Bernoulli parameter <math>\theta</math> is determined for each data point on the basis of one of the constituent distributions.
&#91;&#93;  Draws from the distribution generate membership associations for each data point.  Plug-in estimators can then be used as in the M step of EM to generate a new set of mixture model parameters, and the binomial draw step repeated.</p>

</ss1>
<ss1>
<st>
 Spectral method</st>
<p>

Some problems in mixture model estimation can be solved using <link xlink:type="simple" xlink:href="../110/243110.xml">
spectral method</link>s. 
In particular it becomes useful if data points <math>x_i</math> are points in high-dimensional
<link xlink:type="simple" xlink:href="../697/9697.xml">
Euclidean space</link>, and the hidden distributions are known to be <link xlink:type="simple" xlink:href="../987/4350987.xml">
log-concave</link> (such as <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian distribution</link> or <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../906/45906.xml">
Exponential distribution</link></distribution>
</arrangement>
</structure>
).</p>
<p>

Spectral methods of learning mixture models are based on the use of <link xlink:type="simple" xlink:href="../207/142207.xml">
Singular Value Decomposition</link> of a matrix which contains data points.
The idea is to consider the top <math>k</math> singular vectors, where <math>k</math> is the number of distributions to be learned. The projection 
of each data point to a <link xlink:type="simple" xlink:href="../316/13060316.xml">
linear subspace</link> spanned by those vectors groups points originating from the same distribution 
very close together, while points from different distributions stay far apart. </p>
<p>

One distinctive feature of the spectral method is that it allows us to <link xlink:type="simple" xlink:href="../285/82285.xml">
prove</link> that if 
distributions satisfy certain separation condition (e.g. not too close), then the estimated mixture will be very close to the true one with high probability.</p>

</ss1>
<ss1>
<st>
 Other methods </st>

<p>

Some of them can even provably learn mixtures of <link xlink:type="simple" xlink:href="../200/8092200.xml">
heavy-tailed distribution</link>s including those with 
infinite <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link> (see <link xlink:type="simple" xlink:href="#xpointer(//*[./st=%22Recent+Papers%22])">
links to papers</link> below).
In this setting, EM based methods would not work, since the Expectation step would diverge due to presence of
<link xlink:type="simple" xlink:href="../951/160951.xml">
outlier</link>s.</p>

</ss1>
<ss1>
<st>
 A simulation </st>
<p>

To simulate a sample of size N that is from a mixture of distributions Fi, i=1 to n, with probabilities pi (sum pi=1):
<list>
<entry level="1" type="number">

 Generate N random numbers from a <structure wordnetid="105726345" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../717/10072717.xml">
Categorical distribution</link></distribution>
</arrangement>
</structure>
 of size n and probabilities pi for i=1 to n.  These tell you which of the Fi each of the N values will come from.  Denote by mi the number that came from the ith category.</entry>
<entry level="1" type="number">

 For each i, generate mi random numbers from the Fi distribution.</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 Further reading </st>

<ss1>
<st>
 Books on mixture models </st>
<p>

<list>
<entry level="1" type="number">

 Titterington, D., A. Smith, and U. Makov "Statistical Analysis of Finite Mixture Distributions," John Wiley &amp; Sons (1985). </entry>
<entry level="1" type="number">

 McLachlan, G.J. and Peel, D. <it>Finite Mixture Models,</it> , Wiley (2000)</entry>
<entry level="1" type="number">

 Marin, J.M., Mengersen, K. and Robert, C.P.  "Bayesian modelling and inference on mixtures of distributions". <it>Handbook of Statistics</it> 25, D. Dey and C.R. Rao (eds). Elsevier-Sciences (to appear). <weblink xlink:type="simple" xlink:href="http://www.ceremade.dauphine.fr/%7Exian/mixo.pdf">
available as PDF</weblink></entry>
<entry level="1" type="number">

 Lindsay B.G., Mixture Models: Theory, Geometry, and Applications. NSF-CBMS Regional Conference Series in Probability and Statistics Vol. 5, Institute of Mathematical Statistics, Hayward (1995).</entry>
<entry level="1" type="number">

 McLachlan, G.J. and Basford, K.E. "Mixture Models: Inference and Applications to Clustering", Marcel Dekker (1988)</entry>
<entry level="1" type="number">

 Everitt, B.S. and Hand D.J. "Finite mixture distributions", Chapman &amp; Hall (1981) </entry>
</list>
</p>

</ss1>
<ss1>
<st>
 Application of Gaussian Mixture Models</st>
<p>

<list>
<entry level="1" type="number">

 <cite style="font-style:normal">D.A. Reynolds and R.C. Rose&#32;(1995). "<weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=365379">
Robust text-independent speaker identification using Gaussianmixture speaker models</weblink>".&#32;<it>IEEE Transactions on  Speech and Audio Processing</it>.</cite>&nbsp;</entry>
<entry level="1" type="number">

 <cite style="font-style:normal">H.H. Permuter, J. Francos and I.H. Jarmyn&#32;(2003). "<weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1199538">
Gaussian mixture models of texture and colour for image database retrieval</weblink>".&#32;<it>IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings (ICASSP '03)</it>.</cite>&nbsp;</entry>
<entry level="1" type="number">

 <cite style="font-style:normal" class="book">Wolfgang Lemke&#32;(2005). Term Structure Modeling and Estimation in a State Space Framework.&#32;Springer Verlag. ISBN 978-3540283423.</cite>&nbsp;</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <system wordnetid="104377057" confidence="0.8">
<structure wordnetid="105726345" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<arrangement wordnetid="105726596" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<distribution wordnetid="105729036" confidence="0.8">
<link xlink:type="simple" xlink:href="../242/708242.xml">
Mixture density</link></distribution>
</instrumentality>
</arrangement>
</artifact>
</structure>
</system>
</entry>
<entry level="1" type="bullet">

 The <weblink xlink:type="simple" xlink:href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture">
SOCR demonstrations of EM and Mixture Modeling</weblink></entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/mixturemodel.html">
Mixture modelling page</weblink> (and the <weblink xlink:type="simple" xlink:href="http://www.csse.monash.edu.au/~dld/Snob.html">
Snob</weblink> program for <link xlink:type="simple" xlink:href="../210/302210.xml">
Minimum Message Length</link> (<link xlink:type="simple" xlink:href="../210/302210.xml">
MML</link>) applied to finite mixture models), maintained by D.L. Dowe.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://algorithmics.molgen.mpg.de/Software/PyMix/">
PyMix</weblink> - Python Mixture Package, algorithms and data structures for a broad variety of mixture model based data mining applications in Python</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.ar.media.kyoto-u.ac.jp/members/david/softwares/em/">
em</weblink> - A Python package for learning Gaussian Mixture Models with Expectation Maximization, currently packaged with <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../472/263472.xml">
SciPy</link></software>
</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=18785&amp;objectType=FILE">
GMM.m</weblink> Matlab code for GMM Implementation</entry>
</list>



</p>

</sec>
</bdy>
</distribution>
</causal_agent>
</worker>
</assistant>
</arrangement>
</model>
</person>
</structure>
</physical_entity>
</article>
