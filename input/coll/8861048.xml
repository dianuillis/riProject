<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 23:40:37[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Stanford Smart Memories Project</title>
<id>8861048</id>
<revision>
<id>241258799</id>
<timestamp>2008-09-27T03:04:08Z</timestamp>
<contributor>
<username>SkierRMH</username>
<id>2618808</id>
</contributor>
</revision>
<categories>
<category>Reconfigurable computing</category>
<category>Parallel computing</category>
<category>Computer architecture</category>
<category>Stanford University</category>
<category>Microprocessors</category>
</categories>
</header>
<bdy>

Advances in <link xlink:type="simple" xlink:href="../823/32823.xml">
VLSI</link> technology now permit multiple processors to reside on a <link xlink:type="simple" xlink:href="../782/10699782.xml">
single integrated circuit chip,</link> or <link xlink:type="simple" xlink:href="../782/10699782.xml">
IC</link>.  Such a processing system is known as a chip multiprocessor, or <link xlink:type="simple" xlink:href="../207/3503207.xml">
multi-core CPU system.</link>  Building on this technology, the <b>Stanford Smart Memories Project</b> places several processors on an IC, along with several independent memory blocks.  In addition, the processors can be connected to the memory blocks in various ways, with the ability to form and change connections even while the processors are running.  This ability is a kind of <link xlink:type="simple" xlink:href="../371/188371.xml">
reconfigurable computing</link>.<p>

Depending on how processors talk to memory and to one another, they form a system that can be tailored more or less to a given style of computation.  A fixed (non-reconfigurable) compute system might do well at supporting one style of computation, but consequently perform poorly on a different style.  A reconfigurable computer, however, can adapt to many different styles of computing, and thus provide reasonably good performance across a wide range.  Smart Memories has been shown to be effective for diverse compute styles including <link xlink:type="simple" xlink:href="../518/267518.xml">
MESI-style</link> shared-memory <link xlink:type="simple" xlink:href="../865/176865.xml">
cache coherence,</link> <link xlink:type="simple" xlink:href="../095/8307095.xml">
thread-level speculation (TLS),</link> <link xlink:type="simple" xlink:href="../727/2786727.xml">
streaming</link> and <weblink xlink:type="simple" xlink:href="http://tcc.stanford.edu/">
transaction coherence (TCC).</weblink>
<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></p>
<p>

The <b>Stanford Smart Memories Project</b> is an effort to develop a
computing infrastructure for the next generation of applications. It is a 
<link xlink:type="simple" xlink:href="../207/3503207.xml">
multicore</link> system
with coarse grain reconfiguration capabilities for supporting
diverse computing models, like speculative multithreading and 
<link xlink:type="simple" xlink:href="../727/2786727.xml">
streaming architectures</link>. These features 
allow the system to run a broad range of applications efficiently. 
Research in this area involves <link xlink:type="simple" xlink:href="../823/32823.xml">
VLSI circuits</link>, 
<link xlink:type="simple" xlink:href="../509/6509.xml">
computer architecture</link>, <link xlink:type="simple" xlink:href="../739/5739.xml">
compilers</link>, <link xlink:type="simple" xlink:href="../194/22194.xml">
operating systems</link>, 
<link xlink:type="simple" xlink:href="../210/18567210.xml">
computer graphics</link> and <link xlink:type="simple" xlink:href="../652/5652.xml">
computer networking</link>.</p>
<p>

Smart Memories is a project in the
<weblink xlink:type="simple" xlink:href="http://csl.stanford.edu/">
Computer Systems Laboratory,</weblink> 
a joint laboratory of the
<weblink xlink:type="simple" xlink:href="http://www-ee.stanford.edu/">
Electrical Engineering</weblink> 
and <weblink xlink:type="simple" xlink:href="http://www-cs.stanford.edu/">
Computer Science</weblink>
departments at <weblink xlink:type="simple" xlink:href="http://www.stanford.edu/">
Stanford University</weblink>.</p>

<sec>
<st>
 Project overview </st>
<p>

The Stanford Smart Memories Project aims to design a 
single-chip computing element that provides configurable hardware 
support for diverse computing models, and that maps efficiently to future 
wire-limited VLSI technologies.</p>
<p>

The Smart Memory chip architecture exploits the fact that wire-delay 
limitations in future VLSI chips will impose a fine-grained 
partitioning of processors, memories, and interconnects. Adding 
programmable wires and logic to this inherently modular organization 
allows on-chip memories and communication paths to be customized to 
the particular computing problem at hand. This allows performance
competitive with application-specific architectures, but with lower 
cost and increased flexibility. This fine-grained partitioning of 
processing and memory resources also enables substantial hardware 
parallelism. Effectively exploiting this parallelism in the face of 
global wiring delays requires aggressive methods for reducing on-chip 
communication overhead between the various processing and memory 
structures. </p>
<p>

To develop a configurable micro-architecture, the Smart Memories group
is studying diverse 
classes of computing problems, (such as <link xlink:type="simple" xlink:href="../295/17003295.xml">
ray tracing</link>, multimedia and 
DSP, speech and voice recognition, probabilistic reasoning), and the 
specialized architectures that have been optimized for these problems. 
This will provide insight into the hardware primitives and 
configurable mechanisms required to implement a universal computing 
substrate. The group is mainly interested in the requirements that such 
classes of applications place on the memory system of a multiprocessor 
environment, and are investigating strategies for building a 
reconfigurable memory system.</p>

</sec>
<sec>
<st>
 Architecture overview </st>



<p>

Smart Memories is a multiprocessor system with coarse grain 
reconfiguration capabilities. Processing units in this system are 
in the form of <it>tiles</it> which, when put together in groups of four, 
form <it>quads</it>. These elements connect in a 
hierarchical manner: a set of inter-quad connections provide 
communication facilities for tiles inside a quad, while a mesh 
interconnection network connects quads together. Tiles inside a quad 
share a network interface to connect to the outside world 
(<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/gif/overvi1.gif">
Figure 1</weblink>).</p>


<p>

Each tile in the Smart Memories system consists of four major parts 
(<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/gif/archit1.gif">
Figure 2</weblink>): 
two processor cores, a set of configurable memory mats, a cross bar 
interconnect and a load/store unit (LSU). Either or both of the processors 
inside the tile can be easily turned off, allowing a tile to be just 
a memory resource, and saving power, in the case that excess processing 
power is not required.</p>

<ss1>
<st>
 Tile: Processors, memory mats, crossbar and LSU </st>



<p>

<b>Processors</b></p>
<p>

Smart Memories leverages 
<weblink xlink:type="simple" xlink:href="http://www.tensilica.com/html/xtensa_lx.html">
Xtensa LX</weblink>
commercial configurable processing cores from <weblink xlink:type="simple" xlink:href="http://www.tensilica.com/">
Tensilica.</weblink>
Cores are 32 bit RISC machines with a flexible 16/24 bit instruction length.
The cores have been configured to be 3-way issue VLIW with flexible instruction
formats. The Xtensa LX has a seven stage pipeline, with two stages for memory
access. It has 64 general purpose registers, a 32-bit floating point unit and 32
floating point registers.</p>
<p>

Processors are configured and extended using the TIE <it>(Tensilica Instruction Extension)</it>
Language. The Smart Memories group has defined new interfaces to the
memory, plus state registers and custom instructions for supporting 
different programming models.</p>
<p>

<b>Memory mats</b></p>
<p>

<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/gif/archit2.gif">
Figure 3</weblink>
shows the block diagram of a reconfigurable memory mat.
Each memory mat has 1024 32-bit words in its main data array. Each word is
associated with six control bits in a separate control array. A programmable
PLA performs a read-modify-write operation on the control bits after each access
to the memory word. The mat can perform read, write and compare operations on
each 32-bit data word.</p>
<p>

Each memory mat also has two pairs of pointer/stride registers,
which can be used to implement two separate hardware FIFOs inside. Mats are
connected via a two bit inter-mat communication network, which allows them to
exchange control information. They can be configured to be used as cache, 
FIFO's or scratchpads.</p>
<p>

<b>Crossbar</b></p>
<p>

A crossbar inside the tile connects the memory mats to the two
processor cores inside the tile, and to the tile's quad interface. 
The crossbar has four ports at the processor (LSU) side, two ports to 
the quad interface and 16 ports to the memory mats. </p>
<p>

<b>Load/Store Unit (LSU)</b></p>
<p>

A Load/Store unit interfaces the two Tensilica cores to the rest of the
memory system. It provides basic interfacing and support for the custom 
memory operations that were defined using the TIE language. The LSU also 
communicates with the quad's cache controller to request cache refills, access 
off-tile memory and report other special events, such as synchronization misses.</p>

</ss1>
<ss1>
<st>
 Quad: Four tiles, cache controller, network interface </st>

<p>

Each group of four tiles forms one <it>quad</it>. Each quad has a shared
cache or protocol controller, which provides support for the processors 
inside. It also has a network interface, which sends/receives/routes 
packets on the mesh-like network, and provides an interface to 
the outside world.</p>
<p>

<b>Cache (protocol) controller</b></p>
<p>

The protocol controller is considered to be the heart of the quad. It
can perform a variety of actions to support the processors'
memory access needs under different programming models. Briefly, 
the protocol controller services cache evictions/refills, 
provides access to memory mats in one tile for a processor in 
another tile (off-tile accesses), enforces cache coherence invariance 
(MESI protocol), acts as a DMA engine to move data in and out of the 
quad, and provides support for transactions. </p>
<p>

<b>Network interface</b></p>
<p>

The network interface is a simple router that connects each quad to 
its neighbors via a set of wires. It receives packets from the protocol
controller or other neighbors and routes them to appropriate destinations.</p>

</ss1>
</sec>
<sec>
<st>
 Programming models / software </st>

<p>

Smart Memories is designed to efficiently support different programming models, 
allowing an application to be programmed and run in the model that gives the 
best performance and/or programming ease. Smart Memories can reconfigure its 
memory system to provide the unique memory access requirements for each of 
three major models: shared memory, streaming, and transactional consistency.</p>

<ss1>
<st>
 Shared memory / multi-thread mode </st>

<p>

This programming model gives the programmer a cache coherent shared memory
environment. Multi-thread programs are supported using
different APIs, such as pthreads or ANL macros. There are on-going efforts to
map different application classes to the Smart Memories architecture using
this programming model, including probabilistic reasoning applications,
global illumination and data structure pre-fetching.</p>
<p>

<b>Probabilistic reasoning applications</b></p>
<p>

Probabilistic reasoning is an influential approach in 
<link xlink:type="simple" xlink:href="../164/1164.xml">
artificial intelligence</link>, where it has been shown
to successfully tackle difficult problems in growing fields such as data
mining, image analysis, robotics, and genetics. Given the increasingly complex
models and large data sets used in these emerging applications, the
performance of reasoning algorithms is likely to become important for 
future computing systems. These algorithms tend to be
inherently parallel, but are demanding in compute, memory and bandwidth
resources. By mapping these algorithms onto the Smart Memories architecture,
we can evaluate the effectiveness of various reconfigurable components 
in our design.</p>
<p>

<b>Global illumination on parallel architectures</b></p>
<p>

Monte-Carlo <organ wordnetid="105297523" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<sense_organ wordnetid="105299178" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<part wordnetid="109385911" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<eye wordnetid="105311054" confidence="0.8">
<body_part wordnetid="105220461" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../000/26000.xml">
ray tracing</link></algorithm>
</body_part>
</eye>
</activity>
</procedure>
</psychological_feature>
</act>
</part>
</rule>
</sense_organ>
</event>
</organ>
 to generate scenes with global illumination is an
application that demands a lot from a memory system. The 
<weblink xlink:type="simple" xlink:href="http://www.tacc.utexas.edu/~cslugg/lightray/lightray.php">
application</weblink>
has been coded using pthreads and simulated on the Smart Memories simulator.
Although real-time performance on a single Smart Memories chip is achieved,
higher performance over current processors is possible.
Related publications: C. Burns, 
<it><weblink xlink:type="simple" xlink:href="http://www.tacc.utexas.edu/~cslugg/thesis.pdf">
Global Illumination on Parallel Architectures,</weblink></it>
Senior Thesis, University of Texas Department of Computer Sciences, Dec. 2004</p>
<p>

<b>Data structure pre-fetching</b></p>
<p>

Hardware-based or compiler-assisted pre-fetching techniques work well for
array-based programs but are less effective in hiding memory latency for
pointer-intensive programs. By using a data structure centric approach to
pre-fetching (as opposed to control-flow centric approaches), the Smart 
Memories project exploits libraries of data structures to help with 
pre-fetching data stored in the data structures. Taking advantage of 
the recent success of chip multiprocessors, an idle or under-utilized 
processor can pre-fetch data using a pre-fetch thread. </p>
<p>

A library is modified by adding code for the pre-fetch thread as well as a 
few lines to communicate information from the library code to the pre-fetch 
thread. The pre-fetch thread uses the knowledge about data structures in the 
library to identify the memory traversal patterns and issues pre-fetches 
accordingly. This is contrary to issuing pre-fetches for individual load 
instructions independently. This approach can obtain performance improvements 
without the assistance of any profiling-compiler or costly hardware even while 
restricted to the paradigm of sequential programming languages. Furthermore, 
this approach makes pre-fetching transparent to the programmer (using the 
library) as one need not modify the application code at all.</p>

</ss1>
<ss1>
<st>
 Streaming </st>
<p>

<link xlink:type="simple" xlink:href="../727/2786727.xml">
Streaming</link> is the second programming model supported 
in the Smart Memories system. For data parallel applications as in the 
multimedia and DSP domain, the stream programming model gives high performance. 
By separating a program's computation and communication into kernels and 
streams of data, a compiler can make a lot of static optimizations. A high 
level compiler such as 
<weblink xlink:type="simple" xlink:href="http://www.reservoir.com/r-stream.php">
Reservoir Labs R-Stream</weblink> 
maps compute kernels to stream co-processors and manages the transfer of 
data to software managed local memories. It generates SVM 
(Stream <link xlink:type="simple" xlink:href="../353/32353.xml">
Virtual Machine</link>) code, C with SVM API calls, which is then 
compiled by a Tensilica XCC compiler. The SVM runtime implements the SVM 
API calls to allow a stream program to run on Smart Memories.</p>
<p>

Smart Memories is an active participant in the 
<weblink xlink:type="simple" xlink:href="http://www.morphware.org">
Morphware Forum</weblink>, which develops standards such as the 
<weblink xlink:type="simple" xlink:href="http://www.morphware.org/standards.html">
Stream Virtual Machine.</weblink></p>
<p>

Related publications: F.Labonte, P. Mattson, I. Buck, C. Kozyrakis 
and M. Horowitz, 
<it><weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/svm_pact04.pdf">
The Stream Virtual Machine,</weblink></it> 
PACT, September 2004.</p>

</ss1>
<ss1>
<st>
 Transactional Coherence and Consistency (TCC) </st>
<p>

The last major programming model in the Smart Memories system is 
<link xlink:type="simple" xlink:href="../707/1478707.xml">
transactions</link>.  By executing all 
codes as transactions on the memory system, TCC offers a simpler 
way to parallelize applications than by using different threads. 
For more details about TCC please refer to 
<weblink xlink:type="simple" xlink:href="http://tcc.stanford.edu">
Stanford TCC website.</weblink></p>

</ss1>
</sec>
<sec>
<st>
 Smart Memories test chips </st>

<ss1>
<st>
 Memory test chip </st>

<p>

In February 2003, the Smart Memories group <link xlink:type="simple" xlink:href="../527/31527.xml">
taped out</link> 
a reconfigurable memory test chip on the <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../574/463574.xml">
TSMC</link></company>
 0.18um process. 
The test chip consisted of four memory blocks, a low swing 
<link xlink:type="simple" xlink:href="../456/45456.xml">
crossbar</link>, and testing infrastructure circuits. 
The chips were successfully tested in the lab, operating at 1.1GHz 
clock frequency at nominal voltage of 1.8 volts (Figure 1).
Results were published in the 2004 ISSCC conference (K. Mai, R. Ho, 
E. Alon, D. Liu, Y. Kim, D. Patil, and M. Horowitz. 
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/km_isscc_04.pdf">
Architecture and Circuit Techniques for a Reconfigurable Memory Block</weblink>.
<it>ISSCC,</it> February 2004).</p>
<p>

<it>INSERT FIGURE HERE:</it>
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/gif/ken_chip.JPG">
Figure 1 - Smart Memories test chip, memory blocks and low swing crossbar</weblink></p>

</ss1>
<ss1>
<st>
 Interconnect test chip </st>

<p>

In April 2002, Smart Memories taped out a low swing interconnect test chip on 
the <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../574/463574.xml">
TSMC</link></company>
 0.18um process (Figure 2). The test chip consisted of 
multiple low-swing bus topologies as well as some full-swing buses 
for comparison. The test chip also contained a sense amplifier offset 
measurement block (later re-spawn on a <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../597/100597.xml">
National Semiconductor</link></company>

0.25um process). The chips have been tested and a paper is presented at 
the 2003 VLSI Circuits Symposium (R. Ho, K. Mai, M. Horowitz. 
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/rh_vlsi03.pdf">
Efficient On-Chip Global Interconnects.</weblink>
<it>IEEE Symposium on VLSI Circuits,</it> June 2003).</p>
<p>

<it>INSERT FIGURE HERE:</it>
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/gif/chip.jpg">
Figure 2 - Low swing interconnect test chip</weblink></p>

</ss1>
</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
K. Mai, T. Paaske, N. Jayasena, R. Ho, W. Dally, M. Horowitz, 
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/km_isca_00.pdf">
Smart Memories: A Modular Reconfigurable Architecture,</weblink>
<it><link xlink:type="simple" xlink:href="../099/10029099.xml">
International Symposium on Computer Architecture</link></it>, June 2000.</entry>
</reflist>
</p>

</sec>
<sec>
<st>
 Papers and presentations </st>

<p>

<b>Architecture papers</b></p>


<p>

J. Leverich, H. Arakida, A. Solomatnikov, A. Firoozshahian, C. Kozyrakis, 
<it>Comparative Evaluation of Memory Models for Chip Multiprocessors,</it>
<it><link>
ACM Transactions on Architecture and Code Optimization</link></it>, (to appear in 2008)</p>
<p>

A. Solomatnikov, A. Firoozshahian, W. Qadeer, O. Shacham, K. Kelley, Z. Asgar, M. Wachs, R. Hameed, M. Horowitz, 
<it>Chip Multi-Processor Generator,</it>
Wild and Crazy Ideas session at <it><social_group wordnetid="107950920" confidence="0.8">
<meeting wordnetid="108307589" confidence="0.8">
<gathering wordnetid="107975026" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<conference wordnetid="108308497" confidence="0.8">
<link xlink:type="simple" xlink:href="../980/3487980.xml">
Design Automation Conference</link></conference>
</group>
</gathering>
</meeting>
</social_group>
</it>, June 2007</p>
<p>

J. Leverich, H. Arakida, A. Solomatnikov, A. Firoozshahian, M. Horowitz, C. Kozyrakis, 
<it>Comparing Memory Systems for Chip Multiprocessors,</it>
<it><link xlink:type="simple" xlink:href="../099/10029099.xml">
International Symposium on Computer Architecture</link></it>, June 2007 </p>
<p>

F.Labonte, P. Mattson, I. Buck, C. Kozyrakis and M. Horowitz, 
<it><weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/svm_pact04.pdf">
The Stream Virtual Machine,</weblink></it>
<it><link>
International Conference on Parallel Architectures and Compiler Techniques</link></it>, September 2004.</p>
<p>

K. Mai, T. Paaske, N. Jayasena, R. Ho, W. Dally, M. Horowitz, 
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/km_isca_00.pdf">
Smart Memories: A Modular Reconfigurable Architecture,</weblink>
<it><link xlink:type="simple" xlink:href="../099/10029099.xml">
International Symposium on Computer Architecture</link></it>, June 2000.</p>


<p>

<b>VLSI papers</b></p>


<p>

O. Shacham, M. Wachs, A. Solomatnikov, A. Firoozshahian, S. Richardson and M. Horowitz.
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/scoreboard.pdf">
Verification of Chip Multiprocessor Memory Systems Using A Relaxed Scoreboard</weblink>.
<cite >MICRO-41</cite>, November 2008.
<it>(<weblink xlink:type="simple" xlink:href="http://www-vlsi.stanford.edu/smart_memories/RSB/index.html">
Addendum</weblink>)</it></p>
<p>

K. Mai, R. Ho, E. Alon, D. Liu, Y. Kim, D. Patil, and M. Horowitz. 
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/km_isscc_04.pdf">
Architecture and Circuit Techniques for a Reconfigurable Memory Block.</weblink> 
<it><social_group wordnetid="107950920" confidence="0.8">
<meeting wordnetid="108307589" confidence="0.8">
<gathering wordnetid="107975026" confidence="0.8">
<group wordnetid="100031264" confidence="0.8">
<conference wordnetid="108308497" confidence="0.8">
<link xlink:type="simple" xlink:href="../404/1464404.xml">
International Solid-State Circuits Conference</link></conference>
</group>
</gathering>
</meeting>
</social_group>
</it>, February 2004.</p>
<p>

R. Ho, K. Mai, M. Horowitz. 
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/rh_vlsi03.pdf">
Efficient On-Chip Global Interconnects.</weblink> 
<it><link>
IEEE Symposium on VLSI Circuits</link></it>, June 2003.</p>
<p>

Ho, K. Mai, and M. Horowitz,
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/rh_ieeeproc_01.pdf">
The Future of Wires</weblink>.
<cite >Proceedings of the IEEE</cite>, April 2001, pp. 490-504.</p>
<p>

R. Ho, K. Mai, H. Kapadia and M Horowitz,
<weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/papers/rh_iccad_99.pdf">
Interconnect Scaling Implications For CAD</weblink>,
IEEE/ACM International Conference on Computer-Aided Design,1999, San Jose,CA.</p>
<p>

W. J. Dally and S. Lacy,
<weblink xlink:type="simple" xlink:href="http://cva.stanford.edu/publications/1999/arvlsi99.pdf">
VLSI Architecture: Past, Present, and Future</weblink>,
Proceedings of the Advanced Research in VLSI conference, 1999, Atlanta, GA.</p>
<p>

W. J. Dally and A. Chang,
<weblink xlink:type="simple" xlink:href="http://cva.stanford.edu/publications/2000/dac00.pdf">
The Role of Custom Design in ASIC Chips</weblink>,
Proceedings of the 37th Design Automation Conference, June 2000, Los Angeles, CA.</p>


<p>

<b>Presentations</b></p>
<p>

<list>
<entry level="1" type="bullet">

 Nuwan Jayasena, <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/presentations/DetailedArchitecture.pdf">
Detailed Hardware Architecture</weblink></entry>
<entry level="1" type="bullet">

 DARPA site visit, 14 October 2002</entry>
<entry level="1" type="indent">

   Amin Firoozshahian, <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/presentations/Smart%20Memories%20Hardware%20Presentation%20-%20DARPA%20site%20visit%2010-17-02%20-%20No%20notes.pdf">
Smart Memories Hardware</weblink></entry>
<entry level="1" type="indent">

   Kenneth Mai, Ron Ho, <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/presentations/darpa_review_10_14_02.pdf">
Smart Memories Test Chips</weblink></entry>
</list>
</p>
<p>

<b>Posters</b></p>
<p>

<table>
<row>
<col>
Lance Hammond, January 2002 (Orlando):</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/Jan%2002%20HW%20Layout.pdf">
Hardware</weblink></col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/Jan%2002%20SW%20Layout.pdf">
Software</weblink></col>
</row>
<row>
<col>
Lance Hammond, July 2002 (Colorado Springs):</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20HW%20Poster%2002-07.pdf">
Hardware</weblink></col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20SW%20Poster%2002-07.pdf">
Software</weblink></col>
</row>
<row>
<col>
Lance Hammond, February 2003 (San Diego):</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20HW%20Poster%2003-02.pdf">
Hardware</weblink></col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20SW%20Poster%2003-02.pdf">
Software</weblink></col>
</row>
<row>
<col>
Lance Hammond, September 2003 (Santa Fe):</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20HW%20Poster%2003-09.pdf">
Hardware</weblink></col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20SW%20Poster%2003-09.pdf">
Software</weblink></col>
</row>
<row>
<col>
Lance Hammond, March 2004 (Baltimore):</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20HW%20Poster%2004-03.pdf">
Hardware</weblink></col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20SW%20Poster%2004-03.pdf">
Software</weblink></col>
</row>
<row>
<col>
Lance Hammond, July 2004 (Monterey):</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20HW%20Poster%2004-08.pdf">
Hardware</weblink></col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20SW%20Poster%2004-08.pdf">
Software</weblink></col>
</row>
<row>
<col>
Lance Hammond, March 2005 (Scottsdale):</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/stuff/posters/SM%20Poster%2005-03.pdf">
PDF</weblink></col>
</row>
<row>
<col>
L. Hammond and F. Labonte, March 2006:</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/SMPoster06-03.pdf">
PDF</weblink></col>
</row>
<row>
<col>
L. Hammond and S. Richardson, March 2007:</col>
<col>
 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/SMPoster07-03.pdf">
PDF</weblink></col>
</row>
</table>
</p>



</sec>
<sec>
<st>
 People involved </st>

<p>

<b>Faculty/staff:</b>
<weblink xlink:type="simple" xlink:href="http://www-flash.stanford.edu/~horowitz/">
Mark Horowitz</weblink>;
<weblink xlink:type="simple" xlink:href="http://csl.stanford.edu/~billd/">
Bill Dally</weblink>;
<weblink xlink:type="simple" xlink:href="http://ogun.stanford.edu/~kunle/">
Kunle Olukotun</weblink>;
<weblink xlink:type="simple" xlink:href="http://csl.stanford.edu/~christos/">
Christos Kozyrakis</weblink>;
<weblink xlink:type="simple" xlink:href="http://www.stanford.edu/~engler/">
Dawson Engler</weblink>;
<weblink xlink:type="simple" xlink:href="http://www.stanford.edu/~steveri/">
Stephen Richardson</weblink>.</p>
<p>

<b>Graduate students:</b>
<weblink xlink:type="simple" xlink:href="http://www.stanford.edu/~sols/">
Alex Solomatnikov</weblink>;
<weblink xlink:type="simple" xlink:href="http://www.stanford.edu/~aminf13/">
Amin Firoozshahian</weblink>;
<weblink xlink:type="simple" xlink:href="http://www.stanford.edu/~vicwong/">
Vicky Wong</weblink>;
Varun Sagar Malhotra;
Wajahat Qadeer;
Zain Asgar;
Rehan Hameed;
<weblink xlink:type="simple" xlink:href="http://www.stanford.edu/~shacham/">
Ofer Shacham</weblink>;
Kyle Kelley;
Megan Wachs.</p>
<p>

<b>Alumni:</b>
Mike Chen;
<weblink xlink:type="simple" xlink:href="http://www-lance.stanford.edu/">
Lance Hammond</weblink>;
<weblink xlink:type="simple" xlink:href="http://www-vlsi.stanford.edu/~flabonte/">
Francois Labonte</weblink>;
Ken Mai;
<weblink xlink:type="simple" xlink:href="http://ogun.stanford.edu/~mkprabhu/">
Manohar Prabhu</weblink>;
Ayodele Thomas.</p>



</sec>
<sec>
<st>
 Project resources </st>

<p>

<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/protected/applications.html">
Application Studies </weblink> 
</indent>
: <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/protected/documents.html">
Documents </weblink> 
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/protected/bugzilla.htm">
Internal Bug Database (bugzilla) </weblink> 
</indent>
: <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/sm/html/sm/index.html">
E-mail Archive </weblink> 
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/protected/seminar_slides.html">
Seminar Slides </weblink> 
</indent>
: <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/protected/meetings.html">
Meeting Schedules </weblink> 
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/protected/docs/Smart_Memories_project.html">
Project Schedule </weblink> 
</indent>
: <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/smart_memories/protected/hardware_meetings.htm">
Hardware Meetings </weblink> </p>



</sec>
<sec>
<st>
 Similar projects and links </st>

<ss1>
<st>
 Architecture projects </st>

<p>

<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://iram.cs.berkeley.edu/">
Berkeley IRAM</weblink> - Processor plus DRAM on the same chip.
</indent>
: <weblink xlink:type="simple" xlink:href="http://www-hydra.stanford.edu/fast/fast.shtml">
Stanford FAST</weblink> - FPGA prototype board for e.g. Hydra, below (2003-2006).
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://www-flash.stanford.edu/">
Stanford FLASH Multiprocessor</weblink> - Multiple processor boards on a backplane.  Each board includes a processor, DRAM, L2 cache, and configurable interconnect (1992-1997).
</indent>
: <weblink xlink:type="simple" xlink:href="http://www-hydra.stanford.edu/">
Stanford Hydra</weblink> - Four-processor CMP with support for TLS (1994-2005).
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://cva.stanford.edu/imagine/index.html">
Stanford Imagine</weblink>
</indent>
: <weblink xlink:type="simple" xlink:href="http://www.cs.wisc.edu/~mscalar/">
Wisconsin MultiScalar</weblink> - Origins of speculative multithreading, similar to TLS.  Uses multiple functional units to attack different segments of the out-of-order window in parallel. 1995-2001.</p>
<p>

<b>Reconfigurable/polymorphic architectures</b></p>
<p>

<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://www.ece.cmu.edu/research/piperench/">
CMU PipeRench</weblink> - Pipelineable FPGA, sort of (1997-2000).
</indent>
: <weblink xlink:type="simple" xlink:href="http://www.cag.lcs.mit.edu/raw/">
MIT RAW</weblink> - multiple processor tiles connected by a reconfigurable fabric (1997-2004).
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://www.cs.utexas.edu/users/cart/trips/">
Texas TRIPS</weblink> - Multiple tiles connected via a network.  Each tile has two processors, sixteen ALU's, L1 and L2 caches (2001-2006).
</indent>

</p>
</ss1>
<ss1>
<st>
 Related projects </st>

<p>

<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://graphics.stanford.edu/sss/">
Stanford Streaming Supercomputer (SSS)</weblink>
</indent>
: <weblink xlink:type="simple" xlink:href="http://graphics.stanford.edu/streamlang/">
Streaming Languages</weblink>
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://suif.stanford.edu/">
SUIF Compiler</weblink>
</indent>
: <weblink xlink:type="simple" xlink:href="http://simos.stanford.edu/">
SimOS</weblink>
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://tiny-tera.stanford.edu/tiny-tera/">
Tiny-Tera</weblink>
</indent>

</p>
</ss1>
<ss1>
<st>
 Research groups at Stanford </st>

<p>

<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://mos.stanford.edu/">
VLSI Research Group</weblink>
</indent>
: <weblink xlink:type="simple" xlink:href="http://cva.stanford.edu/">
Concurrent VLSI Architecture Group (CVA)</weblink>
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://www-graphics.stanford.edu/">
Computer Graphics Laboratory</weblink>
</indent>
: <weblink xlink:type="simple" xlink:href="http://getafix.stanford.edu/cad/">
High-Level Design Group</weblink>
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://klamath.stanford.edu/networking/">
Networking Research Groups</weblink>
</indent>
: <weblink xlink:type="simple" xlink:href="http://chronos.stanford.edu/users/nanni/research/">
CAD Synthesis Group</weblink></p>

</ss1>
<ss1>
<st>
 General links </st>

<p>

<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://www.cs.wisc.edu/~arch/www/">
Wisconsin Computer Architecture Home Page</weblink>
</indent>
: <weblink xlink:type="simple" xlink:href="http://www.acm.org/sigarch/">
SIGARCH</weblink> 
<indent level="1">

 <weblink xlink:type="simple" xlink:href="http://www.morphware.org/">
Morphware Forum</weblink>
</indent>





</p>
</ss1>
</sec>
</bdy>
</article>
