<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:50:31[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<algorithm  confidence="0.9511911446218017" wordnetid="105847438">
<header>
<title>Levenberg–Marquardt algorithm</title>
<id>892446</id>
<revision>
<id>243732817</id>
<timestamp>2008-10-07T20:38:02Z</timestamp>
<contributor>
<username>Arriva436</username>
<id>2363746</id>
</contributor>
</revision>
<categories>
<category>Optimization algorithms</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../831/18831.xml">
mathematics</link> and computing, the <b>Levenberg–Marquardt algorithm</b> (or <b>LMA</b>) provides a <link xlink:type="simple" xlink:href="../506/21506.xml">
numerical</link> solution to the problem of minimizing a function, generally nonlinear, over a space of parameters of the function.  These minimization problems arise especially in <link xlink:type="simple" xlink:href="../359/82359.xml">
least squares</link> <link xlink:type="simple" xlink:href="../425/555425.xml">
curve fitting</link> and <link xlink:type="simple" xlink:href="../709/679709.xml">
nonlinear programming</link>. <p>

The LMA interpolates between the <link xlink:type="simple" xlink:href="../753/1164753.xml">
Gauss-Newton algorithm</link> (GNA) and the method of <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link>. The LMA is more <link xlink:type="simple" xlink:href="../849/926849.xml">
robust</link> than the GNA, which means that in many cases it finds a solution even if it starts very far off the final minimum. On the other hand, for well-behaved functions and reasonable starting parameters, the LMA tends to be a bit slower than the GNA. </p>
<p>

The LMA is a very popular curve-fitting algorithm used in many software applications for solving generic curve-fitting problems.</p>

<sec>
<st>
 The problem </st>

<p>

Its main application is in the least squares curve fitting problem: given a set of empirical data pairs of independent and dependent variables, (<it>xi</it>, <it>yi</it>), optimize the parameters <b><it>β</it></b> of the model curve <it>f</it>(<it>x</it>,<b><it>β</it></b>) so that the sum of the squares of the deviations</p>
<p>

<indent level="1">

<math>S(\boldsymbol \beta) = \sum_{i=1}^m [y_i - f(x_i, \ \boldsymbol \beta) ]^2</math>
</indent>

becomes minimal.</p>

</sec>
<sec>
<st>
 The solution </st>

<p>

Like other numeric minimization algorithms, the Levenberg-Marquardt algorithm is an <link xlink:type="simple" xlink:href="../833/68833.xml">
iterative</link> procedure. To start a minimization, the user has to provide an initial guess for the parameter vector <b><it>β</it></b>. In many cases, an uninformed standard guess like <b><it>β</it></b>T=(1,1,...,1) will work fine; in other cases, the algorithm converges only if the initial guess is already somewhat close to the final solution.</p>
<p>

In each iteration step, the parameter vector <b><it>β</it></b> is replaced by a new estimate <b><it>β</it></b> + <b><it>δ</it></b>. To determine <b><it>δ</it></b>, the functions <it>f</it>(<b><it>β</it></b> + <b><it>δ</it></b>) are approximated by their linearizations</p>
<p>

<indent level="1">

<math> f(x_i,\boldsymbol \beta+\boldsymbol \delta) \approx f(x _i,\boldsymbol \beta) + J_i \boldsymbol \delta \!</math>
</indent>

where <math>J_i=\frac{\partial f(x_i,\beta)}{\partial \beta}</math> is the <link xlink:type="simple" xlink:href="../461/12461.xml">
gradient</link> (row-vector in this case)
of <it>f</it> with respect to <b><it>β</it></b>.</p>
<p>

At a minimum of the sum of squares <math>S</math>, the <link xlink:type="simple" xlink:href="../461/12461.xml">
gradient</link> of <math>S</math> with respect to <b><it>δ</it></b> is 0. <link xlink:type="simple" xlink:href="../921/7921.xml">
Differentiating</link> the squares in the definition of <math>S</math>, using the above first-order approximation of <math> f(x_i,\boldsymbol \beta+\boldsymbol \delta)</math>, and setting the result to zero leads to:</p>
<p>

<indent level="1">

<math>\mathbf{(J^{T}J)\boldsymbol \delta  = J^{T} [y - f(\boldsymbol \beta)]} \!</math>
</indent>

where <math>\mathbf{J}</math> is the <link xlink:type="simple" xlink:href="../351/195351.xml">
Jacobian</link> matrix whose i-th row equals <math>J_i</math>, and where <math>\mathbf{f}</math> and <math>\mathbf{y}</math> are vectors with i-th component 
<math>f(x_i,\boldsymbol \beta)</math> and <math>y_i</math>, respectively. 
This is a set of linear equations which can be solved for <b><it>δ</it></b>.</p>
<p>

Levenberg's contribution  is to replace this equation by a 'damped version'</p>
<p>

<indent level="1">

<math>\mathbf{(J^{T}J + \lambda I)\boldsymbol \delta  = J^{T} [y - f(\boldsymbol \beta)]}\!</math>
</indent>

where <b>I</b> is the identity matrix, giving as the increment <b><it>δ</it></b> to the estimated parameter vector <b><it>β</it></b>.</p>
<p>

The (non-negative) damping factor &amp;lambda; is adjusted at each iteration. If reduction of <it>S</it> is rapid a smaller value can be used bringing the algorithm closer to the <link>
Gauss–Newton algorithm</link>, whereas if an iteration gives insufficient reduction in the residual &amp;lambda; can be increased giving a step closer to the gradient descent direction. Note that the <link xlink:type="simple" xlink:href="../461/12461.xml">
gradient</link> of <it>S</it> with respect to <b><it>β</it></b>
equals <math>(-2)(J^{T} [y - f(\boldsymbol \beta) ] )^{T}</math>, therefore, for large values of <b><it>λ</it></b>, the step will
be taken approximately in the direction of the gradient. If either the calculated step length <b><it>δ</it></b>, or the reduction of sum of squares reduction from the latest parameter vector <b><it>β</it></b> + <b><it>δ</it></b>, fall below predefined limits, the iteration is aborted and the last parameter vector <b><it>β</it></b> is considered to be the solution.</p>
<p>

Levenberg's algorithm has the disadvantage that if the value of damping factor &amp;lambda; is large, inverting <b>J</b>T<b>J+λ I</b> is not used at all. Marquardt provided the insight that we can scale each component of the gradient according to the curvature so that there is larger movement along the directions where the gradient is smaller. This avoids slow convergence in the direction of small gradient. Therefore, Marquardt replaced the identity matrix <b>I</b> with the diagonal of the <link>
Hessian</link> matrix <b>J</b>T<b>J</b>, resulting in the Levenberg-Marquardt algorithm</p>
<p>

<indent level="1">

<math>\mathbf{(J^{T}J + \lambda\, diag(J^{T}J))\boldsymbol \delta  = J^{T} [y - f(\boldsymbol \beta)]}\!</math>.
</indent>

A similar damping factor appears in  <link xlink:type="simple" xlink:href="../323/954323.xml">
Tikhonov regularization</link>, which is used to solve linear <link xlink:type="simple" xlink:href="../673/176673.xml">
ill-posed problems</link>, as well as in <link xlink:type="simple" xlink:href="../328/954328.xml">
ridge regression</link>, an <link xlink:type="simple" xlink:href="../926/1565926.xml">
estimation</link> technique in <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>.</p>

<ss1>
<st>
 Choice of damping parameter </st>
<p>

Various more-or-less heuristic arguments have been put forward for the best choice for the damping parameter &amp;lambda;.  Theoretical arguments exist showing why some of these choices guaranteed local convergence of the algorithm; however these choices can make the global convergence of the algorithm suffer from the undesirable properties of <link xlink:type="simple" xlink:href="../489/201489.xml">
 steepest-descent</link>, in particular very slow convergence close to the optimum.</p>
<p>

The absolute values of any choice depends on how well-scaled the initial problem is.  Marquardt recommended starting with a value &amp;lambda;0 and a factor &amp;nu;&amp;gt;1.  Initially setting &amp;lambda;=&amp;lambda;0 and computing the residual sum of squares <it>S</it>(<b><it>β</it></b>) after one step from the starting point with the damping factor of 
&amp;lambda;=&amp;lambda;0 and secondly with &amp;lambda;/&amp;nu;.  If both of these are worse than the initial point then the damping is increased by successive multiplication by &amp;nu; until a better point is found with a new damping factor of &amp;lambda;&amp;nu;k 
for some <it>k</it>.</p>
<p>

If use of the damping factor &amp;lambda;/&amp;nu; results in a reduction in squared residual then this is taken as the new value of &amp;lambda; (and the new optimum location is taken as that obtained with this damping factor) and the process continues; if using &amp;lambda;/&amp;nu; resulted in a worse residual, but using &amp;lambda; resulted in a better residual then &amp;lambda; is left unchanged and the new optimum is taken as the value obtained with &amp;lambda; as damping factor.</p>

</ss1>
</sec>
<sec>
<st>
Example</st>
<p>

In this example we try to fit the function <math>y=a \cos(bX) + b \sin(aX)</math> using the Levenberg–Marquardt algorithm implemented in 
<software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../707/48707.xml">
GNU Octave</link></software>
 as the <it>leasqr</it> function. The 3 graphs Fig 1,2,3 show progressively better fitting for the parameters <it>a</it>=100, <it>b</it>=102 used
in the initial curve. Only when the parameters in Fig 3 are chosen closest to the original,are the curves fitting exactly. This equation
is an example of very sensitive initial conditions for the Levenberg-Marquardt algorithm. One reason for this sensitivity is the existence of multiple minima - the function <math>\cos(\beta x)</math> has minima at parameter value <math>\hat  \beta</math> and <math>\hat \beta +2n \pi.</math> </p>
<p>

<image width="150px" src="Lev-Mar-poor-fit.png">
<caption>

Poor Fit
</caption>
</image>

<image width="150px" src="Lev-Mar-better-fit.png">
<caption>

Better Fit
</caption>
</image>

<image width="150px" src="Lev-Mar-best-fit.png">
<caption>

Best Fit
</caption>
</image>
</p>

</sec>
<sec>
<st>
References</st>
<p>

The algorithm was first published  by Kenneth Levenberg, while working at the <site wordnetid="108651247" confidence="0.8">
<region wordnetid="108630985" confidence="0.8">
<position wordnetid="108621598" confidence="0.8">
<point wordnetid="108620061" confidence="0.8">
<landmark wordnetid="108624891" confidence="0.8">
<location wordnetid="100027167" confidence="0.8">
<tract wordnetid="108673395" confidence="0.8">
<geographical_area wordnetid="108574314" confidence="0.8">
<link xlink:type="simple" xlink:href="../611/4080611.xml">
Frankford Army Arsenal</link></geographical_area>
</tract>
</location>
</landmark>
</point>
</position>
</region>
</site>
. It was rediscovered by <physical_entity wordnetid="100001930" confidence="0.8">
<president wordnetid="110468559" confidence="0.8">
<executive wordnetid="110069645" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<administrator wordnetid="109770949" confidence="0.8">
<corporate_executive wordnetid="109966255" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<head wordnetid="110162991" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../915/3522915.xml">
Donald Marquardt</link></mathematician>
</head>
</scientist>
</causal_agent>
</corporate_executive>
</administrator>
</leader>
</statistician>
</person>
</executive>
</president>
</physical_entity>
 who worked as a <link xlink:type="simple" xlink:href="../661/48661.xml">
statistician</link> at <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../829/201829.xml">
DuPont</link></company>
 and independently by Girard, Wynn and Morrison.</p>

<p>

<list>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Kenneth Levenberg&#32;(<link xlink:type="simple" xlink:href="../622/34622.xml">
1944</link>).&#32;"A Method for the Solution of Certain Non-Linear Problems in Least Squares". <it>The Quarterly of Applied Mathematics</it>&#32;<b>2</b>: 164&ndash;168.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">A. Girard&#32;(1958).&#32;"". <it>Rev. Opt</it>&#32;<b>37</b>: 225, 397.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">C.G. Wynne&#32;(1959).&#32;"Lens Designing by Electronic Digital Computer: I". <it>Proc. Phys. Soc. London</it>&#32;<b>73</b>: 777. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1088%2F0370-1328%2F73%2F5%2F310">
10.1088/0370-1328/73/5/310</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">D.D. Morrison&#32;(1960).&#32;"". <it>Jet Propulsion Laboratory Seminar proceedings</it>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal"><physical_entity wordnetid="100001930" confidence="0.8">
<president wordnetid="110468559" confidence="0.8">
<executive wordnetid="110069645" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<administrator wordnetid="109770949" confidence="0.8">
<corporate_executive wordnetid="109966255" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<head wordnetid="110162991" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<link xlink:type="simple" xlink:href="../915/3522915.xml">
Donald Marquardt</link></mathematician>
</head>
</scientist>
</causal_agent>
</corporate_executive>
</administrator>
</leader>
</statistician>
</person>
</executive>
</president>
</physical_entity>
&#32;(1963).&#32;"An Algorithm for Least-Squares Estimation of Nonlinear Parameters". <it>SIAM Journal on Applied Mathematics</it>&#32;<b>11</b>: 431&ndash;441. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1137%2F0111030">
10.1137/0111030</weblink>.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal">Philip E. Gill and <link xlink:type="simple" xlink:href="../376/3438376.xml">
Walter Murray</link>&#32;(<link xlink:type="simple" xlink:href="../753/34753.xml">
1978</link>).&#32;"Algorithms for the solution of the nonlinear least-squares problem". <it>SIAM Journal on Numerical Analysis</it>&#32;<b>15</b>&#32;(5): 977&ndash;992. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1137%2F0715063">
10.1137/0715063</weblink>.</cite>&nbsp;</entry>
</list>
</p>

</sec>
<sec>
<st>
 Available implementations </st>
<p>

<list>
<entry level="1" type="bullet">

The oldest implementation still in use is <weblink xlink:type="simple" xlink:href="http://www.netlib.org/minpack/">
lmdif</weblink>, from <link xlink:type="simple" xlink:href="../660/6978660.xml">
MINPACK</link>, in <link xlink:type="simple" xlink:href="../168/11168.xml">
FORTRAN</link>, in the <link xlink:type="simple" xlink:href="../551/18935551.xml">
public domain</link>.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.messen-und-deuten.de/lmfit/index.html">
lmfit</weblink>, a translation of lmdif into <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
ANSI C</link></programming_language>
 with an easy-to-use wrapper for curve fitting, public domain.</entry>
<entry level="1" type="bullet">

Several high-level languages and mathematical packages have wrappers for the <link xlink:type="simple" xlink:href="../660/6978660.xml">
MINPACK</link> routines, among them:</entry>
<entry level="2" type="bullet">

Python library <link xlink:type="simple" xlink:href="../472/263472.xml">
scipy</link>, module scipy.optimize.leastsq,</entry>
<entry level="2" type="bullet">

<programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../587/512587.xml">
IDL</link></programming_language>
, add-on <weblink xlink:type="simple" xlink:href="http://cow.physics.wisc.edu/~craigm/idl/fitting.html">
MPFIT</weblink>.</entry>
<entry level="2" type="bullet">

<software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../024/49024.xml">
Mathematica</link></software>
 offers LevenbergMarquardt as a method option for many of its optimization routines.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref></entry>
<entry level="2" type="bullet">

<programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../707/376707.xml">
R (programming language)</link></programming_language>
 has the <weblink xlink:type="simple" xlink:href="http://cran.r-project.org/web/packages/minpack.lm/index.html">
minpack.lm</weblink> package.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

Detailed description of the algorithm can be found in <weblink xlink:type="simple" xlink:href="http://www.fizyka.umk.pl/nrbook/c15-5.pdf">
Numerical Recipes in C, Chapter 15.5: Nonlinear models</weblink></entry>
<entry level="1" type="bullet">

C. T. Kelley, <it>Iterative Methods for Optimization</it>, SIAM Frontiers in Applied Mathematics, no 18, 1999, ISBN: 0-89871-433-8. <weblink xlink:type="simple" xlink:href="http://www.siam.org/books/textbooks/fr18_book.pdf">
 Online copy</weblink> </entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://202.38.126.65/mirror/www.siam.org/siamnews/mtc/mtc1093.htm">
History of the algorithm in SIAM news</weblink></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/638988.html">
A tutorial by Ananth Ranganathan</weblink></entry>
<entry level="1" type="bullet">

The <structure wordnetid="104341686" confidence="0.8">
<artifact wordnetid="100021939" confidence="0.8">
<area wordnetid="102735688" confidence="0.8">
<library wordnetid="103660909" confidence="0.8">
<room wordnetid="104105893" confidence="0.8">
<link xlink:type="simple" xlink:href="../067/1113067.xml">
GNU Scientific Library</link></room>
</library>
</area>
</artifact>
</structure>
 library has a C implementation.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.ics.forth.gr/%7elourakis/levmar/">
levmar</weblink> is a reimplementation in <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../021/6021.xml">
C</link></programming_language>
/<programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../038/72038.xml">
C++</link></programming_language>
, <link xlink:type="simple" xlink:href="../683/18938683.xml">
GNU General Public License</link>.</entry>
<entry level="2" type="bullet">

<link xlink:type="simple" xlink:href="../412/20412.xml">
Matlab</link>, <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../939/23939.xml">
Perl</link></programming_language>
 (<link xlink:type="simple" xlink:href="../764/908764.xml">
PDL</link>) and <programming_language wordnetid="106898352" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../862/23862.xml">
python</link></programming_language>
 interfaces to levmar: see <weblink xlink:type="simple" xlink:href="http://www.johnlapeyre.com/pdl/levmar/src/">
PDL::Fit::Levmar</weblink> and <weblink xlink:type="simple" xlink:href="http://projects.liquidx.net/python/browser/pylevmar">
pylevmar</weblink>.</entry>
<entry level="1" type="bullet">

<message wordnetid="106598915" confidence="0.8">
<request wordnetid="106513366" confidence="0.8">
<link xlink:type="simple" xlink:href="../881/15881.xml">
Java programming language</link></request>
</message>
 implementations: <weblink xlink:type="simple" xlink:href="http://scribblethink.org/Computer/Javanumeric/index.html">
Javanumerics</weblink>, <weblink xlink:type="simple" xlink:href="http://virtualrisk.cvs.sourceforge.net/*checkout*/virtualrisk/util/lma/lma_v1.3.zip">
lma</weblink>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.alglib.net/optimization/levenbergmarquardt.php">
ALGLIB</weblink> has implementations in C# / C++ / Delphi / Visual Basic / etc.</entry>
<entry level="1" type="bullet">

 <software wordnetid="106566077" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../412/4006412.xml">
Fityk</link></software>
 has a C++ implementation.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.sf.net/projects/pythonequations">
Python Equations</weblink> Implementation in Python. BSD license.</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../601/43601.xml">
gnuplot</link> uses its own implementation <weblink xlink:type="simple" xlink:href="http://www.gnuplot.info/">
gnuplot.info</weblink>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://oooconv.free.fr/fitoo/fitoo_en.html">
FitOO</weblink> is an old proof of concept using the algorithm in an OpenOffice.org macro</entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</article>
