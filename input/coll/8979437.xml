<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 23:43:58[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Stochastic approximation</title>
<id>8979437</id>
<revision>
<id>210159487</id>
<timestamp>2008-05-04T19:22:30Z</timestamp>
<contributor>
<username>Spacepotato</username>
<id>271631</id>
</contributor>
</revision>
<categories>
<category>Stochastic processes</category>
<category>Optimization algorithms</category>
</categories>
</header>
<bdy>

<b>Stochastic approximation</b> methods are a family of iterative <link xlink:type="simple" xlink:href="../543/7325543.xml">
stochastic optimization</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link>s that attempt to find zeroes or extrema of functions which cannot be computed directly, but only estimated via noisy observations.  The first, and prototypical, algorithms of this kind were the <b>Robbins-Monro</b> and <b>Kiefer-Wolfowitz</b> algorithms.  
__NOTOC__
<sec>
<st>
 Robbins-Monro algorithm </st>
<p>

In the <physical_entity wordnetid="100001930" confidence="0.8">
<president wordnetid="110468559" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<executive wordnetid="110069645" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<administrator wordnetid="109770949" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<corporate_executive wordnetid="109966255" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<head wordnetid="110162991" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../193/745193.xml">
Robbins</link></scholar>
</mathematician>
</head>
</causal_agent>
</alumnus>
</corporate_executive>
</leader>
</administrator>
</statistician>
</executive>
</scientist>
</intellectual>
</person>
</president>
</physical_entity>
-Monro algorithm, introduced in 1951<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>, one 
has a function <math>M(x)</math> for which one wishes to find the value of <math>x</math>, <math>x_0</math>, satisfying <math>M(x_0)=\alpha</math>.  However, what is observable is not <math>M(x)</math>, but rather a random variable <math>N(x)</math> such that <math>E(N(x)|x)=M(x)</math>.  The algorithm is then to construct a sequence <math>x_1, x_2, \dots</math>   which satisfies
<indent level="2">

<math>x_{n+1}=x_n+a_n(\alpha-N(x_n))</math>.
</indent>
Here, <math>a_1, a_2, \dots</math>   is a sequence of positive step sizes.  Robbins and Monro proved that, if <math>N(x)</math> is uniformly bounded, <math>M(x)</math> is nondecreasing, <math>M'(x_0)</math> exists and is positive, and if <math>a_n</math> satisfies a set of bounds (fulfilled if one takes <math>a_n=1/n</math>), then <math>x_n</math> converges in <math>L^2</math> (and hence also in probability) to <math>x_0</math>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>, Theorem 2.  In general, the <math>a_n's</math> need not equal <math>1/n</math>.  However, to ensure convergence, they should converge to zero, and in order to average out the noise in <math>N(x)</math>, they should converge slowly.
</p>
</sec>
<sec>
<st>
 Kiefer-Wolfowitz algorithm </st>
<p>

In the Kiefer-Wolfowitz algorithm<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>, introduced a year after the Robbins-Monro algorithm, one wishes to find the maximum, <math>x_0</math>, of the unknown <math>M(x)</math>
and constructs a sequence <math>x_1,x_2,\dots</math> such that
<indent level="2">

<math>x_{n+1}=x_n+a_n\frac{N(x_n+c_n)-N(x_n-c_n)}{c_n}</math>.
</indent>
Here, <math>a_1, a_2, \dots</math> is a sequence of positive step sizes which serve the same function as in the Robbins-Monro algorithm, and <math>c_1, c_2, \dots</math> is a sequence of positive step sizes which are used to estimate, via finite differences, the derivative of <math>M</math>.  <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<president wordnetid="110468559" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<executive wordnetid="110069645" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<administrator wordnetid="109770949" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<corporate_executive wordnetid="109966255" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<head wordnetid="110162991" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../344/11873344.xml">
Kiefer</link></scholar>
</mathematician>
</head>
</causal_agent>
</alumnus>
</corporate_executive>
</leader>
</administrator>
</statistician>
</executive>
</associate>
</scientist>
</colleague>
</intellectual>
</person>
</president>
</peer>
</physical_entity>
 and Wolfowitz showed that, if <math>a_n</math> and <math>c_n</math> satisfy various bounds (fulfilled by taking <math>a_n=1/n</math>, <math>c_n=(1/n)^{1/3}</math>), and <math>M(x)</math> and <math>N(x)</math> satisfy some technical conditions, then the sequence <math>x_n</math> converges in probability to <math>x_0</math>.</p>

</sec>
<sec>
<st>
Subsequent developments</st>
<p>

An extensive theoretical literature has grown up around these algorithms, concerning conditions for convergence, rates of convergence,  multivariate and other generalizations, proper choice of step size, possible noise models, and so on.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>   These methods are also applied in <link xlink:type="simple" xlink:href="../039/7039.xml">
control theory</link>, in which case the unknown function which we wish to optimize or find the zero of may vary in time.  In this case, the step size <math>a_n</math> should not converge to zero but should be chosen so as to track the function.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>, 2nd ed., chapter 3</p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../641/1180641.xml">
Stochastic gradient descent</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../543/7325543.xml">
Stochastic optimization</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
A Stochastic Approximation Method, Herbert Robbins and Sutton Monro, <it>Annals of Mathematical Statistics</it> <b>22</b>, #3 (September 1951), pp. 400&ndash;407.</entry>
<entry id="2">
Stochastic Estimation of the Maximum of a Regression Function, <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<president wordnetid="110468559" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<executive wordnetid="110069645" confidence="0.8">
<statistician wordnetid="110653238" confidence="0.8">
<administrator wordnetid="109770949" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<corporate_executive wordnetid="109966255" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<head wordnetid="110162991" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../344/11873344.xml">
J. Kiefer</link></scholar>
</mathematician>
</head>
</causal_agent>
</alumnus>
</corporate_executive>
</leader>
</administrator>
</statistician>
</executive>
</associate>
</scientist>
</colleague>
</intellectual>
</person>
</president>
</peer>
</physical_entity>
 and J. Wolfowitz, <it>Annals of Mathematical Statistics</it> <b>23</b>, #3 (September 1952), pp. 462&ndash;466.</entry>
<entry id="3">
<it>Stochastic Approximation Algorithms and Applications</it>, Harold J. Kushner and G. George Yin, New York: Springer-Verlag, 1997.  ISBN 038794916X; 2nd ed., titled <it>Stochastic Approximation and Recursive Algorithms and Applications</it>, 2003, ISBN 0387008942.</entry>
<entry id="4">
<it>Stochastic Approximation and Recursive Estimation</it>, Mikhail Borisovich Nevel'son and Rafail Zalmanovich Has'minski&#301;, translated by Israel Program for Scientific Translations and B. Silver, Providence, RI: American Mathematical Society, 1973, 1976. ISBN 0821815970.</entry>
</reflist>
</p>

</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
