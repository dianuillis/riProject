<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:54:50[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Boosting</title>
<id>90500</id>
<revision>
<id>228661739</id>
<timestamp>2008-07-29T19:28:17Z</timestamp>
<contributor>
<username>Montrealais</username>
<id>3378</id>
</contributor>
</revision>
<categories>
<category>Ensemble learning</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

For other uses, see <link xlink:type="simple" xlink:href="../920/8033920.xml">
Boosting (disambiguation)</link>.<p>

<b>Boosting</b> is a <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> <link xlink:type="simple" xlink:href="../458/774458.xml">
meta-algorithm</link> for performing <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link>.  Boosting is based on the question posed by Kearns<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>: can a set of <b>weak learners</b> create a single <b>strong learner</b> ?  A weak learner is defined to be a classifier which is only slightly correlated with the true classification.  In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.  </p>
<p>

The affirmative answer to Kearns' question has significant ramifications in <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> and <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>.</p>

<sec>
<st>
 Boosting algorithms </st>

<p>

While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are typically weighted in some way that is usually related to the weak learner's accuracy.  After a weak learner is added, the data is reweighted: examples that are misclassified gain weight and examples that are classified correctly lose weight (some boosting algorithms actually decrease the weight of repeatedly misclassified examples, e.g., <link>
boost by majority</link> and <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../016/11448016.xml">
BrownBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
).  Thus, future weak learners focus more on the examples that previous weak learners misclassified.</p>
<p>

There are many boosting algorithms.  The original ones, proposed by <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../974/14590974.xml">
Robert Schapire</link></scientist>
 (a recursive majority gate formulation <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>) and <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../826/8000826.xml">
Yoav Freund</link></scientist>
 (boost by majority <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>) were not adaptive and could not take full advantage of the weak learners.</p>
<p>

Only algorithms that are provable boosting algorithms in the <link xlink:type="simple" xlink:href="../008/380008.xml">
probably approximately correct learning</link> formulation are boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called "leveraging algorithms", although they are also sometimes incorrectly called boosting algorithms.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref></p>

</sec>
<sec>
<st>
 Examples of boosting algorithms </st>

<p>

The main variation between many boosting algorithms is their method of weighting training data points and hypotheses.  <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 is very popular and perhaps the most significant historically as it was the first algorithm that could adapt to the weak learners.  However, there are many more recent algorithms such as <link xlink:type="simple" xlink:href="../434/9616434.xml">
LPBoost</link>, <link>
TotalBoost</link>, <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../016/11448016.xml">
BrownBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
,<link>
MadaBoost</link>, <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../465/17627465.xml">
LogitBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
, and others.  Many boosting algorithms fit into the <link>
AnyBoost</link> framework,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref> which shows that boosting performs <link xlink:type="simple" xlink:href="../489/201489.xml">
gradient descent</link> in <link xlink:type="simple" xlink:href="../630/340630.xml">
function space</link> using a <link xlink:type="simple" xlink:href="../906/3196906.xml">
convex</link> cost function.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<table>
<row>
<col>
<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../603/1645603.xml">
AdaBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <plant wordnetid="100017222" confidence="0.8">
<tree wordnetid="113104059" confidence="0.8">
<vascular_plant wordnetid="113083586" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<woody_plant wordnetid="113103136" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../998/11288998.xml">
Alternating decision tree</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</woody_plant>
</rule>
</event>
</vascular_plant>
</tree>
</plant>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../911/1307911.xml">
Bootstrap aggregating</link></entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../016/11448016.xml">
BrownBoost</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link>
CoBoosting</link></entry>
</list>
</col>
<col>
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../434/9616434.xml">
LPBoost</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../631/226631.xml">
logistic regression</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../718/201718.xml">
maximum entropy methods</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../542/1729542.xml">
neural network</link>s</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link>s</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../674/17562674.xml">
margin classifier</link>s</entry>
</list>
</col>
<col>
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../612/416612.xml">
cross validation</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link></entry>
</list>
</col>
</row>
</table>
</p>

</sec>
<sec>
<st>
 References </st>

<ss1>
<st>
Footnotes</st>

<p>

<reflist>
<entry id="1">
Michael Kearns.  Thoughts on hypothesis boosting. Unpublished manuscript. 1988</entry>
<entry id="2">
Rob Schapire. Strength of Weak Learnability. Journal of Machine Learning Vol. 5, pages 197-227. 1990</entry>
<entry id="3">
Yoav Freund. Boosting a weak learning algorithm by majority.  Proceedings of the Third Annual Workshop on Computational Learning Theory. 1990</entry>
<entry id="4">
Nir Krause and Yoram Singer. Leveraging the margin more carefully. In Proceedings of the International Conference on Machine Learning (ICML), 2004.</entry>
<entry id="5">
Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms as gradient descent. In S.A. Solla, T.K. Leen, and K.-R. Muller, editors, Advances in Neural Information Processing Systems 12, pages 512--518.  MIT Press, 2000</entry>
</reflist>
</p>

</ss1>
<ss1>
<st>
Notations</st>
<p>

<list>
<entry level="1" type="bullet">

 Yoav Freund and Robert E. Schapire A decision-theoretic generalization of on-line learning and an application to boosting.  Journal of Computer and System Sciences, 55(1):119--139, 1997. http://www.cse.ucsd.edu/~yfreund/papers/adaboost.pdf</entry>
<entry level="1" type="bullet">

 Robert E. Schapire and Yoram Singer. Improved Boosting Algorithms Using Confidence-Rated Predictors. Machine Learning, 37(3):297--336, 1999. http://citeseer.ist.psu.edu/schapire99improved.html</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/489339.html">
 The boosting approach to machine learning: An overview</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/schapire90strength.html">
The strength of weak learnability</weblink></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.princeton.edu/~schapire/boost.html">
An up-to-date collection of papers on boosting</weblink></entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
