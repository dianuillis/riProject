<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 23:50:10[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<social_event  confidence="0.8" wordnetid="107288639">
<contest  confidence="0.8" wordnetid="107456188">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<header>
<title>Netflix Prize</title>
<id>9399111</id>
<revision>
<id>243148709</id>
<timestamp>2008-10-05T10:47:33Z</timestamp>
<contributor>
<username>Maias</username>
<id>1827467</id>
</contributor>
</revision>
<categories>
<category>Computer science competitions</category>
</categories>
</header>
<bdy>

The <b>Netflix Prize</b> is an ongoing open competition for the best <link xlink:type="simple" xlink:href="../289/480289.xml">
collaborative filtering</link> <link xlink:type="simple" xlink:href="../775/775.xml">
algorithm</link> that predicts user ratings for <link xlink:type="simple" xlink:href="../571/10571.xml">
film</link>s, based on previous ratings.  The competition is held by <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../537/175537.xml">
Netflix</link></company>
, an online  <medium wordnetid="106254669" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../498/11014498.xml">
DVD</link></medium>
-rental service, and is opened for anyone (with some exceptions)<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>.  The grand prize of $1,000,000 is reserved for the entry which bests Netflix's own algorithm for predicting ratings by 10%.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>
<sec>
<st>
 Problem and data sets </st>

<p>

Netflix provided a <it>training</it> data set of over 100 million ratings that over 480,000 users gave to nearly 18,000 movies.  Each training rating is a quadruplet .  The user and movie fields are integer IDs, while grades are from 1 to 5 (<link xlink:type="simple" xlink:href="../563/14563.xml">
integral</link>) stars.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

The <it>qualifying</it> data set contains over 2.8 million triplets , with grades known only to the jury.  A participating team's algorithm must predict grades on the entire qualifying set, but they are only informed of the score for half of the data, the <it>quiz</it> set.  The other half is the <it>test</it> set, and performance on this is used by the jury to determine potential prize winners.  Only the judges know which ratings are in the quiz set, and which are in the test set—this arrangement is intended to make it difficult to <link xlink:type="simple" xlink:href="../002/364002.xml">
hill climb</link> on the test set.  Submitted predictions are scored against the true grades in terms of <link xlink:type="simple" xlink:href="../608/8648608.xml">
root mean squared error</link> (RMSE), and the goal is to reduce this error as much as possible. Note that while the actual grades are integers in the range 1 to 5, submitted predictions need not be.</p>
<p>

For each movie, title and year of release is provided in a separate dataset.  No information at all is provided about users.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

In order to protect the privacy of customers, "some of the rating data for some customers in the training and qualifying sets have been deliberately perturbed in one or more of the following ways: deleting ratings; inserting alternative ratings and dates; and modifying rating dates".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

The training set is such that the average user rated over 200 movies, and the average movie was rated by over 5000 users.  But there is wide <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link> in the data—some movies in the training set have as little as 3 ratings,<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>
while one user rated over 17,000 movies.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref></p>
<p>

There was some controversy as to the chosen metric (<link xlink:type="simple" xlink:href="../608/8648608.xml">
root mean squared error</link>). Would a reduction of the RMSE by 10% really benefit the users? It has been claimed that even as small an improvement as .01 RMSE result in a significant difference in the ranking of the "top-10" most recommended movies for a user.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%226%22])">6</ref>.</p>

</sec>
<sec>
<st>
 Prizes </st>

<p>

Prizes are based improvement over the Cinematch algorithm or the previous year's score if a team has made improvement beyond a certain threshold.  A trivial algorithm that predicts for each movie in the quiz set its average grade from the training data produces an RMSE of 1.0540.   Netflix's own algorithm, called <it>Cinematch</it>, uses "straightforward statistical <link xlink:type="simple" xlink:href="../904/17904.xml">
linear model</link>s with a lot of data conditioning".<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%227%22])">7</ref>
Using only the training data, Cinematch scores an RMSE of 0.9514 on the quiz data, roughly a 10% improvement over the trivial algorithm.  Cinematch has a similar performance on the test set, 0.9525.  In order to win the grand prize of $1,000,000, a participating team must improve this by another 10%, to achieve  0.8572 on the test set.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>  (Such an improvement on the quiz set corresponds to an RMSE of 0.8563.)</p>
<p>

If no team wins the grand prize, a <it>progress</it> prize of $50,000 is awarded every year for the best result thus far.  However, in order to win this prize, an algorithm must improve the RMSE on the quiz set by at least 1% over the previous progress prize winner (or over Cinematch, the first year).  If no submission succeeds, the progress prize is not awarded for that year.</p>
<p>

To win a progress or grand prize a participant must provide source code and a description of the algorithm to the jury within one week after being contacted by them.  Following verification the winner must also provide a non-exclusive license to Netflix.  Netflix will publish only the description, not the source code, of the system.  A team may choose to not claim a prize, in order to keep their algorithm and source code secret.  The jury also keeps their predictions secret from other participants.  A team may send as many attempts to predict grades as they wish.  Originally submissions were limited to once a week, but the interval was quickly modified to once a day.  A team's best submission so far counts as their current submission.</p>
<p>

When one of the teams succeeds to improve the RMSE by 10% or more, the jury issues a <it>last call</it>, giving all teams 30 days to send their submissions.  Only then, the team with best submission is asked for the algorithm description, source code, and non-exclusive license, and, after successful verification, declared a grand prize winner.</p>

</sec>
<sec>
<st>
 End of the contest </st>

<p>

The contest lasts till the grand prize winner is declared. If no one receives the grand prize, it lasts for at least 5 years (until <link xlink:type="simple" xlink:href="../527/22527.xml">
October 2</link>, <fundamental_quantity wordnetid="113575869" confidence="0.8">
<time_period wordnetid="115113229" confidence="0.8">
<year wordnetid="115203791" confidence="0.8">
<link xlink:type="simple" xlink:href="../225/36225.xml">
2011</link></year>
</time_period>
</fundamental_quantity>
). After that date, the contest may be terminated any time to Netflix sole discretion.</p>

</sec>
<sec>
<st>
 Progress and current status </st>

<p>

The competition began on <link xlink:type="simple" xlink:href="../527/22527.xml">
October 2</link>, <link xlink:type="simple" xlink:href="../164/36164.xml">
2006</link>.  By October 8, a team called WXYZConsulting had already beaten Cinematch's results.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%228%22])">8</ref>
By October 15, there were three teams who had beaten Cinematch, one of them by 1.06%, enough to qualify for the annual progress prize.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%229%22])">9</ref>  By June, 2007, over 20,000 teams had registered for the competition from over 150 countries. 2,000 teams had submitted over 13,000 prediction sets.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>
<p>

Over the first year of the competition, a handful of front-runners traded first place; the more prominent ones were:
<list>
<entry level="1" type="bullet">

 WXYZConsulting, a team by Yi Zhang and Wei Xu. (A front runner during Nov-Dec 2006.)</entry>
<entry level="1" type="bullet">

 ML@UToronto A, a team from the <university wordnetid="108286163" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../325/7955325.xml">
University of Toronto</link></university>
 led by Prof. <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<research_worker wordnetid="110523076" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../174/507174.xml">
Geoffrey Hinton</link></associate>
</research_worker>
</scientist>
</causal_agent>
</colleague>
</person>
</peer>
</physical_entity>
. (A front runner during parts of Oct-Dec 2006.)</entry>
<entry level="1" type="bullet">

 Gravity, a team of four scientists from the <link xlink:type="simple" xlink:href="../565/1504565.xml">
Budapest University of Technology</link> (A front runner during Jan-May 2007.)</entry>
<entry level="1" type="bullet">

 BellKor, a group of scientists from <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../825/3874825.xml">
AT&amp;T Labs</link></company>
. (A front runner since May 2007.)</entry>
</list>
</p>
<p>

See also a chart <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2210%22])">10</ref> showing the progress through the first year of the competition.</p>
<p>

On <time_off wordnetid="115118453" confidence="0.8">
<fundamental_quantity wordnetid="113575869" confidence="0.8">
<time_period wordnetid="115113229" confidence="0.8">
<vacation wordnetid="115137890" confidence="0.8">
<leisure wordnetid="115137676" confidence="0.8">
<link xlink:type="simple" xlink:href="../491/1491.xml">
August 12</link></leisure>
</vacation>
</time_period>
</fundamental_quantity>
</time_off>
, <link xlink:type="simple" xlink:href="../165/36165.xml">
2007</link>, many contestants gathered at the KDD Cup and Workshop 2007.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2211%22])">11</ref> which was held at San Jose, CA. During the workshop all four of the top teams on the leaderboard at that time presented their techniques.</p>
<p>

On <link xlink:type="simple" xlink:href="../531/27531.xml">
September 2</link>, <link xlink:type="simple" xlink:href="../165/36165.xml">
2007</link>, the competition entered the "last call" period for the 2007 Progress Prize. Teams had thirty days to tender submissions for consideration.  At the beginning of this period the leading team was BellKor, with an RMSE of 0.8728 (8.26% improvement). followed by Dinosaur Planet (RMSE=0.8769; 7.83% improvement),  and Gravity (RMSE=0.8785; 7.66% improvement).  In the last hour of the last call period, an entry by "KorBell" took first place.  This turned out to be an alternate name for Team BellKor.</p>
<p>

Over the second year of the competition, only three teams reached the leading position:
<list>
<entry level="1" type="bullet">

 BellKor, a group of scientists from <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../825/3874825.xml">
AT&amp;T Labs</link></company>
. (front runner during May 2007 - Sept 2008.)</entry>
<entry level="1" type="bullet">

 BigChaos, a team of Austrian scientists from commendo research &amp; consulting (single team front runner since Oct 2008)</entry>
<entry level="1" type="bullet">

 BellKor in BigChaos, a joint team of the two leading single teams (A front runner since Sept. 2008)</entry>
</list>
</p>

<ss1>
<st>
 2007 Progress Prize </st>

<p>

On <link xlink:type="simple" xlink:href="../761/21761.xml">
November 13</link>, <link xlink:type="simple" xlink:href="../165/36165.xml">
2007</link>, team KorBell (aka BellKor) was declared the winner of the $50,000 Progress Prize<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2212%22])">12</ref>
with an  RMSE of 0.8712 (8.43% improvement).  The team consisted of three researchers from <company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../825/3874825.xml">
AT&amp;T Labs</link></company>
, <link>
Yehuda Koren</link>, <link xlink:type="simple" xlink:href="../591/220591.xml">
Robert Bell</link>, and <link>
Chris Volinsky</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2213%22])">13</ref>
As required, they published a description of their algorithm.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%2214%22])">14</ref></p>
<p>

In order to win the 2008 Progress Prize, a participant should obtain an RMSE of 0.8625 or better on the quiz set.</p>

</ss1>
</sec>
<sec>
<st>
References</st>
<p>

<reflist>
<entry id="1">
People who are connected with Netflix (current and former employees, agents,  close relatives of Netflix employees, etc.), residents of <district wordnetid="108552138" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../867/7954867.xml">
Quebec</link></district>
, and residents of <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../481/5042481.xml">
Cuba</link></country>
, <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../653/14653.xml">
Iran</link></country>
, <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../849/7515849.xml">
Syria</link></country>
, <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../255/21255.xml">
North Korea</link></country>
, <link xlink:type="simple" xlink:href="../457/19457.xml">
Myanmar</link>, or <country wordnetid="108544813" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../421/27421.xml">
Sudan</link></country>
 may not participate in the contest.</entry>
<entry id="2">
<weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/rules">
Netflix Prize: Review Rules</weblink></entry>
<entry id="3">
 <cite style="font-style:normal">James Bennett;&#32;Stan Lanning&#32;(<time_off wordnetid="115118453" confidence="0.8">
<fundamental_quantity wordnetid="113575869" confidence="0.8">
<time_period wordnetid="115113229" confidence="0.8">
<vacation wordnetid="115137890" confidence="0.8">
<leisure wordnetid="115137676" confidence="0.8">
<link xlink:type="simple" xlink:href="../491/1491.xml">
August 12</link></leisure>
</vacation>
</time_period>
</fundamental_quantity>
</time_off>
, <link xlink:type="simple" xlink:href="../165/36165.xml">
2007</link>). "<weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/assets/NetflixPrizeKDD_to_appear.pdf">
The Netflix Prize</weblink>".&#32;<it>Proceedings of KDD Cup and Workshop 2007</it>. Retrieved on <link>
2007-08-25</link>.</cite>&nbsp;</entry>
<entry id="4">
Sigmoid Curve&#32;(2006-10-08).&#32;"<weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/community/viewtopic.php?id=164">
"Miss Congeniality"</weblink>".&#32;<it>Netflix Prize Forum</it>.&#32;Retrieved on <link>
2007-08-25</link>.</entry>
<entry id="5">
prodigious&#32;(2006-10-06).&#32;"<weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/community/viewtopic.php?id=141">
"A single customer that rated 17,000 movies"</weblink>".&#32;<it>Netflix Prize Forum</it>.&#32;Retrieved on <link>
2007-08-25</link>.</entry>
<entry id="6">
YehudaKoren&#32;(2007-12-18).&#32;"<weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/community/viewtopic.php?id=828">
How useful is a lower RMSE?</weblink>".&#32;<it>Netflix Prize Forum</it>.</entry>
<entry id="7">
"<weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/faq">
"Netflix Prize Frequently Asked Questions"</weblink>".&#32;Retrieved on <link>
2007-08-21</link>.</entry>
<entry id="8">
"<weblink xlink:type="simple" xlink:href="http://www.hackingnetflix.com/2006/10/netflix_prize_r.html">
"Netflix Prize Rankings"</weblink>".&#32;<it>Hacking NetFlix</it>&#32;(<link xlink:type="simple" xlink:href="../549/22549.xml">
October 9</link>, <link xlink:type="simple" xlink:href="../164/36164.xml">
2006</link>).&#32;Retrieved on <link>
2007-08-21</link>.</entry>
<entry id="9">
"<weblink xlink:type="simple" xlink:href="http://jsnell.iki.fi/blog/archive/2006-10-15-netflix-prize.html">
"Netflix Prize (I tried to resist, but...)"</weblink>".&#32;<it>Juho Snellman's Weblog</it>&#32;(<link xlink:type="simple" xlink:href="../555/22555.xml">
October 15</link>, <link xlink:type="simple" xlink:href="../164/36164.xml">
2006</link>).&#32;Retrieved on <link>
2007-08-21</link>.</entry>
<entry id="10">
"<weblink xlink:type="simple" xlink:href="http://www.research.att.com/~volinsky/netflix/leaders.gif">
Top contenders for Progress Prize 2007 chart</weblink>".</entry>
<entry id="11">
"<weblink xlink:type="simple" xlink:href="http://www.cs.uic.edu/~liub/Netflix-KDD-Cup-2007.html">
The KDD Cup and Workshop 2007</weblink>".</entry>
<entry id="12">
Prizemaster&#32;(2007-11-13).&#32;"<weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/community/viewtopic.php?id=799">
Netflix Progress Prize 2007 awarded to team KorBell</weblink>".&#32;<it>Netflix Prize Forum</it>.</entry>
<entry id="13">
"<weblink xlink:type="simple" xlink:href="http://www.netflix.com/MediaCenter?id=6422">
$50,000 Progress Prize is Awarded on First Anniversary of $1 Million Netflix Prize</weblink>".</entry>
<entry id="14">
R. Bell, Y. Koren, C. Volinsky&#32;(2007).&#32;"<weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/assets/ProgressPrize2007_KorBell.pdf">
"The BellKor solution to the Netflix Prize"</weblink>".</entry>
</reflist>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/">
Official site</weblink></entry>
<entry level="2" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.netflixprize.com/leaderboard">
Leaderboard</weblink></entry>
<entry level="1" type="bullet">

 Kate Greene&#32;(<link xlink:type="simple" xlink:href="../542/22542.xml">
October 6</link>, <link xlink:type="simple" xlink:href="../164/36164.xml">
2006</link>).&#32;"<weblink xlink:type="simple" xlink:href="http://www.technologyreview.com/read_article.aspx?id=17587&amp;ch=biztech">
"The $1 million Netflix challenge"</weblink>".&#32;<it><magazine wordnetid="106595351" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../747/1258747.xml">
Technology Review</link></magazine>
</it>. Interview with NetFlix vice-president Jim Bennet</entry>
</list>
</p>


</sec>
</bdy>
</psychological_feature>
</contest>
</social_event>
</event>
</article>
