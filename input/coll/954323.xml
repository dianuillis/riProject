<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:56:46[mciao0828] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Tikhonov regularization</title>
<id>954323</id>
<revision>
<id>243732990</id>
<timestamp>2008-10-07T20:38:50Z</timestamp>
<contributor>
<username>Billlion</username>
<id>90356</id>
</contributor>
</revision>
<categories>
<category>Estimation theory</category>
<category>Linear algebra</category>
</categories>
</header>
<bdy>

<b>Tikhonov regularization</b> is the most commonly used method of <link xlink:type="simple" xlink:href="../061/2009061.xml">
regularization</link> of <link xlink:type="simple" xlink:href="../673/176673.xml">
ill-posed problem</link>s.  In <link xlink:type="simple" xlink:href="../685/26685.xml">
statistics</link>, the method is also known as <b>ridge regression</b>. It is related to the <link xlink:type="simple" xlink:href="../446/892446.xml">
Levenberg-Marquardt algorithm</link> for <link xlink:type="simple" xlink:href="../764/15652764.xml">
non-linear least squares</link> problems.<p>

The standard approach to solve an <link xlink:type="simple" xlink:href="../606/10160606.xml">
overdetermined system</link> of <link xlink:type="simple" xlink:href="../087/113087.xml">
linear equations</link> given as
<indent level="1">

 <math>A\mathbf{x}=\mathbf{b},</math>
</indent>
is known as <link xlink:type="simple" xlink:href="../872/484872.xml">
linear least squares</link> and seeks to minimize the residual
<indent level="1">

 <math>\|A\mathbf{x}-\mathbf{b}\|^2 </math>
</indent>
where <math>\left \| \cdot \right \|</math> is the <link xlink:type="simple" xlink:href="../534/990534.xml#xpointer(//*[./st=%22Euclidean+norm%22])">
Euclidean norm</link>. However, the matrix <math>A</math> may be <link xlink:type="simple" xlink:href="../934/6934.xml">
ill-conditioned</link> or <link xlink:type="simple" xlink:href="../200/200200.xml">
singular</link> yielding a large number of solutions. In order to give preference to a particular solution with desirable properties, the regularization term is included in this minimization:
<indent level="1">

 <math>\|A\mathbf{x}-\mathbf{b}\|^2+ \|\Gamma \mathbf{x}\|^2</math>
</indent>
for some suitably chosen <it>Tikhonov matrix</it> <math>\Gamma </math>. In many cases, this matrix is chosen as the <link xlink:type="simple" xlink:href="../718/59718.xml">
identity matrix</link> <math>\Gamma= I </math>, giving preference to solutions with smaller norms. In other cases, <link xlink:type="simple" xlink:href="../486/56486.xml">
highpass</link> operators (e.g. a <link xlink:type="simple" xlink:href="../174/230174.xml">
difference operator</link> or a weighted <link xlink:type="simple" xlink:href="../811/8811.xml">
fourier operator</link>) may be used to enforce smoothness if the underlying vector is believed to be mostly continuous.
This regularization improves the conditioning of the problem, thus enabling a numerical solution. An explicit solution, denoted by <math>\hat{x}</math>, is given by:
<indent level="1">

 <math>\hat{x} = (A^{T}A+ \Gamma^{T} \Gamma )^{-1}A^{T}\mathbf{b}</math>
</indent>
The effect of regularization may be varied via the scale of matrix <math>\Gamma</math> (e.g. <math>\Gamma = \alpha I</math>). For <math>\Gamma </math> = 0 this reduces to the unregularized least squares solution provided that (ATA)âˆ’1 exists.</p>

<sec>
<st>
Bayesian interpretation</st>
<p>

Although at first the choice of the solution to this regularized problem may look artificial, and indeed the matrix <math>\Gamma</math> seems rather arbitrary, the process can be justified from a <representation wordnetid="105926676" confidence="0.8">
<interpretation wordnetid="105928513" confidence="0.8">
<link xlink:type="simple" xlink:href="../890/4890.xml">
Bayesian point of view</link></interpretation>
</representation>
. Note that for an ill-posed problem one must necessarily introduce some additional assumptions in order to get a stable solution. Statistically we might assume that <link xlink:type="simple" xlink:href="../624/6772624.xml">
a priori</link> we know that <math>x</math> is a random variable with a <link xlink:type="simple" xlink:href="../347/50347.xml">
multivariate normal distribution</link>. For simplicity we take the mean to be zero and assume that each component is independent with <link xlink:type="simple" xlink:href="../590/27590.xml">
standard deviation</link> <math>\sigma _x</math>. Our data is also subject to errors, and we take the errors in <math>b</math> to be also <link xlink:type="simple" xlink:href="../593/27593.xml">
independent</link> with zero mean and standard deviation <math>\sigma _b</math>. Under these assumptions the Tikhonov-regularized solution is the <link xlink:type="simple" xlink:href="../806/140806.xml">
most likely</link> solution given the data and the a priori distribution of <math>x</math>, according to <statement wordnetid="106722453" confidence="0.8">
<message wordnetid="106598915" confidence="0.8">
<theorem wordnetid="106752293" confidence="0.8">
<proposition wordnetid="106750804" confidence="0.8">
<link xlink:type="simple" xlink:href="../569/49569.xml">
Bayes' theorem</link></proposition>
</theorem>
</message>
</statement>
. The Tikhonov matrix is then <math>\Gamma = \alpha I </math> for tikhonov factor <math>\alpha = \sigma _b / \sigma _x</math>.</p>
<p>

If the assumption of <link xlink:type="simple" xlink:href="../462/21462.xml">
normality</link> is replaced by assumptions of <link xlink:type="simple" xlink:href="../428/294428.xml">
homoscedasticity</link> and uncorrelatedness of <link xlink:type="simple" xlink:href="../509/461509.xml">
errors</link>, and still assume zero mean, then the <link xlink:type="simple" xlink:href="../353/170353.xml">
Gauss-Markov theorem</link> entails that the solution is still optimal in a certain sense.</p>

</sec>
<sec>
<st>
Generalized Tikhonov regularization</st>
<p>

For general multivariate normal distributions for <math>x</math> and the data error, one can apply a transformation of the variables to reduce to the case above. Equivalently, one can seek an <math>x</math> to minimize</p>
<p>

<indent level="1">

<math>\|Ax-b\|_P^2 + \|x-x_0\|_Q^2\,</math>
</indent>

where we have used <math>\left \| x  \right \|_P^2</math> to stand for the weighted norm <math>x^T P x</math>. In the Bayesian interpretation <math>P</math> is the inverse <link xlink:type="simple" xlink:href="../752/191752.xml">
covariance matrix</link> of <math>b</math>, <math>x_0</math> is the <link xlink:type="simple" xlink:href="../653/9653.xml">
expected value</link> of <math>x</math>, and <math>Q</math> is the inverse covariance matrix of <math>x</math>. The Tikhonov matrix is then given as a factorization of the matrix <math> Q = \Gamma^T \Gamma </math> (e.g. the <link xlink:type="simple" xlink:href="../433/134433.xml">
cholesky factorization</link>), and is considered a <link xlink:type="simple" xlink:href="../182/46182.xml#xpointer(//*[./st=%22Whitening+a+random+vector%22])">
whitening filter</link>.</p>
<p>

This generalized problem can be solved explicitly using the formula</p>
<p>

<indent level="1">

 <math>x_0 + (A^T PA + Q)^{-1} A^T P(b-Ax_0).\,</math>
</indent>

</p>
</sec>
<sec>
<st>
Regularization in Hilbert space</st>
<p>

Typically discrete linear ill-conditioned problems result as discretization of <link xlink:type="simple" xlink:href="../234/474234.xml">
integral equation</link>s, and one can formulate Tikhonov regularization in the original infinite dimensional context. In the above we can interpret <math>A</math> as a <link xlink:type="simple" xlink:href="../441/577441.xml">
compact operator</link> on <link xlink:type="simple" xlink:href="../167/18994167.xml">
Hilbert space</link>s, and <math>x</math> and <math>b</math> as elements in the domain and range of <math>A</math>. The operator <math>A^* A + \Gamma^T \Gamma </math> is then a <link xlink:type="simple" xlink:href="../175/667175.xml">
self-adjoint</link> bounded invertible operator.</p>

</sec>
<sec>
<st>
Relation to singular value decomposition and Wiener filter</st>
<p>

With <math>\Gamma = \alpha I </math>, this least squares solution can be analyzed in a special way via the <link xlink:type="simple" xlink:href="../207/142207.xml">
singular value decomposition</link>. Given the singular value decomposition of A
<indent level="1">

<math>A = U \Sigma V^T</math>
</indent>
with singular values <math>\sigma _i</math>, the Tikhonov regularized solution can be expressed as</p>
<p>

<indent level="1">

<math>\hat{x} = V D U^T b</math>
</indent>

where <math>D</math> has diagonal values</p>
<p>

<indent level="1">

<math>D_{ii} = \frac{\sigma _i}{\sigma _i ^2 + \alpha ^2}</math>
</indent>

and is zero elsewhere. This demonstrates the effect of the Tikhonov parameter on the <link xlink:type="simple" xlink:href="../934/6934.xml">
condition number</link> of the regularized problem. For the generalized case a similar representation can be derived using a <link xlink:type="simple" xlink:href="../903/1007903.xml">
generalized singular value decomposition</link>.</p>
<p>

Finally, it is related to the <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<filter wordnetid="103339643" confidence="0.8">
<link xlink:type="simple" xlink:href="../721/1216721.xml">
Wiener filter</link></filter>
</device>
</instrumentality>
</artifact>
:</p>
<p>

<indent level="1">

<math>\hat{x} = \sum _{i=1} ^q f_i \frac{u_i ^T b}{\sigma _i} v_i</math>
</indent>

where the Wiener weights are <math>f_i = \frac{\sigma _i ^2}{\sigma _i ^2 + \alpha ^2}</math> and <math>q</math> is the <link xlink:type="simple" xlink:href="../561/26561.xml">
rank</link> of <math>A</math>.</p>

</sec>
<sec>
<st>
Determination of the Tikhonov factor</st>
<p>

The optimal regularization parameter <math>\alpha</math> is usually unknown and often in practical problems is determined by an ad hoc method. A possible approach relies on the Bayesian interpretation described above. Other approaches include the <link>
discrepancy principle</link>, <link xlink:type="simple" xlink:href="../612/416612.xml">
cross validation</link>, <link>
L-curve method</link>, and <link>
unbiased predictive risk estimator</link>.   Wahba proved that the optimal parameter, in the sense of <link xlink:type="simple" xlink:href="../612/416612.xml#xpointer(//*[./st=%22Leave-one-out+cross-validation%22])">
leave-one-out cross-validation</link> minimizes:</p>
<p>

<indent level="1">

<math>G = \frac{\operatorname{RSS}}{\tau ^2} = \frac{\left \| X \hat{\beta} - y \right \| ^2}{\left[ \operatorname{Tr} \left(I - X (X^T X + \alpha ^2 I) ^{-1} X ^T \right) \right]^2}</math>
</indent>

where <math>\operatorname{RSS}</math> is the <link xlink:type="simple" xlink:href="../303/2473303.xml">
residual sum of squares</link> and <math>\tau</math> is the effective number <link xlink:type="simple" xlink:href="../622/176622.xml">
degree of freedom</link>.</p>
<p>

Using the previous SVD decomposition, we can simplify the above expression:
<indent level="1">

<math>\operatorname{RSS} = \left \| y - \sum _{i=1} ^q (u_i ' b) u_i \right \| ^2 + \left \| \sum _{i=1} ^q \frac{\alpha ^ 2}{\sigma _i ^ 2 + \alpha ^ 2} (u_i ' b) u_i \right \| ^2</math>
</indent>

<indent level="1">

<math>\operatorname{RSS} = \operatorname{RSS} _0 + \left \| \sum _{i=1} ^q \frac{\alpha ^ 2}{\sigma _i ^ 2 + \alpha ^ 2} (u_i ' b) u_i \right \| ^2</math>
</indent>

and</p>
<p>

<indent level="1">

<math>\tau = m - \sum _{i=1} ^q \frac{\sigma _i ^2}{\sigma _i ^2 + \alpha ^2}
= m - q + \sum _{i=1} ^q \frac{\alpha ^2}{\sigma _i ^2 + \alpha ^2}</math>
</indent>

</p>
</sec>
<sec>
<st>
Relation to probabilistic formulation</st>
<p>

The probabilistic formulation of an <link xlink:type="simple" xlink:href="../956/203956.xml">
inverse problem</link> introduces (when all uncertainties are Gaussian) a covariance matrix <math> C_M</math> representing the a priori uncertainties on the model parameters, and a covariance matrix <math> C_D</math> representing the uncertainties on the observed parameters (see, for instance, Tarantola, 2004 <weblink xlink:type="simple" xlink:href="http://www.ipgp.jussieu.fr/~tarantola/Files/Professional/SIAM/index.html">
http://www.ipgp.jussieu.fr/~tarantola/Files/Professional/SIAM/index.html</weblink>). In the special case when these two matrices are diagonal and isotropic, <math> C_M = \sigma_M^2 I </math> and <math> C_D = \sigma_D^2 I </math>, and, in this case, the equations of inverse theory reduce to the equations above, with <math> \alpha = {\sigma_D}/{\sigma_M} </math>.</p>

</sec>
<sec>
<st>
History</st>
<p>

Tikhonov regularization has been invented independently in many different contexts. 
It became widely known from its application to integral equations from the work of 
<physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<hero wordnetid="110325013" confidence="0.8">
<leader wordnetid="109623038" confidence="0.8">
<intellectual wordnetid="109621545" confidence="0.8">
<alumnus wordnetid="109786338" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<scholar wordnetid="110557854" confidence="0.8">
<link xlink:type="simple" xlink:href="../048/616048.xml">
A. N. Tikhonov</link></scholar>
</mathematician>
</scientist>
</causal_agent>
</alumnus>
</intellectual>
</leader>
</hero>
</person>
</physical_entity>
 and D. L. Phillips. 
Some authors use the term <b>Tikhonov-Phillips regularization</b>. 
The finite dimensional case was expounded by A. E. Hoerl, who took a statistical approach, and by M. Foster, who interpreted this method as a <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../185/63185.xml">
Wiener</link></scientist>
</person>
-<link xlink:type="simple" xlink:href="../161/91161.xml">
Kolmogorov</link> filter. Following Hoerl, it is known in the statistical literature as <b>ridge regression</b>.</p>

</sec>
<sec>
<st>
References</st>
<p>

<list>
<entry level="1" type="bullet">

 Tikhonov AN, 1943, <it>On the stability of inverse problems</it>, Dokl. Akad. Nauk SSSR, 39, No. 5, 195-198</entry>
<entry level="1" type="bullet">

 Tikhonov AN, 1963, <it>Solution of incorrectly formulated problems and the regularization method</it>, Soviet Math Dokl 4, 1035-1038 English translation of Dokl Akad Nauk SSSR 151, 1963, 501-504</entry>
<entry level="1" type="bullet">

 Tikhonov AN and Arsenin VA, 1977, <it>Solution of Ill-posed Problems</it>, Winston &amp; Sons, Washington, ISBN 0-470-99124-0.</entry>
<entry level="1" type="bullet">

 Hansen, P.C., 1998, <it>Rank-deficient and Discrete ill-posed problems</it>, SIAM</entry>
<entry level="1" type="bullet">

 Hoerl AE, 1962, <it>Application of ridge analysis to regression problems</it>, Chemical Engineering Progress, 58, 54-59.</entry>
<entry level="1" type="bullet">

 Foster M, 1961, <it>An application of the Wiener-Kolmogorov smoothing theory to matrix inversion</it>, J. SIAM, 9, 387-392 </entry>
<entry level="1" type="bullet">

 Phillips DL, 1962, <it>A technique for the numerical solution of certain integral equations of the first kind</it>, J Assoc Comput Mach, 9, 84-97</entry>
<entry level="1" type="bullet">

 Tarantola A, 2004, <it>Inverse Problem Theory</it> (<weblink xlink:type="simple" xlink:href="http://www.ipgp.jussieu.fr/~tarantola/Files/Professional/SIAM/index.html">
free PDF version</weblink>), Society for Industrial and Applied Mathematics, ISBN 0-89871-572-5</entry>
<entry level="1" type="bullet">

 Wahba, G, 1990, <it>Spline Models for Observational Data</it>, Society for Industrial and Applied Mathematics</entry>
</list>


</p>

</sec>
</bdy>
</article>
