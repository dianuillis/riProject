<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:54:43[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Transduction (machine learning)</title>
<id>960361</id>
<revision>
<id>186408720</id>
<timestamp>2008-01-23T19:57:47Z</timestamp>
<contributor>
<username>Eteru</username>
<id>425509</id>
</contributor>
</revision>
<categories>
<category>Machine learning</category>
</categories>
</header>
<bdy>

In <link xlink:type="simple" xlink:href="../225/3729225.xml">
logic</link>, <link xlink:type="simple" xlink:href="../577/27577.xml">
statistical inference</link>, and <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link>,
<b>transduction</b> or <b>transductive inference</b> is <link xlink:type="simple" xlink:href="../755/89755.xml">
reasoning</link> from
observed, specific (training) cases to specific (test) cases. In contrast,
<link xlink:type="simple" xlink:href="../736/393736.xml">
induction</link> is reasoning from observed training cases
to general rules, which are then applied to the test cases. The distinction is
most interesting in cases where the predictions of the transductive model are
not achievable by any inductive model. Note that this is caused by transductive
inference on different test sets producing mutually inconsistent predictions.<p>

Transduction was introduced by <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<academician wordnetid="109759069" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<adult wordnetid="109605289" confidence="0.8">
<professional wordnetid="110480253" confidence="0.8">
<mathematician wordnetid="110301261" confidence="0.8">
<educator wordnetid="110045713" confidence="0.8">
<link xlink:type="simple" xlink:href="../673/209673.xml">
Vladimir Vapnik</link></educator>
</mathematician>
</professional>
</adult>
</scientist>
</academician>
</causal_agent>
</person>
</physical_entity>
 in the 1990's, motivated by
his view that transduction is preferable to induction since, according to him, induction requires
solving a more general problem (inferring a function) before solving a more
specific problem (computing outputs for new cases): "When solving a problem of
interest, do not solve a more general problem as an intermediate step. Try to
get the answer that you really need but not a more general one."</p>
<p>

An example of learning which is not inductive would be in the case of binary
classification, where the inputs tend to cluster in two groups. A large set of
test inputs may help in finding the clusters, thus providing useful information
about the classification labels. The same predictions would not be obtainable
from a model which induces a function based only on the training cases.  Some
people may call this an example of the closely related <link xlink:type="simple" xlink:href="../632/2829632.xml">
semi-supervised learning</link>, since Vapnik's motivation is quite different. An example of an algorithm in this category is the Transductive <link xlink:type="simple" xlink:href="../309/65309.xml">
Support Vector Machine</link> (TSVM).</p>
<p>

A third possible motivation which leads to transduction arises through the need
to approximate. If exact inference is computationally prohibitive, one may at
least try to make sure that the approximations are good at the test inputs. In
this case, the test inputs could come from an arbitrary distribution (not
necessarily related to the distribution of the training inputs), which wouldn't
be allowed in semi-supervised learning. An example of an algorithm falling in
this category is the <link>
Bayesian Committee Machine</link> (BCM).
</p>
<sec>
<st>
 References </st>

<p>

<list>
<entry level="1" type="bullet">

 V. N. Vapnik. <it>Statistical learning theory</it>. New York: Wiley, 1998. <it>(See pages 339-371)''</it></entry>
</list>
</p>
<p>

<list>
<entry level="1" type="bullet">

 V. Tresp. <it>A Bayesian committee machine</it>, Neural Computation, 12, 2000, <weblink xlink:type="simple" xlink:href="http://www.tresp.org/papers/bcm6.pdf">
pdf</weblink>.</entry>
</list>
</p>

</sec>
</bdy>
</article>
