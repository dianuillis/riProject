<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 23:56:02[mciao0825] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Oja&apos;s rule</title>
<id>9617564</id>
<revision>
<id>218432704</id>
<timestamp>2008-06-10T17:04:12Z</timestamp>
<contributor>
<username>Makkon</username>
<id>557692</id>
</contributor>
</revision>
<categories>
<category>Neural networks</category>
<category>Neuroscience</category>
<category>Biophysics</category>
</categories>
</header>
<bdy>

<b>Oja's Learning Rule</b>, or simply <b>Oja's rule</b>, is a model of how neurons in the brain or in <link xlink:type="simple" xlink:href="../523/21523.xml">
artificial neural networks</link> change connection strength, or learn, over time. It is a modification of the standard Hebb's Rule (see <link xlink:type="simple" xlink:href="../084/404084.xml">
Hebbian learning</link>) that, through multiplicative normalization, solves all stability problems and generates an algorithm for <link xlink:type="simple" xlink:href="../340/76340.xml">
principal components analysis</link>. This is a computational form of an effect which is believed to happen in biological neurons.
<sec>
<st>
Theory</st>
<p>

Oja's rule requires a number of simplifications to derive, but in its final form it is demonstrably stable, unlike Hebb's rule. It is a single-neuron special case of the <link xlink:type="simple" xlink:href="../929/14402929.xml">
Generalized Hebbian Algorithm</link>. However, Oja's rule can also be generalized in other ways to varying degrees of stability and success.</p>

<ss1>
<st>
Formula</st>
<p>

Oja's rule defines the change in presynaptic weights <math>\textbf{w}</math> given the output response <math>y</math> of a neuron to its inputs <math>\textbf{x}</math> to be</p>
<p>

<indent level="1">

<math>\,\Delta \textbf{w} = \textbf{w}(n+1)-\textbf{w}(n) = \eta y(n) (\textbf{x}(n) - y(n)\textbf{w}(n))</math>,
</indent>

where <math>\eta</math> is the <it>learning rate</it> which can also change with time. Note that the bold symbols are <link xlink:type="simple" xlink:href="../688/292688.xml">
vectors</link> and <math>n</math> defines a discrete time iteration. The rule can also be made for continuous iterations as</p>
<p>

<indent level="1">

<math>\,\frac{d\textbf{w}}{d t} = \eta y(t) (\textbf{x}(t) - y(t)\textbf{w}(t))</math>.
</indent>

</p>
</ss1>
<ss1>
<st>
Derivation</st>
<p>

The simplest learning rule known is Hebb's rule, which states in conceptual terms that <it>neurons that fire together, wire together</it>. In component form as a difference equation, it is written</p>
<p>

<indent level="1">

<math>\,w_i (n+1) = w_i (n) + \eta y(\textbf{x}(n)) x_i (n)</math>,
</indent>

where <math>y(\textbf{x}(n))</math> is again the output, this time explicitly dependent on its input vector <math>\textbf{x}</math>.</p>
<p>

Hebb's rule has synaptic weights approaching infinity with a positive learning rate. We can stop this by normalizing the weights so that each weight's magnitude is restricted between 0, corresponding to no weight, and 1, corresponding to being the only input neuron with any weight. Mathematically, this takes the form</p>
<p>

<indent level="1">

<math>\,w_i (n+1) = \frac{w_i (n) + \eta y(\textbf{x}(n)) x_i (n)}{(\sum_{j=1}^m [w_j (n) + \eta y(\textbf{x}(n)) x_j (n)]^p)^{1/p}}</math>.
</indent>

Note that in Oja's original paper<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>, <math>p=2</math>, corresponding to a sum in quadrature, which is the familiar normalization rule. However, any type of normalization, even linear, will give the same result without loss of generality.</p>
<p>

Our next step is to expand this into a <mathematical_relation wordnetid="113783581" confidence="0.8">
<function wordnetid="113783816" confidence="0.8">
<link xlink:type="simple" xlink:href="../448/30448.xml">
Taylor series</link></function>
</mathematical_relation>
 for a small learning rate <math>| \eta | \ll 1</math>, giving</p>
<p>

<indent level="1">

<math>\,w_i (n+1) = \frac{w_i (n)}{(\sum_j w_j^p)^{1/p}} + \eta \left( \frac{y(n) x_i (n)}{(\sum_j w_j^p)^{1/p}} - \frac{w_i (n)\sum_j y(n) x_j (n) w_j (n)}{(\sum_j w_j^p)^{1 + 1/p}} \right) + O(\eta^2)</math>.
</indent>

For small <math>\eta</math>, our higher order terms (<math>O(\eta^2)</math>) go to zero (see <link xlink:type="simple" xlink:href="../578/44578.xml">
Big-O notation</link>). We again make the specification of a linear neuron, that is, the output of the neuron is equal to the sum of the product of each input and its synaptic weight, or</p>
<p>

<indent level="1">

<math>\,y(\textbf{x}(n)) = \sum_{j=1}^m x_j (n) w_j (n)</math>.
</indent>

We also specify that our weights normalize to 1, which will be a necessary condition for stability, so </p>
<p>

<indent level="1">

<math>\,\| \textbf{w} \| = (\sum_{j=1}^m w_j^p)^{1/p} = 1</math>,
</indent>

which, when substituted into our expansion, gives Oja's rule, or</p>
<p>

<indent level="1">

<math>\,w_i (n+1) = w_i (n) + \eta y(n) (x_i (n) - w_i (n) y(n))</math>.
</indent>

</p>
</ss1>
<ss1>
<st>
Stability and PCA</st>
<p>

In analyzing the convergence of a single neuron evolving by Oja's rule, one extracts the first <it>principal component</it>, or feature, of a data set. Furthermore, with extensions using the <link xlink:type="simple" xlink:href="../929/14402929.xml">
Generalized Hebbian Algorithm</link>, one can create a multi-Oja neural network that can extract as many features as desired, allowing for <link xlink:type="simple" xlink:href="../340/76340.xml">
principal components analysis</link>.</p>
<p>

A principal component <math>\,a_j</math> is extracted from a dataset <math>\textbf{x}</math> through some associated vector <math>\textbf{q}_j</math>, or <math>a_j = \textbf{q}_j \cdot \textbf{x}</math>, and we can restore our original dataset by taking </p>
<p>

<indent level="1">

<math>\textbf{x}=\sum_j a_j \textbf{q}_j</math>.
</indent>

In the case of a single neuron trained by Oja's rule, we find the weight vector converges to <math>\textbf{q}_1</math>, or the first principal component, as time or number of iterations approaches infinity. We can also define, given a set of input vectors <math>\,X_i</math>, that its correlation matrix <math>\,R_{ij}=X_i X_j</math> has an associated <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvector</link> given by <math>\textbf{q}_j</math> with <link xlink:type="simple" xlink:href="../429/2161429.xml">
eigenvalue</link> <math>\,\lambda_j</math>. The <link xlink:type="simple" xlink:href="../344/32344.xml">
variance</link> of outputs of our Oja neuron <math>\sigma^2(n)=\langle y^2 (n) \rangle</math> then converges with time iterations to the principal eigenvalue, or</p>
<p>

<indent level="1">

<math>\lim_{n\rightarrow\infty} \sigma^2(n) = \lambda_1</math>.
</indent>

These results are derived using <link xlink:type="simple" xlink:href="../267/352267.xml">
Lyapunov function</link> analysis, and they show that Oja's neuron necessarily converges on strictly the first principal component if certain conditions are met in our original learning rule. Most importantly, our learning rate <math>\,\eta</math> is allowed to vary with time, but only such that its sum is <it>divergent</it> but its power sum is <it>convergent</it>, that is</p>
<p>

<indent level="1">

<math>\sum_{n=1}^\infty \eta(n) = \infty, ~ \sum_{n=1}^\infty \eta(n)^p &amp;lt; \infty, ~ p&amp;gt;1</math>.
</indent>

Our output <link xlink:type="simple" xlink:href="../835/14179835.xml">
activation function</link> <math>y(\textbf{x}(n))</math> is also allowed to be nonlinear and nonstatic, but it must be continuously differentiable in both <math>\textbf{x}</math> and <math>\textbf{w}</math> and have derivatives bounded in time. <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>

</ss1>
</sec>
<sec>
<st>
Applications</st>
<p>

Oja's rule was originally described in Oja's 1982 paper<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref>, but the principle of self-organization to which it is applied is first attributed to <person wordnetid="100007846" confidence="0.9508927676800064">
<scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../208/1208.xml">
Alan Turing</link></scientist>
</person>
 in 1952<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>. PCA has also had a long history of use before Oja's rule formalized its use in network computation in 1989. The model can thus be applied to any problem of <link xlink:type="simple" xlink:href="../996/76996.xml">
self-organizing map</link>ping, in particular those in which feature extraction is of primary interest. Therefore, Oja's rule has an important place in image and speech processing. It is also useful as it expands easily to higher dimensions of processing, thus being able to integrate multiple outputs quickly. A canonical example is its use in <link xlink:type="simple" xlink:href="../280/192280.xml">
binocular vision</link><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>.</p>

<ss1>
<st>
Biology and Oja's Subspace rule</st>
<p>

There is clear evidence for both <link xlink:type="simple" xlink:href="../266/372266.xml">
long-term potentiation</link> and <link xlink:type="simple" xlink:href="../921/487921.xml">
long-term depression</link> in biological neural networks, along with a normalization effect in both input weights and neuron outputs. However, while as of yet there is no direct experimental evidence of Oja's rule active in a biological neural network, a <link xlink:type="simple" xlink:href="../000/54000.xml">
biophysical</link> derivation of a generalization of the rule is possible. Such a derivation requires retrograde signalling from the postsynaptic neuron, which is biologically plausible (see <link xlink:type="simple" xlink:href="../608/14338608.xml">
neural backpropagation</link>), and takes the form of</p>
<p>

<indent level="1">

<math>\Delta w_{ij} \propto \langle x_i y_j \rangle - \epsilon \left\langle (c_\textrm{pre} * \sum_k w_{ik} y_k)\cdot (c_\textrm{post} * y_j)\right\rangle</math>,
</indent>

where as before <math>w_{ij}</math> is the synaptic weight between the <math>i</math>th input and <math>j</math>th output neurons, <math>x</math> is the input, <math>y</math> is the postsynaptic output, and we define <math>\epsilon</math> to be a constant analogous the learning rate, and <math>c_\textrm{pre}</math> and <math>c_\textrm{post}</math> are presynaptic and postsynaptic functions that model the weakening of signals over time. Note that the angle brackets denote the average and the <math>*</math> operator is a <link xlink:type="simple" xlink:href="../519/7519.xml">
convolution</link>. By taking the pre- and post-synaptic functions into frequency space and combining integration terms with the convolution, we find that this gives an arbitrary-dimensional generalization of Oja's rule known as <b>Oja's Subspace</b><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%224%22])">4</ref>, namely</p>
<p>

<indent level="1">

<math>\Delta w \propto C x\cdot w - w\cdot C y</math><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%225%22])">5</ref>.
</indent>
</p>


</ss1>
</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../084/404084.xml">
Hebbian learning</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../011/14200011.xml">
BCM theory</link></entry>
<entry level="1" type="bullet">

<event wordnetid="100029378" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<link xlink:type="simple" xlink:href="../771/423771.xml">
Synaptic plasticity</link></activity>
</procedure>
</psychological_feature>
</act>
</event>
</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../929/14402929.xml">
Generalized Hebbian Algorithm</link> (GHA)</entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../996/76996.xml">
Self-organizing map</link></entry>
<entry level="1" type="bullet">

<link>
PCA network</link></entry>
<entry level="1" type="bullet">

<idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<component wordnetid="105868954" confidence="0.8">
<part wordnetid="105867413" confidence="0.8">
<link xlink:type="simple" xlink:href="../340/76340.xml">
Principal components analysis</link></part>
</component>
</concept>
</idea>
</entry>
<entry level="1" type="bullet">

<link>
ICA network</link></entry>
<entry level="1" type="bullet">

<link xlink:type="simple" xlink:href="../031/598031.xml">
Independent components analysis</link></entry>
</list>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal"><link>
Oja, Erkki</link>&#32;(November 1982).&#32;"<weblink xlink:type="simple" xlink:href="http://www.springerlink.com/content/u9u6120r003825u1/">
Simplified neuron model as a principal component analyzer</weblink>". <it>Journal of Mathematical Biology</it>&#32;<b>15</b>&#32;(3): 267–273. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1007%2FBF00275687">
10.1007/BF00275687</weblink>. BF00275687. Retrieved on <link>
2007-11-22</link>.</cite>&nbsp;</entry>
<entry id="2">
 <cite id="Reference-Haykin-1998" style="font-style:normal" class="book"><link>
Haykin, Simon</link>&#32;(1998). Neural Networks: A Comprehensive Foundation, 2,&#32;Prentice Hall. ISBN 0132733501.</cite>&nbsp;</entry>
<entry id="3">
Intrator, Nathan&#32;(2007).&#32;"<weblink xlink:type="simple" xlink:href="http://www.cs.tau.ac.il/~nin/Courses/NC06/Hebb_PCA.ppt">
Unsupervised Learning</weblink>".&#32;<it>Neural Computation lectures</it>.&#32;  <link xlink:type="simple" xlink:href="../402/161402.xml">
Tel-Aviv University</link>.&#32;Retrieved on <link>
2007-11-22</link>.</entry>
<entry id="4">
 <cite style="font-style:normal"><link>
Oja, Erkki</link>&#32;(1989).&#32;"<weblink xlink:type="simple" xlink:href="http://www.worldscinetarchives.com/cgi-bin/details.cgi?id=pii:S0129065789000475&amp;type=html">
Neural Networks, Principal Components, and Subspaces</weblink>". <it>International Journal of Neural Systems (IJNS)</it>&#32;<b>1</b>&#32;(1): 61–68. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1142%2FS0129065789000475">
10.1142/S0129065789000475</weblink>. Retrieved on <link>
2007-11-22</link>.</cite>&nbsp;</entry>
<entry id="5">
 <cite style="font-style:normal">Friston, K.J.; C.D. Frith, R.S.J. Frackowiak&#32;(October 22 1993).&#32;"<weblink xlink:type="simple" xlink:href="http://links.jstor.org/sici?sici=0962-8452%2819931022%29254%3A1339%3C47%3APCALAA%3E2.0.CO%3B2-W">
Principal Component Analysis Learning Algorithms: A Neurobiological Analysis</weblink>". <it>Proceedings: Biological Sciences</it>&#32;<b>254</b>&#32;(1339): 47–54. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1098%2Frspb.1993.0125">
10.1098/rspb.1993.0125</weblink>. Retrieved on <link>
2007-11-22</link>.</cite>&nbsp;</entry>
</reflist>
</p>

</sec>
<sec>
<st>
Links</st>
<p>

<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.scholarpedia.org/article/Oja_learning_rule">
Oja, Erkki: Oja learning rule in Scholarpedia</weblink></entry>
</list>
</p>

</sec>
</bdy>
</article>
