<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:56:34[mciao0827] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>N-gram</title>
<id>986182</id>
<revision>
<id>240096320</id>
<timestamp>2008-09-21T22:15:55Z</timestamp>
<contributor>
<username>Aarre</username>
<id>4295527</id>
</contributor>
</revision>
<categories>
<category>Natural language processing</category>
<category>Speech recognition</category>
<category>Wikipedia articles needing context</category>
<category>Wikipedia introduction cleanup</category>
<category>Corpus linguistics</category>
<category>Computational linguistics</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_style.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 The introduction to this article provides <b>insufficient context</b> for those unfamiliar with the subject.
Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=N-gram&amp;action=edit">
improve the article</weblink> with a .</col>
</row>
</table>

Not to be confused with <link xlink:type="simple" xlink:href="../457/3722457.xml">
engram</link>.<p>

An <b><it>n</it></b><b>-gram</b> is a sub-sequence of <it>n</it> items from a given <link xlink:type="simple" xlink:href="../838/27838.xml">
sequence</link>.  <it>n</it>-grams are used in various areas of statistical <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link> and genetic sequence analysis.  The items in question can be <link xlink:type="simple" xlink:href="../980/22980.xml">
phoneme</link>s, syllables, letters, words or <link xlink:type="simple" xlink:href="../292/4292.xml">
base pairs</link> according to the application.</p>
<p>

An <it>n</it>-gram of size 1 is a "<link xlink:type="simple" xlink:href="../182/986182.xml">
unigram</link>"; size 2 is a "<link xlink:type="simple" xlink:href="../587/1064587.xml">
bigram</link>" (or, more etymologically sound but less commonly used, a "digram"); size 3 is a "<link xlink:type="simple" xlink:href="../212/312212.xml">
trigram</link>"; and size 4 or more is simply called an "<it>n</it>-gram".  Some <link xlink:type="simple" xlink:href="../810/1911810.xml">
language model</link>s built from n-grams are "(<it>n</it>&nbsp;&amp;minus;&nbsp;1)-order <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
s".</p>

<sec>
<st>
Examples</st>

<p>

Here are examples of <b><it>word</it></b> level 3-grams and 4-grams (and counts of the number of times they appeared) from the <link xlink:type="simple" xlink:href="../182/986182.xml#xpointer(//*[./st=%22Google_use_of_N-gram%22])">
Google n-gram corpus</link>.</p>
<p>

<list>
<entry level="1" type="bullet">

ceramics collectables collectibles (55)</entry>
<entry level="1" type="bullet">

ceramics collectables fine (130)</entry>
<entry level="1" type="bullet">

ceramics collected by (52)</entry>
<entry level="1" type="bullet">

ceramics collectible pottery (50)</entry>
<entry level="1" type="bullet">

ceramics collectibles cooking (45)</entry>
</list>
</p>
<p>

4-grams</p>
<p>

<list>
<entry level="1" type="bullet">

serve as the incoming (92)</entry>
<entry level="1" type="bullet">

serve as the incubator (99)</entry>
<entry level="1" type="bullet">

serve as the independent (794)</entry>
<entry level="1" type="bullet">

serve as the index (223)</entry>
<entry level="1" type="bullet">

serve as the indication (72)</entry>
<entry level="1" type="bullet">

serve as the indicator (120)</entry>
</list>
</p>

</sec>
<sec>
<st>
<it>n</it>-gram models</st>
<p>

An <b><it>n</it></b><b>-gram model</b> models sequences, notably natural languages, using the statistical properties of <it>n</it>-grams.</p>
<p>

This idea can be traced to an experiment by <scientist wordnetid="110560637" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../693/5693.xml">
Claude Shannon</link></scientist>
's work in <link xlink:type="simple" xlink:href="../773/14773.xml">
information theory</link>.  His question was, given a sequence of letters (for example, the sequence "for ex"), what is the <link xlink:type="simple" xlink:href="../968/44968.xml">
likelihood</link> of the next letter?  From training data, one can derive a <link xlink:type="simple" xlink:href="../543/23543.xml">
probability distribution</link> for the next letter given a history of size <math>n</math>:  <it>a</it> = 0.4, <it>b</it> = 0.00001, <it>c</it> = 0, ....; where the probabilities of all possible "next-letters" sum to 1.0.</p>
<p>

More concisely, an <it>n</it>-gram model predicts <math>x_{i}</math> based on <math>x_{i-1}, x_{i-2}, \dots, x_{i-n}</math>. In Probability terms, this is nothing but <math>P(x_{i} | x_{i-1}, x_{i-2}, \dots, x_{i-n})</math>. When used for <link xlink:type="simple" xlink:href="../810/1911810.xml">
language model</link>ing independence assumptions are made so that each word depends only on the last <it>n</it> words.  This <link xlink:type="simple" xlink:href="../876/60876.xml">
Markov model</link> is used as an approximation of the true underlying language.  This assumption is important because it massively simplifies the problem of learning the language model from data.  In addition, because of the open nature of language, it is common to group words unknown to the language model together. </p>
<p>

<it>n</it>-gram models are widely used in statistical <link xlink:type="simple" xlink:href="../652/21652.xml">
natural language processing</link>.  In <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, <link xlink:type="simple" xlink:href="../980/22980.xml">
phonemes</link> and sequences of phonemes are modeled using a <it>n</it>-gram distribution.  For parsing, words are modeled such that each <it>n</it>-gram is composed of <it>n</it> words.  For <link xlink:type="simple" xlink:href="../650/2995650.xml">
language recognition</link>, sequences of letters are modeled for different languages.  For a sequence of words, (for example "the dog smelled like a skunk"), the trigrams would be: "the dog smelled", "dog smelled like", "smelled like a", and "like a skunk".  For sequences of characters, the 3-grams (sometimes referred to as "trigrams") that can be generated from "good morning" are "goo", "ood", "od ", "d m", " mo", "mor" and so forth.  Some practitioners preprocess strings to remove spaces, most simply collapse whitespace to a single space while preserving paragraph marks. Punctuation is also commonly reduced or removed by preprocessing. <it>n</it>-grams can also be used for sequences of words or, in fact, for almost any type of data. They have been used for example for extracting features for clustering large sets of satellite earth images and for determining what part of the Earth a particular image came from.  They have also been very successful as the first pass in genetic sequence search and in the identification of which species short sequences of DNA were taken from.</p>
<p>

N-gram models are often criticized because they lack any explicit representation of long range dependency.  While it is true that the only explicit dependency range is (n-1) tokens for an n-gram model, it is also true that the effective range of dependency is significantly longer than this although long range correlations drop exponentially with distance for any Markov model.  Alternative Markov language models that incorporate some degree of local state can exhibit very long range dependencies.  This is often done using hand-crafted state variables that represent, for instance, the position in a sentence, the general topic of discourse or a grammatical state variable.  Some of the best parsers of English currently in existence are roughly of this form.</p>
<p>

Another criticism that has been leveled is that Markov models of language, including n-gram models, do not explicitly capture the performance/competence distinction introduced by <person wordnetid="100007846" confidence="0.9508927676800064">
<philosopher wordnetid="110423589" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../566/21566.xml">
Noam Chomsky</link></philosopher>
</person>
.  This criticism fails to explain why parsers that are the best at parsing text seem to uniformly lack any such distinction and most even lack any clear distinction between semantics and syntax.  Most proponents of n-gram and related language models opt for a fairly pragmatic approach to language modeling that emphasizes empirical results over theoretical purity.</p>

</sec>
<sec>
<st>
<it>n</it>-grams for approximate matching</st>

<p>

<it>n</it>-grams can also be used for efficient approximate matching.  By converting a sequence of items to a set of <it>n</it>-grams, it can be embedded in a <link xlink:type="simple" xlink:href="../370/32370.xml">
vector space</link> (in other words, represented as a <link xlink:type="simple" xlink:href="../266/13266.xml">
histogram</link>), thus allowing the sequence to be compared to other sequences in an efficient manner. For example, if we convert strings with only letters in the English alphabet into 3-grams, we get a <math>26^3</math>-dimensional space (the first dimension measures the number of occurrences of "aaa", the second "aab", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string. For example, both the strings "abcba" and "bcbab" give rise to exactly the same 2-grams. However, we know empirically that if two strings of real text have a similar vector representation (as measured by <link xlink:type="simple" xlink:href="../093/157093.xml">
cosine distance</link>) then they are likely to be similar. Other metrics have also been applied to vectors of <it>n</it>-grams with varying, sometimes better, results. For example <link xlink:type="simple" xlink:href="../722/221722.xml">
z-score</link>s have been used to compare documents by examining how many standard deviations each <it>n</it>-gram differs from its mean occurrence in a large collection, or <link xlink:type="simple" xlink:href="../887/53887.xml">
text corpus</link>, of documents (which form the "background" vector).  In the event of small counts, the <link>
g-score</link> may give better results for comparing alternative models.  </p>
<p>

It is also possible to take a more principled approach to the statistics of <it>n</it>-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian inference</link>.</p>

</sec>
<sec>
<st>
Other applications</st>

<p>

<it>n</it>-grams find use in several areas of computer science, <link xlink:type="simple" xlink:href="../561/5561.xml">
computational linguistics</link>, and applied mathematics.  </p>
<p>

They have been used to:
<list>
<entry level="1" type="bullet">

 design <link xlink:type="simple" xlink:href="../395/50395.xml">
kernels</link> that allow <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link> algorithms such as <link xlink:type="simple" xlink:href="../309/65309.xml">
support vector machine</link>s to learn from string data</entry>
<entry level="1" type="bullet">

 find likely candidates for the correct spelling of a misspelled word</entry>
<entry level="1" type="bullet">

 improve compression in <link xlink:type="simple" xlink:href="../013/8013.xml">
compression algorithms</link> where a small area of data requires <it>n</it>-grams of greater length</entry>
<entry level="1" type="bullet">

 assess the probability of a given word sequence appearing in text of a language of interest in pattern recognition systems, <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, OCR (<link xlink:type="simple" xlink:href="../091/49091.xml">
optical character recognition</link>), <link xlink:type="simple" xlink:href="../033/3666033.xml">
Intelligent Character Recognition</link> (<link xlink:type="simple" xlink:href="../658/1166658.xml">
ICR</link>), <link xlink:type="simple" xlink:href="../980/19980.xml">
machine translation</link> and similar applications </entry>
<entry level="1" type="bullet">

 improve retrieval in <link xlink:type="simple" xlink:href="../271/15271.xml">
information retrieval</link> systems when it is hoped to find similar "documents" (a term for which the conventional meaning is sometimes stretched, depending on the data set) given a single query document and a database of reference documents</entry>
<entry level="1" type="bullet">

 improve retrieval performance in genetic sequence analysis as in the <link xlink:type="simple" xlink:href="../695/363695.xml">
BLAST</link> family of programs</entry>
<entry level="1" type="bullet">

 identify the language a text is in or the species a small sequence of DNA was taken from </entry>
<entry level="1" type="bullet">

 predict letters or words at random in order to create text, as in the <link xlink:type="simple" xlink:href="../104/2284104.xml">
dissociated press</link> algorithm.</entry>
</list>
</p>

</sec>
<sec>
<st>
 Bias-versus-variance trade-off </st>
<p>

What goes into picking the <it>n</it> for the <it>n</it>-gram?</p>
<p>

There are problems of balance weight between <it>infrequent grams</it> (for example, if a proper name appeared in the training data) and <it>frequent grams</it>.   Also, items not seen in the training data will be given a <link xlink:type="simple" xlink:href="../934/22934.xml">
probability</link> of 0.0 without <link xlink:type="simple" xlink:href="../662/4155662.xml">
smoothing</link>. For unseen but plausible data from a sample, one can introduce <link xlink:type="simple" xlink:href="../598/2045598.xml">
pseudocount</link>s.  Pseudocounts are generally motivated on Bayesian grounds.</p>

<ss1>
<st>
 Smoothing techniques </st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../010/160010.xml">
Linear interpolation</link> (e.g., taking the <link xlink:type="simple" xlink:href="../274/33274.xml">
weighted mean</link> of the unigram, bigram, and trigram)</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../954/6997954.xml">
Good-Turing</link> discounting</entry>
<entry level="1" type="bullet">

 <link>
Witten-Bell discounting</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../197/13911197.xml">
Katz's back-off model</link> (trigram)</entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
Google use of N-gram</st>

<p>

<company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../923/1092923.xml">
Google</link></company>
 uses n-gram models for a variety of R&amp;D projects, such as <link xlink:type="simple" xlink:href="../491/4558491.xml">
statistical machine translation</link>, <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, <idea wordnetid="105833840" confidence="0.8">
<concept wordnetid="105835747" confidence="0.8">
<property wordnetid="105849040" confidence="0.8">
<feature wordnetid="105849789" confidence="0.8">
<link xlink:type="simple" xlink:href="../869/605869.xml">
checking spelling</link></feature>
</property>
</concept>
</idea>
, <link>
entity detection</link>, and <link xlink:type="simple" xlink:href="../162/383162.xml">
data mining</link>.  In September 2006 <weblink xlink:type="simple" xlink:href="http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html">
Google announced</weblink> that they made their n-grams <weblink xlink:type="simple" xlink:href="http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13">
public</weblink> at the <link xlink:type="simple" xlink:href="../254/1436254.xml">
Linguistic Data Consortium</link> (<weblink xlink:type="simple" xlink:href="http://www.ldc.upenn.edu/">
LDC</weblink>).</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../770/98770.xml">
Hidden Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../212/312212.xml">
Trigram</link>, <link xlink:type="simple" xlink:href="../587/1064587.xml">
Bigram</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../729/132729.xml">
n-tuple</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../305/10573305.xml">
k-mer</link></entry>
</list>
</p>

</sec>
<sec>
<st>
Bibliography</st>

<p>

<list>
<entry level="1" type="bullet">

 Christopher D. Manning, Hinrich Sch√ºtze, <it>Foundations of Statistical Natural Language Processing</it>, MIT Press: 1999.  ISBN 0-262-13360-1.</entry>
<entry level="1" type="bullet">

 Ted Dunning, <it>Statistical Identification of Language</it>.  Computing Research Laboratory Memorandum (1994) MCCS-94-273.</entry>
<entry level="1" type="bullet">

 Owen White, Ted Dunning, Granger Sutton, Mark Adams, J.Craig Venter, and Chris Fields. A quality control algorithm for dna sequencing projects. Nucleic Acids Research, 21(16):3829--3838, 1993.</entry>
<entry level="1" type="bullet">

 Frederick J. Damerau, <it>Markov Models and Linguistic Theory</it>.  Mouton.  The Hague, 1971.</entry>
</list>
</p>

</sec>
<sec>
<st>
 External links </st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://n-gram-patterns.sourceforge.net/">
Google n-gram Information Extracter</weblink></entry>
<entry level="1" type="bullet">

 Two visualizations of Google's n-gram dataset: <weblink xlink:type="simple" xlink:href="http://www.chrisharrison.net/projects/wordassociation/">
Word Association</weblink>, <weblink xlink:type="simple" xlink:href="http://www.chrisharrison.net/projects/wordspectrum/">
Word Spectrum</weblink>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/dunning94statistical.html">
N-gram language identification algorithm</weblink></entry>
<entry level="1" type="bullet">

 An online <weblink xlink:type="simple" xlink:href="http://jonathanwellons.com/n-grams/index.cgi">
N-Grams generator</weblink>.</entry>
<entry level="1" type="bullet">

 SSN is a state of the art <weblink xlink:type="simple" xlink:href="http://homepages.inf.ed.ac.uk/jhender6/">
statistical language parser</weblink> that is nearly Markov.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://ngram.sourceforge.net">
Ngram Statistics Package</weblink>, open source package to identify statistically significant Ngrams</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.w3.org/TR/ngram-spec/">
Stochastic Language Models (N-Gram) Specification</weblink></entry>
</list>
</p>


</sec>
</bdy>
</article>
