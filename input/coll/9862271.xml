<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 17.04.2009 00:03:15[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<physical_entity  confidence="0.8" wordnetid="100001930">
<person  confidence="0.8" wordnetid="100007846">
<model  confidence="0.8" wordnetid="110324560">
<assistant  confidence="0.8" wordnetid="109815790">
<worker  confidence="0.8" wordnetid="109632518">
<causal_agent  confidence="0.8" wordnetid="100007347">
<header>
<title>Hierarchical hidden Markov model</title>
<id>9862271</id>
<revision>
<id>209207098</id>
<timestamp>2008-04-30T11:29:47Z</timestamp>
<contributor>
<username>Melcombe</username>
<id>4682566</id>
</contributor>
</revision>
<categories>
<category>Wikipedia articles needing context</category>
<category>Statistical models</category>
<category>Wikipedia introduction cleanup</category>
<category>Machine learning</category>
</categories>
</header>
<bdy>

<table class="metadata plainlinks ambox ">
<row>
<col>

ambox-style" style=""</col>
</row>
<row>
<col class="mbox-image"><p>

<image width="40x40px" src="Ambox_style.png">
</image>
</p>
</col>
<col style="" class="mbox-text">
 The introduction to this article provides <b>insufficient context</b> for those unfamiliar with the subject.
Please help <weblink xlink:type="simple" xlink:href="http://localhost:18088/wiki/index.php?title=Hierarchical_hidden_Markov_model&amp;action=edit">
improve the article</weblink> with a .</col>
</row>
</table>

<p>

The <b>Hierarchical hidden Markov model</b> (HHMM) is a <link xlink:type="simple" xlink:href="../576/27576.xml">
statistical model</link> derived from the <link xlink:type="simple" xlink:href="../770/98770.xml">
hidden Markov model</link> (HMM). In an HHMM each state is considered to be a self contained probabilistic model. More precisely each state
of the HHMM is itself an HHMM.</p>

<sec>
<st>
 Background </st>
<p>

It is sometimes useful to use HMMs in specific structures in order to 
facilitate learning and generalization. For example, even though a fully connected HMM could always be used if enough training data is
available it is often useful to constrain the model by not allowing
arbitrary state transitions. In the same way it can be beneficial to
embed the HMM into a greater structure; which, theoretically,  may not
be able to solve any other problems than the basic HMM but can solve
some problems more efficiently when it comes to the amount of training
data required.</p>

</sec>
<sec>
<st>
 The Hierarchical Hidden Markov Model </st>

<p>

In the hierarchical hidden Markov model (HHMM) each state is considered
to be a self contained probabilistic model. More precisely each state
of the HHMM is itself an HHMM. This implies that the states of the HHMM
emits sequences of observation symbols rather than single
observation symbols as is the case for the standard HMM states. </p>
<p>

<image location="center" width="400px" src="HHMM.png" type="thumbnail">
<caption>

Illustration of the structure of a HHMM. Gray lines shows vertical transitions. The horizontal transitions are shown as black lines. The light gray circles are the internal states and the dark gray circles are the terminal states that returns control to the activating state. The production states are not shown in this figure.
</caption>
</image>
</p>
<p>

When a state in an HHMM is activated, it will activate its own probabilistic
model, i.e. it will activate one of the states of the underlying HHMM,
which in turn may activate its underlying HHMM and so on. The process
is repeated until a special state, called a production state, is
activated. Only the production states emit observation symbols in
the usual HMM sense. When the production state has emitted a symbol,
control returns to the state that activated the production state.
The states that do not directly emit observations symbols are called
internal states. The activation of a state in an HHMM under an internal state is called a <it>vertical transition</it>. After a vertical transition is completed a <it>horizontal transition</it>
occurs to a state within the same level. When a horizontal
transition leads to a <it>terminating</it> state control is returned to
the state in the HHMM, higher up in the hierarchy, that produced the
last vertical transition.</p>
<p>

Remember that a vertical transition can result in more vertical
transitions before reaching a sequence of production states and
finally returning to the top level. Thus the production states visited
gives rise to a sequence of observation symbols that is "produced"
by the state at the top level.</p>
<p>

The methods for estimating the HHMM parameters and model structure are 
more complex than for the HMM and the interested reader is referred to
(Fine <it>et al.</it>, 1998).</p>
<p>

It should be pointed out that the HMM and HHMM belong to the
same class of classifiers. That is, they can be used to solve the
same set of problems. In fact, the HHMM can be
transformed into a standard HMM. However, the HHMM utilizes its structure to solve a subset of the problems more
efficiently.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../802/9862802.xml">
Layered hidden Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../721/11273721.xml">
Hierarchical Temporal Memory</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 References </st>
<p>

S. Fine, Y. Singer and N. Tishby, "The Hierarchical Hidden Markov Model: Analysis and Applications", Machine Learning, vol. 32, p. 41-62, 1998</p>
<p>

K.Murphy and M.Paskin. "Linear Time Inference in Hierarchical HMMs", NIPS-01 (Neural Info. Proc. Systems).</p>
<p>

H. Bui, D. Phung and S. Venkatesh. "Hierarchical Hidden Markov Models with General State Hierarchy", AAAI-04 (National Conference on Artificial Intelligence).</p>

</sec>
</bdy>
</causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</article>
