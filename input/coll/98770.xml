<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:57:44[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<physical_entity  confidence="0.8" wordnetid="100001930">
<person  confidence="0.8" wordnetid="100007846">
<model  confidence="0.8" wordnetid="110324560">
<assistant  confidence="0.8" wordnetid="109815790">
<entity  confidence="0.9511911446218017" wordnetid="100001740">
<worker  confidence="0.8" wordnetid="109632518">
<causal_agent  confidence="0.8" wordnetid="100007347">
<header>
<title>Hidden Markov model</title>
<id>98770</id>
<revision>
<id>243835690</id>
<timestamp>2008-10-08T06:57:30Z</timestamp>
<contributor>
<username>Hakeem.gadi</username>
<id>342229</id>
</contributor>
</revision>
<categories>
<category>Statistical models</category>
<category>Machine learning</category>
<category>Bioinformatics</category>
</categories>
</header>
<bdy>

<image location="right" width="300px" src="HiddenMarkovModel.png" type="thumb">
<caption>

Probabilistic parameters of a hidden Markov model (example)
<it>x</it> &mdash; states
<it>y</it> &mdash; possible observations
<it>a</it> &mdash; state transition probabilities
<it>b</it> &mdash; output probabilities
</caption>
</image>

A <b>hidden Markov model</b> (<b>HMM</b>) is a <link xlink:type="simple" xlink:href="../576/27576.xml">
statistical model</link> in which the system being modeled is assumed to be a <link xlink:type="simple" xlink:href="../772/98772.xml">
Markov process</link> with unknown parameters, and the challenge is to determine the hidden parameters from the <link xlink:type="simple" xlink:href="../248/294248.xml">
observable</link> parameters. The extracted model parameters can then be used to perform further analysis, for example for <link xlink:type="simple" xlink:href="../706/126706.xml">
pattern recognition</link> applications. An HMM can be considered as the simplest <link xlink:type="simple" xlink:href="../713/1242713.xml">
dynamic Bayesian network</link>.<p>

In a regular <link xlink:type="simple" xlink:href="../876/60876.xml">
Markov model</link>, the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters.  In a <it>hidden</it> Markov model, the state is not directly visible, but variables influenced by the state are visible.  Each state has a probability distribution over the possible output tokens.  Therefore the sequence of tokens generated by an HMM gives some information about the sequence of states.</p>
<p>

Hidden Markov models are especially known for their application in <link xlink:type="simple" xlink:href="../012/30012.xml">
 temporal</link> pattern recognition such as <link xlink:type="simple" xlink:href="../468/29468.xml">
speech</link>, <link xlink:type="simple" xlink:href="../619/203619.xml">
handwriting</link>, <link xlink:type="simple" xlink:href="../452/1400452.xml">
gesture recognition</link>, <link xlink:type="simple" xlink:href="../912/746912.xml">
part-of-speech tagging</link>, <link xlink:type="simple" xlink:href="../261/95261.xml">
musical score</link> following, <link xlink:type="simple" xlink:href="../804/467804.xml">
partial discharge</link>s and <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>.</p>

<sec>
<st>
 Architecture of a hidden Markov model </st>
<p>

The diagram below shows the general architecture of an instantiated HMM. Each oval shape represents a random variable that can adopt a number of values. The random variable <math>x(t)</math> is the hidden state at time <math>t</math> (with the model from the above diagram, <math>x(t) \in \{x_1, x_2, x_3\}</math>). The random variable <math>y(t)</math> is the observation at time <math>t</math> (<math>y(t) \in \{y_1, y_2, y_3, y_4\}</math>). The arrows in the diagram (often called a <family wordnetid="108078020" confidence="0.8">
<link xlink:type="simple" xlink:href="../446/12575446.xml">
trellis diagram</link></family>
) denote conditional dependencies.</p>
<p>

From the diagram, it is clear that the value of the hidden variable <math>x(t)</math> (at time <math>t</math>) <it>only</it> depends  on the value of the hidden variable <math>x(t-1)</math> : the values at time <math>t-2</math> and before have no influence. This is called the <link xlink:type="simple" xlink:href="../422/306422.xml">
Markov property</link>. Similarly, the value of the observed variable  <math>y(t)</math> only depends on the value of the hidden variable <math>x(t)</math> (both at time <math>t</math>).</p>
<p>

<image location="center" width="500px" src="hmm_temporal_bayesian_net.svg">
<caption>

Temporal evolution of a hidden Markov model
</caption>
</image>
</p>

</sec>
<sec>
<st>
Probability of an observed sequence</st>
<p>

<image width="400px" src="HMMsequence.svg" type="thumb">
<caption>

The observation sequence above can be produced by the following state sequences.
5 3 2 5 3 2
5 3 1 2 1 2
4 3 2 5 3 2
4 3 1 2 1 2
3 1 2 5 3 2
Transition and observation probabilities are indicated by the line opacity.
</caption>
</image>

The probability of observing a sequence <math>Y=y(0), y(1),\dots,y(L-1)</math> of length <math>L</math> is given by</p>
<p>

<indent level="1">

<math>P(Y)=\sum_{X}P(Y\mid X)P(X),</math>
</indent>

where the sum runs over all possible hidden node sequences  <math>X=x(0), x(1), \dots, x(L-1)</math>.  Brute force calculation of <math>P(Y)</math> is intractable for most real-life problems, as the number of possible hidden node sequences is typically extremely high. The calculation can however be sped up enormously using the <link xlink:type="simple" xlink:href="../015/228015.xml">
forward algorithm</link> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> or the equivalent backward algorithm.</p>

</sec>
<sec>
<st>
Using hidden Markov models</st>

<p>

There are three <link xlink:type="simple" xlink:href="../150/328150.xml">
canonical</link> problems associated with HMM:
<list>
<entry level="1" type="bullet">

 Given the parameters of the model, compute the probability of a particular output sequence, and the probabilities of the hidden state values given that output sequence.  This problem is solved by the <link xlink:type="simple" xlink:href="../749/9292749.xml">
forward-backward algorithm</link>.</entry>
<entry level="1" type="bullet">

 Given the parameters of the model, find the most likely sequence of hidden states that could have generated a given output sequence.  This problem is solved by the <link xlink:type="simple" xlink:href="../015/228015.xml">
Viterbi algorithm</link>.</entry>
<entry level="1" type="bullet">

 Given an output sequence or a set of such sequences, find the most likely set of state transition and output probabilities. In other words, discover the parameters of the HMM given a dataset of sequences. This problem is solved by the <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../778/822778.xml">
Baum-Welch algorithm</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
.</entry>
</list>
</p>

<ss1>
<st>
 A concrete example </st>


<p>

Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.</p>
<p>

Alice believes that the weather operates as a discrete <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../876/60876.xml">
Markov chain</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
. There are two states, "Rainy" and "Sunny", but she cannot observe them directly, that is, they are <it>hidden</it> from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: "walk", "shop", or "clean". Since Bob tells Alice about his activities, those are the <it>observations</it>. The entire system is that of a hidden Markov model (HMM).</p>
<p>

Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be written them down in the <link xlink:type="simple" xlink:href="../862/23862.xml">
Python programming language</link>:</p>
<p>

states = ('Rainy', 'Sunny')</p>
<p>

observations = ('walk', 'shop', 'clean')</p>
<p>

start_probability = {'Rainy': 0.6, 'Sunny': 0.4}</p>
<p>

transition_probability = {
'Rainy' : {'Rainy': 0.7, 'Sunny': 0.3},
'Sunny' : {'Rainy': 0.4, 'Sunny': 0.6},
}</p>
<p>

emission_probability = {
'Rainy' : {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},
'Sunny' : {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},
}</p>
<p>

In this piece of code, start_probability represents Alice's belief about which state the HMM is in when Bob first calls her (all she knows is that it tends to be rainy on average). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) actually approximately {'Rainy': 0.571, 'Sunny': 0.429}. The transition_probability represents the change of the weather in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow will be sunny if today is rainy. The emission_probability represents how likely Bob is to perform a certain activity on each day. If it is rainy, there is a 50% chance that he is cleaning his apartment; if it is sunny, there is a 60% chance that he is outside for a walk.</p>
<p>

<it>This example is further elaborated in the <link xlink:type="simple" xlink:href="../015/228015.xml">
Viterbi algorithm</link> page.</it></p>

</ss1>
<ss1>
<st>
Applications of hidden Markov models</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../715/5715.xml">
Cryptanalysis</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../468/29468.xml">
Speech recognition</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../980/19980.xml">
Machine translation</link> </entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../804/467804.xml">
Partial discharge</link></entry>
</list>
</p>

</ss1>
</sec>
<sec>
<st>
 History </st>

<p>

Hidden Markov Models were first described in a series of statistical papers by <link>
Leonard E. Baum</link> and other authors in the second half of the 1960s. One of the first applications of HMMs was <link xlink:type="simple" xlink:href="../468/29468.xml">
speech recognition</link>, starting in the mid-1970s.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

In the second half of the 1980s, HMMs began to be applied to the analysis of biological sequences, in particular <link xlink:type="simple" xlink:href="../955/7955.xml">
DNA</link>. Since then, they have become ubiquitous in the field of <link xlink:type="simple" xlink:href="../214/4214.xml">
bioinformatics</link>.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref></p>

</sec>
<sec>
<st>
See also</st>
<p>

<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../571/49571.xml">
Bayesian inference</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../926/1565926.xml">
Estimation theory</link></entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../271/9862271.xml">
Hierarchical hidden Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../802/9862802.xml">
Layered hidden Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<model wordnetid="110324560" confidence="0.8">
<assistant wordnetid="109815790" confidence="0.8">
<worker wordnetid="109632518" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<link xlink:type="simple" xlink:href="../977/5850977.xml">
Hidden semi-Markov model</link></causal_agent>
</worker>
</assistant>
</model>
</person>
</physical_entity>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../999/10770999.xml">
Variable-order Markov model</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../530/17590530.xml">
Sequential dynamical system</link></entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../276/4118276.xml">
Conditional random field</link></entry>
</list>
</p>

</sec>
<sec>
<st>
Notes</st>
<p>

<reflist>
<entry id="1">
Rabiner, p. 262</entry>
<entry id="2">
Rabiner, p. 258</entry>
<entry id="3">
Durbin et al.</entry>
</reflist>
</p>

</sec>
<sec>
<st>
References</st>

<p>

<list>
<entry level="1" type="bullet">

 <physical_entity wordnetid="100001930" confidence="0.8">
<peer wordnetid="109626238" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<engineer wordnetid="109615807" confidence="0.8">
<colleague wordnetid="109935990" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<associate wordnetid="109816771" confidence="0.8">
<link xlink:type="simple" xlink:href="../264/638264.xml">
Lawrence R. Rabiner</link></associate>
</causal_agent>
</colleague>
</engineer>
</person>
</peer>
</physical_entity>
, "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition," <it>Proceedings of the <link xlink:type="simple" xlink:href="../938/56938.xml">
IEEE</link></it>, 77 (2), p. 257&ndash;286, February 1989. <weblink xlink:type="simple" xlink:href="http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf">
http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf</weblink> <weblink xlink:type="simple" xlink:href="http://www.cs.cornell.edu/courses/cs481/2004fa/rabiner.pdf">
http://www.cs.cornell.edu/courses/cs481/2004fa/rabiner.pdf</weblink> </entry>
<entry level="1" type="bullet">

  <cite style="font-style:normal" class="book"><person wordnetid="100007846" confidence="0.9508927676800064">
<officeholder wordnetid="110371450" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../005/335005.xml">
Richard Durbin</link></officeholder>
</person>
, <link>
Sean R. Eddy</link>, <physical_entity wordnetid="100001930" confidence="0.8">
<person wordnetid="100007846" confidence="0.8">
<causal_agent wordnetid="100007347" confidence="0.8">
<scientist wordnetid="110560637" confidence="0.8">
<link xlink:type="simple" xlink:href="../086/15718086.xml">
Anders Krogh</link></scientist>
</causal_agent>
</person>
</physical_entity>
, <link>
Graeme Mitchison</link>&#32;(1999). Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.&#32;<company wordnetid="108058098" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../199/73199.xml">
Cambridge University Press</link></company>
. ISBN 0-521-62971-3.</cite>&nbsp;</entry>
<entry level="1" type="bullet">

 Lior Pachter and Bernd Sturmfels. "Algebraic Statistics for Computational Biology". Cambridge University Press, 2005. ISBN 0-521-85700-7.</entry>
<entry level="1" type="bullet">

 Olivier Cappé, Eric Moulines, Tobias Rydén. <it>Inference in Hidden Markov Models</it>, Springer, 2005. ISBN 0-387-40264-0.</entry>
<entry level="1" type="bullet">

 Kristie Seymore, Andrew McCallum, and Roni Rosenfeld. <it>Learning Hidden Markov Model Structure for Information Extraction</it>. AAAI 99 Workshop on Machine Learning for Information Extraction, 1999 <it>(also at <artifact wordnetid="100021939" confidence="0.8">
<instrumentality wordnetid="103575240" confidence="0.8">
<facility wordnetid="103315023" confidence="0.8">
<engine wordnetid="103287733" confidence="0.8">
<motor wordnetid="103789946" confidence="0.8">
<device wordnetid="103183080" confidence="0.8">
<depository wordnetid="103177349" confidence="0.8">
<archive wordnetid="102735086" confidence="0.8">
<machine wordnetid="103699975" confidence="0.8">
<link xlink:type="simple" xlink:href="../949/158949.xml">
CiteSeer</link></machine>
</archive>
</depository>
</device>
</motor>
</engine>
</facility>
</instrumentality>
</artifact>
: <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/seymore99learning.html">
http://citeseer.ist.psu.edu/seymore99learning.html</weblink>)</it>.</entry>
<entry level="1" type="bullet">

 Tutorial from University of Leeds<weblink xlink:type="simple" xlink:href="http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html">
http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html</weblink>.</entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.stat.psu.edu/~jiali">
J. Li</weblink>, A. Najmi, R. M. Gray, Image classification by a two dimensional hidden Markov model, <it>IEEE Transactions on Signal Processing</it>, 48(2):517-33, February 2000.</entry>
<entry level="1" type="bullet">

 Y. Ephraim and N. Merhav, Hidden Markov processes, IEEE Trans. Inform. Theory, vol. 48, pp. 1518-1569, June 2002.</entry>
<entry level="1" type="bullet">

 B. Pardo and W. Birmingham. <weblink xlink:type="simple" xlink:href="http://www.cs.northwestern.edu/~pardo/publications/pardo-birmingham-aaai-05.pdf">
Modeling Form for On-line Following of Musical Performances</weblink>. AAAI-05 Proc., July 2005.</entry>
<entry level="1" type="bullet">

  Thad Starner, Alex Pentland. <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/starner95visual.html">
Visual Recognition of American Sign Language Using Hidden Markov</weblink>. Master's Thesis, MIT, Feb 1995, Program in Media Arts</entry>
<entry level="1" type="bullet">

 L.Satish and B.I.Gururaj.<weblink xlink:type="simple" xlink:href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=212242">
 Use of hidden Markov models for partial discharge pattern classification</weblink>.IEEE Transactions on Dielectrics and Electrical Insulation, Apr 1993.</entry>
</list>

The path-counting algorithm, an alternative to the Baum-Welch algorithm:
<list>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/677948.html">
Comparing and Evaluating HMM Ensemble Training Algorithms Using Train and Test and Condition Number Criteria</weblink>, Journal of Pattern Analysis and Applications, 2003.</entry>
</list>
</p>

</sec>
<sec>
<st>
External links</st>
<p>

<list>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html">
Hidden Markov Model (HMM) Toolbox for Matlab</weblink> <it>(by Kevin Murphy)''</it></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://htk.eng.cam.ac.uk/">
Hidden Markov Model Toolkit (HTK)</weblink> <it>(a portable toolkit for building and manipulating hidden Markov models)''</it></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.cs.brown.edu/research/ai/dynamics/tutorial/Documents/HiddenMarkovModels.html">
Hidden Markov Models</weblink> <it>(an exposition using basic mathematics)''</it></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.ghmm.org">
GHMM Library</weblink> <it>(home page of the GHMM Library project)''</it></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.run.montefiore.ulg.ac.be/~francois/software/jahmm/">
Jahmm Java Library</weblink> <it>(Java library and associated graphical application)''</it></entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html">
A step-by-step tutorial on HMMs</weblink> <it>(University of Leeds)''</it></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.treeage.com/products/overviewHealth.html">
Software for Markov Models and Processes</weblink> <it> (TreeAge Software)''</it></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://jedlik.phy.bme.hu/~gerjanos/HMM/node2.html">
Hidden Markov Models</weblink> <it>(by Narada Warakagoda)''</it></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://www.kanungo.com/software/software.html">
HMM and other statistical programs</weblink> <it>(Implementation in C by Tapas Kanungo)''</it></entry>
<entry level="1" type="bullet">

 <weblink xlink:type="simple" xlink:href="http://hackage.haskell.org/cgi-bin/hackage-scripts/package/hmm">
The hmm package</weblink> A <weblink xlink:type="simple" xlink:href="http://www.haskell.org">
Haskell</weblink> library for working with Hidden Markov Models.</entry>
<entry level="1" type="bullet">

<weblink xlink:type="simple" xlink:href="http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/forward_algorithm/s1_pg7.html">
Forward algorithm</weblink></entry>
</list>
</p>


</sec>
</bdy>
</causal_agent>
</worker>
</entity>
</assistant>
</model>
</person>
</physical_entity>
</article>
