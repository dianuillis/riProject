<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 15:57:45[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<event  confidence="0.8" wordnetid="100029378">
<rule  confidence="0.8" wordnetid="105846932">
<act  confidence="0.8" wordnetid="100030358">
<psychological_feature  confidence="0.8" wordnetid="100023100">
<procedure  confidence="0.8" wordnetid="101023820">
<activity  confidence="0.8" wordnetid="100407535">
<algorithm  confidence="0.8" wordnetid="105847438">
<header>
<title>Linear classifier</title>
<id>98974</id>
<revision>
<id>234624067</id>
<timestamp>2008-08-27T18:54:21Z</timestamp>
<contributor>
<username>Fishal</username>
<id>54119</id>
</contributor>
</revision>
<categories>
<category>Statistical classification</category>
<category>Classification algorithms</category>
</categories>
</header>
<bdy>

In the field of <link xlink:type="simple" xlink:href="../488/233488.xml">
machine learning</link>, the goal of classification is to group items that have similar <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../404/1299404.xml">
feature</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 values, into groups. A <b>linear classifier</b> achieves this by making a classification decision based on the value of the <link xlink:type="simple" xlink:href="../632/55632.xml">
linear combination</link> of the features.
<sec>
<st>
 Definition </st>
<p>

If the input feature vector to the classifier is a <link xlink:type="simple" xlink:href="../491/19725491.xml">
real</link> vector <math>\vec x</math>, then the output score is</p>
<p>

<indent level="1">

<math>y = f(\vec{w}\cdot\vec{x}) = f\left(\sum_j w_j x_j\right),</math>
</indent>

where <math>\vec w</math> is a real vector of weights and <it>f</it> is a function that converts the <link xlink:type="simple" xlink:href="../093/157093.xml">
dot product</link> of the two vectors into the desired output. The weight vector <math>\vec w</math> is learned from a set of labeled training samples. Often <it>f</it> is a simple function that maps all values above a certain threshold to the first class and all other values to the second class. A more complex <it>f</it> might give the probability that an item belongs to a certain class.</p>
<p>

For a two-class classification problem, one can visualize the operation of a linear classifier as splitting a high-dimensional input space with a <link xlink:type="simple" xlink:href="../862/99862.xml">
hyperplane</link>: all points on one side of the hyperplane are classified as "yes", while the others are classified as "no".</p>
<p>

A linear classifier is often used in situations where the speed of classification is an issue, since it is often the fastest classifier, especially when <math>\vec x</math> is sparse. However, <link xlink:type="simple" xlink:href="../602/232602.xml">
decision tree</link>s can be faster. Also, linear classifiers often work very well when the number of dimensions in <math>\vec x</math> is large, as in <link xlink:type="simple" xlink:href="../441/1331441.xml">
document classification</link>, where each element in <math>\vec x</math> is typically the number of counts of a word in a document (see <link xlink:type="simple" xlink:href="../327/1234327.xml">
document-term matrix</link>). In such cases, the classifier should be well-<link xlink:type="simple" xlink:href="../061/2009061.xml">
regularized</link>.</p>

</sec>
<sec>
<st>
Generative models vs. discriminative models</st>
<p>

There are two broad classes of methods for determining the parameters of a linear classifier <math>\vec w</math> <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref><ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref>. The first is by modeling <link xlink:type="simple" xlink:href="../791/5791.xml">
conditional density functions</link> <math>P(\vec x|{\rm class})</math>. Examples of such algorithms include:
<list>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../657/1470657.xml">
Linear Discriminant Analysis (or Fisher's linear discriminant)</link> (LDA) --- assumes <link xlink:type="simple" xlink:href="../462/21462.xml">
Gaussian</link> conditional density models</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../339/87339.xml">
Naive Bayes classifier</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 --- assumes <link xlink:type="simple" xlink:href="../593/27593.xml">
independent</link> <link xlink:type="simple" xlink:href="../876/3876.xml">
binomial</link> conditional density models.</entry>
</list>
</p>
<p>

The second set approaches are called <link xlink:type="simple" xlink:href="../912/12155912.xml">
discriminative model</link>s, which attempt to maximize the quality of the output on a <link xlink:type="simple" xlink:href="../228/1817228.xml">
training set</link>. Additional terms in the training cost function can easily perform <link xlink:type="simple" xlink:href="../061/2009061.xml">
regularization</link> of the final model. Examples of discriminative training of linear classifiers include
<list>
<entry level="1" type="bullet">

 <information wordnetid="105816287" confidence="0.8">
<datum wordnetid="105816622" confidence="0.8">
<link xlink:type="simple" xlink:href="../631/226631.xml">
Logistic regression</link></datum>
</information>
 --- maximum likelihood estimation of <math>\vec w</math> assuming that the observed training set was generated by a binomial model that depends on the output of the classifier.</entry>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../777/172777.xml">
Perceptron</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
 --- an algorithm that attempts to fix all errors encountered in the training set</entry>
<entry level="1" type="bullet">

 <know-how wordnetid="105616786" confidence="0.8">
<event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<method wordnetid="105660268" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../309/65309.xml">
Support vector machine</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</method>
</rule>
</event>
</know-how>
 --- an algorithm that maximizes the <link xlink:type="simple" xlink:href="../352/41352.xml">
margin</link> between the decision hyperplane and the examples in the training set.</entry>
</list>
</p>
<p>

<b>Note:</b>   In contrast to its name, LDA does not belong to the class of discriminative models in this <link xlink:type="simple" xlink:href="../463/30463.xml">
taxonomy</link>. However, its name makes sense when we compare LDA to the other main linear <link xlink:type="simple" xlink:href="../867/579867.xml">
dimensionality reduction</link> algorithm: <link xlink:type="simple" xlink:href="../340/76340.xml">
Principal Components Analysis</link> (PCA). LDA is a <link xlink:type="simple" xlink:href="../926/20926.xml">
supervised learning</link> algorithm that utilizes the labels of the data, while PCA is an <link xlink:type="simple" xlink:href="../497/233497.xml">
unsupervised learning</link> algorithm that ignores the labels. To summarize, the name is a historical artifact (see <ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%223%22])">3</ref>, p.117).</p>
<p>

Discriminative training often yields higher accuracy than modeling the conditional density functions. However, handling missing data is often easier with conditional density models.</p>
<p>

All of the linear classifier algorithms listed above can be converted into non-linear algorithms operating on a different input space <math>\varphi(\vec x)</math>, using the <link xlink:type="simple" xlink:href="../912/303912.xml">
kernel trick</link>.</p>

</sec>
<sec>
<st>
 See also </st>
<p>

<list>
<entry level="1" type="bullet">

 <event wordnetid="100029378" confidence="0.8">
<rule wordnetid="105846932" confidence="0.8">
<act wordnetid="100030358" confidence="0.8">
<psychological_feature wordnetid="100023100" confidence="0.8">
<procedure wordnetid="101023820" confidence="0.8">
<activity wordnetid="100407535" confidence="0.8">
<algorithm wordnetid="105847438" confidence="0.8">
<link xlink:type="simple" xlink:href="../975/1784975.xml">
Quadratic classifier</link></algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</entry>
<entry level="1" type="bullet">

 <link xlink:type="simple" xlink:href="../244/1579244.xml">
Statistical classification</link></entry>
</list>
</p>

</sec>
<sec>
<st>
 Notes </st>
<p>

<reflist>
<entry id="1">
T. Mitchell, Generative and Discriminative Classifiers: Naive Bayes and Logistic Regression. Draft Version, 2005 <weblink xlink:type="simple" xlink:href="http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">
download</weblink></entry>
<entry id="2">
A. Y. Ng and M. I. Jordan. On Discriminative vs. Generative Classifiers: A comparison of logistic regression and Naive Bayes.  in NIPS 14, 2002. <weblink xlink:type="simple" xlink:href="http://www.cs.berkeley.edu/~jordan/papers/ng-jordan-nips01.ps">
download</weblink></entry>
<entry id="3">
R.O. Duda, P.E. Hart, D.G. Stork, "Pattern Classification", Wiley, (2001). ISBN 0-471-05669-3</entry>
</reflist>

See also: 
<list>
<entry level="1" type="number">

 Y. Yang, X. Liu, "A re-examination of text categorization", Proc. ACM SIGIR Conference, pp. 42-49, (1999). <weblink xlink:type="simple" xlink:href="http://citeseer.ist.psu.edu/yang99reexamination.html">
paper @ citeseer</weblink></entry>
<entry level="1" type="number">

 R. Herbrich, "Learning Kernel Classifiers: Theory and Algorithms," MIT Press, (2001). ISBN 0-262-08306-X</entry>
</list>
</p>


</sec>
</bdy>
</algorithm>
</activity>
</procedure>
</psychological_feature>
</act>
</rule>
</event>
</article>
