<?xml version="1.0" encoding="UTF-8"?>
<!-- generated by CLiX/Wiki2XML [MPI-Inf, MMCI@UdS] $LastChangedRevision: 92 $ on 16.04.2009 17:58:27[mciao0826] -->
<!DOCTYPE article SYSTEM "../article.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
<header>
<title>Sentence extraction</title>
<id>996096</id>
<revision>
<id>209365177</id>
<timestamp>2008-05-01T01:27:35Z</timestamp>
<contributor>
<username>Bkkbrad</username>
<id>165575</id>
</contributor>
</revision>
<categories>
<category>Natural language processing</category>
<category>Computational linguistics</category>
</categories>
</header>
<bdy>

<b>Sentence extraction</b> is a technique used for <link xlink:type="simple" xlink:href="../199/637199.xml">
automatic summarization</link>.
In this shallow approach, <link xlink:type="simple" xlink:href="../509/846509.xml">
statistical heuristics</link> are used to identify the most salient sentences of a text. Sentence extraction is a low-cost approach compared to more knowledge-intensive deeper approaches which require additional knowledge bases such as <link xlink:type="simple" xlink:href="../681/49681.xml">
ontologies</link> or <link xlink:type="simple" xlink:href="../526/17526.xml">
linguistic knowledge</link>. In short "sentence extraction" works as a filter which allows only important sentences to pass.<p>

The major downside of applying sentence-extraction techniques to the task of summarization is the loss of coherence in the resulting summary. 
Nevertheless, sentence extraction summaries can give valuable clues to the main points of a document and are frequently sufficiently intelligible to human readers.</p>

<sec>
<st>
 Procedure </st>
<p>

Usually, a combination of heuristics is used to determine the most important sentences within the document. Each heuristic assigns a (positive or negative) score to the sentence. After all heuristics have been applied, the x highest-scoring sentences are included in the summary.
The individual heuristics are weighted according to their importance.</p>

<ss1>
<st>
 Early approaches and some sample heuristics </st>
<p>

Seminal papers which laid the foundations for many techniques used today have been published by <link>
H. P. Luhn</link> in 1958<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%221%22])">1</ref> and <link>
H. P Edmundson</link> in 1969.<ref xlink:type="simple" xlink:href="#xpointer(//reflist/entry[@id=%222%22])">2</ref></p>
<p>

Luhn proposed to assign more weight to sentences at the beginning of the document or a paragraph.
Edmundson stressed the importance of title-words for summarization and was the first to employ stop-lists in order to filter uninformative words of low semantic content (e.g. most grammatical words such as "of", "the", "a"). He also distinguished between <it>bonus words</it> and <it>stigma words</it>, i.e. words that probably occur together with important (e.g. the word form "significant") or unimportant information.
His idea of using key-words, i.e. words which occur significantly frequently in the document, is still one of the core heuristics of today's summarizers. With large linguistic corpora available today, the <link xlink:type="simple" xlink:href="../290/2057290.xml">
TF/IDF</link> value which originated in <link xlink:type="simple" xlink:href="../271/15271.xml">
Information Retrieval</link>, can be successfully applied to identify the key words of a text: If for example the word "cat" occurs significantly more often in the text to be summarized (TF = text frequency) than in the corpus (IDF means "inverse document frequency"; here the corpus is meant by "document"), then "cat" is likely to be an important word of the text; the text may in fact be a text about cats.</p>

</ss1>
</sec>
<sec>
<st>
 References </st>

<p>

<reflist>
<entry id="1">
 <cite style="font-style:normal"><link>
H. P. Luhn</link>&#32;(April <link xlink:type="simple" xlink:href="../953/34953.xml">
1958</link>).&#32;"<weblink xlink:type="simple" xlink:href="http://www.research.ibm.com/journal/rd/022/luhn.pdf">
The Automatic Creation of Literature Abstracts</weblink>". <it><link>
IBM Journal</link></it>: 159&ndash;165.</cite>&nbsp;</entry>
<entry id="2">
 <cite style="font-style:normal"><link>
H. P. Edmundson</link>&#32;(<link xlink:type="simple" xlink:href="../610/34610.xml">
1969</link>).&#32;"<weblink xlink:type="simple" xlink:href="http://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf">
New Methods in Automatic Extracting</weblink>". <it><magazine wordnetid="106595351" confidence="0.9508927676800064">
<link xlink:type="simple" xlink:href="../845/2321845.xml">
Journal of the ACM</link></magazine>
</it>&#32;<b>16</b>&#32;(2): 264&ndash;285. <document wordnetid="106470073" confidence="0.8">
<written_communication wordnetid="106349220" confidence="0.8">
<writing wordnetid="106362953" confidence="0.8">
<link xlink:type="simple" xlink:href="../994/422994.xml">
doi</link></writing>
</written_communication>
</document>
:<weblink xlink:type="simple" xlink:href="http://dx.doi.org/10.1145%2F321510.321519">
10.1145/321510.321519</weblink>.</cite>&nbsp;</entry>
</reflist>
</p>


</sec>
</bdy>
</article>
